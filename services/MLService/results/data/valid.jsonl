{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhy the datasets is not updating the dill ? \nJust want to know if I update the dill version in dill what will be the repucssion. \nFor now in multiplaces I have to update the library like process requirequire dill 0.4.0 so why not datasets. \nAdding a pr too. \n\n### Steps to reproduce the bug\n\n.\n\n### Expected behavior\n\n.\n\n### Environment info\n\n.<end_of_turn>\n<start_of_turn>model\nDill version update<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDon't use the parquet export infos when USE_PARQUET_EXPORT is False.\r\n\r\nOtherwise the `datasets-server` might reuse erroneous data when re-running a job\r\n\r\nthis follows https://github.com/huggingface/datasets/pull/6714<end_of_turn>\n<start_of_turn>model\nFix parquet export infos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n              Hi, if I want to load the dataset from local file, then how to specify the configuration name?\r\n\r\n_Originally posted by @WHU-gentle in https://github.com/huggingface/datasets/issues/2976#issuecomment-1333455222_\r\n            <end_of_turn>\n<start_of_turn>model\nHi,do you know how to load the dataset from local file now?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSame as https://github.com/huggingface/datasets/pull/7708 but for `IterableDataset`<end_of_turn>\n<start_of_turn>model\nConcurrent IterableDataset push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n# problem\r\nI can't resample audio dataset in Kaggle Notebook. It looks like some code in `datasets` library use aliases that were deprecated in NumPy 1.20.\r\n\r\n## code for resampling\r\n```\r\nfrom datasets import load_dataset, Audio\r\nfrom transformers import AutoFeatureExtractor\r\nfrom transformers import AutoModelForAudioClassification, TrainingArguments, Trainer\r\n\r\nminds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\r\n\r\nfeature_extractor = AutoFeatureExtractor.from_pretrained(\"facebook/wav2vec2-base\")\r\n\r\ndef preprocess_function(examples):\r\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\r\n    inputs = feature_extractor(\r\n        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True\r\n    )\r\n    return inputs\r\n\r\ndataset = dataset.map(preprocess_function, remove_columns=\"audio\", batched=True, batch_size=100)\r\n```\r\n\r\n## the error I got\r\n<details>\r\n  <summary>Click to expand</summary>\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[20], line 1\r\n----> 1 dataset = dataset.map(preprocess_function, remove_columns=\"audio\", batched=True, batch_size=100)\r\n      2 dataset\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1955, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   1952 disable_tqdm = not logging.is_progress_bar_enabled()\r\n   1954 if num_proc is None or num_proc == 1:\r\n-> 1955     return self._map_single(\r\n   1956         function=function,\r\n   1957         with_indices=with_indices,\r\n   1958         with_rank=with_rank,\r\n   1959         input_columns=input_columns,\r\n   1960         batched=batched,\r\n   1961         batch_size=batch_size,\r\n   1962         drop_last_batch=drop_last_batch,\r\n   1963         remove_columns=remove_columns,\r\n   1964         keep_in_memory=keep_in_memory,\r\n   1965         load_from_cache_file=load_from_cache_file,\r\n   1966         cache_file_name=cache_file_name,\r\n   1967         writer_batch_size=writer_batch_size,\r\n   1968         features=features,\r\n   1969         disable_nullable=disable_nullable,\r\n   1970         fn_kwargs=fn_kwargs,\r\n   1971         new_fingerprint=new_fingerprint,\r\n   1972         disable_tqdm=disable_tqdm,\r\n   1973         desc=desc,\r\n   1974     )\r\n   1975 else:\r\n   1977     def format_cache_file_name(cache_file_name, rank):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:520, in transmit_tasks.<locals>.wrapper(*args, **kwargs)\r\n    518     self: \"Dataset\" = kwargs.pop(\"self\")\r\n    519 # apply actual function\r\n--> 520 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    521 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    522 for dataset in datasets:\r\n    523     # Remove task templates if a column mapping of the template is no longer valid\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:487, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    480 self_format = {\r\n    481     \"type\": self._format_type,\r\n    482     \"format_kwargs\": self._format_kwargs,\r\n    483     \"columns\": self._format_columns,\r\n    484     \"output_all_columns\": self._output_all_columns,\r\n    485 }\r\n    486 # apply actual function\r\n--> 487 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    488 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    489 # re-apply format to the output\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/fingerprint.py:458, in fingerprint_transform.<locals>._fingerprint.<locals>.wrapper(*args, **kwargs)\r\n    452             kwargs[fingerprint_name] = update_fingerprint(\r\n    453                 self._fingerprint, transform, kwargs_for_fingerprint\r\n    454             )\r\n    456 # Call actual function\r\n--> 458 out = func(self, *args, **kwargs)\r\n    460 # Update fingerprint of in-place transforms + update in-place history of transforms\r\n    462 if inplace:  # update after calling func so that the fingerprint doesn't change if the function fails\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:2356, in Dataset._map_single(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\r\n   2354                 writer.write_table(batch)\r\n   2355             else:\r\n-> 2356                 writer.write_batch(batch)\r\n   2357 if update_data and writer is not None:\r\n   2358     writer.finalize()  # close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_writer.py:507, in ArrowWriter.write_batch(self, batch_examples, writer_batch_size)\r\n    505     col_try_type = try_features[col] if try_features is not None and col in try_features else None\r\n    506     typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 507     arrays.append(pa.array(typed_sequence))\r\n    508     inferred_features[col] = typed_sequence.get_inferred_type()\r\n    509 schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/array.pxi:236, in pyarrow.lib.array()\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/array.pxi:110, in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_writer.py:184, in TypedSequence.__arrow_array__(self, type)\r\n    182     out = numpy_to_pyarrow_listarray(data)\r\n    183 elif isinstance(data, list) and data and isinstance(first_non_null_value(data)[1], np.ndarray):\r\n--> 184     out = list_of_np_array_to_pyarrow_listarray(data)\r\n    185 else:\r\n    186     trying_cast_to_python_objects = True\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1174, in list_of_np_array_to_pyarrow_listarray(l_arr, type)\r\n   1172 \"\"\"Build a PyArrow ListArray from a possibly nested list of NumPy arrays\"\"\"\r\n   1173 if len(l_arr) > 0:\r\n-> 1174     return list_of_pa_arrays_to_pyarrow_listarray(\r\n   1175         [numpy_to_pyarrow_listarray(arr, type=type) if arr is not None else None for arr in l_arr]\r\n   1176     )\r\n   1177 else:\r\n   1178     return pa.array([], type=type)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1163, in list_of_pa_arrays_to_pyarrow_listarray(l_arr)\r\n   1160 null_indices = [i for i, arr in enumerate(l_arr) if arr is None]\r\n   1161 l_arr = [arr for arr in l_arr if arr is not None]\r\n   1162 offsets = np.cumsum(\r\n-> 1163     [0] + [len(arr) for arr in l_arr], dtype=np.object\r\n   1164 )  # convert to dtype object to allow None insertion\r\n   1165 offsets = np.insert(offsets, null_indices, None)\r\n   1166 offsets = pa.array(offsets, type=pa.int32())\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/numpy/__init__.py:324, in __getattr__(attr)\r\n    319     warnings.warn(\r\n    320         f\"In the future `np.{attr}` will be defined as the \"\r\n    321         \"corresponding NumPy scalar.\", FutureWarning, stacklevel=2)\r\n    323 if attr in __former_attrs__:\r\n--> 324     raise AttributeError(__former_attrs__[attr])\r\n    326 if attr == 'testing':\r\n    327     import numpy.testing as testing\r\n\r\nAttributeError: module 'numpy' has no attribute 'object'.\r\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\r\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\r\n```\r\n\r\n</details>\n\n### Steps to reproduce the bug\n\nRun above code in Kaggle Notebook.\n\n### Expected behavior\n\nI can resample audio data without fail.\n\n### Environment info\n\n- `datasets` version: 2.1.0\r\n- Platform: Linux-5.15.133+-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.2.1<end_of_turn>\n<start_of_turn>model\nAttributeError: module 'numpy' has no attribute 'object'. in Kaggle Notebook<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-35-7b6becee3685>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 from datasets import Dataset\r\n\r\n9 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/__init__.py](https://localhost:8080/#) in <module>\r\n     20 __version__ = \"2.15.0\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in <module>\r\n     61 import pyarrow.compute as pc\r\n     62 from huggingface_hub import CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\r\n---> 63 from multiprocess import Pool\r\n     64 from requests import HTTPError\r\n     65 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/__init__.py](https://localhost:8080/#) in <module>\r\n     31 \r\n     32 import sys\r\n---> 33 from . import context\r\n     34 \r\n     35 #\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/context.py](https://localhost:8080/#) in <module>\r\n      4 \r\n      5 from . import process\r\n----> 6 from . import reduction\r\n      7 \r\n      8 __all__ = ()\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/reduction.py](https://localhost:8080/#) in <module>\r\n     14 import os\r\n     15 try:\r\n---> 16     import dill as pickle\r\n     17 except ImportError:\r\n     18     import pickle\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/__init__.py](https://localhost:8080/#) in <module>\r\n     24 \r\n     25 \r\n---> 26 from ._dill import (\r\n     27     dump, dumps, load, loads, copy,\r\n     28     Pickler, Unpickler, register, pickle, pickles, check,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in <module>\r\n    166 try:\r\n    167     from _pyio import open as _open\r\n--> 168     PyTextWrapperType = get_file_type('r', buffering=-1, open=_open)\r\n    169     PyBufferedRandomType = get_file_type('r+b', buffering=-1, open=_open)\r\n    170     PyBufferedReaderType = get_file_type('rb', buffering=-1, open=_open)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in get_file_type(*args, **kwargs)\r\n    154 def get_file_type(*args, **kwargs):\r\n    155     open = kwargs.pop(\"open\", __builtin__.open)\r\n--> 156     f = open(os.devnull, *args, **kwargs)\r\n    157     t = type(f)\r\n    158     f.close()\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in open(file, mode, buffering, encoding, errors, newline, closefd, opener)\r\n    280             return result\r\n    281         encoding = text_encoding(encoding)\r\n--> 282         text = TextIOWrapper(buffer, encoding, errors, newline, line_buffering)\r\n    283         result = text\r\n    284         text.mode = mode\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in __init__(self, buffer, encoding, errors, newline, line_buffering, write_through)\r\n   2043                 encoding = \"utf-8\"\r\n   2044             else:\r\n-> 2045                 encoding = locale.getpreferredencoding(False)\r\n   2046 \r\n   2047         if not isinstance(encoding, str):\r\n\r\nTypeError: <lambda>() takes 0 positional arguments but 1 was given\r\n```\r\n\r\nor \r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-36-652e886d387f>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 import datasets\r\n\r\n9 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/__init__.py](https://localhost:8080/#) in <module>\r\n     20 __version__ = \"2.15.0\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in <module>\r\n     61 import pyarrow.compute as pc\r\n     62 from huggingface_hub import CommitOperationAdd, CommitOperationDelete, DatasetCard, DatasetCardData, HfApi\r\n---> 63 from multiprocess import Pool\r\n     64 from requests import HTTPError\r\n     65 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/__init__.py](https://localhost:8080/#) in <module>\r\n     31 \r\n     32 import sys\r\n---> 33 from . import context\r\n     34 \r\n     35 #\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/context.py](https://localhost:8080/#) in <module>\r\n      4 \r\n      5 from . import process\r\n----> 6 from . import reduction\r\n      7 \r\n      8 __all__ = ()\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/reduction.py](https://localhost:8080/#) in <module>\r\n     14 import os\r\n     15 try:\r\n---> 16     import dill as pickle\r\n     17 except ImportError:\r\n     18     import pickle\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/__init__.py](https://localhost:8080/#) in <module>\r\n     24 \r\n     25 \r\n---> 26 from ._dill import (\r\n     27     dump, dumps, load, loads, copy,\r\n     28     Pickler, Unpickler, register, pickle, pickles, check,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in <module>\r\n    166 try:\r\n    167     from _pyio import open as _open\r\n--> 168     PyTextWrapperType = get_file_type('r', buffering=-1, open=_open)\r\n    169     PyBufferedRandomType = get_file_type('r+b', buffering=-1, open=_open)\r\n    170     PyBufferedReaderType = get_file_type('rb', buffering=-1, open=_open)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/dill/_dill.py](https://localhost:8080/#) in get_file_type(*args, **kwargs)\r\n    154 def get_file_type(*args, **kwargs):\r\n    155     open = kwargs.pop(\"open\", __builtin__.open)\r\n--> 156     f = open(os.devnull, *args, **kwargs)\r\n    157     t = type(f)\r\n    158     f.close()\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in open(file, mode, buffering, encoding, errors, newline, closefd, opener)\r\n    280             return result\r\n    281         encoding = text_encoding(encoding)\r\n--> 282         text = TextIOWrapper(buffer, encoding, errors, newline, line_buffering)\r\n    283         result = text\r\n    284         text.mode = mode\r\n\r\n[/usr/lib/python3.10/_pyio.py](https://localhost:8080/#) in __init__(self, buffer, encoding, errors, newline, line_buffering, write_through)\r\n   2043                 encoding = \"utf-8\"\r\n   2044             else:\r\n-> 2045                 encoding = locale.getpreferredencoding(False)\r\n   2046 \r\n   2047         if not isinstance(encoding, str):\r\n\r\nTypeError: <lambda>() takes 0 positional arguments but 1 was given\r\n```\n\n### Steps to reproduce the bug\n\n`import datasets` on colab\n\n### Expected behavior\n\nwork fine\n\n### Environment info\n\ncolab\r\n\r\n`!pip install datasets`<end_of_turn>\n<start_of_turn>model\nTypeError: <lambda>() takes 0 positional arguments but 1 was given<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen trying to load big json files from a local directory, `load_dataset` throws the following error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/miniconda3/envs/conda-env/lib/python3.10/site-packages/datasets/builder.py\", line 1989, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"miniconda3/envs/conda-env/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 573, in write_table\r\n    pa_table = pa_table.combine_chunks()\r\n  File \"pyarrow/table.pxi\", line 3638, in pyarrow.lib.Table.combine_chunks\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\n```\n\n### Steps to reproduce the bug\n\n1. Download a big file, e.g. `https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-train.json.gz`\r\n2. Load it like `data = load_dataset(\"json\", data_files=[\"nq-train.json\"], split=\"train\")`\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndata = load_dataset(\"json\", data_files=[\"nq-train.json\"], split=\"train\")\r\n```\r\n\r\nA similarly formatted but smaller file, e.g. e.g. `https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz` is loaded without issues\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndata = load_dataset(\"json\", data_files=[\"nq-dev.json\"], split=\"train\")\r\n```\n\n### Expected behavior\n\nIt should load normally\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-5.18.10-76051810-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nError when loading a big local json file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFile \"/home/kas/.conda/envs/diffusers/lib/python3.7/site-packages/datasets/builder.py\", line 1858, in _prepare_split_single\r\nDownloading and preparing dataset json/default to /home/kas/diffusers/examples/dreambooth/cache_data/datasets/json/default-acf423d8c6ef99d0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 0%| | 0/1 [00:00<?, ?it/s] Downloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 84.35it/s]\r\nExtracting data files: 0%| | 0/1 [00:00<?, ?it/s] for _, table in generator:\r\nFile \"/home/kas/.conda/envs/diffusers/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py\", line 114, in _generate_tables\r\nio.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\r\nFile \"pyarrow/_json.pyx\", line 258, in pyarrow._json.read_json\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 27.72it/s]\r\nGenerating train split: 0 examples [00:00, ? examples/s] File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\nFile \"pyarrow/error.pxi\", line 125, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowCapacityError: array cannot contain more than 2147483646 bytes, have 2390448764\n\n### Steps to reproduce the bug\n\n1\u3001data_files = [\"1.json\", \"2.json\", \"3.json\"]\r\n2\u3001dataset = load_dataset('json', data_files=data_files)\n\n### Expected behavior\n\nRead the dataset normally.\r\n\r\n\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-4.15.0-29-generic-x86_64-with-debian-buster-sid\r\n- Python version: 3.7.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 1.3.5<end_of_turn>\n<start_of_turn>model\nI tried to load a custom dataset using the following statement: dataset = load_dataset('json', data_files=data_files). The dataset contains 50 million text-image pairs, but an error occurred.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMore info: [comment](https://github.com/huggingface/datasets/pull/7564#issuecomment-2863391781).<end_of_turn>\n<start_of_turn>model\nFixed typos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCI is broken: https://github.com/huggingface/datasets/actions/runs/9657882317/job/26637998686?pr=6996\r\n```\r\nFAILED tests/test_inspect.py::test_get_dataset_config_names[hf-internal-testing/librispeech_asr_dummy-expected4] - AssertionError: assert ['clean'] == ['clean', 'other']\r\n  \r\n  Right contains one more item: 'other'\r\n  \r\n  Full diff:\r\n    [\r\n        'clean',\r\n  -     'other',\r\n    ]\r\nFAILED tests/test_inspect.py::test_get_dataset_default_config_name[hf-internal-testing/librispeech_asr_dummy-None] - AssertionError: assert 'clean' is None\r\n```\r\n\r\nNote that repository was recently converted to Parquet: https://huggingface.co/datasets/hf-internal-testing/librispeech_asr_dummy/commit/5be91486e11a2d616f4ec5db8d3fd248585ac07a<end_of_turn>\n<start_of_turn>model\nCI is broken for tests using hf-internal-testing/librispeech_asr_dummy<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen i try to create a `Dataset` object with None values inside a dict column, like this: \r\n```python\r\nfrom datasets import Dataset, Features, Value\r\n\r\nDataset.from_dict(\r\n    {\r\n        \"dict\": [{\"a\": 0, \"b\": 0}, None],\r\n    }, features=Features(\r\n        {\"dict\": {\"a\": Value(\"int16\"), \"b\": Value(\"int16\")}}\r\n    )\r\n)\r\n```\r\ni get `ValueError: Got None but expected a dictionary instead`.\r\n\r\n\r\nAt the same time, having None in _nested_ dict feature works, for example, this doesn't throw any errors:\r\n\r\n```python\r\nfrom datasets import Dataset, Features, Value, Sequence\r\n\r\ndataset = Dataset.from_dict(\r\n    {\r\n        \"list_dict\": [[{\"a\": 0, \"b\": 0}], None],\r\n        \"sequence_dict\": [[{\"a\": 0, \"b\": 0}], None],\r\n    }, features=Features({\r\n        \"list_dict\": [{\"a\": Value(\"int16\"), \"b\": Value(\"int16\")}],\r\n        \"sequence_dict\": Sequence({\"a\": Value(\"int16\"), \"b\": Value(\"int16\")}),\r\n    })\r\n)\r\n```\r\nOther types of features also seem to be nullable (but I haven't checked all of them).\r\n\r\nVersion of `datasets` is the latest atm (2.18.0)\r\n\r\n\r\nIs this an expected behavior or a bug?<end_of_turn>\n<start_of_turn>model\nDict feature is non-nullable while nested dict feature is<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDraft about adding `trust_remote_code` to `load_dataset`.\r\n\r\n```python\r\nds = load_dataset(..., trust_remote_code=True)  # run remote code (current default)\r\n```\r\n\r\nIt would default to `True` (current behavior) and in the next major release it will prompt the user to check the code before running it (we'll communicate on this before doing it of course).\r\n\r\n```python\r\n# in the future\r\nds = load_dataset(...)  # prompt the user to check the code before running it (future default)\r\nds = load_dataset(..., trust_remote_code=True)  # run remote code\r\nds = load_dataset(..., trust_remote_code=False)  # disallow remote code\r\n```\r\n\r\nRelated to https://github.com/huggingface/datasets/issues/6400\r\n\r\nWill do a separate PR to use the parquet export when possible<end_of_turn>\n<start_of_turn>model\nAdd trust_remote_code argument<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n## bug\nWhen the map function processes a large dataset, it temporarily stores the data in a cache file on the disk. After the data is stored, the memory occupied by it is released. Therefore, when using the map function to process a large-scale dataset, only a dataset space of the size of `writer_batch_size` will be occupied in memory.\n\nHowever, I found that the map function does not actually reduce memory usage when I used it. At first, I thought there was a bug in the program, causing a memory leak\u2014meaning the memory was not released after the data was stored in the cache. But later, I used a Linux command to check for recently modified files during program execution and found that no new files were created or modified. This indicates that the program did not store the dataset in the disk cache.\n## bug solved\nAfter modifying the parameters of the map function multiple times, I discovered the `cache_file_name` parameter. By changing it, the cache file can be stored in the specified directory. After making this change, I noticed that the cache file appeared. Initially, I found this quite incredible, but then I wondered if the cache file might have failed to be stored in a certain folder. This could be related to the fact that I don't have root privileges.\n\nSo, I delved into the source code of the map function to find out where the cache file would be stored by default. Eventually, I found the function `def _get_cache_file_path(self, fingerprint):`, which automatically generates the storage path for the cache file. The output was as follows: `/tmp/hf_datasets-j5qco9ug/cache-f2830487643b9cc2.arrow`. My hypothesis was confirmed: the lack of root privileges indeed prevented the cache file from being stored, which in turn prevented the release of memory. Therefore, changing the storage location to a folder where I have write access resolved the issue.\n\n### Steps to reproduce the bug\n\nmy code\n`train_data = train_data.map(process_fun, remove_columns=['image_name', 'question_type', 'concern', 'question', 'candidate_answers', 'answer'])`\n\n### Expected behavior\n\nAlthough my bug has been resolved, it still took me nearly a week to search for relevant information and debug the program. However, if a warning or error message about insufficient cache file write permissions could be provided during program execution, I might have been able to identify the cause more quickly. Therefore, I hope this aspect can be improved. I am documenting this bug here so that friends who encounter similar issues can solve their problems in a timely manner.\n\n### Environment info\n\npython: 3.10.15\ndatasets: 3.5.0<end_of_turn>\n<start_of_turn>model\nThe memory-disk mapping failure issue of the map function\uff08resolved, but there are some suggestions.\uff09<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n# Issue: Type Mismatch in Dataset Mapping\r\n\r\n## Description\r\nThere is an issue with the `map` function in the `datasets` library where the mapped output does not reflect the expected type change. After applying a mapping function to convert an integer label to a string, the resulting type remains an integer instead of a string.\r\n\r\n## Reproduction Code\r\nBelow is a Python script that demonstrates the problem:\r\n\r\n```python\r\nfrom datasets import Dataset\r\n\r\n# Original data\r\ndata = {\r\n    'text': ['Hello', 'world', 'this', 'is', 'a', 'test'],\r\n    'label': [0, 1, 0, 1, 1, 0]\r\n}\r\n\r\n# Creating a Dataset object\r\ndataset = Dataset.from_dict(data)\r\n\r\n# Mapping function to convert label to string\r\ndef add_one(example):\r\n    example['label'] = str(example['label'])\r\n    return example\r\n\r\n# Applying the mapping function\r\ndataset = dataset.map(add_one)\r\n\r\n# Iterating over the dataset to show results\r\nfor item in dataset:\r\n    print(item)\r\n    print(type(item['label']))\r\n```\r\n\r\n## Expected Output\r\nAfter applying the mapping function, the expected output should have the `label` field as strings:\r\n\r\n```plaintext\r\n{'text': 'Hello', 'label': '0'}\r\n<class 'str'>\r\n{'text': 'world', 'label': '1'}\r\n<class 'str'>\r\n{'text': 'this', 'label': '0'}\r\n<class 'str'>\r\n{'text': 'is', 'label': '1'}\r\n<class 'str'>\r\n{'text': 'a', 'label': '1'}\r\n<class 'str'>\r\n{'text': 'test', 'label': '0'}\r\n<class 'str'>\r\n```\r\n\r\n## Actual Output\r\nThe actual output still shows the `label` field values as integers:\r\n\r\n```plaintext\r\n{'text': 'Hello', 'label': 0}\r\n<class 'int'>\r\n{'text': 'world', 'label': 1}\r\n<class 'int'>\r\n{'text': 'this', 'label': 0}\r\n<class 'int'>\r\n{'text': 'is', 'label': 1}\r\n<class 'int'>\r\n{'text': 'a', 'label': 1}\r\n<class 'int'>\r\n{'text': 'test', 'label': 0}\r\n<class 'int'>\r\n```\r\n\r\n## Why necessary\r\nIn the case of Image process we often need to convert PIL to tensor with same column name.\r\n\r\nThank for every dev who review this issue. \ud83e\udd17<end_of_turn>\n<start_of_turn>model\nBug: Type Mismatch in Dataset Mapping<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nAs far as I understand, automated construction of splits for hub datasets is currently based on either file names or directory structure ([as described here](https://huggingface.co/docs/datasets/en/repository_structure))\r\n\r\nIt would seem to be pretty useful to also allow splits to be based on identifiers of individual examples\r\n\r\nThis could be configured like \r\n{\"split_name\": {\"column_name\": [column values in split]}}\r\n\r\n(This in turn requires unique 'index' columns, which could be explicitly supported or just assumed to be defined appropriately by the user).\r\n\r\nI guess a potential downside would be that shards would end up spanning different splits - is this something that can be handled somehow? Would this only affect streaming from hub?\r\n\r\n### Motivation\r\n\r\nThe main motivation would be that all data files could be stored in a single directory, and multiple sets of splits could be generated from the same data. This is often useful for large datasets with multiple distinct sets of splits.\r\n\r\nThis could all be configured via the README.md yaml configs\r\n\r\n### Your contribution\r\n\r\nMay be able to contribute if it seems like a good idea<end_of_turn>\n<start_of_turn>model\nSupport for identifier-based automated split construction<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7765 \r\n\r\nThe problem here is that polars uses pyarrow large_string for images, while pandas and others just use the string type. This PR solves that and adds a test.\r\n\r\n```python\r\nimport polars as pl\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nfrom pathlib import Path\r\n\r\nshared_datadir = Path(\"tests/features/data\")\r\nimage_path = str(shared_datadir / \"test_image_rgb.jpg\")\r\n\r\n# Load via polars\r\ndf_polars = pl.DataFrame({\"image_path\": [image_path]})\r\ndataset_polars = Dataset.from_polars(df_polars)\r\nprint(\"Polars DF is large string:\", pa.types.is_large_string(df_polars.to_arrow().schema[0].type))\r\nprint(\"Polars DF is string:\", pa.types.is_string(df_polars.to_arrow().schema[0].type))\r\n\r\n# Load via pandas\r\ndf_pandas = pd.DataFrame({\"image_path\": [image_path]})\r\ndataset_pandas = Dataset.from_pandas(df_pandas)\r\narrow_table_pd = pa.Table.from_pandas(df_pandas)\r\nprint(\"Pandas DF is large string\", pa.types.is_large_string(arrow_table_pd.schema[0].type))\r\nprint(\"Pandas DF is string\", pa.types.is_string(arrow_table_pd.schema[0].type))\r\n```\r\nOutputs:\r\n```bash\r\nPolars DF is large string: True\r\nPolars DF is string: False\r\nPandas DF is large string False\r\nPandas DF is string True\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nFix polars cast column image<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\nimport datasets\r\nimport torch.utils.data\r\n\r\n\r\nnum_shards = 1024\r\n\r\n\r\ndef gen(shards):\r\n    for shard in shards:\r\n        if shard < 25:\r\n            yield {\"shard\": shard}\r\n\r\n\r\ndef main():\r\n    dataset = datasets.IterableDataset.from_generator(\r\n        gen,\r\n        gen_kwargs={\"shards\": list(range(num_shards))},\r\n    )\r\n    dataset = dataset.shuffle(buffer_size=1)\r\n    dataset = datasets.interleave_datasets(\r\n        [dataset, dataset], probabilities=[1, 0], stopping_strategy=\"all_exhausted\"\r\n    )\r\n    dataset = dataset.shuffle(buffer_size=1)\r\n\r\n    dataloader = torch.utils.data.DataLoader(\r\n        dataset,\r\n        batch_size=8,\r\n        num_workers=8,\r\n    )\r\n\r\n    for i, batch in enumerate(dataloader):\r\n        print(batch)\r\n        if i >= 10:\r\n            break\r\n    print()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    for _ in range(100):\r\n        main()\r\n```\n\n### Steps to reproduce the bug\n\nRunning the script above, at some point it will freeze.\r\n\r\n- Changing `num_shards` from 1024 to 25 avoids the issue\r\n- Commenting out the final shuffle avoids the issue\r\n- Commenting out the interleave_datasets call avoids the issue\r\n\r\nAs an aside, if you comment out just the final shuffle, the output from interleave_datasets is not shuffled at all even though there's the shuffle before it. So something about that shuffle config is not being propagated to interleave_datasets.\n\n### Expected behavior\n\nThe script should not freeze.\n\n### Environment info\n\n- `datasets` version: 3.0.0\r\n- Platform: macOS-14.6.1-arm64-arm-64bit\r\n- Python version: 3.12.5\r\n- `huggingface_hub` version: 0.24.7\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1\r\n\r\n\r\nI observed this with 2.21.0 initially, then tried upgrading to 3.0.0 and could still repro.<end_of_turn>\n<start_of_turn>model\nIterableDataset strange deadlock<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUPD: [Fixed](https://github.com/huggingface/datasets/issues/6568#issuecomment-1880817794) . But a new issue came up :(<end_of_turn>\n<start_of_turn>model\nkeep_in_memory=True does not seem to work<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport JSON file with an array of strings.\r\n\r\nFix #6695.<end_of_turn>\n<start_of_turn>model\nMake JSON builder support an array of strings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I use the `map` function to convert images into features, datasets saves nparray as a list. Some people use the `set_format` function to convert the column back, but doesn't this lose precision?\n\n### Steps to reproduce the bug\n\nthe map function\r\n```python\r\ndef convert_image_to_features(inst, processor, image_dir):\r\n    image_file = inst[\"image_url\"]\r\n    file = image_file.split(\"/\")[-1]\r\n    image_path = os.path.join(image_dir, file)\r\n    image = Image.open(image_path)\r\n    image = image.convert(\"RGBA\")\r\n\r\n    inst[\"pixel_values\"] = processor(images=image, return_tensors=\"np\")[\"pixel_values\"]\r\n    return inst\r\n```\r\n\r\nmain function\r\n```python\r\nmap_fun = partial(\r\n      convert_image_to_features, processor=processor, image_dir=image_dir\r\n  )\r\nds = ds.map(map_fun, batched=False, num_proc=20)\r\nprint(type(ds[0][\"pixel_values\"])\r\n```\n\n### Expected behavior\n\n(type < list>) \n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-4.19.91-009.ali4000.alios7.x86_64-x86_64-with-glibc2.35\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nSave nparray as list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI want to be able to use cached dataset from HuggingFace even when I have no Internet connection (or when HuggingFace servers are down, or my company has network issues).  \r\nThe problem why I can't use it:\r\n`data_files` argument from `datasets.load_dataset()` function get it updates from the server before calculating hash for caching. As a result, when I run the same code with and without Internet I get different dataset configuration directory name.\n\n### Steps to reproduce the bug\n\n```\r\nimport datasets\r\n\r\nc4_dataset = datasets.load_dataset(\r\n    path=\"allenai/c4\",\r\n    data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"},\r\n    split=\"train\",\r\n    cache_dir=\"/datesets/cache\",\r\n    download_mode=\"reuse_cache_if_exists\",\r\n    token=False,\r\n)\r\n```\r\n1. Run this code with the Internet.\r\n2. Run the same code without the Internet.\n\n### Expected behavior\n\nWhen running without the Internet connection, the loader should be able to get dataset from cache\n\n### Environment info\n\n- `datasets` version: 2.19.0\r\n- Platform: Windows-10-10.0.19044-SP0\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 16.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\nCannot use cached dataset without Internet connection (or when servers are down)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.3.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nI would like to load a dataset from S3 using the imagefolder option \r\nsomething like \r\n`dataset = datasets.load_dataset('imagefolder', data_dir='s3://.../lsun/train/bedroom', fs=S3FileSystem(), streaming=True)  `\r\n\r\n### Motivation\r\n\r\nno need of data_files\r\n\r\n### Your contribution\r\n\r\nno experience with this<end_of_turn>\n<start_of_turn>model\nload_dataset imageflder for aws s3 path <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\u0634\u0628\u06a9\u0647 \u0647\u0627\u06cc \u0645\u062a\u0646 \u0628\u0647 \u06af\u0641\u062a\u0627\u0631 \u0627\u0628\u062a\u062f\u0627 \u0645\u062a\u0646 \u062f\u0627\u062f\u0647 \u0634\u062f\u0647 \u0631\u0627 \u0628\u0647 \u0628\u0627\u0632\u0646\u0645\u0627\u06cc\u06cc \u0645\u06cc\u0627\u0646\u06cc\r\n<end_of_turn>\n<start_of_turn>model\n\u0634\u0628\u06a9\u0647 \u0647\u0627\u06cc \u0645\u062a\u0646 \u0628\u0647 \u06af\u0641\u062a\u0627\u0631 \u0627\u0628\u062a\u062f\u0627 \u0645\u062a\u0646 \u062f\u0627\u062f\u0647 \u0634\u062f\u0647 \u0631\u0627 \u0628\u0647 \u0628\u0627\u0632\u0646\u0645\u0627\u06cc\u06cc \u0645\u06cc\u0627\u0646\u06cc<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIn https://huggingface.co/models there are task categories for RL and robotics but none in https://huggingface.co/datasets\r\n\r\nOur lab is currently moving our datasets over to hugging face and would like to be able to add those 2 tags\r\n\r\nMoreover we see some older datasets that do have that tag, but we can't seem to add it ourselves.\n\n### Steps to reproduce the bug\n\n1. Create a new dataset on Hugging face\r\n2. Try to type reinforcemement-learning or robotics into the tasks categories, it does not allow you to commit\n\n### Expected behavior\n\nExpected to be able to add RL and robotics as task categories as some previous datasets have these tags\n\n### Environment info\n\nN/A<end_of_turn>\n<start_of_turn>model\nReinforcement Learning and Robotics are not task categories in HF datasets metadata<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nA regression was introduced for pandas < 2.0.0 in PR:\r\n- #6914\r\n\r\nAs described in pandas docs, the `dtype_backend` parameter was first added in pandas 2.0.0: https://pandas.pydata.org/docs/reference/api/pandas.read_json.html\r\n\r\nThis PR fixes the regression by passing (or not) the `dtype_backend` parameter depending on pandas version.\r\n\r\nMaybe, in a future 3.0 `datasets` release, we could just require pandas > 2.0.\r\n\r\nReported by:\r\n- #6977<end_of_turn>\n<start_of_turn>model\nFix regression for pandas < 2.0.0 in JSON loader<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTemporarily pin fsspec < 2023.9.0 until permanent solution is found.\r\n\r\nHot fix #6209.<end_of_turn>\n<start_of_turn>model\nTemporarily pin fsspec < 2023.9.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove torchaudio remnants from code.\r\n\r\nFollow-up on:\r\n- #5573<end_of_turn>\n<start_of_turn>model\nRemove torchaudio remnants from code<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm trying A full colab demo notebook of zero-shot-distillation from https://github.com/huggingface/transformers/tree/main/examples/research_projects/zero-shot-distillation but i got this type of error when importing  datasets on my google colab (python version is 3.10.12)\r\n\r\n![image](https://github.com/huggingface/datasets/assets/15389235/6f7758a2-681d-4436-87d0-5e557838e368)\r\n\r\nI found the same problem that have been solved in [#3326 ] but it seem still error on the google colab. I can't try on my local using jupyter notebook because of my laptop resource doesn't fulfill the requirements. \r\n\r\nPlease can anyone help me solve this problem. Thank you \ud83d\ude05 \n\n### Steps to reproduce the bug\n\nError:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-8-b6e092f83978>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 from datasets import load_dataset\r\n      2 \r\n      3 # Print all the available datasets\r\n      4 from huggingface_hub import list_datasets\r\n      5 print([dataset.id for dataset in list_datasets()])\r\n\r\n6 frames\r\n[/usr/lib/python3.10/functools.py](https://localhost:8080/#) in update_wrapper(wrapper, wrapped, assigned, updated)\r\n     59     # Issue #17482: set __wrapped__ last so we don't inadvertently copy it\r\n     60     # from the wrapped function when updating __dict__\r\n---> 61     wrapper.__wrapped__ = wrapped\r\n     62     # Return the wrapper so this can be used as a decorator via partial()\r\n     63     return wrapper\r\n\r\nAttributeError: readonly attribute\r\n```\n\n### Expected behavior\n\nRun success on Google Colab (free)\n\n### Environment info\n\nWindows 11 x64, Google Colab free<end_of_turn>\n<start_of_turn>model\nCannot import datasets on google colab (python 3.10.12)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRun CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis mistake was a bit confusing, so I thought it was worth sending a PR over.<end_of_turn>\n<start_of_turn>model\nfixed typo in comment<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHaving installed the latest versions of transformers==4.38.1 and datasets==2.17.1 Unable to load the dataset in a kaggle notebook.\r\nGet this Error:\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[8], line 3\r\n      1 from datasets import load_dataset\r\n----> 3 dataset = load_dataset(\"llm-blender/mix-instruct\")\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:1664, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1661 ignore_verifications = ignore_verifications or save_infos\r\n   1663 # Create a dataset builder\r\n-> 1664 builder_instance = load_dataset_builder(\r\n   1665     path=path,\r\n   1666     name=name,\r\n   1667     data_dir=data_dir,\r\n   1668     data_files=data_files,\r\n   1669     cache_dir=cache_dir,\r\n   1670     features=features,\r\n   1671     download_config=download_config,\r\n   1672     download_mode=download_mode,\r\n   1673     revision=revision,\r\n   1674     use_auth_token=use_auth_token,\r\n   1675     **config_kwargs,\r\n   1676 )\r\n   1678 # Return iterable dataset in case of streaming\r\n   1679 if streaming:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:1490, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1488     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1489     download_config.use_auth_token = use_auth_token\r\n-> 1490 dataset_module = dataset_module_factory(\r\n   1491     path,\r\n   1492     revision=revision,\r\n   1493     download_config=download_config,\r\n   1494     download_mode=download_mode,\r\n   1495     data_dir=data_dir,\r\n   1496     data_files=data_files,\r\n   1497 )\r\n   1499 # Get dataset builder class from the processing script\r\n   1500 builder_cls = import_main_class(dataset_module.module_path)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:1242, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1237             if isinstance(e1, FileNotFoundError):\r\n   1238                 raise FileNotFoundError(\r\n   1239                     f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1240                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   1241                 ) from None\r\n-> 1242             raise e1 from None\r\n   1243 else:\r\n   1244     raise FileNotFoundError(\r\n   1245         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory.\"\r\n   1246     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:1230, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1215             return HubDatasetModuleFactoryWithScript(\r\n   1216                 path,\r\n   1217                 revision=revision,\r\n   (...)\r\n   1220                 dynamic_modules_path=dynamic_modules_path,\r\n   1221             ).get_module()\r\n   1222         else:\r\n   1223             return HubDatasetModuleFactoryWithoutScript(\r\n   1224                 path,\r\n   1225                 revision=revision,\r\n   1226                 data_dir=data_dir,\r\n   1227                 data_files=data_files,\r\n   1228                 download_config=download_config,\r\n   1229                 download_mode=download_mode,\r\n-> 1230             ).get_module()\r\n   1231 except Exception as e1:  # noqa: all the attempts failed, before raising the error we should check if the module is already cached.\r\n   1232     try:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:846, in HubDatasetModuleFactoryWithoutScript.get_module(self)\r\n    836     token = self.download_config.use_auth_token\r\n    837 hfh_dataset_info = HfApi(config.HF_ENDPOINT).dataset_info(\r\n    838     self.name,\r\n    839     revision=self.revision,\r\n    840     token=token,\r\n    841     timeout=100.0,\r\n    842 )\r\n    843 patterns = (\r\n    844     sanitize_patterns(self.data_files)\r\n    845     if self.data_files is not None\r\n--> 846     else get_patterns_in_dataset_repository(hfh_dataset_info)\r\n    847 )\r\n    848 data_files = DataFilesDict.from_hf_repo(\r\n    849     patterns,\r\n    850     dataset_info=hfh_dataset_info,\r\n    851     allowed_extensions=ALL_ALLOWED_EXTENSIONS,\r\n    852 )\r\n    853 infered_module_names = {\r\n    854     key: infer_module_for_data_files(data_files_list, use_auth_token=self.download_config.use_auth_token)\r\n    855     for key, data_files_list in data_files.items()\r\n    856 }\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/data_files.py:471, in get_patterns_in_dataset_repository(dataset_info)\r\n    469 resolver = partial(_resolve_single_pattern_in_dataset_repository, dataset_info)\r\n    470 try:\r\n--> 471     return _get_data_files_patterns(resolver)\r\n    472 except FileNotFoundError:\r\n    473     raise FileNotFoundError(\r\n    474         f\"The dataset repository at '{dataset_info.id}' doesn't contain any data file.\"\r\n    475     ) from None\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/data_files.py:99, in _get_data_files_patterns(pattern_resolver)\r\n     97 try:\r\n     98     for pattern in patterns:\r\n---> 99         data_files = pattern_resolver(pattern)\r\n    100         if len(data_files) > 0:\r\n    101             non_empty_splits.append(split)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/data_files.py:303, in _resolve_single_pattern_in_dataset_repository(dataset_info, pattern, allowed_extensions)\r\n    301 data_files_ignore = FILES_TO_IGNORE\r\n    302 fs = HfFileSystem(repo_info=dataset_info)\r\n--> 303 glob_iter = [PurePath(filepath) for filepath in fs.glob(PurePath(pattern).as_posix()) if fs.isfile(filepath)]\r\n    304 matched_paths = [\r\n    305     filepath\r\n    306     for filepath in glob_iter\r\n    307     if filepath.name not in data_files_ignore and not filepath.name.startswith(\".\")\r\n    308 ]\r\n    309 if allowed_extensions is not None:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/fsspec/spec.py:606, in AbstractFileSystem.glob(self, path, maxdepth, **kwargs)\r\n    602         depth = None\r\n    604 allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\r\n--> 606 pattern = glob_translate(path + (\"/\" if ends_with_sep else \"\"))\r\n    607 pattern = re.compile(pattern)\r\n    609 out = {\r\n    610     p: info\r\n    611     for p, info in sorted(allpaths.items())\r\n   (...)\r\n    618     )\r\n    619 }\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/fsspec/utils.py:734, in glob_translate(pat)\r\n    732     continue\r\n    733 elif \"**\" in part:\r\n--> 734     raise ValueError(\r\n    735         \"Invalid pattern: '**' can only be an entire path component\"\r\n    736     )\r\n    737 if part:\r\n    738     results.extend(_translate(part, f\"{not_sep}*\", not_sep))\r\n\r\nValueError: Invalid pattern: '**' can only be an entire path component\r\n```\r\n```\r\nAfter loading this dataset \r\n### Steps to reproduce the bug\r\n```\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"llm-blender/mix-instruct\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset should load with desired split.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.17.1\r\n- Platform: Linux-5.15.133+-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\nUnable to Load Dataset in Kaggle<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nDisallow video push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n\n\nWhen I run the following python script using datasets I get a segfault.\n```python\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nprogress_bar = tqdm(total=(1000), unit='cols', desc='cols ')\nprogress_bar.update(1)\n```\n\n```\n% lldb -- python3 crashmin.py \n(lldb) target create \"python3\"\nCurrent executable set to '/Users/ian/bug/venv/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"crashmin.py\"\n(lldb) r\nProcess 8095 launched: '/Users/ian/bug/venv/bin/python3' (arm64)\nProcess 8095 stopped\n* thread #2, stop reason = exec\n    frame #0: 0x0000000100014b30 dyld`_dyld_start\ndyld`_dyld_start:\n->  0x100014b30 <+0>: mov    x0, sp\n    0x100014b34 <+4>: and    sp, x0, #0xfffffffffffffff0\n    0x100014b38 <+8>: mov    x29, #0x0 ; =0 \nTarget 0: (Python) stopped.\n(lldb) c\nProcess 8095 resuming\ncols :   0% 0/1000 [00:00<?, ?cols/s]Process 8095 stopped\n* thread #2, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x10)\n    frame #0: 0x0000000101783454 _datetime.cpython-313-darwin.so`delta_new + 188\n_datetime.cpython-313-darwin.so`delta_new:\n->  0x101783454 <+188>: ldr    x3, [x20, #0x10]\n    0x101783458 <+192>: adrp   x0, 10\n    0x10178345c <+196>: add    x0, x0, #0x6fc ; \"seconds\"\nTarget 0: (Python) stopped.\n(lldb) bt\n* thread #2, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x10)\n  * frame #0: 0x0000000101783454 _datetime.cpython-313-darwin.so`delta_new + 188\n    frame #1: 0x0000000100704b60 Python`type_call + 96\n    frame #2: 0x000000010067ba34 Python`_PyObject_MakeTpCall + 120\n    frame #3: 0x00000001007aae3c Python`_PyEval_EvalFrameDefault + 30236\n    frame #4: 0x000000010067c900 Python`PyObject_CallOneArg + 112\n    frame #5: 0x000000010070f0a0 Python`slot_tp_finalize + 116\n    frame #6: 0x000000010070c3b4 Python`subtype_dealloc + 788\n    frame #7: 0x00000001006c378c Python`insertdict + 756\n    frame #8: 0x00000001006db2b0 Python`_PyModule_ClearDict + 660\n    frame #9: 0x000000010080a9a8 Python`finalize_modules + 1772\n    frame #10: 0x0000000100809a44 Python`_Py_Finalize + 264\n    frame #11: 0x0000000100837630 Python`Py_RunMain + 252\n    frame #12: 0x0000000100837ef8 Python`pymain_main + 304\n    frame #13: 0x0000000100837f98 Python`Py_BytesMain + 40\n    frame #14: 0x000000019cfcc274 dyld`start + 2840\n(lldb) register read x20\n     x20 = 0x0000000000000000\n(lldb) \n```\n\n### Steps to reproduce the bug\n\nRun the script above, and observe the segfault.\n\n### Expected behavior\n\nNo segfault\n\n### Environment info\n```\n% pip freeze datasets | grep -i datasets\ndatasets==4.2.0\n(venv) 0 ~/bug 14:58:06 \n% pip freeze tqdm | grep -i tqdm\ntqdm==4.67.1\n(venv) 0 ~/bug 14:58:16 \n% python --version\nPython 3.13.7\n```<end_of_turn>\n<start_of_turn>model\nSIGSEGV when Python exits due to near null deref<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix https://github.com/huggingface/datasets/pull/6722<end_of_turn>\n<start_of_turn>model\nget_dataset_default_config_name docstring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsave a useless call to /api/datasets/repo_id<end_of_turn>\n<start_of_turn>model\nNo need for dataset_info<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIt seems that saving a dataset infos back into the card file is not working for datasets without a loading script.\r\n\r\nAfter tracking the problem a bit it looks like saving the infos uses `Builder.get_imported_module_dir()` as its destination directory.\r\nInternally this is a call to `inspect.getfile()` but since the actual builder class used is dynamically created (cf. `datasets.load.configure_builder_class`) this method actually return te path to the parent builder class (e.g. `datasets.packaged_modules.json.JSON`). \r\n\r\n\n\n### Steps to reproduce the bug\n\n1. Have a local dataset without any loading script\r\n2. Make sure there are no dataset infos in the README.md\r\n3. Load with `save_infos=True`\r\n4. No change in the dataset README.md\r\n5. A new README.md file is created in the directory of the parent builder class (e.g. for json in `.../site-packages/datasets/packaged_modules/json/README.md`)\n\n### Expected behavior\n\nThe dataset README.md should be updated and no file should be created in the python environment.\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.6.0\r\n<end_of_turn>\n<start_of_turn>model\n`load_dataset(...,save_infos=True)` not working without loading script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis is a proof of concept for #7310 . The idea is to enable the access to others column of the dataset row when loading an audio file into a table. This is to allow sliced reading. As stated in the issue, many people have very long audio files and use start and stop slicing in this audio file.\r\n\r\nRight now, this code work as a PoC on my dataset. However, this is **just to illustrate** the idea. Many things are messed up, the first being that the shards have wildly varying sizes. \r\n\r\nCould be of interest to @lhoestq and @sanchit-gandhi ? \r\n\r\nHappy to test better ideas locally. <end_of_turn>\n<start_of_turn>model\n[Audio Features - DO NOT MERGE] PoC for adding an offset+sliced reading to audio file.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUnable to load xglue dataset\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndataset = datasets.load_dataset(\"xglue\", \"ntg\")\r\n```\r\n\r\n> ConnectionError: Couldn't reach https://xglue.blob.core.windows.net/xglue/xglue_full_dataset.tar.gz (error 409)\n\n### Expected behavior\n\nExpected the dataset to load\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.15.107+-x86_64-with-glibc2.31\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nGetting a 409 error while loading xglue dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I run the following code I get an exception;\r\n`go_emotions = load_dataset(\"go_emotions\")`\r\n\r\n> AttributeError                            Traceback (most recent call last)\r\nCell In[6], [line 1](vscode-notebook-cell:?execution_count=6&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=6&line=1) go_emotions = load_dataset(\"go_emotions\")\r\n      [2](vscode-notebook-cell:?execution_count=6&line=2) data = go_emotions.data\r\n\r\nFile [c:\\Users\\hijik\\anaconda3\\Lib\\site-packages\\datasets\\load.py:2523](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2523), in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   [2518](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2518) verification_mode = VerificationMode(\r\n   [2519](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2519)     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\r\n   [2520](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2520) )\r\n   [2522](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2522) # Create a dataset builder\r\n-> [2523](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2523) builder_instance = load_dataset_builder(\r\n   [2524](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2524)     path=path,\r\n   [2525](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2525)     name=name,\r\n   [2526](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2526)     data_dir=data_dir,\r\n   [2527](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2527)     data_files=data_files,\r\n   [2528](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2528)     cache_dir=cache_dir,\r\n   [2529](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2529)     features=features,\r\n   [2530](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2530)     download_config=download_config,\r\n   [2531](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2531)     download_mode=download_mode,\r\n   [2532](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2532)     revision=revision,\r\n   [2533](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2533)     token=token,\r\n   [2534](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2534)     storage_options=storage_options,\r\n   [2535](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2535)     trust_remote_code=trust_remote_code,\r\n   [2536](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/load.py:2536)     _require_default_config_name=name is None,\r\n...\r\n---> [63](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/utils/_dill.py:63)         if issubclass(obj_type, transformers.PreTrainedTokenizerBase):\r\n     [64](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/utils/_dill.py:64)             pklregister(obj_type)(_save_transformersPreTrainedTokenizerBase)\r\n     [66](file:///C:/Users/hijik/anaconda3/Lib/site-packages/datasets/utils/_dill.py:66) # Unwrap `torch.compile`-ed functions\r\n\r\nAttributeError: module 'transformers' has no attribute 'PreTrainedTokenizerBase'\r\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?10bc0728-6947-456e-9a3e-f056872b04c6) or open in a [text editor](command:workbench.action.openLargeOutput?10bc0728-6947-456e-9a3e-f056872b04c6). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\ngo_emotions = load_dataset(\"go_emotions\")\r\n```\n\n### Expected behavior\n\nShould simply load the variable with the data from the file\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.16.1\r\n- Platform: Windows-10-10.0.22631-SP0\r\n- Python version: 3.11.4\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nCannot load the dataset go_emotions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nInstall transformers with numpy-2 CI.\r\n\r\nNote that transformers no longer pins numpy < 2 since transformers-4.43.0:\r\n- https://github.com/huggingface/transformers/pull/32018\r\n- https://github.com/huggingface/transformers/releases/tag/v4.43.0<end_of_turn>\n<start_of_turn>model\nInstall transformers with numpy-2 CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI have downloaded laion2B-en, and I'm receiving the following error trying to load it:\r\n```\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128/128 [00:00<00:00, 1173.79it/s]\r\nTraceback (most recent call last):\r\n  File \"D:\\Art-Workspace\\src\\artworkspace\\tokeneval\\compute_frequencies.py\", line 31, in <module>\r\n    count = compute_frequencies()\r\n            ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"D:\\Art-Workspace\\src\\artworkspace\\tokeneval\\compute_frequencies.py\", line 17, in compute_frequencies\r\n    laion2b_dataset = load_dataset(\"laion/laion2B-en\", split=\"train\", cache_dir=_CACHE_DIR, keep_in_memory=False)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\load.py\", line 2165, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1187, in as_dataset\r\n    datasets = map_nested(\r\n               ^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\utils\\py_utils.py\", line 456, in map_nested\r\n    return function(data_struct)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1217, in _build_single_dataset\r\n    ds = self._as_dataset(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\builder.py\", line 1291, in _as_dataset\r\n    dataset_kwargs = ArrowReader(cache_dir, self.info).read(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 244, in read\r\n    return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 265, in read_files\r\n    pa_table = self._read_files(files, in_memory=in_memory)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 200, in _read_files\r\n    pa_table: Table = self._get_table_from_filename(f_dict, in_memory=in_memory)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 336, in _get_table_from_filename\r\n    table = ArrowReader.read_table(filename, in_memory=in_memory)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\arrow_reader.py\", line 357, in read_table\r\n    return table_cls.from_file(filename)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\table.py\", line 1059, in from_file\r\n    table = _memory_mapped_arrow_table_from_file(filename)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\bberman\\Documents\\Art-Workspace\\venv\\Lib\\site-packages\\datasets\\table.py\", line 66, in _memory_mapped_arrow_table_from_file\r\n    pa_table = opened_stream.read_all()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\\ipc.pxi\", line 757, in pyarrow.lib.RecordBatchReader.read_all\r\n  File \"pyarrow\\error.pxi\", line 91, in pyarrow.lib.check_status\r\nOSError: [WinError 8] PrefetchVirtualMemory failed. Detail: [Windows error 8] Not enough memory resources are available to process this command.\r\n```\r\n\r\nThis error is probably a red herring: https://stackoverflow.com/questions/50263929/numpy-memmap-returns-not-enough-memory-while-there-are-plenty-available In other words, the issue is related to asking for a memory mapping of length N > M the length of the file on Windows. This gracefully succeeds on Linux.\r\n\r\nI have 1024 arrow files in my cache instead of 128 like in the repository for it. Probably related. I don't know why `datasets` reorganized/rewrote the dataset in my cache to be 1024 slices instead of the original 128.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\n# as a huggingface developer, you may already have laion2B-en somewhere\r\n_CACHE_DIR = \".\"\r\n\r\nfrom datasets import load_dataset\r\nload_dataset(\"laion/laion2B-en\", split=\"train\", cache_dir=_CACHE_DIR, keep_in_memory=False)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThis should correctly load as a memory mapped Arrow dataset.\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Windows-10-10.0.20348-SP0 (this is windows 2022)\r\n- Python version: 3.11.4\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\nlaion2B-en failed to load on Windows with PrefetchVirtualMemory failed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nPassing files with `.arrow` extension into data_files argument, at least when `streaming=True` is very slow.\n\n### Steps to reproduce the bug\n\n Basically it goes through the `_get_extraction_protocol` method located [here](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L820)\r\nThe method then looks at some base known extensions where `arrow` is not defined so it proceeds to determine the compression with the magic number method which is slow when dealing with a lot of files which are stored in s3 and by looking at this predefined list, I don't see `arrow` in there either so in the end it return None:\r\n\r\n```\r\nMAGIC_NUMBER_TO_COMPRESSION_PROTOCOL = {\r\n    bytes.fromhex(\"504B0304\"): \"zip\",\r\n    bytes.fromhex(\"504B0506\"): \"zip\",  # empty archive\r\n    bytes.fromhex(\"504B0708\"): \"zip\",  # spanned archive\r\n    bytes.fromhex(\"425A68\"): \"bz2\",\r\n    bytes.fromhex(\"1F8B\"): \"gzip\",\r\n    bytes.fromhex(\"FD377A585A00\"): \"xz\",\r\n    bytes.fromhex(\"04224D18\"): \"lz4\",\r\n    bytes.fromhex(\"28B52FFD\"): \"zstd\",\r\n}\r\n```\n\n### Expected behavior\n\nMy expectation is that `arrow` would be in the known lists so it would return None without going through the magic number method.\n\n### Environment info\n\ndatasets 2.19.0<end_of_turn>\n<start_of_turn>model\nExtraction protocol for arrow files is not defined<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Use Case\r\nI use `dataset.map(process_fn, batched=True)` to process the dataset, with data **augmentations or filtering**. However, when all examples within a batch is filtered out, i.e. **an empty batch is returned**, the following error will be thrown:\r\n  ```\r\n  ValueError: Schema and number of arrays unequal\r\n  ```\r\nThis is because the empty batch does not comply with the schema of other batches. I think an empty batch should be allowed to facilitate coding (one does not need to assign an empty list manually for all keys.)\r\n\r\nA simple fix is to check the length of `batch` before writing:\r\n```\r\nif len(batch):\r\n  writer.write_batch(batch)\r\n```\r\ninstead of \r\n\r\nhttps://github.com/huggingface/datasets/blob/74d60213dcbd7c99484c62ce1d3dfd90a1df0770/src/datasets/arrow_dataset.py#L3493\r\n<end_of_turn>\n<start_of_turn>model\n[Feature Request] Check the length of batch before writing so that empty batch is allowed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using the newer pytorch 3.10 kernel, only 2 cores are being used by huggingface filter and map functions.  The Pytorch 3.9 kernel would use as many cores as specified in the num_proc field.\r\n\r\nWe have solved this in our own code by placing the following snippet in the code that is called inside subprocesses:\r\n\r\n```os.sched_setaffinity(0, {i for i in range(1000)})```\r\n\r\nThe problem, as near as we can tell, us that once upon a time, cpu affinity was set using a bitmask (\"0xfffff\" and the like), and affinity recently changed to a list of processors rather than to using the mask.  As such, only processors 1 and 17 are shown to be working in htop.\r\n![Selection_072](https://github.com/huggingface/datasets/assets/107141022/04c5a824-5321-4531-afca-7bc84dff36b4)\r\n\r\n\r\nWhen running functions via `map`, the above resetting of affinity works to spread across the cores.  When using `filter`, however, only two cores are active.\n\n### Steps to reproduce the bug\n\nRepro steps:\r\n\r\n1.  Create an aws sagemaker instance\r\n2. use the pytorch 3_10 kernel\r\n3. Load a dataset\r\n4. run a filter operation\r\n5. watch as only 2 cores are used when num_proc > 2\r\n6. run a map operation\r\n7. watch as only 2 cores are used when num_proc > 2\r\n8. run a map operation with processor affinity reset inside the function called via map\r\n9. Watch as all cores run\r\n\r\n\n\n### Expected behavior\n\nAll specified cores are used via the num_proc argument.\n\n### Environment info\n\nAWS sagemaker with the following init script run in the terminal after instance creation:\r\n\r\nconda init bash\r\nbash\r\nconda activate pytorch_p310\r\npip install Wand PyPDF pytesseract datasets seqeval pdfplumber transformers pymupdf sentencepiece timm donut-python accelerate optimum xgboost\r\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\r\nsudo yum -y install htop\r\nsudo yum -y update\r\nsudo yum -y install wget libstdc++ autoconf automake libtool autoconf-archive pkg-config gcc gcc-c++ make libjpeg-devel libpng-devel libtiff-devel zlib-devel<end_of_turn>\n<start_of_turn>model\nOnly two cores are getting used in sagemaker with pytorch 3.10 kernel<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nContext: we're splitting an iterable dataset by node and then passing it to a torch data loader with multiple workers\r\n\r\nWhen we iterate over it for 5 steps, we don't get an error\r\n\r\nWhen we instead iterate over it for 8 steps, we get an `IndexError` when fetching the data if we have too many workers\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere, we have 2 JAX processes (`jax.process_count() = 2`) which we split the dataset over. The dataset loading script can be found here: https://huggingface.co/datasets/distil-whisper/librispeech_asr/blob/c6a1e805cbfeed5057400ac5937327d7e30281b8/librispeech_asr.py#L310\r\n\r\n<details>\r\n\r\n<summary> Code to reproduce </summary>\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nimport jax\r\nfrom datasets.distributed import split_dataset_by_node\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\n\r\n# load an example dataset (https://huggingface.co/datasets/distil-whisper/librispeech_asr)\r\ndataset = load_dataset(\"distil-whisper/librispeech_asr\", \"all\", split=\"train.clean.100\", streaming=True)\r\n# just keep the text column -> no need to define a collator\r\ndataset_text = dataset.remove_columns(set(dataset.features.keys()) - {\"text\"})\r\n\r\n# define some constants\r\nbatch_size = 256\r\nnum_examples = 5  # works for 5 examples, doesn't for 8\r\nnum_workers = dataset_text.n_shards\r\n\r\n# try with multiple workers\r\ndataloader = DataLoader(dataset_text, batch_size=batch_size, num_workers=num_workers, drop_last=True)\r\n\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Multiple workers\"):\r\n    if i == num_examples:\r\n        break\r\n\r\n# try splitting by node (we can't do this with `dataset_text` since `split_dataset_by_node` expects the Audio column for an ASR dataset)\r\ndataset = split_dataset_by_node(dataset, rank=jax.process_index(), world_size=jax.process_count())\r\n# remove the text column again\r\ndataset_text = dataset.remove_columns(set(dataset.features.keys()) - {\"text\"})\r\ndataloader = DataLoader(dataset_text, batch_size=16, num_workers=num_workers // 2, drop_last=True)\r\n\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Split by node\"):\r\n    if i == num_examples:\r\n        break\r\n\r\n# too many workers\r\ndataloader = DataLoader(dataset_text, batch_size=256, num_workers=num_workers, drop_last=True)\r\nfor i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Too many workers\"):\r\n    if i == num_examples:\r\n        break\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n\r\n<summary> With 5 examples: </summary>\r\n\r\n```\r\nMultiple workers: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:16<00:00,  3.33s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.                                                       \r\nSplit by node: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:13<00:00,  2.76s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.                                                       \r\nToo many dataloader workers: 14 (max is dataset.n_shards=7). Stopping 7 dataloader workers.                             \r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary t\r\no have a number of workers greater than dataset.n_shards=7. To enable more parallelism, please split the dataset in more\r\n files than 7.                                                                                                          \r\nToo many workers: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:15<00:00,  3.03s/it]\r\n```\r\n\r\n</details>\r\n\r\n<details>\r\n\r\n<summary> With 7 examples: </summary>\r\n\r\n```\r\nMultiple workers: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:13<00:00,  1.71s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.\r\nSplit by node: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:11<00:00,  1.38s/it]\r\nAssigning 7 shards (or data sources) of the dataset to each node.\r\nToo many dataloader workers: 14 (max is dataset.n_shards=7). Stopping 7 dataloader workers.\r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary to have a number of workers greater than dataset.n_shards=7. To enable more parallelism, please split the dataset in more files than 7.\r\nToo many workers:  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b        | 7/8 [00:13<00:01,  1.89s/it]\r\nTraceback (most recent call last):\r\n  File \"distil-whisper/test_librispeech.py\", line 36, in <module>\r\n    for i, batch in tqdm(enumerate(dataloader), total=num_examples, desc=\"Too many workers\"):\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/tqdm/std.py\", line 1178, in __iter__\r\n    for obj in iterable:\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\r\n    data = self._next_data()\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1325, in _next_data\r\n    return self._process_data(data)\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data\r\n    data.reraise()\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/_utils.py\", line 644, in reraise\r\n    raise exception\r\nIndexError: Caught IndexError in DataLoader worker process 7.\r\nOriginal Traceback (most recent call last):\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\r\n    data = fetcher.fetch(index)\r\n  File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch\r\n    data.append(next(self.dataset_iter))\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 986, in __iter__\r\n    yield from self._iter_pytorch(ex_iterable)\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 920, in _iter_pytorch\r\n    for key, example in ex_iterable.shard_data_sources(worker_info.id, worker_info.num_workers):\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 540, in shard_data_sources\r\n    self.ex_iterable.shard_data_sources(worker_id, num_workers),\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 796, in shard_data_sources\r\n    self.ex_iterable.shard_data_sources(worker_id, num_workers),\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 126, in shard_data_sources\r\n    requested_gen_kwargs = _merge_gen_kwargs([gen_kwargs_list[i] for i in shard_indices])\r\n  File \"/home/sanchitgandhi/datasets/src/datasets/utils/sharding.py\", line 76, in _merge_gen_kwargs\r\n    for key in gen_kwargs_list[0]\r\nIndexError: list index out of range\r\n```\r\n\r\n</details>\r\n\r\n### Expected behavior\r\n\r\nShould pass for both 5 and 7 examples\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.12.1.dev0\r\n- Platform: Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1<end_of_turn>\n<start_of_turn>model\nSplit dataset by node: index error when sharding iterable dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nTo display images with their associated metadata in the dataset viewer, a `metadata.csv` file is required. In the case of a dataset with multiple subsets, this would require the CSVs to be contained in the same folder as the images since they all need to be named `metadata.csv`. The request is that this be made more flexible for datasets with multiple subsets to avoid the need to put a `metadata.csv` into each image directory where they are not as easily accessed. \r\n\r\n### Motivation\r\n\r\nWhen creating datasets with multiple subsets I can't get the images to display alongside their associated metadata (it's usually one or the other that will show up). Since this requires a file specifically named `metadata.csv`, I then have to place that file within the image directory, which makes it much more difficult to access. Additionally, it still doesn't necessarily display the images alongside their metadata correctly (see, for instance, [this discussion](https://huggingface.co/datasets/imageomics/2018-NEON-beetles/discussions/8)).\r\n\r\nIt was suggested I bring this discussion to GitHub on another dataset struggling with a similar issue ([discussion](https://huggingface.co/datasets/imageomics/fish-vista/discussions/4)). In that case, it's a mix of data subsets, where some just reference the image URLs, while others actually have the images uploaded. The ones with images uploaded are not displaying images, but renaming that file to just `metadata.csv` would diminish the clarity of the construction of the dataset itself (and I'm not entirely convinced it would solve the issue).\r\n\r\n### Your contribution\r\n\r\nI can make a suggestion for one approach to address the issue:\r\n\r\nFor instance, even if it could just end in `_metadata.csv` or `-metadata.csv`, that would be very helpful to allow for more flexibility of dataset structure without impacting clarity. I would think that the functionality on the backend looking for `metadata.csv` could reasonably be adapted to look for such an ending on a filename (maybe also check that it has a `file_name` column?).\r\n\r\nPresumably, requiring the `configs` in a setup like on [this dataset](https://huggingface.co/datasets/imageomics/rare-species/blob/main/README.md) could also help in figuring out how it should work?\r\n```\r\nconfigs:\r\n  - config_name: <image subset>\r\n    data_files:\r\n      - <image-metadata>.csv\r\n      - <path/to/images>/*.jpg\r\n```\r\n\r\nI'd also be happy to look at whatever solution is decided upon and contribute to the ideation.\r\n\r\nThanks for your time and consideration! The dataset viewer really is fabulous when it works :)<end_of_turn>\n<start_of_turn>model\nMake dataset viewer more flexible in displaying metadata alongside images<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNot 100% about this one, but it seems to be recommended.\r\n\r\n```\r\n/fsx/qgallouedec/miniconda3/envs/trl/lib/python3.12/site-packages/datasets/utils/_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\r\n```\r\nTests pass locally. And the warning is gone with this change.\r\n\r\nhttps://peps.python.org/pep-0626/#backwards-compatibility<end_of_turn>\n<start_of_turn>model\nUpdate `_dill.py` to use `co_linetable` for Python 3.10+ in place of `co_lnotab`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nMissing line from previous pr<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(revival of #6832)\r\n\r\nhttps://github.com/huggingface/datasets/pull/7648#issuecomment-3084050130\r\n\r\nClose https://github.com/huggingface/datasets/issues/4101, and more\r\n\r\n---\r\n### PR under work!!!!\r\n<end_of_turn>\n<start_of_turn>model\nReimplemented partial split download support (revival of #6832)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCI is broken for test_load_dataset_distributed_with_script. See: https://github.com/huggingface/datasets/actions/runs/8614926216/job/23609378127\r\n```\r\nFAILED tests/test_load.py::test_load_dataset_distributed_with_script[None] - assert False\r\n +  where False = all(<generator object test_load_dataset_distributed_with_script.<locals>.<genexpr> at 0x7f0c741de3b0>)\r\nFAILED tests/test_load.py::test_load_dataset_distributed_with_script[force_redownload] - assert False\r\n +  where False = all(<generator object test_load_dataset_distributed_with_script.<locals>.<genexpr> at 0x7f0be45f6ea0>)\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken due to hf-internal-testing/dataset_with_script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7591\r\n\r\n### Add num_proc support to `push_to_hub()` for parallel shard upload\r\n\r\nThis PR adds support for parallel upload of dataset shards via the `num_proc` argument in `Dataset.push_to_hub()`.\r\n\r\n\ud83d\udccc While the `num_proc` parameter was already present in the `push_to_hub()` signature and correctly passed to `_push_parquet_shards_to_hub()`, it was not being used to parallelize the upload.\r\n\r\n\ud83d\udd27 This PR updates the internal `_push_parquet_shards_to_hub()` function to:\r\n\r\n- Use `multiprocessing.Pool` and `iflatmap_unordered()` for concurrent shard upload when `num_proc > 1`\r\n- Preserve original serial upload behavior if `num_proc` is `None` or \u2264 1\r\n- Keep tqdm progress and commit behavior unchanged\r\n\r\nLet me know if any test coverage or further changes are needed!\r\n<end_of_turn>\n<start_of_turn>model\nEnable parallel shard upload in push_to_hub() using num_proc<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate process.mdx: Fix code listing in Shard section<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI try to use `load_dataset()` to load several local `.jsonl` files as a dataset. Every line of these files is a json structure only containing one key `text` (yeah it is a dataset for NLP model). The code snippet is as:\r\n```python\r\nds = load_dataset(\"json\", data_files=LIST_OF_FILE_PATHS, num_proc=16)['train']\r\n```\r\nHowever, I found that the loading process can get stuck -- the progress bar `Generating train split` no more proceed. When I was trying to find the cause and solution, I found a really strange behavior. If I load the dataset in this way:\r\n```python\r\ndlist = list()\r\nfor _ in LIST_OF_FILE_PATHS:\r\n    dlist.append(load_dataset(\"json\", data_files=_)['train'])\r\nds = concatenate_datasets(dlist)\r\n```\r\nI can actually successfully load all the files despite its slow speed. But if I load them in batch like above, things go wrong. I did try to use Control-C to trace the stuck point but the program cannot be terminated in this way when `num_proc` is set to `None`. The only thing I can do is use Control-Z to hang it up then kill it. If I use more than 2 cpus, a Control-C would simply cause the following error:\r\n```bash\r\n^C\r\nProcess ForkPoolWorker-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 114, in worker\r\n    task = get()\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/queues.py\", line 368, in get\r\n    res = self._reader.recv_bytes()\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 224, in recv_bytes\r\n    buf = self._recv_bytes(maxlength)\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 422, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 387, in _recv\r\n    chunk = read(handle, remaining)\r\nKeyboardInterrupt\r\nGenerating train split: 92431 examples [01:23, 1104.25 examples/s]  \r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 1373, in iflatmap_unordered\r\n    yield queue.get(timeout=0.05)\r\n  File \"<string>\", line 2, in get\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/managers.py\", line 818, in _callmethod\r\n    kind, result = conn.recv()\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 258, in recv\r\n    buf = self._recv_bytes()\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 422, in _recv_bytes\r\n    buf = self._recv(4)\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/connection.py\", line 387, in _recv\r\n    chunk = read(handle, remaining)\r\nKeyboardInterrupt\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/data/liyongyuan/source/batch_load.py\", line 11, in <module>\r\n    a = load_dataset(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2133, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1049, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1842, in _prepare_split\r\n    for job_id, done, content in iflatmap_unordered(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 1387, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 1387, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py\", line 770, in get\r\n    raise TimeoutError\r\nmultiprocess.context.TimeoutError\r\n```\r\nI have validated the basic correctness of these `.jsonl` files. They are correctly formatted (or they cannot be loaded singly by `load_dataset`) though some of the json may contain too long text (more than 1e7 characters). I do not know if this could be the problem. And there should not be any bottleneck in system's resource. The whole dataset is ~300GB, and I am using a cloud server with plenty of storage and 1TB ram. \r\nThanks for your efforts and patience! Any suggestion or help would be appreciated.\n\n### Steps to reproduce the bug\n\n1. use load_dataset() with `data_files = LIST_OF_FILES`\n\n### Expected behavior\n\nAll the files should be smoothly loaded. \n\n### Environment info\n\n- Datasets: A private dataset. ~2500 `.jsonl` files. ~300GB in total. Each json structure only contains one key: `text`. Format checked.\r\n- `datasets` version: 2.14.2\r\n- Platform: Linux-4.19.91-014.kangaroo.alios7.x86_64-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 10.0.1.dev0+ga6eabc2b.d20230609\r\n- Pandas version: 1.5.2<end_of_turn>\n<start_of_turn>model\nLoading local datasets got strangely stuck<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd support for NIfTI.\r\n\r\nsupports #7804\r\n\r\nThis PR follows https://github.com/huggingface/datasets/pull/7325 very closely\r\n\r\nI am a bit unsure what we need to add to the `document_dataset.mdx` and `document_load.mdx`. I should probably create a dataset on the hub first to create this guide instead of copy+pasting from PDF. \r\n\r\nOpen todos:\r\n - [x] create nifti dataset on the hub\r\n - ~[ ] update `document_dataset.mdx` and `document_load.mdx`~\r\n \r\n EDIT:\r\n I tested with two datasets I created on the hub: \r\n  - https://huggingface.co/datasets/TobiasPitters/test-nifti-unzipped\r\n  - https://huggingface.co/datasets/TobiasPitters/test-nifti\r\n  \r\n for zipped (file extension `.nii.gz` and unzipped `.nii`) files and both seem to work fine. Also tested loading locally and that seems to work as well.\r\nHere is the scriptsthat I ran against the hub:\r\n\r\n```python\r\nfrom pathlib import Path\r\n\r\nfrom datasets import load_dataset\r\nimport nibabel as nib\r\n\r\n\r\ndataset = load_dataset(\r\n        \"TobiasPitters/test-nifti-unzipped\",\r\n        split=\"test\"  # Load as single Dataset, not DatasetDict\r\n)\r\n\r\nprint(\"length dataset unzipped:\", len(dataset))\r\nfor item in dataset:\r\n    isinstance(item[\"nifti\"], nib.nifti1.Nifti1Image)\r\n\r\ndataset = load_dataset(\r\n        \"TobiasPitters/test-nifti\",\r\n        split=\"train\"  # Load as single Dataset, not DatasetDict\r\n)\r\nprint(\"length dataset zipped:\", len(dataset))\r\nfor item in dataset:\r\n    isinstance(item[\"nifti\"], nib.nifti1.Nifti1Image)\r\n```<end_of_turn>\n<start_of_turn>model\nAdd nifti support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi there,\n\nI am trying to load a dataset from the Hugging Face Hub and split it into train and validation splits. Somehow, when I try to do it with `load_dataset`, it exhausts my disk quota. So, I tried manually downloading the parquet files from the hub and loading them as follows:\n\n```python\n ds = DatasetDict(\n        {\n            \"train\": load_dataset(\n                \"parquet\", \n                data_dir=f\"{local_dir}/{tok}\", \n                cache_dir=cache_dir, \n                num_proc=min(12, os.cpu_count()),   # type: ignore\n                split=ReadInstruction(\"train\", from_=0, to=NUM_TRAIN, unit=\"abs\"),  # type: ignore\n            ),\n            \"validation\": load_dataset(\n                \"parquet\", \n                data_dir=f\"{local_dir}/{tok}\", \n                cache_dir=cache_dir, \n                num_proc=min(12, os.cpu_count()),   # type: ignore\n                split=ReadInstruction(\"train\", from_=NUM_TRAIN, unit=\"abs\"),  # type: ignore\n            )\n        }\n    )\n\n```\n\nwhich still strangely creates 720GB of cache. In addition, if I remove the raw parquet file folder (`f\"{local_dir}/{tok}\"` in this example), I am not able to load anything. So, I am left wondering what this cache is doing. Am I missing something? Is there a solution to this problem?\n\nThanks a lot in advance for your help!\n\nA related issue: https://github.com/huggingface/transformers/issues/10204#issue-809007443.\n\n---\n\nPython: 3.11.11\ndatasets: 3.5.0\n<end_of_turn>\n<start_of_turn>model\n`load_dataset` of size 40GB creates a cache of >720GB<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen using distributed training, the code of `os.remove(filename)` may be executed separately by each rank, leading to `FileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmprxxxxxxx.arrow'`\r\n\r\n```python\r\nfrom torch import distributed as dist\r\n\r\nif dist.get_rank() == 0:\r\n    dataset = process_dataset(*args, **kwargs)\r\n    objects = [dataset]\r\nelse:\r\n    objects = [None]\r\ndist.broadcast_object_list(objects, src=0)\r\ndataset = objects[0]\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nRemove `Table.__getstate__` and `Table.__setstate__`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse [`array.flatten`](https://arrow.apache.org/docs/python/generated/pyarrow.ListArray.html#pyarrow.ListArray.flatten) that takes `.offset` into account instead of `array.values` in array cast/embed.<end_of_turn>\n<start_of_turn>model\nBetter list array values handling in cast/embed storage<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIntroduce a HF_DATASET_CACHE_NETWORK_LOCATION configuration (e.g. an environment variable) together with a companion network cache service.\n\nEnable a three-tier cache lookup for datasets:\n\n1. Local on-disk cache\n2. Configurable network cache proxy\n3. Official Hugging Face Hub\n\n### Motivation\n\n- Distributed training & ephemeral jobs: In high-performance or containerized clusters, relying solely on a local disk cache either becomes a streaming bottleneck or incurs a heavy cold-start penalty as each job must re-download datasets.\n- Traffic & cost reduction: A pull-through network cache lets multiple consumers share a common cache layer, reducing duplicate downloads from the Hub and lowering egress costs.\n- Better streaming adoption: By offloading repeat dataset pulls to a locally managed cache proxy, streaming workloads can achieve higher throughput and more predictable latency.\n- Proven pattern: Similar proxy-cache solutions (e.g. Harbor\u2019s Proxy Cache for Docker images) have demonstrated reliability and performance at scale: https://goharbor.io/docs/2.1.0/administration/configure-proxy-cache/\n\n### Your contribution\n\nI\u2019m happy to draft the initial PR for adding HF_DATASET_CACHE_NETWORK_LOCATION support in datasets and sketch out a minimal cache-service prototype.\n\nI have limited bandwidth so I would be looking for collaborators if anyone else is interested. <end_of_turn>\n<start_of_turn>model\nNetworked Pull Through Cache<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n+ mark as legacy in some parts of the docs since we'll not build new features for script datasets<end_of_turn>\n<start_of_turn>model\nless script docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt's too cumbersome to write this command every time we perform a dataset merging operation. ```pythonfrom datasets import concatenate_datasets``` We have added a simple `__add__` magic method to each class using `concatenate_datasets.`\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nbookcorpus = load_dataset(\"bookcorpus\", split=\"train\")\r\nwiki = load_dataset(\"wikimedia/wikipedia\", \"20231101.ab\", split=\"train\")\r\nwiki = wiki.remove_columns([col for col in wiki.column_names if col != \"text\"])  # only keep the 'text' column\r\n\r\nbookcorpus + wiki\r\n\r\n#Dataset({\r\n#   features: ['text'],\r\n#   num_rows: 74004228\r\n#})\r\n#Dataset({\r\n#    features: ['text'],\r\n#    num_rows: 6152\r\n#})\r\n#Dataset({\r\n#   features: ['text'],\r\n#    num_rows: 74010380\r\n#})\r\n```<end_of_turn>\n<start_of_turn>model\n__add__ for Dataset, IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nA faster way to sort a dataset which contains a large number of rows.\r\n\r\n### Motivation\r\n\r\nThe current sorting implementations took significantly longer than expected when I was running on a dataset trying to sort by timestamps. \r\n\r\n**Code snippet:**\r\n```python\r\nds = datasets.load_dataset( \"json\", **{\"data_files\": {\"train\": \"path-to-jsonlines\"}, \"split\": \"train\"}, num_proc=os.cpu_count(), keep_in_memory=True) \r\nsorted_ds = ds.sort(\"pubDate\", keep_in_memory=True)\r\n```\r\n\r\nHowever, once I switched to a different method which\r\n1. unpacked to a list of tuples\r\n2. sorted tuples by key\r\n3. run `.select` with the sorted list of indices\r\nIt was significantly faster (orders of magnitude, especially with M's of rows)\r\n\r\n### Your contribution\r\n\r\nI'd be happy to implement a crude single key sorting algorithm so that other users can benefit from this trick. Broadly, this would take a `Dataset` and perform;\r\n\r\n```python\r\n# ds is a Dataset object\r\n# key_name is the sorting key\r\n\r\nclass Dataset:\r\n   ...\r\n   def _sort(key_name: str) -> Dataset:\r\n      index_keys = [(i,x) for i,x in enumerate(self[key_name])]\r\n      sorted_rows = sorted(row_pubdate, key=lambda x: x[1])\r\n      sorted_indicies = [x[0] for x in sorted_rows]\r\n      return self.select(sorted_indicies)\r\n```<end_of_turn>\n<start_of_turn>model\nFaster sorting for single key items<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI follow this [example](https://discuss.huggingface.co/t/error-handling-in-iterabledataset/72827/3) to handle errors in custom dataset. I am writing a dataset script which read jsonl files and i need to handle errors and continue reading files without raising exception and exit the execution.\r\n\r\n```\r\ndef _generate_examples(self, filepaths):\r\n        errors=[]\r\n        \r\n        id_ = 0\r\n        for filepath in filepaths:\r\n            try:\r\n                with open(filepath, 'r') as f:\r\n                    for line in f:\r\n                        json_obj = json.loads(line)\r\n                        \r\n                        yield id_, json_obj\r\n                        id_ += 1\r\n            except Exception as exc:     \r\n                logger.error(f\"error occur at filepath: {filepath}\")      \r\n                errors.append(error)\r\n```\r\n\r\nseems the logger.error is printed but still exception is raised the the run is exit.\r\n\r\n```\r\nDownloading and preparing dataset custom_dataset/default to /home/myuser/.cache/huggingface/datasets/custom_dataset/default-a14cdd566afee0a6/1.0.0/acfcc9fb9c57034b580c4252841\r\nERROR: datasets_modules.datasets.custom_dataset.acfcc9fb9c57034b580c4252841bb890a5617cbd28678dd4be5e52b81188ad02.custom_dataset: 2024-07-22 10:47:42,167: error occur at filepath: '/home/myuser/ds/corrupted-file.jsonl\r\nTraceback (most recent call last):\r\n  File \"/home/myuser/.cache/huggingface/modules/datasets_modules/datasets/custom_dataset/ac..2/custom_dataset.py\", line 48, in _generate_examples\r\n    json_obj = json.loads(line)\r\n  File \"myenv/lib/python3.8/json/__init__.py\", line 357, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"myenv/lib/python3.8/json/decoder.py\", line 337, in decode\r\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\r\n  File \"myenv/lib/python3.8/json/decoder.py\", line 353, in raw_decode\r\n    obj, end = self.scan_once(s, idx)\r\njson.decoder.JSONDecodeError: Invalid control character at: line 1 column 4 (char 3)\r\nGenerating train split: 0 examples [00:06, ? examples/s]>\r\nRemoteTraceback: \r\n\"\"\"\r\nTraceback (most recent call last):\r\n  File \"myenv/lib/python3.8/site-packages/datasets/builder.py\", line 1637, in _prepare_split_single\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"myenv/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 594, in finalize\r\n    raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\ndatasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"myenv/lib/python3.8/site-packages/multiprocess/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"myenv/lib/python3.8/site-packages/datasets/utils/py_utils.py\", line 1353, in \r\n_write_generator_to_queue\r\n    for i, result in enumerate(func(**kwargs)):\r\n  File \"myenv/lib/python3.8/site-packages/datasets/builder.py\", line 1646, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n\r\n\u2502                                                                                                  \u2502\r\n\u2502 myenv/lib/python3.8/site-packages/datasets/utils/py_utils. \u2502\r\n\u2502 py:1377 in <listcomp>                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1374 \u2502   \u2502   \u2502   \u2502   if all(async_result.ready() for async_result in async_results) and queue  \u2502\r\n\u2502   1375 \u2502   \u2502   \u2502   \u2502   \u2502   break                                                                 \u2502\r\n\u2502   1376 \u2502   \u2502   # we get the result in case there's an error to raise                             \u2502\r\n\u2502 \u2771 1377 \u2502   \u2502   [async_result.get() for async_result in async_results]                            \u2502\r\n\u2502   1378                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                      \u2502\r\n\u2502 \u2502           .0 = <list_iterator object at 0x7f2cc1f0ce20>                 \u2502                      \u2502\r\n\u2502 \u2502 async_result = <multiprocess.pool.ApplyResult object at 0x7f2cc1f79c10> \u2502                      \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                      \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 myenv/lib/python3.8/site-packages/multiprocess/pool.py:771 \u2502\r\n\u2502 in get                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   768 \u2502   \u2502   if self._success:                                                                  \u2502\r\n\u2502   769 \u2502   \u2502   \u2502   return self._value                                                             \u2502\r\n\u2502   770 \u2502   \u2502   else:                                                                              \u2502\r\n\u2502 \u2771 771 \u2502   \u2502   \u2502   raise self._value                                                              \u2502\r\n\u2502   772 \u2502                                                                                          \u2502\r\n\u2502   773 \u2502   def _set(self, i, obj):                                                                \u2502\r\n\u2502   774 \u2502   \u2502   self._success, self._value = obj                                                   \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 \u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 locals \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e                           \u2502\r\n\u2502 \u2502    self = <multiprocess.pool.ApplyResult object at 0x7f2cc1f79c10> \u2502                           \u2502\r\n\u2502 \u2502 timeout = None                                                     \u2502                           \u2502\r\n\u2502 \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f                           \u2502\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Steps to reproduce the bug\n\nsame as above\n\n### Expected behavior\n\nshould handle error and continue reading remaining files\n\n### Environment info\n\npython 3.9<end_of_turn>\n<start_of_turn>model\nCustom Dataset | Still Raise Error while handling errors in _generate_examples<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a JSON file in my s3 file system(minio), I can use load_dataset to get the file link, but I can't iterate it\r\n<img width=\"816\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/59083384/cc0778d3-36f3-45b5-ac68-4e7c664c2ed0\">\r\n<img width=\"1144\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/59083384/76872af3-8b3c-42ff-9f55-528c920a7af1\">\r\n\r\nwe can change 4 lines to fix this bug, you can check whether it is ok for us.\r\n<img width=\"941\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/59083384/5a22155a-ece7-496c-8506-047e5c235cd3\">\n\n### Steps to reproduce the bug\n\n1. storage a file in you s3 file system\r\n2. use load_dataset to read it through streaming\r\n3. iterate it\n\n### Expected behavior\n\ncan iterate it successfully\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1\r\n<end_of_turn>\n<start_of_turn>model\nload_dataset from s3 file system through streaming can't not iterate data <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe full trace:\r\n```\r\n/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/datasets/load.py:1744: FutureWarning: 'ignore_verifications' was de\r\nprecated in favor of 'verification_mode' in version 2.9.1 and will be removed in 3.0.0.\r\nYou can remove this warning by passing 'verification_mode=no_checks' instead.\r\n  warnings.warn(               \r\nDownloading and preparing dataset parquet/laion--laion-coco to /home/bian/.cache/huggingface/datasets/laion___parquet/laion--\r\nlaion-coco-cb4205d7f1863066/0.0.0/bcacc8bdaa0614a5d73d0344c813275e590940c6ea8bc569da462847103a1afd...\r\nDownloading data: 100%|\u2588| 1.89G/1.89G [04:57<00:00,           \r\nDownloading data files: 100%|\u2588| 1/1 [04:59<00:00, 2           \r\nExtracting data files: 100%|\u2588| 1/1 [00:00<00:00, 13           \r\nGenerating train split: 0 examples [00:00, ? examples/s]<_io.BufferedReader \r\nname='/home/bian/.cache/huggingface/datasets/downlo           \r\nads/26d7a016d25bbd9443115cfa3092136e8eb2f1f5bcd4154           \r\n0cb9234572927f04c'>                                           \r\n                                                   Traceback (most recent call last):\r\n  File \"/home/bian/data/ZOC/download_laion_coco.py\", line 4, in <module>\r\n    dataset = load_dataset(\"laion/laion-coco\", ignore_verifications=True)\r\n  File \"/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/datasets/load.py\", line 1791, in load_dataset\r\n    builder_instance.download_and_prepare(                    \r\n  File \"/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/datasets/builder.py\", line 891, in download_and_prepare\r\n    self._download_and_prepare(                               \r\n  File \"/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/datasets/builder.py\", line 986, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/datasets/builder.py\", line 1748, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/datasets/builder.py\", line 1842, in _prepare_split_single\r\n    generator = self._generate_tables(**gen_kwargs)           \r\n  File \"/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 67, in \r\n_generate_tables                                              \r\n    parquet_file = pq.ParquetFile(f)                          \r\n  File \"/home/bian/anaconda3/envs/sd/lib/python3.10/site-packages/pyarrow/parquet/core.py\", line 323, in __init__\r\n    self.reader.open(          \r\n  File \"pyarrow/_parquet.pyx\", line 1227, in pyarrow._parquet.ParquetReader.open\r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file\r\n.\r\n```\r\n\r\n\r\nI have carefully followed the instructions in  #5264  but still get the same error.\r\n\r\nOther helpful information: \r\n\r\n\r\n```\r\nds = load_dataset(\"parquet\", data_files=\r\n    ...: \"https://huggingface.co/datasets/laion/l\r\n    ...: aion-coco/resolve/d22869de3ccd39dfec1507\r\n    ...: f7ded32e4a518dad24/part-00000-2256f782-1\r\n    ...: 26f-4dc6-b9c6-e6757637749d-c000.snappy.p\r\n    ...: arquet\")\r\nFound cached dataset parquet (/home/bian/.cache/huggingface/datasets/parquet/default-a02eea00aeb08b0e/0.0.0/bb8ccf89d9ee38581ff5e51506d721a9b37f14df8090dc9b2d8fb4a40957833f)\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00,  4.55it/s]\r\n```\n\n### Steps to reproduce the bug\n\n``` \r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"laion/laion-coco\", ignore_verifications=True/False)\r\n```\n\n### Expected behavior\n\nProperly load Laion-coco dataset\n\n### Environment info\n\ndatasets==2.11.0 torch==1.12.1 python 3.10<end_of_turn>\n<start_of_turn>model\nlaion-coco download error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIterating over a [`Column`](https://github.com/huggingface/datasets/blob/8b1bd4ec1cc9e9ce022f749abb6485ef984ae7c0/src/datasets/arrow_dataset.py#L633-L692) iterates through the parent [`Dataset`](https://github.com/huggingface/datasets/blob/8b1bd4ec1cc9e9ce022f749abb6485ef984ae7c0/src/datasets/arrow_dataset.py#L695) and applies all formatting/transforms on each row, regardless of which column is being accessed. This causes an error when transforms depend on columns not present in the projection.\n\n### Steps to reproduce the bug\n\n### Load a dataset with multiple columns\n\n```python\nds = load_dataset(\"mrbrobot/isic-2024\", split=\"train\")\n```\n\n### Define a transform that specifies an input column\n\n```python\ndef image_transform(batch):\n    batch[\"image\"] = batch[\"image\"]  # KeyError when batch doesn't contain \"image\"\n    return batch\n\n# apply transform only to image column\nds = ds.with_format(\"torch\")\nds = ds.with_transform(image_transform, columns=[\"image\"], output_all_columns=True)\n```\n\n### Iterate over non-specified column\n\n```python\n# iterate over a different column, triggers the transform on each row, but batch doesn't contain \"image\"\nfor t in ds[\"target\"]:  # KeyError: 'image'\n    print(t)\n```\n\n### Expected behavior\n\nIf a user iterates over `ds[\"target\"]` and the transform specifies `columns=[\"image\"]`, the transform should be skipped.\n\n### Environment info\n\n`datasets`: 4.2.0\nPython: 3.12.12\nLinux: Debian 11.11<end_of_turn>\n<start_of_turn>model\nTransform with columns parameter triggers on non-specified column access<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR aims to implement a resumable `BufferShuffledExamplesIterable`.\r\nInstead of saving the entire buffer content, which is very memory-intensive, the newly implemented `BufferShuffledExamplesIterable` saves only the minimal state necessary for recovery, e.g., the random generator states and the state of the first example in the buffer dict.\r\n\r\nThe idea is that since the buffer size is limited, even if the entire buffer is discarded, we can rebuild it as long as the state of the oldest example is recorded. For buffer size $B$, the expected distance between when an example is pushed and when it is yielded is \r\n$d = \\sum_{k=1}^{\\infty} k\\frac{1}{B} (1 - \\frac{1}{B} )^{k-1} =B$.\r\nSimulation experiments support these claims:\r\n```py\r\nfrom random import randint\r\n\r\nBUFFER_SIZE = 1024\r\n\r\ndists = []\r\nbuffer = []\r\nfor i in range(10000000):\r\n    if i < BUFFER_SIZE:\r\n        buffer.append(i)\r\n    else:\r\n        index = randint(0, BUFFER_SIZE - 1)\r\n        dists.append(i - buffer[index])\r\n        buffer[index] = i\r\n\r\nprint(f\"MIN DIST: {min(dists)}\\nMAX DIST: {max(dists)}\\nAVG DIST: {sum(dists) / len(dists):.2f}\\n\")\r\n```\r\nwhich produces the following output:\r\n```py\r\nMIN DIST: 1\r\nMAX DIST: 15136\r\nAVG DIST: 1023.95\r\n\r\n```\r\nThe overall time for reconstructing the buffer and recovery should not be too long.\r\nThe following code mimics the cases of resuming online tokenization by `datasets` and `StatefulDataLoader` under distributed scenarios, \r\n\r\n```py\r\nimport pickle\r\nimport time\r\nfrom itertools import chain\r\nfrom typing import Any, Dict, List\r\n\r\nimport torch\r\nfrom datasets import load_dataset\r\nfrom torchdata.stateful_dataloader import StatefulDataLoader\r\nfrom tqdm import tqdm\r\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling\r\n\r\ntokenizer = AutoTokenizer.from_pretrained('fla-hub/gla-1.3B-100B')\r\ntokenizer.pad_token = tokenizer.eos_token\r\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\r\n\r\ntorch.manual_seed(42)\r\n\r\n\r\ndef tokenize(examples: Dict[str, List[Any]]) -> Dict[str, List[List[int]]]:\r\n    input_ids = tokenizer(examples['text'])['input_ids']\r\n    input_ids = list(chain(*input_ids))\r\n    total_length = len(input_ids)\r\n    chunk_size = 2048\r\n    total_length = (total_length // chunk_size) * chunk_size\r\n    # the last chunk smaller than chunk_size will be discarded\r\n    return {'input_ids': [input_ids[i: i+chunk_size] for i in range(0, total_length, chunk_size)]}\r\n\r\n\r\nbatch_size = 16\r\nnum_workers = 5\r\ncontext_length = 2048\r\nrank = 1\r\nworld_size = 32\r\nprefetch_factor = 2\r\nsteps = 2048\r\npath = 'fla-hub/slimpajama-test'\r\ndataset = load_dataset(\r\n    path=path,\r\n    split='train',\r\n    streaming=True,\r\n    trust_remote_code=True\r\n)\r\ndataset = dataset.map(tokenize, batched=True, remove_columns=next(iter(dataset)).keys())\r\ndataset = dataset.shuffle(seed=42)\r\nloader = StatefulDataLoader(dataset=dataset,\r\n                            batch_size=batch_size,\r\n                            collate_fn=data_collator,\r\n                            num_workers=num_workers,\r\n                            persistent_workers=False,\r\n                            prefetch_factor=prefetch_factor)\r\nstart = time.time()\r\nfor i, batch in tqdm(enumerate(loader)):\r\n    if i == 0:\r\n        print(f'{i}\\n{batch[\"input_ids\"]}')\r\n    if i == steps - 1:\r\n        print(f'{i}\\n{batch[\"input_ids\"]}')\r\n        state_dict = loader.state_dict()\r\n    if i == steps:\r\n        print(f'{i}\\n{batch[\"input_ids\"]}')\r\n        break\r\nprint(f\"{time.time() - start:.2f}s elapsed\")\r\nprint(f\"{len(pickle.dumps(state_dict)) / 1024**2:.2f}MB states in total\")\r\nfor worker in state_dict['_snapshot']['_worker_snapshots'].keys():\r\n    print(f\"{worker} {len(pickle.dumps(state_dict['_snapshot']['_worker_snapshots'][worker])) / 1024**2:.2f}MB\")\r\nprint(state_dict['_snapshot']['_worker_snapshots']['worker_0']['dataset_state'])\r\n\r\nloader = StatefulDataLoader(dataset=dataset,\r\n                            batch_size=batch_size,\r\n                            collate_fn=data_collator,\r\n                            num_workers=num_workers,\r\n                            persistent_workers=False,\r\n                            prefetch_factor=prefetch_factor)\r\nprint(\"Loading state dict\")\r\nloader.load_state_dict(state_dict)\r\nstart = time.time()\r\nfor batch in loader:\r\n    print(batch['input_ids'])\r\n    break\r\n\r\nprint(f\"{time.time() - start:.2f}s elapsed\")\r\n\r\n```\r\nand the outputs are\r\n```py\r\n0\r\ntensor([[  909,   395, 19082,  ..., 13088, 16232,   395],\r\n        [  601, 28705, 28770,  ..., 28733,   923,   288],\r\n        [21753, 15071, 13977,  ...,  9369, 28723,   415],\r\n        ...,\r\n        [21763, 28751, 20300,  ..., 28781, 28734,  4775],\r\n        [  354,   396, 10214,  ...,   298,   429, 28770],\r\n        [  333,  6149, 28768,  ...,  2773,   340,   351]])\r\n2047\r\ntensor([[28723,   415,  3889,  ...,   272,  3065,  2609],\r\n        [  403,  3214,  3629,  ...,   403, 21163, 16434],\r\n        [28723,    13, 28749,  ..., 28705, 28750, 28734],\r\n        ...,\r\n        [ 2778,  2251, 28723,  ...,   354,   684,   429],\r\n        [ 5659,   298,  1038,  ...,  5290,   297, 22153],\r\n        [  938, 28723,  1537,  ...,  9123, 28733, 12154]])\r\n2048\r\ntensor([[  769,   278, 12531,  ..., 28721, 19309, 28739],\r\n        [  415, 23347,   622,  ...,  3937,  2426, 28725],\r\n        [28745,  4345, 28723,  ...,   338, 28725,   583],\r\n        ...,\r\n        [ 1670, 28709,  5809,  ..., 28734, 28760,   393],\r\n        [  340,  1277,   624,  ...,   325, 28790,  1329],\r\n        [  523,  1144,  3409,  ...,   359,   359, 17422]])\r\n65.97s elapsed\r\n0.00MB states in total\r\nworker_0 0.00MB\r\nworker_1 0.00MB\r\nworker_2 0.00MB\r\nworker_3 0.00MB\r\nworker_4 0.00MB\r\n{'ex_iterable': {'ex_iterable': {'shard_idx': 0, 'shard_example_idx': 14000}, 'num_examples_since_previous_state': 166, 'previous_state_example_idx': 7394, 'previous_state': {'shard_idx': 0, 'shard_example_idx': 13000}}, 'num_taken': 6560, 'global_example_idx': 7560, 'buffer_state_dict': {'num_taken': 6560, 'global_example_idx': 356, 'index_offset': 0, 'first_state': {'ex_iterable': {'shard_idx': 0, 'shard_example_idx': 1000}, 'num_examples_since_previous_state': 356, 'previous_state_example_idx': 0, 'previous_state': {'shard_idx': 0, 'shard_example_idx': 0}}, 'bit_generator_state': {'state': {'state': 274674114334540486603088602300644985544, 'inc': 332724090758049132448979897138935081983}, 'bit_generator': 'PCG64', 'has_uint32': 0, 'uinteger': 0}}}\r\nLoading state dict\r\ntensor([[  769,   278, 12531,  ..., 28721, 19309, 28739],\r\n        [  415, 23347,   622,  ...,  3937,  2426, 28725],\r\n        [28745,  4345, 28723,  ...,   338, 28725,   583],\r\n        ...,\r\n        [ 1670, 28709,  5809,  ..., 28734, 28760,   393],\r\n        [  340,  1277,   624,  ...,   325, 28790,  1329],\r\n        [  523,  1144,  3409,  ...,   359,   359, 17422]])\r\n24.60s elapsed\r\n```\r\n\r\nNot sure if this PR complies with the `datasets` code style. Looking for your help @lhoestq, also very willing to further improve the code if any suggestions are given.\r\n<end_of_turn>\n<start_of_turn>model\nMake `BufferShuffledExamplesIterable` resumable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am interested in creating train, test and eval splits from a pandas Dataframe, therefore I was looking at the possibilities I can follow. I noticed the split parameter and was hopeful to use it in order to generate the 3 at once, however, while trying to understand the code, i noticed that it has no added value (correct me if I am wrong or misunderstood the code). \r\n\r\n\r\nfrom_pandas function code :\r\n\r\n```python\r\n    if info is not None and features is not None and info.features != features:\r\n            raise ValueError(\r\n                f\"Features specified in `features` and `info.features` can't be different:\\n{features}\\n{info.features}\"\r\n            )\r\n        features = features if features is not None else info.features if info is not None else None\r\n        if info is None:\r\n            info = DatasetInfo()\r\n        info.features = features\r\n        table = InMemoryTable.from_pandas(\r\n            df=df,\r\n            preserve_index=preserve_index,\r\n        )\r\n        if features is not None:\r\n            # more expensive cast than InMemoryTable.from_pandas(..., schema=features.arrow_schema)\r\n            # needed to support the str to Audio conversion for instance\r\n            table = table.cast(features.arrow_schema)\r\n        return cls(table, info=info, split=split)\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset\r\n# Filling the split parameter with whatever causes no harm at all\r\ndata = Dataset.from_pandas(self.raw_data, split='egiojegoierjgoiejgrefiergiuorenvuirgurthgi')\r\n```\n\n### Expected behavior\n\nWould be great if there is no split parameter (if it isn't working), or to add a concrete example of how it can be used.\n\n### Environment info\n\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.15.0-127-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.27.1\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nA parameter is specified but not used in datasets.arrow_dataset.Dataset.from_pandas()<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen a dataset saved with a specified inner sequence length is loaded without specifying that length, the original data is altered and becomes inconsistent.\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset, Features, Value, Sequence, load_dataset\r\n\r\n# Repository ID\r\nrepo_id = \"my_repo_id\"\r\n\r\n# Define features with a specific length of 3 for each inner sequence\r\nspecified_features = Features({\"key\": Sequence(Sequence(Value(\"float32\"), length=3))})\r\n\r\n# Create a dataset with the specified features\r\ndata = [\r\n    [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],\r\n    [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]],\r\n]\r\ndataset = Dataset.from_dict({\"key\": data}, features=specified_features)\r\n\r\n# Push the dataset to the hub\r\ndataset.push_to_hub(repo_id)\r\n\r\n# Define features without specifying the length\r\nunspecified_features = Features({\"key\": Sequence(Sequence(Value(\"float32\")))})\r\n\r\n# Load the dataset from the hub with this new feature definition\r\ndataset = load_dataset(f\"qgallouedec/{repo_id}\", split=\"train\", features=unspecified_features)\r\n\r\n# The obtained data is altered\r\nprint(dataset.to_dict())  # {'key': [[[1.0], [2.0]], [[3.0], [4.0]]]}\r\n```\r\n\n\n### Expected behavior\n\n```python\r\nprint(dataset.to_dict())  # {'key': [[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]], [[7.0, 8.0, 9.0], [10.0, 11.0, 12.0]]]}\r\n```\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-6.2.0-32-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nData alteration when loading dataset with unspecified inner sequence length<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHow to do np.fromfile  to use it like np.load\r\n\r\n\r\n```python\r\ndef xnumpy_fromfile(filepath_or_buffer, *args, download_config: Optional[DownloadConfig] = None, **kwargs):\r\n    import numpy as np\r\n\r\n    if hasattr(filepath_or_buffer, \"read\"):\r\n        return np.fromfile(filepath_or_buffer, *args, **kwargs)\r\n    else:\r\n        filepath_or_buffer = str(filepath_or_buffer)\r\n        return np.fromfile(xopen(filepath_or_buffer, \"rb\", download_config=download_config).read(), *args, **kwargs)\r\n```\r\nthis is not work\r\n<end_of_turn>\n<start_of_turn>model\nnp.fromfile not supported<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen streaming a [large ASR dataset](https://huggingface.co/datasets/sanchit-gandhi/concatenated-train-set) from the Hug (~3TB) I often encounter 502 Server Errors seemingly randomly during streaming:\r\n```\r\nhuggingface_hub.utils._errors.HfHubHTTPError: 502 Server Error: Bad Gateway for url: https://huggingface.co/datasets/sanchit-gandhi/concatenated-train-set/resolve/7d2acc5c59de848e456e951a76e805304d6fb350/train/train-00288-of-07135.parquet \r\n```\r\n\r\nThis is despite the parquet file definitely existing on the Hub: https://huggingface.co/datasets/sanchit-gandhi/concatenated-train-set/blob/main/train/train-00228-of-07135.parquet\r\nAnd having the correct commit id: [7d2acc5c59de848e456e951a76e805304d6fb350](https://huggingface.co/datasets/sanchit-gandhi/concatenated-train-set/commits/main/train)\r\n\r\nI\u2019m wondering whether this is coming from datasets? Or from the Hub side? \r\n\r\n### Steps to reproduce the bug\r\n\r\nReproducer:\r\n```python\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\nfrom tqdm import tqdm\r\n\r\nNUM_EPOCHS = 20\r\n\r\ndataset = load_dataset(\"sanchit-gandhi/concatenated-train-set\", \"train\", streaming=True)\r\ndataset = dataset.with_format(\"torch\")\r\ndataloader = DataLoader(dataset[\"train\"], batch_size=256, drop_last=True, pin_memory=True, num_workers=16)\r\n\r\nfor epoch in tqdm(range(NUM_EPOCHS), desc=\"Epoch\", position=0):\r\n    for batch in tqdm(dataloader, desc=\"Batch\", position=1):\r\n        continue\r\n```\r\n\r\nRunning the above script tends to fail within about 2 hours with a traceback like the following:\r\n\r\n<details>\r\n\r\n<summary> Traceback: </summary>\r\n\r\n```python\r\n1029     for batch in train_loader: \r\n1030   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__ \r\n1031     data = self._next_data() \r\n1032   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1325, in _next_data \r\n1033     return self._process_data(data) \r\n1034   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1371, in _process_data \r\n1035     data.reraise() \r\n1036   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/_utils.py\", line 694, in reraise \r\n1037     raise exception \r\n1038 huggingface_hub.utils._errors.HfHubHTTPError: Caught HfHubHTTPError in DataLoader worker process 10. \r\n1039 Original Traceback (most recent call last): \r\n1040   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py\", line 286, in hf_raise_for_status \r\n1041     response.raise_for_status() \r\n1042   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/requests/models.py\", line 1021, in raise_for_status \r\n1043     raise HTTPError(http_error_msg, response=self) \r\n1044 requests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://huggingface.co/datasets/sanchit-gandhi/concatenated-train-set/resolve/7d2acc5c59de848e456e951a76e805304d6fb350/train/train-00288-of-07135.parquet \r\n1045 The above exception was the direct cause of the following exception: \r\n1046 Traceback (most recent call last): \r\n1047   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop \r\n1048     data = fetcher.fetch(index) \r\n1049   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 32, in fetch \r\n1050     data.append(next(self.dataset_iter)) \r\n1051   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 1363, in __iter__ \r\n1052     yield from self._iter_pytorch() \r\n1053   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 1298, in _iter_pytorch \r\n1054     for key, example in ex_iterable: \r\n1055   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 983, in __iter__ \r\n1056     for x in self.ex_iterable: \r\n1057   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 863, in __iter__ \r\n1058     yield from self._iter() \r\n1059   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 900, in _iter \r\n1060     for key, example in iterator: \r\n1061   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 679, in __iter__ \r\n1062     yield from self._iter() \r\n1063   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 741, in _iter \r\n1064     for key, example in iterator: \r\n1065   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 863, in __iter__ \r\n1066     yield from self._iter() \r\n1067   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 900, in _iter \r\n1068     for key, example in iterator: \r\n1069   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 1115, in __iter__ \r\n1070     for key, example in self.ex_iterable: \r\n1071   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 679, in __iter__ \r\n1072     yield from self._iter() \r\n1073   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 741, in _iter \r\n1074     for key, example in iterator: \r\n1075   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 1115, in __iter__ \r\n1076     for key, example in self.ex_iterable: \r\n1077   File \"/home/sanchitgandhi/datasets/src/datasets/iterable_dataset.py\", line 282, in __iter__ \r\n1078     for key, pa_table in self.generate_tables_fn(**self.kwargs): \r\n1079   File \"/home/sanchitgandhi/datasets/src/datasets/packaged_modules/parquet/parquet.py\", line 87, in _generate_tables \r\n1080     for batch_idx, record_batch in enumerate( \r\n1081   File \"pyarrow/_parquet.pyx\", line 1367, in iter_batches \r\n1082   File \"pyarrow/types.pxi\", line 88, in pyarrow.lib._datatype_to_pep3118 \r\n1083   File \"/home/sanchitgandhi/datasets/src/datasets/download/streaming_download_manager.py\", line 341, in read_with_retries \r\n1084     out = read(*args, **kwargs) \r\n1085   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/fsspec/spec.py\", line 1856, in read \r\n1086     out = self.cache._fetch(self.loc, self.loc + length) \r\n1087   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/fsspec/caching.py\", line 189, in _fetch \r\n1088     self.cache = self.fetcher(start, end)  # new block replaces old \r\n1089   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/huggingface_hub/hf_file_system.py\", line 626, in _fetch_range \r\n1090     hf_raise_for_status(r) \r\n1091   File \"/home/sanchitgandhi/hf/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py\", line 333, in hf_raise_for_status \r\n1092     raise HfHubHTTPError(str(e), response=response) from e \r\n1093 huggingface_hub.utils._errors.HfHubHTTPError: 502 Server Error: Bad Gateway for url: https://huggingface.co/datasets/sanchit-gandhi/concatenated-train-set/resolve/7d2acc5c59de848e456e951a76e805304d6fb350/train/train-00288-of-07135.parquet\r\n```\r\n\r\n</details>\r\n\r\n### Expected behavior\r\n\r\nShould be able to stream the dataset without any 502 error.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.2.dev0\r\n- Platform: Linux-5.13.0-1023-gcp-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- `huggingface_hub` version: 0.20.1\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\n502 Server Errors when streaming large dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.3.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nhelpful for the next PR to enable parallel image/audio/video decoding and make multimodal datasets go brr (e.g. for lerobot)\r\n\r\n- fix with_indices\r\n- fix resuming with save_state_dict() / load_state_dict() - omg that wasn't easy\r\n- remove unnecessary decoding in map() to enable parallelism in FormattedExampleIterable later\r\n\r\nsmall bonus: keeping features in batch()<end_of_turn>\n<start_of_turn>model\nFix small bugs with async map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAs outlined [here](https://discuss.huggingface.co/t/length-error-using-map-with-datasets/40969/3?u=sachin), the following collate function drops 5 out of possible 6 elements in the batch (it is 6 because out of the eight, two are bad links in laion). A reproducible [kaggle kernel ](https://www.kaggle.com/sachin/laion-hf-dataset/edit) can be found here.\r\n\r\nThe weirdest part is when inspecting the sizes of the tensors as shown below, both `tokenized_captions[\"input_ids\"]` and `image_features` show the correct shapes. Simply the output only has one element (with the batch dimension squeezed out).\r\n\r\n```python\r\nclass CollateFn:        \r\n    def get_image(self, url):\r\n        try:\r\n            response = requests.get(url)\r\n            return Image.open(io.BytesIO(response.content)).convert(\"RGB\")\r\n        except PIL.UnidentifiedImageError:\r\n            logger.info(f\"Reading error: Could not transform f{url}\")\r\n            return None\r\n        except requests.exceptions.ConnectionError:\r\n            logger.info(f\"Connection error: Could not transform f{url}\")\r\n            return None\r\n        \r\n        \r\n    def __call__(self, batch):\r\n        images = [self.get_image(url) for url in batch[\"url\"]]\r\n        captions = [caption for caption, image in zip(batch[\"caption\"], images) if image is not None]\r\n        images = [image for image in images if image is not None]\r\n        \r\n        tokenized_captions = tokenizer(\r\n            captions,\r\n            padding=\"max_length\",\r\n            truncation=True,\r\n            max_length=tokenizer.model_max_length,\r\n            return_tensors=\"pt\",\r\n        )\r\n        \r\n        image_features = torch.stack([torch.Tensor(feature_extractor(image)[\"pixel_values\"][0]) for image in images])\r\n#         import pdb; pdb.set_trace()\r\n        return {\"input_ids\": tokenized_captions[\"input_ids\"], \"images\": image_features}\r\n\r\ncollate_fn = CollateFn()\r\nlaion_ds = datasets.load_dataset(\"laion/laion400m\", split=\"train\", streaming=True)\r\nlaion_ds_batched = laion_ds.map(collate_fn, batched=True, batch_size=8, remove_columns=next(iter(laion_ds)).keys())\r\n```\n\n### Steps to reproduce the bug\n\nA reproducible [kaggle kernel ](https://www.kaggle.com/sachin/laion-hf-dataset/edit) can be found here.\n\n### Expected behavior\n\nWould expect `next(iter(laion_ds_batched))` to produce two tensors of shape `(batch_size, 77)` and `batch_size, image_shape`.\n\n### Environment info\n\ndatasets==2.12.0\r\npython==3.10<end_of_turn>\n<start_of_turn>model\nMissing elements in `map` a batched dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ne.g. a field named `url.txt` should be a treated as text\r\n\r\nI also included a small fix to support .npz correctly<end_of_turn>\n<start_of_turn>model\nFix decoding multi part extension<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nrequests.get(..., streaming=True) is faster than using HTTP range requests when streaming large TAR files\r\n\r\nit can be enabled using block_size=0 in fsspec\r\n\r\ncc @rwightman <end_of_turn>\n<start_of_turn>model\nFaster webdataset streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfollowing https://github.com/huggingface/datasets/pull/5893<end_of_turn>\n<start_of_turn>model\nOptimize IterableDataset.from_file using ArrowExamplesIterable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix wrong type hints in data_files introduced in:\r\n- #6493<end_of_turn>\n<start_of_turn>model\nFix wrong type hints in data_files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm encountering a \"FileNotFoundError\" while attempting to use the \"paws-x\" dataset to retrain the DistilRoBERTa-base model. The error message is as follows: \r\n\r\nFileNotFoundError: Couldn't find a module script at /content/paws-x/paws-x.py. Module 'paws-x' doesn't exist on the Hugging Face Hub either.\n\n### Steps to reproduce the bug\n\nhttps://colab.research.google.com/drive/11xUUFxloClpmqLvDy_Xxfmo3oUzjY5nx#scrollTo=kUn74FigzhHm\n\n### Expected behavior\n\nThe the trained model\n\n### Environment info\n\ncolab, \"paws-x\" dataset , DistilRoBERTa-base model<end_of_turn>\n<start_of_turn>model\nFileNotFoundError: Couldn't find a module script at /content/paws-x/paws-x.py. Module 'paws-x' doesn't exist on the Hugging Face Hub either.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI have an image dataset with 300k images, the size of image is 768 * 768.\r\n\r\nWhen I run `dataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/img_dir\", split='train')` in second time, it takes 50 minutes to finish \"Resolving data files\" part, what's going on in this part?\r\n\r\nFrom my understand, after Arrow files been created in the first run, the second run should not take time longer than one or two minutes.\r\n\r\n### Steps to reproduce the bug\r\n\r\n# Run following code two times\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/img_dir\", split='train')\r\n\r\n### Expected behavior\r\n\r\nFast dataset building\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.0-60-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nStuck in \"Resolving data files...\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #6791 \r\n\r\nSmall PR to raise a better error when a dataset is not embedded properly.<end_of_turn>\n<start_of_turn>model\n#6791 Improve type checking around FAISS<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI get a FileNotFoundError:\r\n<img width=\"944\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1336bc08-06f6-4682-a3c0-071ff65efa87\">\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nSee screenshot.\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nI want to load one audiofile from the dataset. \r\n\r\n### Environment info\r\n\r\nMacOs Intel 14.6.1 (23G93)\r\nPython 3.10.9\r\nNumpy 1.23\r\nDatasets latest version<end_of_turn>\n<start_of_turn>model\nFile not found error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCSV builder raises a deprecation warning on `verbose` parameter:\r\n```\r\nFutureWarning: The 'verbose' keyword in pd.read_csv is deprecated and will be removed in a future version.\r\n```\r\n\r\nSee:\r\n- https://github.com/pandas-dev/pandas/pull/56556\r\n- https://github.com/pandas-dev/pandas/pull/57450<end_of_turn>\n<start_of_turn>model\nCSV builder raises deprecation warning on verbose parameter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\npolars is also quite popular now, thus this tiny update can tell users polars is supported<end_of_turn>\n<start_of_turn>model\n(Super tiny doc update) Mention to_polars<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHello,\n\nI have a use case where my team is sharing models and dataset in shared directory to avoid duplication.\nI noticed that the [cache documentation for datasets](https://huggingface.co/docs/datasets/main/en/cache) only mention the `HF_HOME` environment variable but never the `HF_DATASETS_CACHE`.\n\nIt should be nice to add `HF_DATASETS_CACHE` to datasets documentation if it's an intended feature.\nIf it's not, I think a depreciation warning would be appreciated.\n\n### Motivation\n\nThis variable is fully working and similar to what `HF_HUB_CACHE` does for models, so it's nice to know that this exists. This seems to be a quick change to implement.\n\n### Your contribution\n\nI could contribute since this is only affecting a small portion of the documentation<end_of_turn>\n<start_of_turn>model\nDocument the HF_DATASETS_CACHE env variable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a small typo in an error message in `src/datasets/fingerprint.py`:\r\n\r\nhttps://github.com/huggingface/datasets/blob/910fab20606893f69b4fccac5fcc883dddf5a14d/src/datasets/fingerprint.py#L63\r\n\r\n```diff\r\n- occured\r\n+ occurred\r\n```<end_of_turn>\n<start_of_turn>model\nFix typo in error message for cache directory deletion<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport Python 3.11: https://peps.python.org/pep-0664/<end_of_turn>\n<start_of_turn>model\nSupport Python 3.11<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport features in metadata configs, like:\r\n```\r\nconfigs:\r\n  - config_name: default\r\n    features:\r\n      - name: id\r\n        dtype: int64\r\n      - name:  name\r\n        dtype: string\r\n      - name: score\r\n        dtype: float64\r\n```\r\n\r\nThis will allow to avoid inference of data types.\r\n\r\nCurrently, we allow passing this information in the `dataset_info` (instead of `configs`) field, but this is not intuitive and it is not properly documented.\r\n\r\nTODO:\r\n- [ ] Document usage<end_of_turn>\n<start_of_turn>model\nSupport features in metadata configs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nthe load_dataset() for local functions support file types like csv, json etc but not of type tsv (tab separated values).\n\n### Motivation\n\ncant easily load files of type tsv, have to convert them to another type like csv then load\n\n### Your contribution\n\nCan try by raising a PR with a little help, currently went through the code but didn't fully understand<end_of_turn>\n<start_of_turn>model\nload_dataset() does not support tsv<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nwhen I use load_dataset methods to load mozilla-foundation/common_voice_7_0, it can successfully download and extracted the dataset but It cannot generating the arrow document,\r\nThis bug happened in my server, my laptop, so as #6906 , but it won't happen in the google colab. I work for it for days, even I load the datasets from local path, it can Generating\u2007train\u2007split and validation split but bug happen again in test\u2007split.\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nfrom datasets import load_dataset, load_metric, Audio\r\n\r\ncommon_voice_train = load_dataset(\"mozilla-foundation/common_voice_7_0\", \"ja\", split=\"train\", token=selftoken, trust_remote_code=True)\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\n```\r\n{\r\n\t\"name\": \"ValueError\",\r\n\t\"message\": \"Instruction \\\"train\\\" corresponds to no data!\",\r\n\t\"stack\": \"---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[2], line 3\r\n      1 from datasets import load_dataset, load_metric, Audio\r\n----> 3 common_voice_train = load_dataset(\\\"mozilla-foundation/common_voice_7_0\\\", \\\"ja\\\", split=\\\"train\\\",token='hf_REDACTED')#,trust_remote_code=True)#,streaming=True)\r\n      4 common_voice_test = load_dataset(\\\"mozilla-foundation/common_voice_7_0\\\", \\\"ja\\\", split=\\\"test\\\",token='hf_REDACTED')#,trust_remote_code=True)#,streaming=True)\r\n\r\nFile c:\\\\Users\\\\cybes\\\\.conda\\\\envs\\\\ECoG\\\\lib\\\\site-packages\\\\datasets\\\\load.py:2626, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2622 # Build dataset for splits\r\n   2623 keep_in_memory = (\r\n   2624     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2625 )\r\n-> 2626 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n   2627 # Rename and cast features to match task schema\r\n   2628 if task is not None:\r\n   2629     # To avoid issuing the same warning twice\r\n\r\nFile c:\\\\Users\\\\cybes\\\\.conda\\\\envs\\\\ECoG\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:1266, in DatasetBuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\r\n   1263 verification_mode = VerificationMode(verification_mode or VerificationMode.BASIC_CHECKS)\r\n   1265 # Create a dataset for each of the given splits\r\n-> 1266 datasets = map_nested(\r\n   1267     partial(\r\n   1268         self._build_single_dataset,\r\n   1269         run_post_process=run_post_process,\r\n   1270         verification_mode=verification_mode,\r\n   1271         in_memory=in_memory,\r\n   1272     ),\r\n   1273     split,\r\n   1274     map_tuple=True,\r\n   1275     disable_tqdm=True,\r\n   1276 )\r\n   1277 if isinstance(datasets, dict):\r\n   1278     datasets = DatasetDict(datasets)\r\n\r\nFile c:\\\\Users\\\\cybes\\\\.conda\\\\envs\\\\ECoG\\\\lib\\\\site-packages\\\\datasets\\\\utils\\\\py_utils.py:484, in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\r\n    482 if batched:\r\n    483     data_struct = [data_struct]\r\n--> 484 mapped = function(data_struct)\r\n    485 if batched:\r\n    486     mapped = mapped[0]\r\n\r\nFile c:\\\\Users\\\\cybes\\\\.conda\\\\envs\\\\ECoG\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:1296, in DatasetBuilder._build_single_dataset(self, split, run_post_process, verification_mode, in_memory)\r\n   1293     split = Split(split)\r\n   1295 # Build base dataset\r\n-> 1296 ds = self._as_dataset(\r\n   1297     split=split,\r\n   1298     in_memory=in_memory,\r\n   1299 )\r\n   1300 if run_post_process:\r\n   1301     for resource_file_name in self._post_processing_resources(split).values():\r\n\r\nFile c:\\\\Users\\\\cybes\\\\.conda\\\\envs\\\\ECoG\\\\lib\\\\site-packages\\\\datasets\\\\builder.py:1370, in DatasetBuilder._as_dataset(self, split, in_memory)\r\n   1368 if self._check_legacy_cache():\r\n   1369     dataset_name = self.name\r\n-> 1370 dataset_kwargs = ArrowReader(cache_dir, self.info).read(\r\n   1371     name=dataset_name,\r\n   1372     instructions=split,\r\n   1373     split_infos=self.info.splits.values(),\r\n   1374     in_memory=in_memory,\r\n   1375 )\r\n   1376 fingerprint = self._get_dataset_fingerprint(split)\r\n   1377 return Dataset(fingerprint=fingerprint, **dataset_kwargs)\r\n\r\nFile c:\\\\Users\\\\cybes\\\\.conda\\\\envs\\\\ECoG\\\\lib\\\\site-packages\\\\datasets\\\\arrow_reader.py:256, in BaseReader.read(self, name, instructions, split_infos, in_memory)\r\n    254     msg = f'Instruction \\\"{instructions}\\\" corresponds to no data!'\r\n    255     #msg = f'Instruction \\\"{self._path}\\\",\\\"{name}\\\",\\\"{instructions}\\\",\\\"{split_infos}\\\" corresponds to no data!'\r\n--> 256     raise ValueError(msg)\r\n    257 return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n\r\nValueError: Instruction \\\"train\\\" corresponds to no data!\"\r\n}\r\n```\r\n\r\n### Environment info\r\n\r\nEnvironment:\r\npython 3.9\r\nwindows 11 pro\r\nVScode+jupyter<end_of_turn>\n<start_of_turn>model\ncannot split dataset when using load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, \r\n\r\nDecord is a dev dependency not maintained since couple years. \r\nIt does not have an ARM package available rendering it uninstallable on non-intel based macs \r\n\r\nSuggestion is to move to eva-decord (https://github.com/georgia-tech-db/eva-decord)  which doesnt have this problem. \r\n\r\nHappy to raise a PR\r\n\n\n### Steps to reproduce the bug\n\nSource installation as mentioned in contributinog.md\n\n### Expected behavior\n\nInstallation without decord failing to be installed.\n\n### Environment info\n\npython=3.10, M3 Mac <end_of_turn>\n<start_of_turn>model\nSource installation fails on Macintosh with python 3.10<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello everyone,\n\ni am trying to load `mozilla-foundation--common_voice_11_0` and it fails. Reproducer \n\n```\nimport datasets\ndatasets.load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True, trust_remote_code=True)\n```\nand it fails with\n```\nFile ~/opt/envs/.../lib/python3.10/site-packages/datasets/utils/file_utils.py:827, in _add_retries_to_file_obj_read_method.<locals>.read_with_retries(*args, **kwargs)\n    825 for retry in range(1, max_retries + 1):\n    826     try:\n--> 827         out = read(*args, **kwargs)\n    828         break\n    829     except (\n    830         _AiohttpClientError,\n    831         asyncio.TimeoutError,\n    832         requests.exceptions.ConnectionError,\n    833         requests.exceptions.Timeout,\n    834     ) as err:\n\nFile /usr/lib/python3.10/codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\n    319 def decode(self, input, final=False):\n    320     # decode input (taking the buffer into account)\n    321     data = self.buffer + input\n--> 322     (result, consumed) = self._buffer_decode(data, self.errors, final)\n    323     # keep undecoded input until the next call\n    324     self.buffer = data[consumed:]\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\n```\nWhen i remove streaming then everything is good but i need `streaming=True`\n\n### Steps to reproduce the bug\n\n```\nimport datasets\ndatasets.load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True, trust_remote_code=True)\n```\n\n### Expected behavior\n\nExpected that it will download dataset\n\n### Environment info\n\ndatasets==3.6.0 \npython3.10\non all platforms linux/win/mac<end_of_turn>\n<start_of_turn>model\nloading mozilla-foundation--common_voice_11_0 fails<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds `WebDatasetConfig`.\r\n\r\nCloses #7055 <end_of_turn>\n<start_of_turn>model\nWebDataset BuilderConfig<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTemporarily pin numpy<2.1 to fix CI.\r\n\r\nFix #7111.<end_of_turn>\n<start_of_turn>model\nTemporarily pin numpy<2.1 to fix CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAt `save_to_disk` step, the `max_shard_size` by default is `500MB`. However, one row of the dataset might be larger than `500MB` then the saving will throw an index error. Without looking at the source code, the bug is due to wrong calculation of number of shards which i think is \r\n`total_size / min(max_shard_size, row_size)` which should be `total_size / max(max_shard_size, row_size)`\r\n\r\nThe fix is setting a larger `max_shard_size`\n\n### Steps to reproduce the bug\n\n1. create a dataset with large dense tensors per row\r\n2. set a small `max_shard_size` say 1MB\r\n3. `save_to_disk`\n\n### Expected behavior\n\n```\r\nraise IndexError(f\"Index {index} out of range for dataset of size {size}.\")\r\n\r\nIndexError: Index 10 out of range for dataset of size 10.\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.0\r\n- Platform: Linux-5.10.201-168.748.amzn2int.x86_64-x86_64-with-glibc2.26\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\nIndex error when data is large<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nlast section ``to_pandas()\"<end_of_turn>\n<start_of_turn>model\nUpdate use_with_pandas.mdx: to_pandas() correction in last section<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe dataset `rewardsignal/reddit_writing_prompts` is missing in Huggingface Hub.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nprompt_response_dataset = load_dataset(\"rewardsignal/reddit_writing_prompts\", data_files=\"prompt_responses_full.csv\", split='train[:80%]')\r\n```\r\n\r\n### Expected behavior\r\n\r\nDatasetNotFoundError: Dataset 'rewardsignal/reddit_writing_prompts' doesn't exist on the Hub or cannot be accessed\r\n\r\n### Environment info\r\n\r\nNothing to do with versions<end_of_turn>\n<start_of_turn>model\nDataset 'rewardsignal/reddit_writing_prompts' doesn't exist on the Hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nto_csv, to_json, to_sql, to_pandas, to_polars, to_dict, to_list<end_of_turn>\n<start_of_turn>model\nDocs and more methods for IterableDataset: push_to_hub, to_parquet...<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nThere should be a way to specify the datatype of a column in `datasets.add_column()`. \n\n### Motivation\n\nTo specify a custom datatype, we have to use `datasets.add_column()` followed by `datasets.cast_column()` which is slow for large datasets. Another workaround is to pass a `numpy.array()` of desired type to the `datasets.add_column()` function.\r\n\r\nIMO this functionality should be natively supported.\r\n\r\nhttps://discuss.huggingface.co/t/add-column-with-a-particular-type-in-datasets/95674\r\n\r\n\n\n### Your contribution\n\nI can submit a PR for this.<end_of_turn>\n<start_of_turn>model\nSpecifying datatype when adding a column to a dataset.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nColumn returned after mapping is not found in new instance of the dataset.\n\n### Steps to reproduce the bug\n\nCode for reproduction. After running get_total_audio_length, it is errored out due to `data` not having `duration`\n```\ndef compute_duration(x):\n    return {\"duration\": len(x[\"audio\"][\"array\"]) / x[\"audio\"][\"sampling_rate\"]}\n\n\ndef get_total_audio_length(dataset):\n    data = dataset.map(compute_duration, num_proc=NUM_PROC)\n    print(data)\n    durations=data[\"duration\"]\n    total_seconds = sum(durations)\n    return total_seconds\n```\n\n### Expected behavior\n\nNew datasets.Dataset instance should have new columns attached.\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: Linux-5.4.0-124-generic-x86_64-with-glibc2.31\n- Python version: 3.10.13\n- `huggingface_hub` version: 0.33.2\n- PyArrow version: 20.0.0\n- Pandas version: 2.3.0\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\nDatasets 4.0 map function causing column not found<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 4.3.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR aligns the `tqdm` logic with `huggingface_hub` (without introducing breaking changes), as the current one is error-prone.\r\n\r\nAdditionally, it improves the doc page about the `datasets`' utilities, and the handling of local `fsspec` paths in `cached_path`.\r\n \r\nFix #6409 <end_of_turn>\n<start_of_turn>model\nBetter `tqdm` wrapper<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPass `detail=False` to the `fsspec` `listdir`  to avoid unnecessarily fetching expensive metadata about the paths.<end_of_turn>\n<start_of_turn>model\nFaster `xlistdir`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n# TL:DR\n\n`Dataset.from_generator` function passes all of its arguments to `BuilderConfig.create_config_id`, including `generator` function itself. `BuilderConfig.create_config_id` function tries to hash all the args, which can take a large amount of time or even cause MemoryError if the dataset processed in a generator function is large enough.\n\nMaybe we should pop `generator` from `config_kwargs_to_add_to_suffix` before hashing to avoid it.\n\n# Full description\n\nI have a pretty large spatial imagery dataset that is generated from two xbatcher.BatchGenerators via custom `dataset_generator` function that looks like this if simplified:\n```\ndef dataset_generator():\n    for index in samples:\n        data_dict = {\n            \"key\": index,\n            \"x\": x_batches[index].data,\n            \"y\":  y_batches[index].data,\n        }\n        yield data_dict\n```\nThen I use `datasets.Dataset.from_generator` to generate the dataset itself.\n```\n# Create dataset\nds = datasets.Dataset.from_generator(\n    dataset_generator,\n    features=feat,\n    cache_dir=(output / \".cache\"),\n)\n```\n\nIt works nicely with pretty small data, but if the dataset is huge and barely fits in memory, it crashes with memory error:\n<details>\n<summary>Full stack trace</summary>\n\n```\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\remote_sensing_processor\\segmentation\\semantic\\tiles.py:248](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/remote_sensing_processor/segmentation/semantic/tiles.py#line=247), in generate_tiles(x, y, output, tile_size, shuffle, split, x_dtype, y_dtype, x_nodata, y_nodata)\n    245         yield data_dict\n    247 # Create dataset\n--> 248 ds = datasets.Dataset.from_generator(\n    249     dataset_generator,\n    250     features=feat,\n    251     cache_dir=(output / \".cache\"),\n    252 )\n    254 # Save dataset\n    255 ds.save_to_disk(output / name)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\arrow_dataset.py:1105](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/arrow_dataset.py#line=1104), in Dataset.from_generator(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, split, **kwargs)\n   1052 \"\"\"Create a Dataset from a generator.\n   1053 \n   1054 Args:\n   (...)   1101 ```\n   1102 \"\"\"\n   1103 from .io.generator import GeneratorDatasetInputStream\n-> 1105 return GeneratorDatasetInputStream(\n   1106     generator=generator,\n   1107     features=features,\n   1108     cache_dir=cache_dir,\n   1109     keep_in_memory=keep_in_memory,\n   1110     gen_kwargs=gen_kwargs,\n   1111     num_proc=num_proc,\n   1112     split=split,\n   1113     **kwargs,\n   1114 ).read()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\io\\generator.py:29](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/io/generator.py#line=28), in GeneratorDatasetInputStream.__init__(self, generator, features, cache_dir, keep_in_memory, streaming, gen_kwargs, num_proc, split, **kwargs)\n      9 def __init__(\n     10     self,\n     11     generator: Callable,\n   (...)     19     **kwargs,\n     20 ):\n     21     super().__init__(\n     22         features=features,\n     23         cache_dir=cache_dir,\n   (...)     27         **kwargs,\n     28     )\n---> 29     self.builder = Generator(\n     30         cache_dir=cache_dir,\n     31         features=features,\n     32         generator=generator,\n     33         gen_kwargs=gen_kwargs,\n     34         split=split,\n     35         **kwargs,\n     36     )\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\builder.py:343](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/builder.py#line=342), in DatasetBuilder.__init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\n    341     config_kwargs[\"data_dir\"] = data_dir\n    342 self.config_kwargs = config_kwargs\n--> 343 self.config, self.config_id = self._create_builder_config(\n    344     config_name=config_name,\n    345     custom_features=features,\n    346     **config_kwargs,\n    347 )\n    349 # prepare info: DatasetInfo are a standardized dataclass across all datasets\n    350 # Prefill datasetinfo\n    351 if info is None:\n    352     # TODO FOR PACKAGED MODULES IT IMPORTS DATA FROM src/packaged_modules which doesn't make sense\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\builder.py:604](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/builder.py#line=603), in DatasetBuilder._create_builder_config(self, config_name, custom_features, **config_kwargs)\n    598 builder_config._resolve_data_files(\n    599     base_path=self.base_path,\n    600     download_config=DownloadConfig(token=self.token, storage_options=self.storage_options),\n    601 )\n    603 # compute the config id that is going to be used for caching\n--> 604 config_id = builder_config.create_config_id(\n    605     config_kwargs,\n    606     custom_features=custom_features,\n    607 )\n    608 is_custom = (config_id not in self.builder_configs) and config_id != \"default\"\n    609 if is_custom:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\builder.py:187](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/builder.py#line=186), in BuilderConfig.create_config_id(self, config_kwargs, custom_features)\n    185             suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\n    186     else:\n--> 187         suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\n    189 if custom_features is not None:\n    190     m = Hasher()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\fingerprint.py:188](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/fingerprint.py#line=187), in Hasher.hash(cls, value)\n    186 @classmethod\n    187 def hash(cls, value: Any) -> str:\n--> 188     return cls.hash_bytes(dumps(value))\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:109](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=108), in dumps(obj)\n    107 \"\"\"Pickle an object to a string.\"\"\"\n    108 file = BytesIO()\n--> 109 dump(obj, file)\n    110 return file.getvalue()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:103](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=102), in dump(obj, file)\n    101 def dump(obj, file):\n    102     \"\"\"Pickle an object to a file.\"\"\"\n--> 103     Pickler(file, recurse=True).dump(obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:420](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=419), in Pickler.dump(self, obj)\n    418 def dump(self, obj): #NOTE: if settings change, need to update attributes\n    419     logger.trace_setup(self)\n--> 420     StockPickler.dump(self, obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:484](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=483), in _Pickler.dump(self, obj)\n    482 if self.proto >= 4:\n    483     self.framer.start_framing()\n--> 484 self.save(obj)\n    485 self.write(STOP)\n    486 self.framer.end_framing()\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1985](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1984), in save_function(pickler, obj)\n   1982 if state_dict:\n   1983     state = state, state_dict\n-> 1985 _save_with_postproc(pickler, (_create_function, (\n   1986         obj.__code__, globs, obj.__name__, obj.__defaults__,\n   1987         closure\n   1988 ), state), obj=obj, postproc_list=postproc_list)\n   1990 # Lift closure cell update to earliest function (#458)\n   1991 if _postproc:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1117](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1116), in _save_with_postproc(pickler, reduction, is_pickler_dill, obj, postproc_list)\n   1115         continue\n   1116 else:\n-> 1117     pickler.save_reduce(*reduction)\n   1118 # pop None created by calling preprocessing step off stack\n   1119 pickler.write(POP)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:690](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=689), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    688 else:\n    689     save(func)\n--> 690     save(args)\n    691     write(REDUCE)\n    693 if obj is not None:\n    694     # If the object is already in the memo, this means it is\n    695     # recursive. In this case, throw away everything we put on the\n    696     # stack, and fetch the object back from the memo.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:905](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=904), in _Pickler.save_tuple(self, obj)\n    903 if n <= 3 and self.proto >= 2:\n    904     for element in obj:\n--> 905         save(element)\n    906     # Subtle.  Same as in the big comment below.\n    907     if id(obj) in memo:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\n    [... skipping similar frames: Pickler.save at line 70 (1 times), Pickler.save at line 414 (1 times)]\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:905](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=904), in _Pickler.save_tuple(self, obj)\n    903 if n <= 3 and self.proto >= 2:\n    904     for element in obj:\n--> 905         save(element)\n    906     # Subtle.  Same as in the big comment below.\n    907     if id(obj) in memo:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:905](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=904), in _Pickler.save_tuple(self, obj)\n    903 if n <= 3 and self.proto >= 2:\n    904     for element in obj:\n--> 905         save(element)\n    906     # Subtle.  Same as in the big comment below.\n    907     if id(obj) in memo:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:690](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=689), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    688 else:\n    689     save(func)\n--> 690     save(args)\n    691     write(REDUCE)\n    693 if obj is not None:\n    694     # If the object is already in the memo, this means it is\n    695     # recursive. In this case, throw away everything we put on the\n    696     # stack, and fetch the object back from the memo.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:920](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=919), in _Pickler.save_tuple(self, obj)\n    918 write(MARK)\n    919 for element in obj:\n--> 920     save(element)\n    922 if id(obj) in memo:\n    923     # Subtle.  d was not in memo when we entered save_tuple(), so\n    924     # the process of saving the tuple's elements must have saved\n   (...)    928     # could have been done in the \"for element\" loop instead, but\n    929     # recursive tuples are a rare thing.\n    930     get = self.get(memo[id(obj)][0])\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1019](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1018), in _Pickler._batch_setitems(self, items)\n   1017     k, v = tmp[0]\n   1018     save(k)\n-> 1019     save(v)\n   1020     write(SETITEM)\n   1021 # else tmp is empty, and we're done\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\n    [... skipping similar frames: Pickler.save at line 70 (1 times), Pickler.save at line 414 (1 times)]\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:1217](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=1216), in save_module_dict(pickler, obj)\n   1214     if is_dill(pickler, child=False) and pickler._session:\n   1215         # we only care about session the first pass thru\n   1216         pickler._first_pass = False\n-> 1217     StockPickler.save_dict(pickler, obj)\n   1218     logger.trace(pickler, \"# D2\")\n   1219 return\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:990](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=989), in _Pickler.save_dict(self, obj)\n    987     self.write(MARK + DICT)\n    989 self.memoize(obj)\n--> 990 self._batch_setitems(obj.items())\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:83](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=82), in Pickler._batch_setitems(self, items)\n     80     from datasets.fingerprint import Hasher\n     82     items = sorted(items, key=lambda x: Hasher.hash(x[0]))\n---> 83 dill.Pickler._batch_setitems(self, items)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:1014](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=1013), in _Pickler._batch_setitems(self, items)\n   1012     for k, v in tmp:\n   1013         save(k)\n-> 1014         save(v)\n   1015     write(SETITEMS)\n   1016 elif n:\n\n    [... skipping similar frames: Pickler.save at line 70 (1 times), Pickler.save at line 414 (1 times)]\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:601](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=600), in _Pickler.save(self, obj, save_persistent_id)\n    597     raise PicklingError(\"Tuple returned by %s must have \"\n    598                         \"two to six elements\" % reduce)\n    600 # Save the reduce() output and finally memoize the object\n--> 601 self.save_reduce(obj=obj, *rv)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:715](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=714), in _Pickler.save_reduce(self, func, args, state, listitems, dictitems, state_setter, obj)\n    713 if state is not None:\n    714     if state_setter is None:\n--> 715         save(state)\n    716         write(BUILD)\n    717     else:\n    718         # If a state_setter is specified, call it instead of load_build\n    719         # to update obj's with its previous state.\n    720         # First, push state_setter and its tuple of expected arguments\n    721         # (obj, state) onto the stack.\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:920](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=919), in _Pickler.save_tuple(self, obj)\n    918 write(MARK)\n    919 for element in obj:\n--> 920     save(element)\n    922 if id(obj) in memo:\n    923     # Subtle.  d was not in memo when we entered save_tuple(), so\n    924     # the process of saving the tuple's elements must have saved\n   (...)    928     # could have been done in the \"for element\" loop instead, but\n    929     # recursive tuples are a rare thing.\n    930     get = self.get(memo[id(obj)][0])\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\datasets\\utils\\_dill.py:70](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/datasets/utils/_dill.py#line=69), in Pickler.save(self, obj, save_persistent_id)\n     68 if obj_type is FunctionType:\n     69     obj = getattr(obj, \"_torchdynamo_orig_callable\", obj)\n---> 70 dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\site-packages\\dill\\_dill.py:414](file:///C:/ProgramData/miniforge3/envs/geo/Lib/site-packages/dill/_dill.py#line=413), in Pickler.save(self, obj, save_persistent_id)\n    412     msg = \"Can't pickle %s: attribute lookup builtins.generator failed\" % GeneratorType\n    413     raise PicklingError(msg)\n--> 414 StockPickler.save(self, obj, save_persistent_id)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:558](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=557), in _Pickler.save(self, obj, save_persistent_id)\n    556 f = self.dispatch.get(t)\n    557 if f is not None:\n--> 558     f(self, obj)  # Call unbound method with explicit self\n    559     return\n    561 # Check private dispatch table if any, or else\n    562 # copyreg.dispatch_table\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:809](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=808), in _Pickler.save_bytes(self, obj)\n    806         self.save_reduce(codecs.encode,\n    807                          (str(obj, 'latin1'), 'latin1'), obj=obj)\n    808     return\n--> 809 self._save_bytes_no_memo(obj)\n    810 self.memoize(obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:797](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=796), in _Pickler._save_bytes_no_memo(self, obj)\n    795     self._write_large_bytes(BINBYTES8 + pack(\"<Q\", n), obj)\n    796 elif n >= self.framer._FRAME_SIZE_TARGET:\n--> 797     self._write_large_bytes(BINBYTES + pack(\"<I\", n), obj)\n    798 else:\n    799     self.write(BINBYTES + pack(\"<I\", n) + obj)\n\nFile [C:\\ProgramData\\miniforge3\\envs\\geo\\Lib\\pickle.py:254](file:///C:/ProgramData/miniforge3/envs/geo/Lib/pickle.py#line=253), in _Framer.write_large_bytes(self, header, payload)\n    247 # Perform direct write of the header and payload of the large binary\n    248 # object. Be careful not to concatenate the header and the payload\n    249 # prior to calling 'write' as we do not want to allocate a large\n    250 # temporary bytes object.\n    251 # We intentionally do not insert a protocol 4 frame opcode to make\n    252 # it possible to optimize file.read calls in the loader.\n    253 write(header)\n--> 254 write(payload)\n\nMemoryError:\n\n```\n\n</details>\n\nMemory error is an expected type of error in such case, but when I started digging down, I found out that it occurs in a kinda unexpected place - in `create_config_id` function. It tries to hash `config_kwargs_to_add_to_suffix`, including generator function itself. \n\nI modified the `BuilderConfig.create_config_id` code like this to check which values are hashed and how much time it takes to hash them and ran it on a toy dataset:\n```\nprint(config_kwargs_to_add_to_suffix)\nstart_time = time.time()\n\nif all(isinstance(v, (str, bool, int, float)) for v in config_kwargs_to_add_to_suffix.values()):\n    suffix = \",\".join(\n        str(k) + \"=\" + urllib.parse.quote_plus(str(v)) for k, v in config_kwargs_to_add_to_suffix.items()\n    )\n    if len(suffix) > 32:  # hash if too long\n        suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\nelse:\n    suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\nend_time = time.time()\n\nprint(f\"Execution time: {end_time - start_time:.4f} seconds\")\nprint(suffix)\n```\n\nIn my case the content of `config_kwargs_to_add_to_suffix` was like this:\n```\n{'features': {'key': Value(dtype='int64', id=None), 'x': Array3D(shape=(44, 128, 128), dtype='float32', id=None), 'y_class': Array2D(shape=(128, 128), dtype='int32', id=None)}, 'gen_kwargs': None, 'generator': <function generate_tiles.<locals>.dataset_generator at 0x00000139D10D7920>, 'split': NamedSplit('train')}\n```\n\nAlso I noticed that hashing took a significant amount of time - 43.1482 seconds, while the overall function execution (with data loading, batching and saving dataset) took 2min 45s. The output of `create_config_id` is just a dataset id, so, it is inappropirately large amount of time.\n\nBut when I added `config_kwargs_to_add_to_suffix.pop(\"generator\", None)`, the hashing took only 0.0060 seconds.\n\nMaybe we shouldn't hash the generator function, as it can be really computationally and memory expensive.\n\n\n\n### Steps to reproduce the bug\n\nThis is a simplified example of a workflow I used to generate dataset. But I think that you can use almost any workflow to reproduce that bug.\n\n```\nimport pystac\nimport pystac_client\nimport planetary_computer\n\nimport numpy as np\nimport xarray as xr\nimport rioxarray as rxr\nimport dask\nimport xbatcher\n\nimport datasets\n\n\n# Loading a dataset, in our case - single Landsat image\ncatalog = pystac_client.Client.open(\n    \"https://planetarycomputer.microsoft.com/api/stac/v1\",\n    modifier=planetary_computer.sign_inplace,\n)\nbrazil = [-60.2, -3.31]\ntime_of_interest = \"2021-06-01/2021-08-31\"\nsearch = catalog.search(collections=[\"landsat-c2-l2\"], intersects={\"type\": \"Point\", \"coordinates\": brazil}, datetime=time_of_interest)\nitems = search.item_collection()\nitem = min(items, key=lambda item: pystac.extensions.eo.EOExtension.ext(item).cloud_cover)\n\n# Getting x data\nbands = []\nfor band in [\"red\", \"green\", \"blue\", \"nir08\", \"coastal\", \"swir16\", \"swir22\", \"lwir11\"]:\n    with rxr.open_rasterio(item.assets[band].href, chunks=True, lock=True) as raster:\n        raster = raster.to_dataset('band')\n        #print(raster)\n        raster = raster.rename({1: band})\n        bands.append(raster)\nx = xr.merge(bands).squeeze().to_array(\"band\").persist()\n\n# Getting y data\nwith rxr.open_rasterio(item.assets['qa_pixel'].href, chunks=True, lock=True) as raster:\n    y = raster.squeeze().persist()\n\n# Setting up batches generators\nx_batches = xbatcher.BatchGenerator(ds=x, input_dims={\"x\": 256, \"y\": 256})\ny_batches = xbatcher.BatchGenerator(ds=y, input_dims={\"x\": 256, \"y\": 256})\n\n# Filtering samples that contain only nodata\nsamples = list(range(len(x_batches)))\nsamples_filtered = []\nfor i in samples:\n    if not np.array_equal(np.unique(x_batches[i]), np.array([0.])) and not np.array_equal(np.unique(y_batches[i]), np.array([0])):\n        samples_filtered.append(i)\nsamples = samples_filtered\nnp.random.shuffle(samples)\n\n# Setting up features\nfeat = {\n    \"key\": datasets.Value(dtype=\"int64\"), \n    \"x\": datasets.Array3D(dtype=\"float32\", shape=(4, 256, 256)),\n    \"y\": datasets.Array2D(dtype=\"int32\", shape=(256, 256))\n}\nfeat = datasets.Features(feat)\n\n# Setting up a generator\ndef dataset_generator():\n    for index in samples:\n        data_dict = {\n            \"key\": index,\n            \"x\": x_batches[index].data,\n            \"y\": y_batches[index].data,\n        }\n        yield data_dict\n\n# Create dataset\nds = datasets.Dataset.from_generator(\n    dataset_generator,\n    features=feat,\n    cache_dir=\"temp/cache\",\n)\n```\nPlease, try adding `config_kwargs_to_add_to_suffix.pop(\"generator\", None)` to `BuilderConfig.create_config_id` and then measuring how much time it takes to run \n``` \nif all(isinstance(v, (str, bool, int, float)) for v in config_kwargs_to_add_to_suffix.values()):\n    suffix = \",\".join(\n        str(k) + \"=\" + urllib.parse.quote_plus(str(v)) for k, v in config_kwargs_to_add_to_suffix.items()\n    )\n    if len(suffix) > 32:  # hash if too long\n        suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\nelse:\n    suffix = Hasher.hash(config_kwargs_to_add_to_suffix)\n```\ncode block with and without `config_kwargs_to_add_to_suffix.pop(\"generator\", None)`\n\nIn my case the difference was 3.3828 seconds without popping generator function and 0.0010 seconds with popping.\n\n### Expected behavior\n\nMuch faster hashing and no MemoryErrors\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Windows-11-10.0.26100-SP0\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 17.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nMemoryError while creating dataset from generator<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport `pyarrow` 14.0.1 and fix vulnerability [CVE-2023-47248](https://github.com/advisories/GHSA-5wvp-7f3h-6wmm).\r\n\r\nFix #6396.<end_of_turn>\n<start_of_turn>model\nSupport pyarrow 14.0.1 and fix vulnerability CVE-2023-47248<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen mapping some datasets with `batched=True`, datasets may raise an exeception:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Users/codingl2k1/Work/datasets/venv/lib/python3.11/site-packages/multiprocess/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n                    ^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/utils/py_utils.py\", line 1328, in _write_generator_to_queue\r\n    for i, result in enumerate(func(**kwargs)):\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/arrow_dataset.py\", line 3483, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/arrow_writer.py\", line 549, in write_batch\r\n    array = cast_array_to_feature(col_values, col_type) if col_type is not None else col_values\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/table.py\", line 1831, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/table.py\", line 1831, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/table.py\", line 2063, in cast_array_to_feature\r\n    return feature.cast_storage(array)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/features/features.py\", line 1098, in cast_storage\r\n    if min_max[\"max\"] >= self.num_classes:\r\n       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: '>=' not supported between instances of 'NoneType' and 'int'\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"/Users/codingl2k1/Work/datasets/t1.py\", line 33, in <module>\r\n    ds = ds.map(transforms, num_proc=14, batched=True, batch_size=5)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/dataset_dict.py\", line 850, in map\r\n    {\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/dataset_dict.py\", line 851, in <dictcomp>\r\n    k: dataset.map(\r\n       ^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/arrow_dataset.py\", line 577, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/arrow_dataset.py\", line 542, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/arrow_dataset.py\", line 3179, in map\r\n    for rank, done, content in iflatmap_unordered(\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/utils/py_utils.py\", line 1368, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/utils/py_utils.py\", line 1368, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/venv/lib/python3.11/site-packages/multiprocess/pool.py\", line 774, in get\r\n    raise self._value\r\nTypeError: '>=' not supported between instances of 'NoneType' and 'int'\r\n```\n\n### Steps to reproduce the bug\n\n1. Checkout the latest main of datasets.\r\n2. Run the code:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndef transforms(examples):\r\n    # examples[\"pixel_values\"] = [image.convert(\"RGB\").resize((100, 100)) for image in examples[\"image\"]]\r\n    return examples\r\n\r\nds = load_dataset(\"scene_parse_150\")\r\nds = ds.map(transforms, num_proc=14, batched=True, batch_size=5)\r\nprint(ds)\r\n```\n\n### Expected behavior\n\nmap without exception.\n\n### Environment info\n\nDatasets: https://github.com/huggingface/datasets/commit/b8067c0262073891180869f700ebef5ac3dc5cce\r\nPython: 3.11.4\r\nSystem: Macos<end_of_turn>\n<start_of_turn>model\nBatch map raises TypeError: '>=' not supported between instances of 'NoneType' and 'int'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport JSON lines with missing struct fields.\r\n\r\nFix #7159.\r\n\r\nThe implemented test raised:\r\n```\r\nTypeError: Couldn't cast array of type\r\nstruct<age: int64>\r\nto\r\n{'age': Value(dtype='int32', id=None), 'name': Value(dtype='string', id=None)}\r\n```<end_of_turn>\n<start_of_turn>model\nSupport JSON lines with missing struct fields<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn order to convert script-datasets to no-script datasets, we need to support setting a default config name for those scripts that set one.<end_of_turn>\n<start_of_turn>model\nSupport setting a default config name in push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n![1702472610561](https://github.com/huggingface/datasets/assets/28867010/4b7ef5e7-32c3-4b73-84cb-5de059caa0b6)\r\nWhen I learn from https://huggingface.co/docs/datasets/image_load and try to load image data from a folder. I noticed that the image was read twice in the returned data. As you can see in the attached image, there are only four images in the train folder, but reading brings up eight images\n\n### Steps to reproduce the bug\n\nfrom datasets import Dataset, load_dataset\r\n\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"data/\", drop_labels=False)\r\n# print(dataset[\"train\"][0][\"image\"] == dataset[\"train\"][1][\"image\"])\r\n\r\nprint(dataset)\r\nprint(dataset[\"train\"][\"image\"])\r\nprint(len(dataset[\"train\"][\"image\"]))\n\n### Expected behavior\n\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'label'],\r\n        num_rows: 8\r\n    })\r\n})\r\n[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D1CA8B0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D2452E0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D245310>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D2453A0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D245460>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2877x2129 at 0x1BD1D245430>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D2454F0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=4208x3120 at 0x1BD1D245550>]\r\n8\n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.19.4\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nImage Data loaded Twice<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe cache used to only check for the latest cache directory with a given config_name, but it was wrong (e.g. `default-data_dir=data%2Ffortran-data_dir=data%2Ffortran` instead of `default-data_dir=data%2Ffortran`)\r\n\r\nI fixed this by not passing the `config_kwargs` to the parent Builder `__init__`, and passing the config_id forged from the `config_kwargs` directly\r\n\r\nclose https://github.com/huggingface/datasets/issues/6609<end_of_turn>\n<start_of_turn>model\nFix reload cache with data dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIn the context of a private hub deployment, customers would like to use load_dataset() to load datasets from their hub, not from the public hub. This doesn't seem to be configurable at the moment and it would be nice to add this feature.\n\nThe obvious workaround is to clone the repo first and then load it from local storage, but this adds an extra step. It'd be great to have the same experience regardless of where the hub is hosted.\n\nThis issue was raised before here: https://github.com/huggingface/datasets/issues/3679\n@juliensimon\n\n### Motivation\n\nnone\n\n### Your contribution\n\nnone<end_of_turn>\n<start_of_turn>model\nDownload datasets from a private hub in 2025<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR implements `builder.as_iterable_dataset(split=...)` as discussed in #5481.\r\n\r\nIt allows users to load an `IterableDataset` directly from cached Arrow files (using ArrowReader and ArrowExamplesIterable), without loading the full dataset into memory.\r\n\r\nThis is useful for large-scale training scenarios where memory is constrained. A test has also been added in `test_builder.py`.\r\n\r\nRelated to: #5481\r\n<end_of_turn>\n<start_of_turn>model\nAdd `as_iterable_dataset()` method to DatasetBuilder for streaming from cached Arrow files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAllow numpy-2.1 and test it without audio extra.\r\n\r\nThis PR reverts:\r\n- #7114\r\n\r\nNote that audio extra tests can be included again with numpy-2.1 once next numba-0.61.0 version is released.<end_of_turn>\n<start_of_turn>model\nAllow numpy-2.1 and test it without audio extra<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR updates the documentation to include the HF_DATASETS_CACHE environment variable, which allows users to customize the cache location for datasets\u2014similar to HF_HUB_CACHE for models.<end_of_turn>\n<start_of_turn>model\nCloses #7457<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nBump hfh to 0.24 to fix ci<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAt Google Colab\n```!pip install faiss-cpu``` works\n```import faiss``` no error\nbut\n```embeddings_dataset.add_faiss_index(column='embeddings')```\nreturns\n```\n[/usr/local/lib/python3.11/dist-packages/datasets/search.py](https://localhost:8080/#) in init(self, device, string_factory, metric_type, custom_index)\n247 self.faiss_index = custom_index\n248 if not _has_faiss:\n--> 249 raise ImportError(\n250 \"You must install Faiss to use FaissIndex. To do so you can run conda install -c pytorch faiss-cpu or conda install -c pytorch faiss-gpu. \"\n251 \"A community supported package is also available on pypi: pip install faiss-cpu or pip install faiss-gpu. \"\n```\n\nbecause\n```_has_faiss = importlib.util.find_spec(\"faiss\") is not None```    at the beginning of ```datasets/search.py``` returns ```False```\nwhen\nthe same code at colab notebook returns\n```ModuleSpec(name='faiss', loader=<_frozen_importlib_external.SourceFileLoader object at 0x7b7851449f50>, origin='/usr/local/lib/python3.11/dist-packages/faiss/init.py', submodule_search_locations=['/usr/local/lib/python3.11/dist-packages/faiss'])```\n\nBut\n```\nimport datasets\ndatasets.search._has_faiss\n```\nat ```colab notebook``` also returns ```False```\n\nThe same story with   ```_has_elasticsearch```\n\n### Steps to reproduce the bug\n\n1. Follow https://huggingface.co/learn/nlp-course/chapter5/6?fw=pt at Google Colab\n2. till ```embeddings_dataset.add_faiss_index(column='embeddings')```\n3. ```embeddings_dataset.add_elasticsearch_index(column='embeddings')```\n4. https://colab.research.google.com/drive/1h2cjuiClblqzbNQgrcoLYOC8zBqTLLcv#scrollTo=3ddzRp72auOF\n\n### Expected behavior\n\nI've only started Tutorial and don't know exactly. But something tells me that ```embeddings_dataset.add_faiss_index(column='embeddings')```\nshould work without ```Import Error```\n\n### Environment info\n\nGoogle Colab notebook with default config<end_of_turn>\n<start_of_turn>model\n.add_faiss_index and .add_elasticsearch_index returns ImportError at Google Colab<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\r\nThis pull request updates the Datasets documentation to include the `HF_DATASETS_CACHE` environment variable. While the current documentation only mentions `HF_HOME` for overriding the default cache directory, `HF_DATASETS_CACHE` is also a supported and useful option for specifying a custom cache location for datasets stored in Arrow format.\r\n\r\nThis addition is based on the discussion in (https://github.com/huggingface/datasets/issues/7457), where users noted the absence of this variable in the documentation despite its functionality. The update adds a new section to `cache.mdx` that explains how to use `HF_DATASETS_CACHE` with an example.\r\n\r\nThis change aims to improve clarity and help users better manage their cache directories when working in shared environments or with limited local storage.\r\n\r\nCloses #7457.<end_of_turn>\n<start_of_turn>model\nDocument the HF_DATASETS_CACHE environment variable in the datasets cache documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nCreate battery_analysis.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n... to be consistent with `transformers` and `huggingface_hub`.<end_of_turn>\n<start_of_turn>model\nDeprecate `use_auth_token` in favor of `token`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nEnable parallel downloads using multiprocessing when `num_proc` is passed to `load_dataset`.\r\n\r\nIt was enabled for datasets with scripts already (if they passed lists to `dl_manager.download`) but not for no-script datasets (we pass dicts {split: [list of files]} to `dl_manager.download` for those ones).\r\n\r\nI fixed this by parallelising on the lists contained in the data files dicts when possible.\r\n\r\nI also added a context manager `stack_multiprocessing_download_progress_bars` in `DownloadManager` to stack the progress bard of the downloads (from `cached_path(...)` calls). Otherwise the progress bars overlap each other with an annoying flickering effect.<end_of_turn>\n<start_of_turn>model\nFix parallel downloads for datasets without scripts<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nSupport data files with `.ndjson` extension.\n\n### Motivation\n\nWe already support data files with `.jsonl` extension.\n\n### Your contribution\n\nI am opening a PR.<end_of_turn>\n<start_of_turn>model\nSupport data files with .ndjson extension<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIm trying to run a fairly simple map that is converting a dataset into numpy arrays. however, it just piles up on memory and doesnt write to disk. Ive tried multiple cache techniques such as specifying the cache dir, setting max mem, +++ but none seem to work. What am I missing here?\n\n### Steps to reproduce the bug\n\n```\r\nfrom pydub import AudioSegment\r\nimport io\r\nimport base64\r\nimport numpy as np\r\nimport os\r\nCACHE_PATH = \"/mnt/extdisk/cache\" # \"/root/.cache/huggingface/\"# \r\nos.environ[\"HF_HOME\"] = CACHE_PATH\r\nimport datasets\r\nimport logging\r\nlogger = logging.getLogger()\r\nlogger.setLevel(logging.INFO)\r\n# Create a handler for Jupyter notebook\r\nhandler = logging.StreamHandler()\r\nformatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\r\nhandler.setFormatter(formatter)\r\nlogger.addHandler(handler)\r\n\r\n#datasets.config.IN_MEMORY_MAX_SIZE= 1000#*(2**30) #50 gb\r\nprint(datasets.config.HF_CACHE_HOME)\r\nprint(datasets.config.HF_DATASETS_CACHE)\r\n# Decode the base64 string into bytes\r\ndef convert_mp3_to_audio_segment(example):\r\n    \"\"\"\r\n    example = ds['train'][0]\r\n    \"\"\"\r\n    try:\r\n        audio_data_bytes = base64.b64decode(example['audio'])\r\n        # Use pydub to load the MP3 audio from the decoded bytes\r\n        audio_segment = AudioSegment.from_file(io.BytesIO(audio_data_bytes), format=\"mp3\")\r\n        # Resample to 24_000\r\n        audio_segment = audio_segment.set_frame_rate(24_000)\r\n        audio = {'sampling_rate' : audio_segment.frame_rate,\r\n        'array' : np.array(audio_segment.get_array_of_samples(), dtype=\"float\")}\r\n        del audio_segment\r\n        duration = len(audio['array']) / audio['sampling_rate']\r\n    except Exception as e:\r\n        logger.warning(f\"Failed to convert audio for {example['id']}. Error: {e}\")\r\n        audio = {'sampling_rate' : 0,\r\n        'array' : np.array([]), duration : 0}\r\n    return {'audio' : audio, 'duration' : duration}\r\n\r\nds = datasets.load_dataset(\"NbAiLab/nb_distil_speech_noconcat_stortinget\", cache_dir=CACHE_PATH, keep_in_memory=False)\r\n\r\n#%%\r\nnum_proc=32\r\nds_processed = (\r\n    ds\r\n    #.select(range(10))\r\n    .map(convert_mp3_to_audio_segment, num_proc=num_proc, desc=\"Converting mp3 to audio segment\") #, cache_file_name=f\"{CACHE_PATH}/stortinget_audio\" # , cache_file_name=\"test\"\r\n)\r\n```\n\n### Expected behavior\n\nthe map should write to disk\n\n### Environment info\n\n- `datasets` version: 3.2.0\r\n- Platform: Linux-6.8.0-45-generic-x86_64-with-glibc2.39\r\n- Python version: 3.12.7\r\n- `huggingface_hub` version: 0.26.3\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\n.map() is not caching and ram goes OOM<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen exporting a Dataset object to a JSON Lines file using the .to_json(lines=True) method, the process consumes a very large amount of memory. The memory usage is proportional to the size of the entire Dataset object being saved, rather than being a low, constant memory operation.\n\nThis behavior is unexpected, as the JSONL format is line-oriented and ideally suited for streaming writes. This issue can easily lead to Out-of-Memory (OOM) errors when exporting large datasets, especially in memory-constrained environments like Docker containers.\n\n<img width=\"1343\" height=\"329\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/518b4263-ad12-422d-9672-28ffe97240ce\" />\n\n### Steps to reproduce the bug\n\n```\nimport os\nfrom datasets import load_dataset, Dataset\nfrom loguru import logger\n\n# A public dataset to test with\nREPO_ID = \"adam89/TinyStoriesChinese\"\nSUBSET = \"default\"\nSPLIT = \"train\"\nNUM_ROWS_TO_LOAD = 10  # Use a reasonably large number to see the memory spike\n\n\ndef run_test():\n    \"\"\"Loads data into memory and then saves it, triggering the memory issue.\"\"\"\n    logger.info(\"Step 1: Loading data into an in-memory Dataset object...\")\n\n    # Create an in-memory Dataset object from a stream\n    # This simulates having a processed dataset ready to be saved\n    iterable_dataset = load_dataset(REPO_ID, name=SUBSET, split=SPLIT, streaming=True)\n    limited_stream = iterable_dataset.take(NUM_ROWS_TO_LOAD)\n    in_memory_dataset = Dataset.from_generator(limited_stream.__iter__)\n\n    logger.info(f\"Dataset with {len(in_memory_dataset)} rows created in memory.\")\n\n    output_path = \"./test_output.jsonl\"\n    logger.info(f\"Step 2: Saving the dataset to {output_path} using .to_json()...\")\n    logger.info(\"Please monitor memory usage during this step.\")\n\n    # This is the step that causes the massive memory allocation\n    in_memory_dataset.to_json(output_path, force_ascii=False)\n\n    logger.info(\"Save operation complete.\")\n    os.remove(output_path)\n\n\nif __name__ == \"__main__\":\n    # To see the memory usage clearly, run this script with a memory profiler:\n    # python -m memray run your_script_name.py\n    # python -m memray tree xxx.bin\n    run_test()\n\n```\n\n### Expected behavior\n\nI would expect the .to_json(lines=True) method to be a memory-efficient, streaming operation. The memory usage should remain low and relatively constant, as data is converted and written to the file line-by-line or in small batches. The memory footprint should not be proportional to the total number of rows in the in_memory_dataset.\n\n### Environment info\n\ndatasets version:3.6.0\nPython version:3.9.18\nos:macOS 15.3.1 (arm64)<end_of_turn>\n<start_of_turn>model\nDataset.to_json consumes excessive memory, appears to not be a streaming operation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`diffusers` (https://github.com/huggingface/diffusers/pull/7116) and `huggingface_hub` (https://github.com/huggingface/huggingface_hub/pull/2072) also use `uv` to install their dependencies, so we can do the same here.\r\n\r\nIt seems to make the \"Install dependencies\" step in the `ubuntu` jobs 5-8x faster and 1.5-2x in the `windows` one.\r\n\r\nBesides introducing `uv` in CI, this PR bumps the `tensorflow` minimal version requirement to align with Transformers and simplifies the SpaCy hashing tests (use blank language models instead of the pre-trained ones)\r\n <end_of_turn>\n<start_of_turn>model\nInstall dependencies with `uv` in CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI have a dataset of 2500 images that can be used for color-blind machine-learning algorithms. Since , there was no dataset available online , I made this dataset myself and would like to contribute this now to community<end_of_turn>\n<start_of_turn>model\nWould like to Contribute a dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen calling filter on an iterable dataset, the features get set to None\n\n### Steps to reproduce the bug\n\nimport numpy as np\r\nimport time\r\nfrom datasets import Dataset, Features, Array3D\r\n\r\n```python\r\nfeatures=Features(**{\"array0\": Array3D((None, 10, 10), dtype=\"float32\"), \"array1\": Array3D((None,10,10), dtype=\"float32\")})\r\ndataset = Dataset.from_dict({f\"array{i}\": [np.zeros((x,10,10), dtype=np.float32) for x in [2000,1000]*25] for i in range(2)}, features=features)\r\nds = dataset.to_iterable_dataset()\r\norig_column_names = ds.column_names\r\nds = ds.filter(lambda x: True)\r\nassert ds.column_names == orig_column_names\r\n```\n\n### Expected behavior\n\nFilter should preserve features information\n\n### Environment info\n\n3.0.2<end_of_turn>\n<start_of_turn>model\nIterable dataset.filter should not override features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI came across this, perhaps niche, bug where `Features` does not/cannot account for pyarrow's `nullable=False` option in Fields. Interestingly, I found that in regular \"flat\" fields this does not necessarily lead to conflicts, but when a non-nullable field is in a struct, an incompatibility arises.\n\nIt's not easy to explain in words, so the minimal example below should help I hope.\n\nNote that I suggest a solution in the comments in the code, simply allowing `Dataset.to_parquet` to allow for a `schema` argument which, when provided, will override the default ds.features.arrow_schema.\n\n### Steps to reproduce the bug\n\n```python\nimport os\nfrom datasets import Dataset, Features\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n# HF datasets is destructive when you call Features.from_arrow_schema(schema) on a schema \n# because it will not account for nullable and non-nullable fields in structs (it will always allow nullable)\n# Reloading the same dataset with the original schema will raise an error because the schema is not the same anymore\nnon_nullable_schema = pa.schema(\n    [\n        pa.field(\"text\", pa.string(), nullable=False),\n        pa.field(\"meta\",\n            pa.struct(\n                [\n                    pa.field(\"date\", pa.list_(pa.string()), nullable=False),\n                ],\n            ),\n        ),\n\n    ]\n)\nprint(\"ORIGINAL SCHEMA\")\nprint(non_nullable_schema)\nprint()\n\nfeats = Features.from_arrow_schema(non_nullable_schema)\n\nprint(\"FEATUR-IZED SCHEMA (nullable-restrictions are gone)\")\nprint(feats.arrow_schema)\nprint()\n\nds = Dataset.from_dict(\n    {\n        \"text\": [\"a\", \"b\", \"c\"],\n        \"meta\": [{\"date\": [\"2021-01-01\"]}, {\"date\": [\"2021-01-02\"]}, {\"date\": [\"2021-01-03\"]}],\n    },\n    features=feats,\n)\n\nfname = \"tmp.parquet\"\n\n# This is not possible: TypeError: pyarrow.parquet.core.ParquetWriter() got multiple values for keyword argument 'schema'\n# Though I believe this would be the easiest fix: allow schema to be passed to to_parquet and overwrite the schema in the dataset\n# ds.to_parquet(fname, schema=non_nullable_schema)\n\nds.to_parquet(fname)\n\ntry:\n    _ = pq.read_table(fname, schema=non_nullable_schema)\nfinally:\n    os.unlink(fname)\n```\n\n\n### Expected behavior\n\n- Non-destructive behavior when converting an arrow schema to Features; or\n- the ability to override the default arrow schema with a custom one\n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-5.14.0-427.20.1.el9_4.x86_64-x86_64-with-glibc2.34\n- Python version: 3.11.10\n- `huggingface_hub` version: 0.27.1\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nFeatures.from_arrow_schema is destructive<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nLet's have more hub-centric documentation in the datasets docs\r\n\r\nTutorials\r\n- Add \u201cConfigure the dataset viewer\u201d page\r\n- Change order:\r\n  - Overview\r\n    - and more focused on the Hub rather than the library\r\n  - Then all the hub related things\r\n    - and mention how to read/write with other tools like pandas\r\n  - Then all the datasets lib related things in a subsection\r\n\r\nAlso:\r\n- Rename \u201cknow your dataset\u201d page to \u201cExplore your dataset\u201d\r\n- Remove \u201cEvaluate Predictions\u201d page since it's 'evaluate' stuff (or move to legacy section ?)\r\n\r\nTODO:\r\n- [ ] write the \u201cConfigure the dataset viewer\u201d page<end_of_turn>\n<start_of_turn>model\nMore hub centric docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI noticed that you can use a Model Card instead of a Dataset Card when pushing a dataset to the Hub, but this isn\u2019t clearly mentioned in [the docs.](https://huggingface.co/docs/datasets/dataset_card)\r\n\r\n- Update the docs to clarify that a Model Card can work for datasets too.\r\n- It might be worth creating a dedicated DatasetCard module, similar to the ModelCard module, for consistency and better support.\r\n\r\nNot sure if this belongs here or on the [Hub repo](https://github.com/huggingface/huggingface_hub), but thought I\u2019d bring it up!\n\n### Motivation\n\nI just spent an hour like on [this issue](https://github.com/huggingface/trl/pull/2491) trying to create a `DatasetCard` for a script.\n\n### Your contribution\n\nmight later<end_of_turn>\n<start_of_turn>model\nClarify documentation or Create DatasetCard<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis adds a Arrow dataset builder to be able to load and stream from already preprocessed Arrow files.\r\nIt's related to https://github.com/huggingface/datasets/issues/3035<end_of_turn>\n<start_of_turn>model\nArrow dataset builder to be able to load and stream Arrow datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI want to improve the error message for datasets like https://huggingface.co/datasets/m-a-p/COIG-CQIA\r\n\r\nCc @albertvillanova @severo is this new error ok ? Or should I use a dedicated error class ?\r\n\r\nNew:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1920, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/arrow_writer.py\", line 574, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/table.py\", line 2322, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/table.py\", line 2276, in cast_table_to_schema\r\n    raise CastError(\r\ndatasets.table.CastError: Couldn't cast\r\ninstruction: string\r\nother: string\r\nindex: string\r\ndomain: list<item: string>\r\n  child 0, item: string\r\noutput: string\r\ntask_type: struct<major: list<item: string>, minor: list<item: string>>\r\n  child 0, major: list<item: string>\r\n      child 0, item: string\r\n  child 1, minor: list<item: string>\r\n      child 0, item: string\r\ntask_name_in_eng: string\r\ninput: string\r\nto\r\n{'answer_from': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'human_verified': Value(dtype='bool', id=None), 'domain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'output': Value(dtype='string', id=None), 'task_type': {'major': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'minor': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'copyright': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/quentinlhoest/hf/datasets/playground/ttest.py\", line 74, in <module>\r\n    load_dataset(\"m-a-p/COIG-CQIA\")\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/load.py\", line 2529, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 936, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1031, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1791, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1922, in _prepare_split_single\r\n    raise DatasetGenerationCastError.from_cast_error(\r\ndatasets.exceptions.DatasetGenerationCastError: An error occurred while generating the dataset\r\n\r\nAll the data files must have the same columns, but at some point there are 3 new columns (other, index, task_name_in_eng) and 3 missing columns (answer_from, copyright, human_verified).\r\n\r\nThis happened while the json dataset builder was generating data using\r\n\r\nhf://datasets/m-a-p/COIG-CQIA/coig_pc/coig_pc_core_sample.json (at revision b7b7ecf290f6515036c7c04bd8537228ac2eb474)\r\n\r\nPlease either edit the data files to have matching columns, or separate them into different configurations (see docs at https://hf.co/docs/hub/datasets-manual-configuration#multiple-configurations)\r\n```\r\n\r\nPreviously:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1931, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/arrow_writer.py\", line 574, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/table.py\", line 2295, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/table.py\", line 2253, in cast_table_to_schema\r\n    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\r\nValueError: Couldn't cast\r\ntask_type: struct<major: list<item: string>, minor: list<item: string>>\r\n  child 0, major: list<item: string>\r\n      child 0, item: string\r\n  child 1, minor: list<item: string>\r\n      child 0, item: string\r\nother: string\r\ninstruction: string\r\ntask_name_in_eng: string\r\ndomain: list<item: string>\r\n  child 0, item: string\r\nindex: string\r\noutput: string\r\ninput: string\r\nto\r\n{'human_verified': Value(dtype='bool', id=None), 'task_type': {'major': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'minor': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}, 'answer_from': Value(dtype='string', id=None), 'copyright': Value(dtype='string', id=None), 'instruction': Value(dtype='string', id=None), 'domain': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'output': Value(dtype='string', id=None), 'input': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/quentinlhoest/hf/datasets/playground/ttest.py\", line 74, in <module>\r\n    load_dataset(\"m-a-p/COIG-CQIA\")\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/load.py\", line 2529, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 949, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1044, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1804, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/Users/quentinlhoest/hf/datasets/src/datasets/builder.py\", line 1949, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```<end_of_turn>\n<start_of_turn>model\nBetter cast error when generating dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am iterating over a large dataset that I load using streaming=True to avoid running out of memory. Unfortunately, I am observing that memory usage increases over time and I'm finally running in an oom.\n\n### Steps to reproduce the bug\n\n```\n  ds = load_dataset(\"openslr/librispeech_asr\", split=\"train.clean.360\", streaming=True)\n  for i,sample in enumerate(tqdm(ds)):\n    target_file = os.path.join(NSFW_TARGET_FOLDER, f'audio{i}.wav')\n    try:\n      sf.write(target_file, sample['audio']['array'], samplerate=sample['audio']['sampling_rate'])\n    except Exception as e:\n      print(f\"Could not write audio {i} in ds: {e}\")\n```\n\n\n### Expected behavior\n\nI'd expect to have a small memory footprint and memory being freed after each iteration of the for loop. Instead the memory usage is increasing. I tried to remove the logic to write the sound file and just print the sample but the issue remains the same.\n\n### Environment info\n\nPython 3.12.11\nUbuntu 24\ndatasets 4.0.0 and 3.6.0<end_of_turn>\n<start_of_turn>model\nOut of memory even though using load_dataset(..., streaming=True)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nFor most large speech dataset, we do not wish to generate hundreds of millions of small audio samples. Instead, it is quite common to provide larger audio files with frame offset (soundfile start and stop arguments). We should be able to pass these arguments to Audio() (column ID corresponding in the dataset row).\n\n### Motivation\n\nI am currently generating a fairly big dataset to .parquet(). Unfortunately, it does not work because all existing functions load the whole .wav file corresponding to the row. All my attempts at bypassing this failed. We should be able to put in the Table only the bytes corresponding to what soundfile reads with an offset (and subset of the audio file). \n\n### Your contribution\n\nI can totally test whatever code on my large dataset creation script. <end_of_turn>\n<start_of_turn>model\nEnable the Audio Feature to decode / read with an offset + duration<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFaiss tests on Windows make the CI run indefinitely until maximum execution time (360 minutes) is reached.\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/9712659783\r\n```\r\ntest (integration, windows-latest, deps-minimum)\r\nThe job running on runner GitHub Actions 60 has exceeded the maximum execution time of 360 minutes.\r\ntest (integration, windows-latest, deps-latest)\r\nThe job running on runner GitHub Actions 238 has exceeded the maximum execution time of 360 minutes.\r\n```\r\n```\r\n____________________________ tests/test_search.py _____________________________\r\n[gw1] win32 -- Python 3.8.10 C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python.exe\r\nworker 'gw1' crashed while running 'tests/test_search.py::IndexableDatasetTest::test_add_faiss_index'\r\n____________________________ tests/test_search.py _____________________________\r\n[gw2] win32 -- Python 3.8.10 C:\\hostedtoolcache\\windows\\Python\\3.8.10\\x64\\python.exe\r\nworker 'gw2' crashed while running 'tests/test_search.py::IndexableDatasetTest::test_add_faiss_index'\r\n```\r\n```\r\ntests/test_search.py::IndexableDatasetTest::test_add_faiss_index \r\n[gw0] node down: Not properly terminated\r\n[gw0] FAILED tests/test_search.py::IndexableDatasetTest::test_add_faiss_index \r\n\r\nreplacing crashed worker gw0\r\n\r\ntests/test_search.py::IndexableDatasetTest::test_add_faiss_index \r\n[gw1] node down: Not properly terminated\r\n[gw1] FAILED tests/test_search.py::IndexableDatasetTest::test_add_faiss_index \r\n\r\nreplacing crashed worker gw1\r\n\r\ntests/test_search.py::IndexableDatasetTest::test_add_faiss_index \r\n[gw2] node down: Not properly terminated\r\n[gw2] FAILED tests/test_search.py::IndexableDatasetTest::test_add_faiss_index \r\n\r\nreplacing crashed worker gw2\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken for faiss tests on Windows: node down: Not properly terminated<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport `pyarrow` 14.0.0.\r\n\r\nFix #6377 and fix #6374 (root cause).\r\n\r\nThis fix is analog to a previous one:\r\n- #6175<end_of_turn>\n<start_of_turn>model\nSupport pyarrow 14.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n\r\n```\r\n  File \"/root/miniconda3/envs/gino_conda/lib/python3.9/site-packages/datasets/load.py\", line 1852, in dataset_module_factory\r\n    raise DatasetNotFoundError(msg + f\" at revision '{revision}'\" if revision else msg)\r\ndatasets.exceptions.DatasetNotFoundError: Dataset 'xxx' doesn't exist on the Hub or cannot be accessed\r\n>>> dataset = load_dataset(\"xxxx\", token=True)\r\n\r\n404 error 404 Client Error. (Request ID: Root=xxxx)\r\n\r\nRepository Not Found for url: https://huggingface.co/api/datasets/xxx/xxx.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/root/miniconda3/envs/gino_conda/lib/python3.9/site-packages/datasets/load.py\", line 2593, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/root/miniconda3/envs/gino_conda/lib/python3.9/site-packages/datasets/load.py\", line 2265, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/root/miniconda3/envs/gino_conda/lib/python3.9/site-packages/datasets/load.py\", line 1910, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/root/miniconda3/envs/gino_conda/lib/python3.9/site-packages/datasets/load.py\", line 1852, in dataset_module_factory\r\n    raise DatasetNotFoundError(msg + f\" at revision '{revision}'\" if revision else msg)\r\ndatasets.exceptions.DatasetNotFoundError: Dataset 'xxx' doesn't exist on the Hub or cannot be accessed\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n123\r\n\r\n### Expected behavior\r\n\r\n123\r\n\r\n### Environment info\r\n\r\n123<end_of_turn>\n<start_of_turn>model\nMy Private Dataset doesn't exist on the Hub or cannot be accessed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 3.5.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a script downloading `amazon_reviews_multi`.\r\n\r\nWhen the download starts, I get\r\n\r\n```\r\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\r\nDownloading data: 243B [00:00, 1.43MB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.54s/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 842.40it/s]\r\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\r\nDownloading data: 243B [00:00, 928kB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.42s/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 832.70it/s]\r\nDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\r\nDownloading data: 243B [00:00, 1.81MB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01<00:00,  1.40s/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 1294.14it/s]\r\nGenerating train split:   0%|          | 0/200000 [00:00<?, ? examples/s]\r\n```\r\n\r\nthe file is clearly too small to contain the requested dataset, in fact it contains en error message:\r\n\r\n```\r\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<Error><Code>AccessDenied</Code><Message>Access Denied</Message><RequestId>AGJWSY3ZADT2QVWE</RequestId><HostId>Gx1O2KXnxtQFqvzDLxyVSTq3+TTJuTnuVFnJL3SP89Yp8UzvYLPTVwd1PpniE4EvQzT3tCaqEJw=</HostId></Error>\r\n```\r\n\r\nobviously the script fails:\r\n\r\n```\r\n>           raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\nE           datasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. load_dataset(\"amazon_reviews_multi\", name=\"en\", split=\"train\", cache_dir=\"ADDYOURPATHHERE\")\n\n### Expected behavior\n\nI would expect the dataset to be downloaded and processed\n\n### Environment info\n\n* The problem is present with both datasets 2.12.0 and 2.14.2\r\n* python version 3.10.12<end_of_turn>\n<start_of_turn>model\nProblems in downloading Amazon reviews from HF<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds support for tabular HDF5 file(s) by converting each row to an Arrow table. It supports columns with the usual dtypes including up to 5-dimensional arrays as well as support for complex/compound types by using `Features(dict)`. All datasets within the HDF5 file should have rows on the first dimension (groups/subgroups are still allowed). Closes #3113.\r\n\r\nReplaces #7625 which only supports a relatively small subset of HDF5.<end_of_turn>\n<start_of_turn>model\nHDF5 support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs proposed by @BramVanroy, it would be neat to have this functionality through the API.<end_of_turn>\n<start_of_turn>model\nAllow deleting a subset/config from a no-script dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.16.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSince updating to >2.14 we have very slow access to our parquet files on GCS when loading a dataset (>30 min vs 3s). Our GCS bucket has many objects and resolving globs is very slow. I could track down the problem to this change:\r\nhttps://github.com/huggingface/datasets/blame/bade7af74437347a760830466eb74f7a8ce0d799/src/datasets/data_files.py#L348 \r\nThe underlying implementation with gcsfs is really slow. Could you go back to the old way if we are simply giving the parquet files and no glob pattern?\r\n\r\nThank you.\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\nLoad a dataset from a GCS bucket that has many files.\n\n### Expected behavior\n\nUsed to be fast (3s) in 2.13\n\n### Environment info\n\ndatasets==2.14.5<end_of_turn>\n<start_of_turn>model\nLoading dataset from large GCS bucket very slow since 2.14<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe current link leads to a 404 page.<end_of_turn>\n<start_of_turn>model\nfix link for rotten_tomatoes dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm trying to load [piuba-bigdata/articles_and_comments] and I'm stumbling with this error on `2.14.5`. However, this works on  `2.10.0`.\r\n\n\n### Steps to reproduce the bug\n\n[Colab link](https://colab.research.google.com/drive/1SAftFMQnFE708ikRnJJHIXZV7R5IBOCE#scrollTo=r2R2ipCCDmsg)\r\n\r\n```python\r\nDownloading readme: 100%\r\n1.19k/1.19k [00:00<00:00, 30.9kB/s]\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[<ipython-input-2-807c3583d297>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 load_dataset(\"piuba-bigdata/articles_and_comments\", split=\"train\")\r\n\r\n2 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2127 \r\n   2128     # Create a dataset builder\r\n-> 2129     builder_instance = load_dataset_builder(\r\n   2130         path=path,\r\n   2131         name=name,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\r\n   1813         download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1814         download_config.storage_options.update(storage_options)\r\n-> 1815     dataset_module = dataset_module_factory(\r\n   1816         path,\r\n   1817         revision=revision,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1506                     raise e1 from None\r\n   1507                 if isinstance(e1, FileNotFoundError):\r\n-> 1508                     raise FileNotFoundError(\r\n   1509                         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1510                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n\r\nFileNotFoundError: Couldn't find a dataset script at /content/piuba-bigdata/articles_and_comments/articles_and_comments.py or any data file in the same directory. Couldn't find 'piuba-bigdata/articles_and_comments' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in piuba-bigdata/articles_and_comments.\r\n```\n\n### Expected behavior\n\nIt should load normally.\n\n### Environment info\n\n```\r\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.18.0\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3\r\n```<end_of_turn>\n<start_of_turn>model\nCannot load dataset with `2.14.5`: `FileNotFound` error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently the DIll version is less than 3.9 and now major libraries like Multiprocess, gepa requires Dill version as 0.4.0 and this is making a conflict in installation. So added this small PR to update the DIll. <end_of_turn>\n<start_of_turn>model\nFix: Update Dill Version in Setup py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n* A small update to the documentation, noting the ability to load jsonl files. <end_of_turn>\n<start_of_turn>model\nUpdate loading.mdx to include \"jsonl\" file loading.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nRunning load_dataset using the newest `datasets` library like below on the ted_talks_iwslt using year pair data will throw an error \"Config name is missing\"\r\n\r\nsee also:\r\nhttps://huggingface.co/datasets/ted_talks_iwslt/discussions/3\r\n\r\nlikely caused by #6493, where the `and not config_kwargs` part in the if logic was removed \r\n https://github.com/huggingface/datasets/blob/ef3b5dd3633995c95d77f35fb17f89ff44990bc4/src/datasets/builder.py#L512\n\n### Steps to reproduce the bug\n\nrun: \r\n```python\r\nload_dataset(\"ted_talks_iwslt\", language_pair=(\"ja\", \"en\"), year=\"2015\")\r\n```\n\n### Expected behavior\n\nLoad the data without error\n\n### Environment info\n\ndatasets 2.16.0\r\n<end_of_turn>\n<start_of_turn>model\nted_talks_iwslt | Error: Config name is missing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease v3.1.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix decoding tests<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nSupporting FFCV, https://github.com/libffcv/ffcv\n\n### Motivation\n\nAccording to the benchmark, FFCV seems to be fastest image loading method.\n\n### Your contribution\n\nno<end_of_turn>\n<start_of_turn>model\nSupporting FFCV: Fast Forward Computer Vision<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix release instructions.\r\n\r\nDuring last release, I had to make this additional update.<end_of_turn>\n<start_of_turn>model\nFix release instructions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix empty dataset to_parquet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI have commonvoice 8.0.0 downloaded in `~/.cache/huggingface/datasets/mozilla-foundation___common_voice_8_0/en/8.0.0/b2f8b72f8f30b2e98c41ccf855954d9e35a5fa498c43332df198534ff9797a4a`.  The folder contains all the arrow files etc, and was used as the cached version last time I touched the ec2 instance I'm working on.  Now, with the same command that downloaded it initially:\r\n\r\n```\r\ndataset = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"en\", use_auth_token=\"<mytoken>\")\r\n```\r\n\r\nit tries to redownload the dataset to `~/.cache/huggingface/datasets/mozilla-foundation___common_voice_8_0/en/8.0.0/05bdc7940b0a336ceeaeef13470c89522c29a8e4494cbeece64fb472a87acb32`\r\n\r\n### Steps to reproduce the bug\r\n\r\nSteps to reproduce the behavior:\r\n\r\n1.  ```dataset = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"en\", use_auth_token=\"<mytoken>\")```\r\n2. dataset is updated by maintainers\r\n3. ```dataset = load_dataset(\"mozilla-foundation/common_voice_8_0\", \"en\", use_auth_token=\"<mytoken>\")```\r\n\r\n### Expected behavior\r\n\r\nI expect that it uses the already downloaded data in `~/.cache/huggingface/datasets/mozilla-foundation___common_voice_8_0/en/8.0.0/b2f8b72f8f30b2e98c41ccf855954d9e35a5fa498c43332df198534ff9797a4a`.\r\n\r\nNot sure what's happening in 2. but if, say it's an issue with the dataset referenced by \"mozilla-foundation/common_voice_8_0\" being modified by the maintainers, how would I force datasets to point to the original version I downloaded?\r\n\r\nEDIT:  It was indeed that the maintainers had updated the dataset (v 8.0.0).  However I still cant load the dataset from disk instead of redownloading, with for example:\r\n\r\n```\r\nload_dataset(\".cache/huggingface/datasets/downloads/extracted/<hash>/cv-corpus-8.0-2022-01-19/en/\", \"en\")\r\n\r\n> ...\r\n> File [~/miniconda3/envs/aa_torch2/lib/python3.10/site-packages/datasets/table.py:1938](.../ python3.10/site-packages/datasets/table.py:1938), in cast_array_to_feature(array, feature, allow_number_to_str)\r\n   1937 elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n-> 1938     return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n...\r\n   1794         e = e.__context__\r\n-> 1795     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1797 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n\r\n### Environment info\r\n\r\ndatasets==2.7.0\r\npython==3.10.8\r\nOS:  AWS Linux<end_of_turn>\n<start_of_turn>model\nCache not being used when loading commonvoice 8.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nadd an s to be consistent with pandas' on_bad_lines<end_of_turn>\n<start_of_turn>model\ntypo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nhttps://github.com/huggingface/datasets/blob/8a19de052e3d79f79cea26821454bbcf0e9dcd68/src/datasets/arrow_dataset.py#L3652C29-L3652C37\n\nHere we can see the writer is initialized on `i==0`. However, there can be cases where in the user mapping function, the first example is filtered out (length constraints, etc).\n\nIn this case, the writer would be a `None` type and the code will report `NoneType has no write function`.\n\nA simple fix is available, simply change line 3652 from `if i == 0:` to `if writer is None:`\n\n### Steps to reproduce the bug\n\nPrepare a dataset\n\nhave this function\n\n```\nimport datasets\n\n\ndef make_map_fn(split, max_prompt_tokens=3):\n    def process_fn(example, idx):\n\n        question = example['question']\n        reasoning_steps = example['reasoning_steps']\n        label = example['label']\n\n        answer_format = \"\"\n        for i in range(len(reasoning_steps)):\n            system_message = \"Dummy\"\n\n        all_steps_formatted = []\n\n        content = f\"\"\"Dummy\"\"\"\n\n        prompt = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": content},\n        ]\n        tokenized = tokenizer.apply_chat_template(prompt, return_tensors=\"pt\", truncation=False)\n        if tokenized.shape[1] > max_prompt_tokens:\n            return None  # skip overly long examples\n\n        data = {\n            \"dummy\": \"dummy\"\n        }\n\n        return data\n\n    return process_fn\n\n...\n# load your dataset\n...\ntrain = train.map(function=make_map_fn('train'), with_indices=True)\n```\n\n### Expected behavior\n\nThe dataset mapping shall behave even when the first example is filtered out.\n\n### Environment info\n\nI am using `datasets==3.6.0` but I have observed this issue in the github repo too: https://github.com/huggingface/datasets/blob/8a19de052e3d79f79cea26821454bbcf0e9dcd68/src/datasets/arrow_dataset.py#L3652C29-L3652C37<end_of_turn>\n<start_of_turn>model\nMapping function not working if the first example is returned as None<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReported on [Discord](https://discord.com/channels/879548962464493619/1144295822209581168/1144295822209581168)<end_of_turn>\n<start_of_turn>model\nFix import in `image_load` doc<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nchore: fix typos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix YAML error in README files appearing on GitHub.\r\n\r\nSee error message:\r\n![Screenshot from 2024-05-14 06-58-02](https://github.com/huggingface/datasets/assets/8515462/7984cc4e-96ee-4e83-99a4-4c0c5791fa05)\r\n\r\n\r\nFix #6897.<end_of_turn>\n<start_of_turn>model\nFix YAML error in README files appearing on GitHub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen I use `Dataset.from_generator(my_gen)` to load my dataset, it simply skips my changes to the generator function.\nI tried `datasets.disable_caching`, but it doesn't work!<end_of_turn>\n<start_of_turn>model\n`datasets.disable_caching` doesn't work<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, \n\na issue related to #4760 here when loading a single file from a dataset, unable to access it in offline mode afterwards\n\n### Steps to reproduce the bug\n\n```python\nimport os\n# os.environ[\"HF_HUB_OFFLINE\"] = \"1\"\nos.environ[\"HF_TOKEN\"] = \"xxxxxxxxxxxxxx\"\n\nimport datasets\n\ndataset_name = \"uonlp/CulturaX\"\ndata_files = \"fr/fr_part_00038.parquet\"\n\nds = datasets.load_dataset(dataset_name, split='train', data_files=data_files)\nprint(f\"Dataset loaded   : {ds}\")\n```\nOnce the file has been cached, I rerun with the HF_HUB_OFFLINE activated an get this error : \n```\nValueError: Couldn't find cache for uonlp/CulturaX for config 'default-1e725f978350254e'\nAvailable configs in the cache: ['default-2935e8cdcc21c613']\n```\n\n### Expected behavior\n\nShould be able to access the previously cached files\n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-5.4.0-215-generic-x86_64-with-glibc2.31\n- Python version: 3.12.0\n- `huggingface_hub` version: 0.27.0\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.3.1\n<end_of_turn>\n<start_of_turn>model\nIssue with offline mode and partial dataset cached<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello, everyone. I met a problem when I am trying to load a data file using load_dataset method on a Debian 10 system. The data file is not very large, only 1.63MB with 600 records. \r\nHere is my code:\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('json', data_files='mypath/oaast_rm_zh.json')\r\n\r\nI waited it for 20 minutes. It still no response. I cannot using Ctrl+C to cancel the command. I have to use Ctrl+Z to kill it. I also try it with a txt file, it still no response in a long time. \r\n\r\nI can load the same file successfully using my laptop (windows 10, python 3.8.5, datasets==2.14.5). I can also make it on another computer (Ubuntu 20.04.5 LTS, python 3.10.13, datasets 2.14.7). It only takes me 1-2 miniutes.\r\n\r\nCould you give me some suggestions? Thank you.\r\n\r\n\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('json', data_files='mypath/oaast_rm_zh.json')\n\n### Expected behavior\n\nI hope it can load the file successfully.\n\n### Environment info\n\nOS: Debian GNU/Linux 10\r\nPython: Python 3.10.13\r\nPip list:\r\nPackage                   Version\r\n------------------------- ------------\r\naccelerate                0.25.0\r\naddict                    2.4.0\r\naiofiles                  23.2.1\r\naiohttp                   3.9.1\r\naiosignal                 1.3.1\r\naliyun-python-sdk-core    2.14.0\r\naliyun-python-sdk-kms     2.16.2\r\naltair                    5.2.0\r\nannotated-types           0.6.0\r\nanyio                     3.7.1\r\nasync-timeout             4.0.3\r\nattrs                     23.1.0\r\ncertifi                   2023.11.17\r\ncffi                      1.16.0\r\ncharset-normalizer        3.3.2\r\nclick                     8.1.7\r\ncontourpy                 1.2.0\r\ncrcmod                    1.7\r\ncryptography              41.0.7\r\ncycler                    0.12.1\r\ndatasets                  2.14.7\r\ndill                      0.3.7\r\ndocstring-parser          0.15\r\neinops                    0.7.0\r\nexceptiongroup            1.2.0\r\nfastapi                   0.105.0\r\nffmpy                     0.3.1\r\nfilelock                  3.13.1\r\nfonttools                 4.46.0\r\nfrozenlist                1.4.1\r\nfsspec                    2023.10.0\r\ngast                      0.5.4\r\ngradio                    3.50.2\r\ngradio_client             0.6.1\r\nh11                       0.14.0\r\nhttpcore                  1.0.2\r\nhttpx                     0.25.2\r\nhuggingface-hub           0.19.4\r\nidna                      3.6\r\nimportlib-metadata        7.0.0\r\nimportlib-resources       6.1.1\r\njieba                     0.42.1\r\nJinja2                    3.1.2\r\njmespath                  0.10.0\r\njoblib                    1.3.2\r\njsonschema                4.20.0\r\njsonschema-specifications 2023.11.2\r\nkiwisolver                1.4.5\r\nmarkdown-it-py            3.0.0\r\nMarkupSafe                2.1.3\r\nmatplotlib                3.8.2\r\nmdurl                     0.1.2\r\nmodelscope                1.10.0\r\nmpmath                    1.3.0\r\nmultidict                 6.0.4\r\nmultiprocess              0.70.15\r\nnetworkx                  3.2.1\r\nnltk                      3.8.1\r\nnumpy                     1.26.2\r\nnvidia-cublas-cu12        12.1.3.1\r\nnvidia-cuda-cupti-cu12    12.1.105\r\nnvidia-cuda-nvrtc-cu12    12.1.105\r\nnvidia-cuda-runtime-cu12  12.1.105\r\nnvidia-cudnn-cu12         8.9.2.26\r\nnvidia-cufft-cu12         11.0.2.54\r\nnvidia-curand-cu12        10.3.2.106\r\nnvidia-cusolver-cu12      11.4.5.107\r\nnvidia-cusparse-cu12      12.1.0.106\r\nnvidia-nccl-cu12          2.18.1\r\nnvidia-nvjitlink-cu12     12.3.101\r\nnvidia-nvtx-cu12          12.1.105\r\norjson                    3.9.10\r\noss2                      2.18.3\r\npackaging                 23.2\r\npandas                    2.1.4\r\npeft                      0.7.1\r\nPillow                    10.1.0\r\npip                       23.3.1\r\nplatformdirs              4.1.0\r\nprotobuf                  4.25.1\r\npsutil                    5.9.6\r\npyarrow                   14.0.1\r\npyarrow-hotfix            0.6\r\npycparser                 2.21\r\npycryptodome              3.19.0\r\npydantic                  2.5.2\r\npydantic_core             2.14.5\r\npydub                     0.25.1\r\nPygments                  2.17.2\r\npyparsing                 3.1.1\r\npython-dateutil           2.8.2\r\npython-multipart          0.0.6\r\npytz                      2023.3.post1\r\nPyYAML                    6.0.1\r\nreferencing               0.32.0\r\nregex                     2023.10.3\r\nrequests                  2.31.0\r\nrich                      13.7.0\r\nrouge-chinese             1.0.3\r\nrpds-py                   0.13.2\r\nsafetensors               0.4.1\r\nscipy                     1.11.4\r\nsemantic-version          2.10.0\r\nsentencepiece             0.1.99\r\nsetuptools                68.2.2\r\nshtab                     1.6.5\r\nsimplejson                3.19.2\r\nsix                       1.16.0\r\nsniffio                   1.3.0\r\nsortedcontainers          2.4.0\r\nsse-starlette             1.8.2\r\nstarlette                 0.27.0\r\nsympy                     1.12\r\ntiktoken                  0.5.2\r\ntokenizers                0.15.0\r\ntomli                     2.0.1\r\ntoolz                     0.12.0\r\ntorch                     2.1.2\r\ntqdm                      4.66.1\r\ntransformers              4.36.1\r\ntriton                    2.1.0\r\ntrl                       0.7.4\r\ntyping_extensions         4.9.0\r\ntyro                      0.6.0\r\ntzdata                    2023.3\r\nurllib3                   2.1.0\r\nuvicorn                   0.24.0.post1\r\nwebsockets                11.0.3\r\nwheel                     0.41.2\r\nxxhash                    3.4.1\r\nyapf                      0.40.2\r\nyarl                      1.9.4\r\nzipp                      3.17.0\r\n\r\n<end_of_turn>\n<start_of_turn>model\nGot stuck when I trying to load a dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHave the possibility to do `ds.push_to_hub(..., append=True)`.\n\n### Motivation\n\nRequested in this [comment](https://huggingface.co/datasets/laion/dalle-3-dataset/discussions/3#65252597c4edc168202a5eaa) and \r\nthis [comment](https://huggingface.co/datasets/laion/dalle-3-dataset/discussions/4#6524f675c9607bdffb208d8f). Discussed internally on [slack](https://huggingface.slack.com/archives/C02EMARJ65P/p1696950642610639?thread_ts=1690554266.830949&cid=C02EMARJ65P).\r\n\n\n### Your contribution\n\nWhat I suggest to do for parquet datasets is to use `CommitOperationCopy` + `CommitOperationDelete` from `huggingface_hub`:\r\n1. list files\r\n2. copy files from parquet-0001-of-0004 to parquet-0001-of-0005\r\n3. delete files like parquet-0001-of-0004\r\n4. generate + add last parquet file parquet-0005-of-0005\r\n\r\n=> make a single commit with all commit operations at once\r\n\r\n\r\nI think it should be quite straightforward to implement. Happy to review a PR (maybe conflicting with the ongoing \"1 commit push_to_hub\" PR https://github.com/huggingface/datasets/pull/6269)<end_of_turn>\n<start_of_turn>model\nIncremental dataset (e.g. `.push_to_hub(..., append=True)`)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n```python\r\nfrom datasets import load_dataset\r\n\r\non_bad_file = \"error\"  # default\r\n# on_bad_file = \"warn\"  # warn and skip bad file\r\n# on_bad_file = \"skip\"  # skip bad file\r\nds = load_dataset(parquet_dataset_id, on_bad_file=on_bad_file)\r\n```<end_of_turn>\n<start_of_turn>model\nParquet: add `on_bad_file` argument to error/warn/skip bad files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMark tests that require `librosa`.\r\n\r\nNote that `librosa` is an optional dependency (installed with `audio` option) and we should be able to test environments without that library installed. This is the case if we want to test Numpy 2.0, which is currently incompatible with `librosa` due to its dependency on `soxr`:\r\n- https://github.com/dofuuz/python-soxr/issues/28<end_of_turn>\n<start_of_turn>model\nMark tests that require librosa<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI have noticed that in some cases the split order is not preserved.\r\n\r\nFor example, consider a no-script dataset with configs:\r\n```yaml\r\nconfigs:\r\n- config_name: default\r\n  data_files:\r\n  - split: train\r\n    path: train.csv\r\n  - split: test\r\n    path: test.csv\r\n```\r\n- Note the defined split order is [train, test]\r\n\r\nOnce the dataset is loaded, the split order is not preserved:\r\n```python\r\nIn [16]: ds\r\nOut[16]: \r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 1\r\n    })\r\n    train: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n- Note the obtained split order is [test, train]<end_of_turn>\n<start_of_turn>model\nSplit order is not preserved<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nBefore the fix, `get_data_patterns` inferred wrongly the split name for paths with the word \"data\" twice:\r\n- For the URL path: `hf://datasets/piuba-bigdata/articles_and_comments@f328d536425ae8fcac5d098c8408f437bffdd357/data/train-00001-of-00009.parquet` (note the org name `piuba-bigdata/` ending with `data/`)\r\n- The inferred split name was: `articles_and_comments@f328d536425ae8fcac5d098c8408f437bffdd357/data/train` instead of `train`\r\n\r\nThis PR fixes this issue by passing the `base_path` (`hf://datasets/piuba-bigdata/articles_and_comments@f328d536425ae8fcac5d098c8408f437bffdd357`) to `_get_data_files_patterns` and prepending it to the regex split pattern (`data/{split}-[0-9][0-9][0-9][0-9][0-9]-of-[0-9][0-9][0-9][0-9][0-9].*\\\\..*`).\r\n\r\nFix #6305.\r\nFix https://huggingface.co/datasets/piuba-bigdata/articles_and_comments/discussions/1<end_of_turn>\n<start_of_turn>model\nFix get_data_patterns for directories with the word data twice<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSince the release of `datasets` 2.14, private/gated datasets do not load when passing `token`: they raise  `EmptyDatasetError`.\r\n\r\nThis is a non-planned  backward incompatible breaking change.\r\n\r\nNote that private datasets do load if instead `download_config` is passed:\r\n```python\r\nfrom datasets import DownloadConfig, load_dataset\r\n\r\nds = load_dataset(\"albertvillanova/tmp-private\", split=\"train\", download_config=DownloadConfig(token=\"<MY-TOKEN>\"))\r\nds\r\n```\r\ngives\r\n```\r\nDataset({\r\n    features: ['text'],\r\n    num_rows: 4\r\n})\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"albertvillanova/tmp-private\", split=\"train\", token=\"<MY-TOKEN>\")\r\n```\r\ngives\r\n```\r\n---------------------------------------------------------------------------\r\nEmptyDatasetError                         Traceback (most recent call last)\r\n[<ipython-input-2-25b48732107a>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 ds = load_dataset(\"albertvillanova/tmp-private\", split=\"train\", token=\"<MY-TOKEN>\")\r\n\r\n5 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2107 \r\n   2108     # Create a dataset builder\r\n-> 2109     builder_instance = load_dataset_builder(\r\n   2110         path=path,\r\n   2111         name=name,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\r\n   1793         download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1794         download_config.storage_options.update(storage_options)\r\n-> 1795     dataset_module = dataset_module_factory(\r\n   1796         path,\r\n   1797         revision=revision,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1484                     raise ConnectionError(f\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\") from None\r\n   1485                 if isinstance(e1, EmptyDatasetError):\r\n-> 1486                     raise e1 from None\r\n   1487                 if isinstance(e1, FileNotFoundError):\r\n   1488                     raise FileNotFoundError(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1474                     download_config=download_config,\r\n   1475                     download_mode=download_mode,\r\n-> 1476                 ).get_module()\r\n   1477         except (\r\n   1478             Exception\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in get_module(self)\r\n   1030             sanitize_patterns(self.data_files)\r\n   1031             if self.data_files is not None\r\n-> 1032             else get_data_patterns(base_path, download_config=self.download_config)\r\n   1033         )\r\n   1034         data_files = DataFilesDict.from_patterns(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/data_files.py](https://localhost:8080/#) in get_data_patterns(base_path, download_config)\r\n    457         return _get_data_files_patterns(resolver)\r\n    458     except FileNotFoundError:\r\n--> 459         raise EmptyDatasetError(f\"The directory at {base_path} doesn't contain any data files\") from None\r\n    460 \r\n    461 \r\n\r\nEmptyDatasetError: The directory at hf://datasets/albertvillanova/tmp-private@79b9e4fe79670a9a050d6ebc385464891915a71d doesn't contain any data files\r\n```\n\n### Expected behavior\n\nThe dataset should load.\n\n### Environment info\n\n- `datasets` version: 2.14.3\r\n- Platform: Linux-5.15.109+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nPrivate datasets do not load when passing token<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThis bug generates when I use torch.compile(model) in my code, which seems to raise an error in datasets lib.\n\n### Steps to reproduce the bug\n\nI use the following code to fine-tune Falcon on my private dataset.\r\n\r\n```python\r\nimport transformers\r\nfrom transformers import (\r\n    AutoModelForCausalLM, \r\n    AutoTokenizer,\r\n    AutoConfig,\r\n    DataCollatorForSeq2Seq,\r\n    Trainer,\r\n    Seq2SeqTrainer,\r\n    HfArgumentParser,\r\n    Seq2SeqTrainingArguments,\r\n    BitsAndBytesConfig,\r\n)\r\n\r\nfrom peft import (\r\n    LoraConfig,\r\n    get_peft_model,\r\n    get_peft_model_state_dict,\r\n    prepare_model_for_int8_training,\r\n    set_peft_model_state_dict,\r\n)\r\n\r\nimport torch\r\nimport os\r\nimport evaluate\r\nimport functools\r\nfrom datasets import load_dataset\r\nimport bitsandbytes as bnb\r\nimport logging\r\nimport json\r\nimport copy\r\nfrom typing import Dict, Optional, Sequence\r\nfrom dataclasses import dataclass, field\r\n\r\n\r\n# Lora settings\r\nLORA_R = 8\r\nLORA_ALPHA = 16\r\nLORA_DROPOUT= 0.05\r\nLORA_TARGET_MODULES = [\"query_key_value\"]\r\n\r\n\r\n@dataclass\r\nclass ModelArguments:\r\n    model_name_or_path: Optional[str] = field(default=\"Salesforce/codegen2-7B\")\r\n\r\n\r\n@dataclass\r\nclass DataArguments:\r\n    data_path: str = field(default=None, metadata={\"help\": \"Path to the training data.\"})\r\n    train_file: str = field(default=None, metadata={\"help\": \"Path to the evaluation data.\"})\r\n    eval_file: str = field(default=None, metadata={\"help\": \"Path to the evaluation data.\"})\r\n    cache_path: str = field(default=None, metadata={\"help\": \"Path to the cache directory.\"})\r\n    num_proc: int = field(default=4, metadata={\"help\": \"Number of processes to use for data preprocessing.\"})\r\n\r\n\r\n@dataclass\r\nclass TrainingArguments(transformers.TrainingArguments):\r\n    # cache_dir: Optional[str] = field(default=None)\r\n    optim: str = field(default=\"adamw_torch\")\r\n    model_max_length: int = field(\r\n        default=512,\r\n        metadata={\"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"},\r\n    )\r\n    is_lora: bool = field(default=True, metadata={\"help\": \"Whether to use LORA.\"})\r\n\r\n\r\ndef tokenize(text, tokenizer, max_seq_len=512, add_eos_token=True):\r\n    result = tokenizer(\r\n        text,\r\n        truncation=True,\r\n        max_length=max_seq_len,\r\n        padding=False,\r\n        return_tensors=None,\r\n    )\r\n    if (\r\n        result[\"input_ids\"][-1] != tokenizer.eos_token_id\r\n        and len(result[\"input_ids\"]) < max_seq_len\r\n        and add_eos_token\r\n    ):\r\n        result[\"input_ids\"].append(tokenizer.eos_token_id)\r\n        result[\"attention_mask\"].append(1)\r\n\r\n    if add_eos_token and len(result[\"input_ids\"]) >= max_seq_len:\r\n        result[\"input_ids\"][max_seq_len - 1] = tokenizer.eos_token_id\r\n        result[\"attention_mask\"][max_seq_len - 1] = 1\r\n\r\n    result[\"labels\"] = result[\"input_ids\"].copy()\r\n    return result\r\n\r\n\r\ndef main():\r\n    parser = HfArgumentParser((ModelArguments, DataArguments, TrainingArguments))\r\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\r\n\r\n    config = AutoConfig.from_pretrained(\r\n        model_args.model_name_or_path, \r\n        cache_dir=data_args.cache_path,\r\n        trust_remote_code=True,\r\n    )\r\n\r\n    if training_args.is_lora:\r\n        model = AutoModelForCausalLM.from_pretrained(\r\n            model_args.model_name_or_path,\r\n            cache_dir=data_args.cache_path,\r\n            torch_dtype=torch.float16,\r\n            trust_remote_code=True,\r\n            load_in_8bit=True,\r\n            quantization_config=BitsAndBytesConfig(\r\n                load_in_8bit=True,\r\n                llm_int8_threshold=6.0\r\n            ),\r\n        )\r\n        model = prepare_model_for_int8_training(model)\r\n\r\n        config = LoraConfig(\r\n            r=LORA_R,\r\n            lora_alpha=LORA_ALPHA,\r\n            target_modules=LORA_TARGET_MODULES,\r\n            lora_dropout=LORA_DROPOUT,\r\n            bias=\"none\",\r\n            task_type=\"CAUSAL_LM\",\r\n        )\r\n        model = get_peft_model(model, config)\r\n    else:\r\n        model = AutoModelForCausalLM.from_pretrained(\r\n            model_args.model_name_or_path,\r\n            torch_dtype=torch.float16,\r\n            cache_dir=data_args.cache_path,\r\n            trust_remote_code=True,\r\n        )\r\n    model.config.use_cache = False\r\n    \r\n    def print_trainable_parameters(model):\r\n        \"\"\"\r\n        Prints the number of trainable parameters in the model.\r\n        \"\"\"\r\n        trainable_params = 0\r\n        all_param = 0\r\n        for _, param in model.named_parameters():\r\n            all_param += param.numel()\r\n            if param.requires_grad:\r\n                trainable_params += param.numel()\r\n        print(\r\n            f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\r\n        )\r\n    print_trainable_parameters(model)\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained(\r\n        model_args.model_name_or_path,\r\n        cache_dir=data_args.cache_path,\r\n        model_max_length=training_args.model_max_length,\r\n        padding_side=\"left\",\r\n        use_fast=True,\r\n        trust_remote_code=True,\r\n    )\r\n    tokenizer.pad_token = tokenizer.eos_token\r\n    \r\n    # Load dataset\r\n\r\n    def generate_and_tokenize_prompt(sample):\r\n        input_text = sample[\"input\"]\r\n        target_text = sample[\"output\"] + tokenizer.eos_token\r\n        full_text = input_text + target_text\r\n        tokenized_full_text = tokenize(full_text, tokenizer, max_seq_len=512)\r\n        tokenized_input_text = tokenize(input_text, tokenizer, max_seq_len=512)\r\n        input_len = len(tokenized_input_text[\"input_ids\"]) - 1 # -1 for eos token\r\n        tokenized_full_text[\"labels\"] = [-100] * input_len + tokenized_full_text[\"labels\"][input_len:]\r\n        return tokenized_full_text\r\n\r\n    data_files = {}\r\n    if data_args.train_file is not None:\r\n        data_files[\"train\"] = data_args.train_file\r\n    if data_args.eval_file is not None:\r\n        data_files[\"eval\"] = data_args.eval_file\r\n    \r\n    dataset = load_dataset(data_args.data_path, data_files=data_files)\r\n    train_dataset = dataset[\"train\"]\r\n    eval_dataset = dataset[\"eval\"]\r\n    train_dataset = train_dataset.map(generate_and_tokenize_prompt, num_proc=data_args.num_proc)\r\n    eval_dataset = eval_dataset.map(generate_and_tokenize_prompt, num_proc=data_args.num_proc)\r\n    data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True)\r\n\r\n    # Evaluation metrics\r\n    def compute_metrics(eval_preds, tokenizer):\r\n        metric = evaluate.load('exact_match')\r\n        preds, labels = eval_preds\r\n        # In case the model returns more than the prediction logits\r\n        if isinstance(preds, tuple):\r\n            preds = preds[0]\r\n\r\n        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True, clean_up_tokenization_spaces=False)\r\n\r\n        # Replace -100s in the labels as we can't decode them\r\n        labels[labels == -100] = tokenizer.pad_token_id\r\n        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True, clean_up_tokenization_spaces=False)\r\n\r\n        # Some simple post-processing\r\n        decoded_preds = [pred.strip() for pred in decoded_preds]\r\n        decoded_labels = [label.strip() for label in decoded_labels]\r\n\r\n        result = metric.compute(predictions=decoded_preds, references=decoded_labels)\r\n        return {'exact_match': result['exact_match']} \r\n    \r\n    compute_metrics_fn = functools.partial(compute_metrics, tokenizer=tokenizer)\r\n\r\n    model = torch.compile(model)\r\n\r\n    # Training\r\n    trainer = Trainer(\r\n        model=model, \r\n        train_dataset=train_dataset,\r\n        eval_dataset=eval_dataset,  \r\n        args=training_args,\r\n        data_collator=data_collator,\r\n        compute_metrics=compute_metrics_fn,\r\n    )\r\n    trainer.train()\r\n    trainer.save_state()\r\n    trainer.save_model(output_dir=training_args.output_dir)\r\n    tokenizer.save_pretrained(save_directory=training_args.output_dir)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nWhen I didn't use `torch.cpmpile(model)`, my code worked well. But when I added this line to my code, It produced the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"falcon_sft.py\", line 230, in <module>\r\n    main()\r\n  File \"falcon_sft.py\", line 223, in main\r\n    trainer.train()\r\n  File \"python3.10/site-packages/transformers/trainer.py\", line 1539, in train\r\n    return inner_training_loop(\r\n  File \"python3.10/site-packages/transformers/trainer.py\", line 1787, in _inner_training_loop\r\n    for step, inputs in enumerate(epoch_iterator):\r\n  File \"python3.10/site-packages/accelerate/data_loader.py\", line 384, in __iter__\r\n    current_batch = next(dataloader_iter)\r\n  File \"python3.10/site-packages/torch/utils/data/dataloader.py\", line 633, in __next__\r\n    data = self._next_data()\r\n  File \"python3.10/site-packages/torch/utils/data/dataloader.py\", line 677, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\r\n    data = self.dataset.__getitems__(possibly_batched_index)\r\n  File \"python3.10/site-packages/datasets/arrow_dataset.py\", line 2807, in __getitems__\r\n    batch = self.__getitem__(keys)\r\n  File \"python3.10/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\r\n    return self._getitem(key)\r\n  File \"python3.10/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\r\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n  File \"python3.10/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\r\n    _check_valid_index_key(key, size)\r\n  File \"python3.10/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\r\n    _check_valid_index_key(int(max(key)), size=size)\r\n  File \"python3.10/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\r\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\nIndexError: Invalid key: 88 is out of bounds for size 0\r\n```\r\n\r\nSo I'm confused about why this error was generated, and how to fix it. Is this error produced by datasets or `torch.compile`?\n\n### Expected behavior\n\nI want to use `torch.compile` in my code.\n\n### Environment info\n\n- `datasets` version: 2.14.3\r\n- Platform: Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nIndexError: Invalid key: 88 is out of bounds for size 0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDeprecate the Beam API and download from the HF GCS bucked. \r\n\r\nTODO:\r\n- [x] Convert the Beam-based [`wikipedia`](https://huggingface.co/datasets/wikipedia) to an Arrow-based dataset ([Hub PR](https://huggingface.co/datasets/wikipedia/discussions/19))\r\n- [x] Make [`natural_questions`](https://huggingface.co/datasets/natural_questions) a no-code dataset ([Hub PR](https://huggingface.co/datasets/natural_questions/discussions/7))\r\n- [x] Make [`wiki40b`](https://huggingface.co/datasets/wiki40b) a no-code dataset ([Hub PR](https://huggingface.co/datasets/wiki40b/discussions/5))\r\n- [x] Make [`wiki_dpr`](https://huggingface.co/datasets/wiki_dpr) an Arrow-based dataset ([Hub PR](https://huggingface.co/datasets/wiki_dpr/discussions/14))<end_of_turn>\n<start_of_turn>model\nDeprecate Beam API and download from HF GCS bucket<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPreserve order of configs and splits, as defined in dataset infos.\r\n\r\nFix #6521.<end_of_turn>\n<start_of_turn>model\nPreserve order of configs and splits when using Parquet exports<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently `datasets` does not correctly indicate to the Python type-checker (e.g. `pyright` / `Pylance`) that the output of `with_format` is compatible with PyTorch's `Dataloader` since it does not indicate that the HuggingFace `Dataset` is compatible with the PyTorch `Dataset` class. It would be great if we could get the typing to work nicely.\n\n### Motivation\n\nTo avoid casting types in our Python code.\n\n### Your contribution\n\nI would be happy to contribute a PR if this is something that may be accepted and could work with the current approach.\nThis doesn't have to be for just PyTorch, but I imagine that the same thing would be useful for `tensorflow` and such, but we only have a need for PyTorch at this stage.<end_of_turn>\n<start_of_turn>model\nMake `with_format` correctly indicate that a `Dataset` is compatible with PyTorch's `Dataset` class<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nwe're not using the benchmarks anymore + they were not working anyway due to token permissions\r\n\r\nI keep the code in case we ever want to re-run the benchmark manually<end_of_turn>\n<start_of_turn>model\nfix ci benchmark<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI want to be able to overwrite/update/delete splits in my dataset. Currently the only way to do is to manually go into the dataset and delete the split. If I try to overwrite programmatically I end up in an error state and (somewhat) corrupting the dataset. Read below.\r\n\r\n**Current Behavior**\r\nWhen I push to an existing split I get this error:\r\n`ValueError: Split complexRoofLocation_01Apr2023_to_31May2023test already present` \r\nThis seems to suggest that the library doesn't support overwriting splits. \r\n\r\n**Potential Bug**\r\nWhat\u2019s strange is that datasets, despite the operation erroring out with the ValueError above, does, in fact, overwrite the split:\r\n\r\n`Pushing dataset shards to the dataset hub: 100% [.....................] 1/1 [00:00<00:00, 55.04it/s]` \r\nEven though you got an error message and your code fails, your dataset is now changed. That seems like a bug. Either don't change the dataset, or don't throw the error and allow the script to proceed. \r\n\r\nAdditional Bug\r\nWhile it overwrites the split, it doesn\u2019t overwrite the split\u2019s information. Because of this when you pull down the dataset you may end up getting a `NonMatchingSplitsSizesError` if the size of the dataset during the overwrite is different. For example, my original split had 5 rows, but on my overwrite, I only had 4. Then when I try to download the dataset, I get a `NonMatchingSplitsSizesError` because the dataset's data.json states there\u2019s 5 but only 4 exist in the split.\r\n\r\nExpected Behavior\r\nThis corrupts the dataset rendering it unusable (until you take manual intervention). Either the library should let the overwrite happen (which it does but should also update the metadata) or it shouldn\u2019t do anything.\n\n### Steps to reproduce the bug\n\n[Colab Notebook](https://colab.research.google.com/drive/1bqVkD06Ngs9MQNdSk_ygCG6y1UqXA4pC?usp=sharing)\r\n\n\n### Expected behavior\n\nThe split should be overwritten and I should be able to use the new version of the dataset without issue. \n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3\r\n<end_of_turn>\n<start_of_turn>model\nOverwriting Split overwrites data but not metadata, corrupting dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to get CLIP to fine-tuning with my code.\r\nWhen I tried to run it on multiple GPUs using accelerate, I encountered the following phenomenon.\r\n\r\n- Validation dataloader stuck in 2nd epoch only on multi-GPU\r\nSpecifically, when the \"for inputs in valid_loader:\" process is finished, it does not proceed to the next step. train_loader process is completed. Also, both train and valid are working correctly in the first epoch.\r\nThe accelerate command at that time is as follows.\r\n`accelerate launch --multi_gpu --num_processes=2 {script_name.py} {--arg1} {--arg2} ...`\r\n\r\n- This will not happen when single GPU is used.\r\n`CUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...`\r\n- Setting num_workers=0 in dataloader did not change the result.\n\n### Steps to reproduce the bug\n\n1. The codes for fine-tuning the regular CLIP were updated for accelerate.\r\n2. Run the code with the accelerate command as `accelerate launch --multi_gpu --num_processes=2 {script_name.py} {--arg1} {--arg2} ...` and the above problem will occur.\r\n3. CUDA_VISIBLE_DEVICES=\"0\" accelerate launch {script_name.py} --arg1 --arg2 ...` , it works fine.\n\n### Expected behavior\n\nIt Should end normally as if it was run on a single GPU.\n\n### Environment info\n\nSince `datasets-cli env` did not work, the environment is described below.\r\n\r\n- OS: Ubuntu 22.04 with Docker\r\n- Docker: 24.0.5, build ced0996\r\n- Python: 3.10.12\r\n- torch==2.0.1\r\n- accelerate==0.21.0\r\n- transformers==4.33.1<end_of_turn>\n<start_of_turn>model\nDataloader stuck on multiple GPUs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdds the StreamHandler (as `hfh` and `transformers` do) to the library's logger to log INFO messages and logs the messages about \"loading a cached result\" (and some other warnings) as INFO\r\n\r\n(Also removes the `leave=False` arg in the progress bars to be consistent with `hfh` and `transformers` - progress bars serve as an indicator that a result is not cached, so it makes more sense not to delete them)\r\n\r\nFix #2832, fix https://github.com/huggingface/datasets/issues/1948, fix https://github.com/huggingface/datasets/issues/5444<end_of_turn>\n<start_of_turn>model\nImprove logging<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n<img width=\"1001\" alt=\"Screenshot 2024-05-01 at 3 37 06 AM\" src=\"https://github.com/huggingface/datasets/assets/130903099/00fcf12c-fcc1-4749-8592-d263d4efcbcc\">\r\n\r\nAs you can see I logged in to my account and the write token is valid.\r\n\r\nBut I can't upload on my main account and I am getting that error. It was okay on my test account at first try.\r\n\r\n(I refreshed the token, tried a new token but still doesn't work)\n\n### Steps to reproduce the bug\n\n1. I loaded a dataset.\r\n\r\n2. I logged in using both cli and huggingface_hub\r\n\r\n3. I pushed to my down dataset\r\n(It went well without any issues on my test account)\n\n### Expected behavior\n\nIt should have gone smoothly and this is not even my first time uploading to huggingface datasets\n\n### Environment info\n\ncolab, dataset (tried multiple versions)<end_of_turn>\n<start_of_turn>model\nWrite token isn't working while pushing to datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nArrow has a very useful dictionary/categorical type (https://arrow.apache.org/docs/python/generated/pyarrow.dictionary.html). This data type has significant speed, memory and disk benefits over pa.string() when there are only a few unique text strings in a column.\r\n\r\nUnfortunately, huggingface datasets currently does not support this type. So huggingface datasets cannot natively read many parquet files that use this datatype .This PR adds support for Huggingface Datasets to read categorical/dictionary data.\r\n\r\nNote: This PR functions by simply converting those dictionary/categorical types to strings. This means that huggingface datasets cannot take advantage of the compute benefits of categoricals, but it significantly simplifies logic. At this time, I do not think it makes sense to optimize categorical support within huggingface datasets and that we should only try to optimize later, if necessary.\r\n\r\nCloses #5706<end_of_turn>\n<start_of_turn>model\nAdd support for categorical/dictionary types<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to https://github.com/huggingface/huggingface_hub/issues/3340.\r\n\r\nThis PR adapts `datasets` to be compatible with both huggingface_hub v0.x and v1.x. \r\n\r\nIn practice nothing else should change (I've checked the codebase). The `HfHubHTTPError` is a base error defined in `huggingface_hub` that inherits from `requests.HTTPError` in v0.x and will inherit from `httpx.HTTPError` in v1.x. It has been introduced ~2 years ago so it's fine to use it right now (i.e. no need to wait for v1.x release or bump minimal version).\r\n\r\nMost of the changes have been around the test suite to make sure that tests are passing with both `requests` and `httpx` backends. Mid-term it would be good to completely remove the `requests` dependency from `datasets` but that's an orthogonal topic. <end_of_turn>\n<start_of_turn>model\nSupport huggingface_hub v0.x and v1.x<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollowing [documentation](https://huggingface.co/docs/datasets/repository_structure#define-your-splits-and-subsets-in-yaml) I had defined different configs for [`Dataception`](https://huggingface.co/datasets/bigdata-pw/Dataception), a dataset of datasets:\r\n\r\n```yaml\r\nconfigs:\r\n- config_name: dataception\r\n  data_files:\r\n  - path: dataception.parquet\r\n    split: train\r\n  default: true\r\n- config_name: dataset_5423\r\n  data_files:\r\n  - path: datasets/5423.tar\r\n    split: train\r\n...\r\n- config_name: dataset_721736\r\n  data_files:\r\n  - path: datasets/721736.tar\r\n    split: train\r\n```\r\n\r\nThe intent was for metadata to be browsable via Dataset Viewer, in addition to each individual dataset, and to allow datasets to be loaded by specifying the config/name to `load_dataset`.\r\n\r\nWhile testing `load_dataset` I encountered the following error:\r\n\r\n```python\r\n>>> dataset = load_dataset(\"bigdata-pw/Dataception\", \"dataset_7691\")\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 467k/467k [00:00<00:00, 1.99MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 71.0M/71.0M [00:02<00:00, 26.8MB/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"datasets\\load.py\", line 2145, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"datasets\\builder.py\", line 1027, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"datasets\\builder.py\", line 1100, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"datasets\\packaged_modules\\parquet\\parquet.py\", line 58, in _split_generators\r\n    self.info.features = datasets.Features.from_arrow_schema(pq.read_schema(f))\r\n                                                             ^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow\\parquet\\core.py\", line 2325, in read_schema\r\n    file = ParquetFile(\r\n           ^^^^^^^^^^^^\r\n  File \"pyarrow\\parquet\\core.py\", line 318, in __init__\r\n    self.reader.open(\r\n  File \"pyarrow\\_parquet.pyx\", line 1470, in pyarrow._parquet.ParquetReader.open\r\n  File \"pyarrow\\error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Parquet magic bytes not found in footer. Either the file is corrupted or this is not a parquet file.\r\n```\r\n\r\nThe correct file is downloaded, however the incorrect builder type is detected; `parquet` due to other content of the repository. It would appear that the config needs to be taken into account.\r\n\r\nNote that I have removed the additional configs from the repository because of this issue and there is a limit of 3000 configs anyway so the Dataset Viewer doesn't work as I intended. I'll add them back in if it assists with testing.\r\n\r\n<end_of_turn>\n<start_of_turn>model\n`load_dataset` from Hub with `name` to specify `config` using incorrect builder type when multiple data formats are present<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMinor fix.<end_of_turn>\n<start_of_turn>model\nFix minor typo in docs loading.mdx<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCorrects an issue where `self._ex_iterable` was erroneously used instead of `ex_iterable`, when both Distributed Data Parallel (DDP) and multi num_worker are used concurrently. This improper usage led to the generation of incorrect `shards_indices`, subsequently causing issues with the control flow responsible for worker creation. The fix ensures the appropriate iterable is used, thus providing a more accurate determination of whether a new worker should be instantiated or not.<end_of_turn>\n<start_of_turn>model\nFix for Incorrect ex_iterable used with multi num_worker<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfollowing https://github.com/huggingface/datasets/pull/5944<end_of_turn>\n<start_of_turn>model\nAdd Arrow builder docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfor the viewer, this way it can use Video(decode=False) and doesn't need decord (which causes segfaults)<end_of_turn>\n<start_of_turn>model\nAllow video with disabeld decoding without decord<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I load_dataset with num_proc > 0 in a deamon process, I got an error:\r\n\r\n```python\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/download/download_manager.py\", line 564, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/download/download_manager.py\", line 427, in download\r\n    downloaded_path_or_paths = map_nested(\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/utils/py_utils.py\", line 468, in map_nested\r\n    mapped = parallel_map(function, iterable, num_proc, types, disable_tqdm, desc, _single_map_nested)\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/utils/experimental.py\", line 40, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/parallel/parallel.py\", line 34, in parallel_map\r\n    return _map_with_multiprocessing_pool(\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/Work/datasets/src/datasets/parallel/parallel.py\", line 64, in _map_with_multiprocessing_pool\r\n    with Pool(num_proc, initargs=initargs, initializer=initializer) as pool:\r\n      ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/context.py\", line 119, in Pool\r\n    return Pool(processes, initializer, initargs, maxtasksperchild,\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/pool.py\", line 215, in __init__\r\n    self._repopulate_pool()\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/pool.py\", line 306, in _repopulate_pool\r\n    return self._repopulate_pool_static(self._ctx, self.Process,\r\n    ^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/pool.py\", line 329, in _repopulate_pool_static\r\n    w.start()\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/multiprocessing/process.py\", line 118, in start\r\n    assert not _current_process._config.get('daemon'),     ^^^^^^^^^^^^^^^^^\r\nAssertionError: daemonic processes are not allowed to have children\r\n```\r\n\r\nThe download is io-intensive computing, may be datasets can replece the multi processing pool by a multi threading pool if in a deamon process.\n\n### Steps to reproduce the bug\n\n1. start a deamon process\r\n2. run load_dataset with num_proc > 0\n\n### Expected behavior\n\nNo error.\n\n### Environment info\n\nPython 3.11.4\r\ndatasets latest master<end_of_turn>\n<start_of_turn>model\nAssertionError: daemonic processes are not allowed to have children<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen you try to download a dataset that uses [biopython](https://github.com/biopython/biopython), like `load_dataset(\"InstaDeepAI/multi_species_genomes\")`, you get the error:\r\n\r\n```\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"InstaDeepAI/multi_species_genomes\")\r\n/home/j.vangoey/.pyenv/versions/multi_species_genomes/lib/python3.10/site-packages/datasets/load.py:1454: FutureWarning: The repository for InstaDeepAI/multi_species_genomes contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/InstaDeepAI/multi_species_genomes\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.51k/7.51k [00:00<00:00, 7.67MB/s]\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 17.2k/17.2k [00:00<00:00, 11.0MB/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/j.vangoey/.pyenv/versions/multi_species_genomes/lib/python3.10/site-packages/datasets/load.py\", line 2548, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/j.vangoey/.pyenv/versions/multi_species_genomes/lib/python3.10/site-packages/datasets/load.py\", line 2220, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/j.vangoey/.pyenv/versions/multi_species_genomes/lib/python3.10/site-packages/datasets/load.py\", line 1871, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/home/j.vangoey/.pyenv/versions/multi_species_genomes/lib/python3.10/site-packages/datasets/load.py\", line 1844, in dataset_module_factory\r\n    ).get_module()\r\n  File \"/home/j.vangoey/.pyenv/versions/multi_species_genomes/lib/python3.10/site-packages/datasets/load.py\", line 1466, in get_module\r\n    local_imports = _download_additional_modules(\r\n  File \"/home/j.vangoey/.pyenv/versions/multi_species_genomes/lib/python3.10/site-packages/datasets/load.py\", line 346, in _download_additional_modules\r\n    raise ImportError(\r\nImportError: To be able to use InstaDeepAI/multi_species_genomes, you need to install the following dependency: Bio.\r\nPlease install it using 'pip install Bio' for instance.\r\n>>> \r\n```\r\n\r\n`Bio` comes from the `biopython` package that can be installed with `pip install biopython`, not with  `pip install Bio` as suggested.\r\n\r\n\r\nThis PR adds special logic to show the correct package name in the error message of ` _download_additional_modules`, similarly as is done for `sklearn` / `scikit-learn`  already.\r\n\r\nThere are more packages where importable module name differs from the PyPI package name, so this could be made more generic, like:\r\n\r\n```\r\n# Mapping of importable module names to their PyPI package names\r\npackage_map = {\r\n    \"sklearn\": \"scikit-learn\",\r\n    \"Bio\": \"biopython\",\r\n    \"PIL\": \"Pillow\",\r\n    \"bs4\": \"beautifulsoup4\"\r\n}\r\n\r\nfor module_name, pypi_name in package_map.items():\r\n    if module_name in needs_to_be_installed.keys():\r\n        needs_to_be_installed[module_name] = pypi_name\r\n```<end_of_turn>\n<start_of_turn>model\nfix: show correct package name to install biopython<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n@lhoestq wondering if the team has thought about this and if there are any recommendations?\n\nCurrently when processing datasets some examples are bound to get filtered out, whether it's due to bad format, or length is too long, or any other custom filters that might be getting applied. Let's just focus on the filter by length for now, since that would be something that gets applied dynamically for each training run. Say we want to show a graph in W&B with the running total of the number of filtered examples so far.\n\nWhat would be a good way to go about hooking this up? Because the map/filter operations happen before the DataLoader batches are created, at training time if we're just grabbing batches from the DataLoader then we won't know how many things have been filtered already. But there's not really a good way to include a 'num_filtered' key into the dataset itself either because dataset map/filter process examples independently and don't have a way to track a running sum.\n\nThe only approach I can kind of think of is having a 'is_filtered' key in the dataset, and then creating a custom batcher/collator that reads that and tracks the metric?<end_of_turn>\n<start_of_turn>model\nGetting statistics about filtered examples<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nUsing .set_format(\"torch\") leads to images having shape (H, W, C), the same as in numpy. \r\nHowever, pytorch normally uses (C, H, W) format.\r\n\r\nMaybe I'm missing something but this makes the format a lot less useful as I then have to permute it anyways.\r\nIf not using the format it is possible to directly use torchvision transforms but any non-transformed value will not be a tensor.\r\n\r\nIs there a reason for this choice?\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Dataset, Features, Audio, Image\r\nimages = [\"path/to/image.png\"] * 10\r\nfeatures = Features({\"image\": Image()})\r\nds = Dataset.from_dict({\"image\": images}, features=features) \r\nds = ds.with_format(\"torch\")\r\nds[0][\"image\"].shape\r\n```\r\n```python\r\ntorch.Size([512, 512, 4])\r\n```\r\n\r\n### Expected behavior\r\n\r\n```python\r\nfrom datasets import Dataset, Features, Audio, Image\r\nimages = [\"path/to/image.png\"] * 10\r\nfeatures = Features({\"image\": Image()})\r\nds = Dataset.from_dict({\"image\": images}, features=features) \r\nds = ds.with_format(\"torch\")\r\nds[0][\"image\"].shape\r\n```\r\n```python\r\ntorch.Size([4, 512, 512])\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-6.5.9-100.fc37.x86_64-x86_64-with-glibc2.31\r\n- Python version: 3.11.6\r\n- Huggingface_hub version: 0.18.0\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2<end_of_turn>\n<start_of_turn>model\nTorchFormatter images (H, W, C) instead of (C, H, W) format<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd `pyarrow-hotfix` to release docs.<end_of_turn>\n<start_of_turn>model\nAdd pyarrow-hotfix to release docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/6066<end_of_turn>\n<start_of_turn>model\nfix tqdm lock<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n[According](https://huggingface.co/MIT/ast-finetuned-speech-commands-v2) to `MIT/ast-finetuned-speech-commands-v2`, the model was trained on the Speech Commands v2 dataset. However, while the model config says the model should have 35 class labels, the dataset itself has 36 class labels. Moreover, the class labels themselves don't match between the model config and the dataset. It is difficult to reproduce the data used to fine tune `MIT/ast-finetuned-speech-commands-v2`.\n\n### Steps to reproduce the bug\n\n```\r\n>>> model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-speech-commands-v2\")\r\n>>> model.config.id2label\r\n{0: 'backward', 1: 'follow', 2: 'five', 3: 'bed', 4: 'zero', 5: 'on', 6: 'learn', 7: 'two', 8: 'house', 9: 'tree', 10: 'dog', 11: 'stop', 12: 'seven', 13: 'eight', 14: 'down', 15: 'six', 16: 'forward', 17: 'cat', 18: 'right', 19: 'visual', 20: 'four', 21: 'wow', 22: 'no', 23: 'nine', 24: 'off', 25: 'three', 26: 'left', 27: 'marvin', 28: 'yes', 29: 'up', 30: 'sheila', 31: 'happy', 32: 'bird', 33: 'go', 34: 'one'}\r\n\r\n>>> dataset = load_dataset(\"speech_commands\", \"v0.02\", split=\"test\")\r\n>>> torch.unique(torch.Tensor(dataset['label']))\r\ntensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13.,\r\n        14., 15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27.,\r\n        28., 29., 30., 31., 32., 33., 34., 35.])\r\n```\r\nIf you try to explore the [dataset itself](https://huggingface.co/datasets/speech_commands/viewer/v0.02/test), you can see that the id to label does not match what is provided by `model.config.id2label`.\r\n\n\n### Expected behavior\n\nThe labels should match completely and there should be the same number of label classes between the model config and the dataset itself.\n\n### Environment info\n\ndatasets = 2.14.6, transformers =  4.33.3<end_of_turn>\n<start_of_turn>model\nSpeech Commands v2 dataset doesn't match AST-v2 config<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAfter the recent update of `datasets`, Dataset.save_to_disk does not accept a pathlib.Path anymore. It was supported in 2.18.0 and previous versions. Is this intentional? Was it supported before only because of a Python dusk-typing miracle?\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import Dataset\r\nimport pathlib\r\n\r\npath = pathlib.Path(\"./my_out_path\")\r\nDataset.from_dict(\r\n    {\"text\": [\"hello world\"], \"label\": [777], \"split\": [\"train\"]}\r\n.save_to_disk(path)\r\n```\r\nThis results in an error when using datasets 2.19:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 3, in <module>\r\n  File \"/Users/jb/scratch/venv/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 1515, in save_to_disk\r\n    fs, _ = url_to_fs(dataset_path, **(storage_options or {}))\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jb/scratch/venv/lib/python3.11/site-packages/fsspec/core.py\", line 383, in url_to_fs\r\n    chain = _un_chain(url, kwargs)\r\n            ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/jb/scratch/venv/lib/python3.11/site-packages/fsspec/core.py\", line 323, in _un_chain\r\n    if \"::\" in path\r\n       ^^^^^^^^^^^^\r\nTypeError: argument of type 'PosixPath' is not iterable\r\n```\r\n\r\nConverting to str works, however.\r\n```\r\nDataset.from_dict(\r\n     {\"text\": [\"hello world\"], \"label\": [777], \"split\": [\"train\"]}\r\n).save_to_disk(str(path))\r\n```\n\n### Expected behavior\n\nMy dataset gets saved to disk without an error.\n\n### Environment info\n\naiohttp==3.9.5\r\naiosignal==1.3.1\r\nattrs==23.2.0\r\ncertifi==2024.2.2\r\ncharset-normalizer==3.3.2\r\ndatasets==2.19.0\r\ndill==0.3.8\r\nfilelock==3.14.0\r\nfrozenlist==1.4.1\r\nfsspec==2024.3.1\r\nhuggingface-hub==0.23.2\r\nidna==3.7\r\nmultidict==6.0.5\r\nmultiprocess==0.70.16\r\nnumpy==1.26.4\r\npackaging==24.0\r\npandas==2.2.2\r\npyarrow==16.1.0\r\npyarrow-hotfix==0.6\r\npython-dateutil==2.9.0.post0\r\npytz==2024.1\r\nPyYAML==6.0.1\r\nrequests==2.32.3\r\nsix==1.16.0\r\ntqdm==4.66.4\r\ntyping_extensions==4.12.0\r\ntzdata==2024.1\r\nurllib3==2.2.1\r\nxxhash==3.4.1\r\nyarl==1.9.4<end_of_turn>\n<start_of_turn>model\nSupport for pathlib.Path in datasets 2.19.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nAccessing audio dataset value throws `Format not recognised error`\r\n\r\n### Steps to reproduce the bug\r\n\r\n**code:**\r\n```py\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"fawazahmed0/bug-audio\")\r\n\r\nfor data in dataset[\"train\"]:\r\n    print(data)\r\n\r\n```\r\n\r\n**output:**\r\n```bash\r\n(mypy) C:\\Users\\Nawaz-Server\\Documents\\ml>python myest.py\r\n[C:\\vcpkg\\buildtrees\\mpg123\\src\\0d8db63f9b-3db975bc05.clean\\src\\libmpg123\\layer3.c:INT123_do_layer3():1801] error: dequantization failed!\r\n{'audio': {'path': 'C:\\\\Users\\\\Nawaz-Server\\\\.cache\\\\huggingface\\\\hub\\\\datasets--fawazahmed0--bug-audio\\\\snapshots\\\\fab1398431fed1c0a2a7bff0945465bab8b5daef\\\\data\\\\Ghamadi\\\\037135.mp3', 'array': array([ 0.00000000e+00, -2.86519935e-22, -2.56504911e-21, ...,\r\n       -1.94239747e-02, -2.42924765e-02, -2.99104657e-02]), 'sampling_rate': 22050}, 'reciter': 'Ghamadi', 'transcription': '\u0627\u0644\u0627 \u0639\u062c\u0648\u0632 \u0627 \u0641\u064a \u0627\u0644\u063a\u0628\u0631\u064a\u0646', 'line': 3923, 'chapter': 37, 'verse': 135, 'text': '\u0627\u0650\u0655\u0644\u064e\u0651\u0627 \u0639\u064e\u062c\u064f\u0648\u0632\u08f0 \u0627 \u0641\u0650\u064a \u0671\u0644\u06e1\u063a\u064e\u0670\u0628\u0650\u0631\u0650\u064a\u0646\u064e'}\r\nTraceback (most recent call last):\r\n  File \"C:\\Users\\Nawaz-Server\\Documents\\ml\\myest.py\", line 5, in <module>\r\n    for data in dataset[\"train\"]:\r\n                ~~~~~~~^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 2372, in __iter__\r\n    formatted_output = format_table(\r\n                       ^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 639, in format_table\r\n    return formatter(pa_table, query_type=query_type)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 403, in __call__\r\n    return self.format_row(pa_table)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 444, in format_row\r\n    row = self.python_features_decoder.decode_row(row)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\formatting\\formatting.py\", line 222, in decode_row\r\n    return self.features.decode_example(row) if self.features else row\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\features\\features.py\", line 2042, in decode_example\r\n    column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\features\\features.py\", line 1403, in decode_nested_example\r\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\datasets\\features\\audio.py\", line 184, in decode_example\r\n    array, sampling_rate = sf.read(f)\r\n                           ^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\soundfile.py\", line 285, in read\r\n    with SoundFile(file, 'r', samplerate, channels,\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\soundfile.py\", line 658, in __init__\r\n    self._file = self._open(file, mode_int, closefd)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"C:\\Users\\Nawaz-Server\\.conda\\envs\\mypy\\Lib\\site-packages\\soundfile.py\", line 1216, in _open\r\n    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\r\nsoundfile.LibsndfileError: Error opening <_io.BufferedReader name='C:\\\\Users\\\\Nawaz-Server\\\\.cache\\\\huggingface\\\\hub\\\\datasets--fawazahmed0--bug-audio\\\\snapshots\\\\fab1398431fed1c0a2a7bff0945465bab8b5daef\\\\data\\\\Ghamadi\\\\037136.mp3'>: Format not recognised.\r\n```\r\n\r\n### Expected behavior\r\n\r\nEverything should work fine, as loading the problematic audio file directly with soundfile package works fine\r\n\r\n**code:**\r\n```\r\nimport soundfile as sf\r\n\r\nprint(sf.read('C:\\\\Users\\\\Nawaz-Server\\\\.cache\\\\huggingface\\\\hub\\\\datasets--fawazahmed0--bug-audio\\\\snapshots\\\\fab1398431fed1c0a2a7bff0945465bab8b5daef\\\\data\\\\Ghamadi\\\\037136.mp3'))\r\n```\r\n\r\n**output:**\r\n```bash\r\n(mypy) C:\\Users\\Nawaz-Server\\Documents\\ml>python myest.py\r\n[C:\\vcpkg\\buildtrees\\mpg123\\src\\0d8db63f9b-3db975bc05.clean\\src\\libmpg123\\layer3.c:INT123_do_layer3():1801] error: dequantization failed!\r\n(array([ 0.00000000e+00, -8.43723821e-22, -2.45370628e-22, ...,\r\n       -7.71464454e-03, -6.90496899e-03, -8.63333419e-03]), 22050)\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.0.2\r\n- Platform: Windows-11-10.0.22621-SP0\r\n- Python version: 3.12.7\r\n- `huggingface_hub` version: 0.26.2\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.10.0\r\n- soundfile: 0.12.1<end_of_turn>\n<start_of_turn>model\nAccessing audio dataset value throws Format not recognised error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTo be used to train models it allows to load an IterableDataset from the cached Arrow file.  \r\nSee https://github.com/huggingface/datasets/issues/5481<end_of_turn>\n<start_of_turn>model\nLoad cached dataset as iterable <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfollowing https://github.com/huggingface/datasets/pull/7708<end_of_turn>\n<start_of_turn>model\nUpdate dataset_dict push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #2494<end_of_turn>\n<start_of_turn>model\nAdded specific use cases in Improve Performace<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn the documentation for [ClassLabel](https://huggingface.co/docs/datasets/v2.21.0/en/package_reference/main_classes#datasets.ClassLabel), there is an example of usage with the following code:\r\n\r\n````\r\nfrom datasets import Features\r\nfeatures = Features({'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'])})\r\nfeatures\r\n````\r\n\r\nwhich expects to output (as stated in the documentation):\r\n\r\n````\r\n{'label': ClassLabel(num_classes=3, names=['bad', 'ok', 'good'], id=None)}\r\n````\r\n\r\nbut it generates the following\r\n\r\n````\r\n{'label': ClassLabel(names=['bad', 'ok', 'good'], id=None)}\r\n````\r\n\r\nIf my understanding is correct, this happens because although num_classes is used during the init of the object, it is afterward ignored:\r\n\r\nhttps://github.com/huggingface/datasets/blob/be5cff059a2a5b89d7a97bc04739c4919ab8089f/src/datasets/features/features.py#L975\r\n\r\nI would like to work on this issue if this is something needed \ud83d\ude04\r\n<end_of_turn>\n<start_of_turn>model\nInconsistent output in documentation example: `num_classes` not displayed in `ClassLabel` output<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nMore Parquet streaming docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI want to download Common Voice 17.0 hy-AM but it returns an error. \r\n```\r\n\r\nThe version_base parameter is not specified.\r\nPlease specify a compatability version level, or None.\r\nWill assume defaults for version 1.1\r\n  @hydra.main(config_name='hfds_config', config_path=None)\r\n/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\r\nSee https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\r\n  ret = run_job(\r\n/usr/local/lib/python3.10/dist-packages/datasets/load.py:1429: FutureWarning: The repository for mozilla-foundation/common_voice_17_0 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mozilla-foundation/common_voice_17_0\r\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\r\nPassing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\r\n  warnings.warn(\r\nReading metadata...: 6180it [00:00, 133224.37it/s]les/s]\r\nGenerating train split: 0 examples [00:00, ? examples/s]\r\nHuggingFace datasets failed due to some reason (stack trace below).\r\nFor certain datasets (eg: MCV), it may be necessary to login to the huggingface-cli (via `huggingface-cli login`).\r\nOnce logged in, you need to set `use_auth_token=True` when calling this script.\r\n\r\nTraceback error for reference :\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1743, in _prepare_split_single\r\n    example = self.info.features.encode_example(record) if self.info.features is not None else record\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\", line 1878, in encode_example\r\n    return encode_nested_example(self, example)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\", line 1243, in encode_nested_example\r\n    {\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/features/features.py\", line 1243, in <dictcomp>\r\n    {\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 326, in zip_dict\r\n    yield key, tuple(d[key] for d in dicts)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py\", line 326, in <genexpr>\r\n    yield key, tuple(d[key] for d in dicts)\r\nKeyError: 'sentence_id'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/workspace/nemo/scripts/speech_recognition/convert_hf_dataset_to_nemo.py\", line 358, in main\r\n    dataset = load_dataset(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2549, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1005, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1767, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1100, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1605, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1762, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ncv_17 = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"hy-AM\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt works fine with common_voice_16_1\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.15.0-1042-nvidia-x86_64-with-glibc2.35\r\n- Python version: 3.11.6\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nCant Downlaod Common Voice 17.0 hy-AM <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix https://github.com/huggingface/datasets/issues/6675<end_of_turn>\n<start_of_turn>model\nAdd `mode` parameter to `Image` feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nWe need to speed up SDXL dataset pre-process. Please make it possible to use multiple GPUs for the [official SDXL trainer](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py) :)\r\n\r\n### Motivation\r\n\r\nPre-computing 3 million of images takes around 2 days.\r\nWould be nice to be able to be able to do multi-GPU (or even better \u2013 multi-GPU + multi-node) vae and embedding precompute...\r\n\r\n### Your contribution\r\n\r\nI'm not sure i can wrap my head around the multi-GPU mapping...\r\nPlus it's too expensive for me to take x2 A100 and spend a day just figuring out the staff since I don't have a job right now.<end_of_turn>\n<start_of_turn>model\nFeature request: Multi-GPU dataset mapping for SDXL training<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAfter waiting for 2 hours, it still presents ``The dataset viewer should be available soon. Please retry later.''\n\n### Steps to reproduce the bug\n\ndataset link: https://huggingface.co/datasets/BryanW/HI_EDIT\n\n### Expected behavior\n\nPresent the dataset viewer.\n\n### Environment info\n\nNA<end_of_turn>\n<start_of_turn>model\nThe dataset viewer should be available soon. Please retry later.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt was not taken into account e.g. when passing to a DataLoader with num_workers>0\r\n\r\nFix https://github.com/huggingface/datasets/issues/6565<end_of_turn>\n<start_of_turn>model\n[IterableDataset] Fix `drop_last_batch`in map after shuffling or sharding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWe build a dataset which contains several hdf5 files and write a script using `h5py` to generate the dataset. The hdf5 files are large and the processed dataset cache takes more disk space. So we hope to try streaming iterable dataset. Unfortunately, `h5py` can't convert a remote URL into a hdf5 file descriptor. So we use `fsspec` as an interface like below:\r\n\r\n```python\r\ndef _generate_examples(self, filepath, split):\r\n        for file in filepath:\r\n            with fsspec.open(file, \"rb\") as fs:\r\n                with h5py.File(fs, \"r\") as fp:\r\n                    # for event_id in sorted(list(fp.keys())):\r\n                    event_ids = list(fp.keys())\r\n                    ......\r\n```\n\n### Steps to reproduce the bug\n\nThe `fsspec` works, but it takes 10+ min to print the first 10 examples, which is even longer than the downloading time. I'm not sure if it just caches the whole hdf5 file and generates the examples.\n\n### Expected behavior\n\nSo does the following make sense so far?\r\n\r\n1. download the files\r\n```python\r\ndataset = datasets.load('path/to/myscripts', split=\"train\", name=\"event\", trust_remote_code=True)\r\n```\r\n\r\n2. load the iterable dataset faster (using the raw file cache at path `.cache/huggingface/datasets/downloads`)\r\n```python\r\ndataset = datasets.load('path/to/myscripts', split=\"train\", name=\"event\", trust_remote_code=True, streaming=true)\r\n```\r\n\r\nI made some tests, but the code above can't get the expected result. I'm not sure if this is supported. I also find the issue #6327 . It seemed similar to mine, but I couldn't find a solution.\n\n### Environment info\n\n- `datasets` = 2.18.0\r\n- `h5py` = 3.10.0\r\n- `fsspec` = 2023.10.0<end_of_turn>\n<start_of_turn>model\nload `streaming=True` dataset with downloaded cache<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix audio bytes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://github.com/huggingface/datasets-server/issues/2082#issuecomment-1805716557\r\n\r\nWe have the same error for:\r\n- https://huggingface.co/datasets/severo/a_dataset_that_does_not_exist: a dataset that does not exist\r\n- https://huggingface.co/datasets/severo/test_files_without_extension: a dataset with files without a known extension\r\n\r\n```\r\n>>> import datasets\r\n>>> datasets.get_dataset_config_names('severo/a_dataset_that_does_not_exist')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 351, in get_dataset_config_names\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1508, in dataset_module_factory\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at /home/slesage/hf/datasets-server/services/worker/severo/a_dataset_that_does_not_exist/a_dataset_that_does_not_exist.py or any data file in the same directory. Couldn't find 'severo/a_dataset_that_does_not_exist' on the Hugging Face Hub either: FileNotFoundError: Dataset 'severo/a_dataset_that_does_not_exist' doesn't exist on the Hub. If the repo is private or gated, make sure to log in with `huggingface-cli login`.\r\n>>> datasets.get_dataset_config_names('severo/test_files_without_extension')\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 351, in get_dataset_config_names\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1508, in dataset_module_factory\r\n    raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at /home/slesage/hf/datasets-server/services/worker/severo/test_files_without_extension/test_files_without_extension.py or any data file in the same directory. Couldn't find 'severo/test_files_without_extension' on the Hugging Face Hub either: FileNotFoundError: No (supported) data files or dataset script found in severo/test_files_without_extension.\r\n```\r\n\r\nTo differentiate, we must parse the error message (only the end is different). We should have a different exception for these two errors.<end_of_turn>\n<start_of_turn>model\nRaise a different exception for inexisting dataset vs files without known extension<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFolderBase Dataset automatically resolves under current directory when data_dir is not specified.\r\n\r\nFor example:\r\n```\r\nload_dataset(\"audiofolder\")\r\n```\r\ntakes long time to resolve and collect data_files from current directory. But I think it should reach out to this line for error handling https://github.com/huggingface/datasets/blob/cb8c5de5145c7e7eee65391cb7f4d92f0d565d62/src/datasets/packaged_modules/folder_based_builder/folder_based_builder.py#L58-L59\n\n### Steps to reproduce the bug\n\n```\r\nload_dataset(\"audiofolder\")\r\n```\n\n### Expected behavior\n\nError report\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.15.0-78-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.15\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nFolderBase Dataset automatically resolves under current directory when data_dir is not specified<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI used map to store raw audio waveforms of variable lengths in a column of a dataset the `map` call fails with ArrowInvalid: Value X too large to fit in C integer type.\n\n```\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"...lib/python3.12/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3526, in _map_single\n    writer.write_batch(batch)\n  File \"...lib/python3.12/site-packages/datasets/arrow_writer.py\", line 605, in write_batch\n    arrays.append(pa.array(typed_sequence))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/array.pxi\", line 252, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 114, in pyarrow.lib._handle_arrow_array_protocol\n  File \"...lib/python3.12/site-packages/datasets/arrow_writer.py\", line 225, in __arrow_array__\n    out = list_of_np_array_to_pyarrow_listarray(data)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/features/features.py\", line 1538, in list_of_np_array_to_pyarrow_listarray\n    return list_of_pa_arrays_to_pyarrow_listarray(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/features/features.py\", line 1530, in list_of_pa_arrays_to_pyarrow_listarray\n    offsets = pa.array(offsets, type=pa.int32())\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/array.pxi\", line 362, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 87, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Value 2148479376 too large to fit in C integer type\n```\n\n\n\n### Steps to reproduce the bug\n\nCalling map on a dataset that returns a column with long 1d numpy arrays of variable length.\n\nExample:\n```python\n# %%\nimport logging\nimport datasets\nimport pandas as pd\nimport numpy as np\n# %%\n\n\ndef process_batch(batch, rank):\n    res = []\n    for _ in batch[\"id\"]:\n        res.append(np.zeros((2**30)).astype(np.uint16))\n    \n    return {\"audio\": res}\n\n\nif __name__ == \"__main__\":\n    df = pd.DataFrame(\n        {\n            \"id\": list(range(400)),\n        }\n    )\n\n    ds = datasets.Dataset.from_pandas(df)\n    try:\n        from multiprocess import set_start_method\n\n        set_start_method(\"spawn\")\n    except RuntimeError:\n        print(\"Spawn method already set, continuing...\")\n    mapped_ds = ds.map(\n        process_batch,\n        batched=True,\n        batch_size=2,\n        with_rank=True,\n        num_proc=2,\n        cache_file_name=\"path_to_cache/tmp.arrow\",\n        writer_batch_size=200,\n        remove_columns=ds.column_names,\n        # disable_nullable=True,\n    )\n\n```\n\n### Expected behavior\n\nI think the offsets should be pa.int64() if needed  and not forced to be `pa.int32()` \nin https://github.com/huggingface/datasets/blob/3e13d30823f8ec498d56adbc18c6880a5463b313/src/datasets/features/features.py#L1535\n\n### Environment info\n\n- `datasets` version: 3.3.1\n- Platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.29.0\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nBuilding a dataset with large variable size arrays results in error ArrowInvalid: Value X too large to fit in C integer type<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nr/hub/filesystem in save_to_disk<end_of_turn>\n<start_of_turn>model\n[MINOR:TYPO] Update save_to_disk docstring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThis bug is triggered under the following conditions:\r\n\r\n- datasets repo ids without organization names trigger errors, such as `bookcorpus`, `gsm8k`, `wikipedia`, rather than in the form of `A/B`.\r\n- If `HF_ENDPOINT` is set and the hostname is not in the form of `(hub-ci.)?huggingface.co`.\r\n- This issue occurs with `datasets>2.15.0` or `huggingface-hub>0.19.4`. For example, using the latest versions: `datasets==2.18.0` and `huggingface-hub==0.21.4`, \r\n\r\n### Steps to reproduce the bug\r\n\r\nthe issue can be reproduced with the following code:\r\n1. install specific datasets and huggingface_hub.\r\n```bash\r\npip install datasets==2.18.0\r\npip install huggingface_hub==0.21.4\r\n```\r\n2. execute python code.\r\n```Python\r\nimport os\r\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\r\nfrom datasets import load_dataset\r\nbookcorpus = load_dataset('bookcorpus', split='train')\r\n```\r\nconsole output:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/datasets/load.py\", line 2556, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/datasets/load.py\", line 2228, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/datasets/load.py\", line 1879, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/datasets/load.py\", line 1830, in dataset_module_factory\r\n    with fs.open(f\"datasets/{path}/{filename}\", \"r\", encoding=\"utf-8\") as f:\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/fsspec/spec.py\", line 1295, in open\r\n    self.open(\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/fsspec/spec.py\", line 1307, in open\r\n    f = self._open(\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 228, in _open\r\n    return HfFileSystemFile(self, path, mode=mode, revision=revision, block_size=block_size, **kwargs)\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 615, in __init__\r\n    self.resolved_path = fs.resolve_path(path, revision=revision)\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 180, in resolve_path\r\n    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 117, in _repo_and_revision_exist\r\n    self._api.repo_info(repo_id, revision=revision, repo_type=repo_type)\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2413, in repo_info\r\n    return method(\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2286, in dataset_info\r\n    hf_raise_for_status(r)\r\n  File \"/home/padeoe/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 362, in hf_raise_for_status\r\n    raise HfHubHTTPError(str(e), response=response) from e\r\nhuggingface_hub.utils._errors.HfHubHTTPError: 401 Client Error: Unauthorized for url: https://hf-mirror.com/api/datasets/bookcorpus/bookcorpus.py (Request ID: Root=1-65ee8659-5ab10eec5960c63e71f2bb58;b00bdbea-fd6e-4a74-8fe0-bc4682ae090e)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset was downloaded correctly without any errors.\r\n\r\n### Environment info\r\n\r\ndatasets==2.18.0\r\nhuggingface-hub==0.21.4<end_of_turn>\n<start_of_turn>model\nIssue Downloading Certain Datasets After Setting Custom `HF_ENDPOINT`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport `push_to_hub` canonical datasets.\r\n\r\nThis is necessary in the Space to convert script-datasets to Parquet: https://huggingface.co/spaces/albertvillanova/convert-dataset-to-parquet\r\n\r\nNote that before this PR, the `repo_id` \"dataset_name\" was transformed to \"user/dataset_name\". This behavior was introduced by:\r\n- #6269<end_of_turn>\n<start_of_turn>model\nSupport push_to_hub canonical datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee: https://github.com/huggingface/datasets/actions/runs/7104781624/job/19340572394\r\n```\r\nFAILED tests/test_load.py::test_loading_from_the_datasets_hub - NotADirectoryError: [WinError 267] The directory name is invalid: 'C:\\\\Users\\\\RUNNER~1\\\\AppData\\\\Local\\\\Temp\\\\tmpfcnps56i\\\\hf-internal-testing___dataset_with_script\\\\default\\\\0.0.0\\\\c240e2be3370bdbd\\\\dataset_with_script-train.arrow'\r\n```<end_of_turn>\n<start_of_turn>model\nCI on windows is broken: PermissionError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nNote that this bug is only triggered when `streaming=True`. #5459 introduced always calling fsspec with `client_kwargs={\"requote_redirect_url\": False}`, which seems to have incompatibility issues even in the newest versions.\r\n\r\nAnalysis of what's happening:\r\n\r\n1. `datasets` passes the `client_kwargs` through `fsspec`\r\n2. `fsspec` passes the `client_kwargs` through `s3fs`\r\n3. `s3fs` passes the `client_kwargs` to `aiobotocore` which uses `aiohttp`\r\n```\r\ns3creator = self.session.create_client(\r\n    \"s3\", config=conf, **init_kwargs, **client_kwargs\r\n)\r\n```\r\n4. The `session` tries to create an `aiohttp` session but the `**kwargs` are not just kept as unfolded `**kwargs` but passed in as individual variables (`requote_redirect_url` and `trust_env`).\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/cxrh/Documents/GitHub/nlp_foundation/nlp_train/test.py\", line 14, in <module>\r\n    batch = next(iter(ds))\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1353, in __iter__\r\n    for key, example in ex_iterable:\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 255, in __iter__\r\n    for key, pa_table in self.generate_tables_fn(**self.kwargs):\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/datasets/packaged_modules/json/json.py\", line 78, in _generate_tables\r\n    for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py\", line 840, in __iter__\r\n    yield from self.generator(*self.args, **self.kwargs)\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py\", line 921, in _iter_from_urlpaths\r\n    elif xisdir(urlpath, download_config=download_config):\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py\", line 305, in xisdir\r\n    return fs.isdir(inner_path)\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/fsspec/spec.py\", line 721, in isdir\r\n    return self.info(path)[\"type\"] == \"directory\"\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/fsspec/archive.py\", line 38, in info\r\n    self._get_dirs()\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/datasets/filesystems/compression.py\", line 64, in _get_dirs\r\n    f = {**self.file.fs.info(self.file.path), \"name\": self.uncompressed_name}\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/fsspec/asyn.py\", line 118, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/fsspec/asyn.py\", line 103, in sync\r\n    raise return_result\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/fsspec/asyn.py\", line 56, in _runner\r\n    result[0] = await coro\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/s3fs/core.py\", line 1302, in _info\r\n    out = await self._call_s3(\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/s3fs/core.py\", line 341, in _call_s3\r\n    await self.set_session()\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/s3fs/core.py\", line 524, in set_session\r\n    s3creator = self.session.create_client(\r\n  File \"/Users/cxrh/miniconda3/envs/s3_data_loader/lib/python3.10/site-packages/aiobotocore/session.py\", line 114, in create_client\r\n    return ClientCreatorContext(self._create_client(*args, **kwargs))\r\nTypeError: AioSession._create_client() got an unexpected keyword argument 'requote_redirect_url'\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Install the necessary libraries, datasets having a requirement for being at least 2.19.0: \r\n\r\n```\r\npip install s3fs fsspec aiohttp aiobotocore botocore 'datasets>=2.19.0'\r\n```\r\n\r\n2. Run this code:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\r\n    \"json\",\r\n    data_files=\"s3://your_path/*.jsonl.gz\",\r\n    streaming=True,\r\n    split=\"train\",\r\n)\r\n\r\nbatch = next(iter(ds))\r\n\r\nprint(batch)\r\n```\r\n\r\n3. You get the `unexpected keyword argument 'requote_redirect_url'` error.\r\n\r\n### Expected behavior\r\n\r\nThe datasets is able to load a batch from the dataset stored on S3, without triggering this `requote_redirect_url` error.\r\n\r\nFix: I could fix this by directly removing the `requote_redirect_url` and `trust_env` - then it loads properly.\r\n\r\n<img width=\"1127\" alt=\"image\" src=\"https://github.com/user-attachments/assets/4c40efa9-8787-4919-b613-e4908c3d1ab2\">\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.1.0\r\n- Platform: macOS-15.1-arm64-arm-64bit\r\n- Python version: 3.10.15\r\n- `huggingface_hub` version: 0.26.2\r\n- PyArrow version: 18.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\n[BUG]: Streaming from S3 triggers `unexpected keyword argument 'requote_redirect_url'`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently, I'm reuploading datasets for [`MTEB`](https://github.com/embeddings-benchmark/mteb/). Some of them have many splits (more than 20), and I'm encountering the error:\r\n`Too many requests for https://huggingface.co/datasets/repo/create`.\r\n\r\nIt seems that this issue occurs because the dataset tries to recreate itself every time a split is uploaded. To resolve this, I've added a check to ensure that if the dataset already exists, it won't attempt to recreate it.<end_of_turn>\n<start_of_turn>model\nadd check if repo exists for dataset uploading<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIncludes both https://github.com/huggingface/datasets/pull/6458 and https://github.com/huggingface/datasets/pull/6459\r\n\r\nThis PR should be merged instead of the two individually, since they are conflicting\r\n\r\n## Offline cache reload\r\n\r\n it can reload datasets that were pushed to hub if they exist in the cache.\r\n\r\nexample:\r\n\r\n```python\r\n>>> Dataset.from_dict({\"a\": [1, 2]}).push_to_hub(\"lhoestq/tmp\")\r\n>>> load_dataset(\"lhoestq/tmp\")\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\nand later, without connection:\r\n\r\n```python\r\n>>> load_dataset(\"lhoestq/tmp\")\r\nUsing the latest cached version of the dataset since lhoestq/tmp couldn't be found on the Hugging Face Hub\r\nFound the latest cached dataset configuration 'default' at /Users/quentinlhoest/.cache/huggingface/datasets/lhoestq___tmp/default/0.0.0/da0e902a945afeb9 (last modified on Wed Dec 13 14:55:52 2023).\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\n- Updated `CachedDatasetModuleFactory` to look for datasets in the cache at `<namespace>___<dataset_name>/<config_id>`\r\n- Since the metadata configs parameters are not available in offline mode, we don't know which folder to load (config_id and hash change), so I simply load the latest one\r\n  - I instantiate a BuilderConfig even if there is no metadata config with the right config_name\r\n  - Its config_id is equal to the config_name to be able to retrieve it in the cache (no more suffix for configs from metadata configs)\r\n  - We can reload this config if offline mode by specifying the right config_name (same as online !)\r\n- Consequences of this change:\r\n  - Only when there are user's parameters it creates a custom builder config with config_id = config_name + user parameters hash\r\n  - the hash used to name the cache folder takes into account the metadata config and the dataset info, so that the right cache can be reloaded when there is internet connection without redownloading the data or resolving the data files. For local directories I hash the builder configs and dataset info, and for datasets on the hub I use the commit sha as hash.\r\n  - cache directories now look like `config/version/commit_sha` for hub datasets which is clean :)\r\n\r\nFix https://github.com/huggingface/datasets/issues/3547\r\n\r\n## Lazy data files resolution\r\n\r\nthis makes this code run in 2sec instead of >10sec\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"glue\", \"sst2\", streaming=True, trust_remote_code=False)\r\n```\r\n\r\nFor some datasets with many configs and files it can be up to 100x faster.\r\nThis is particularly important now that some datasets will be loaded from the Parquet export instead of the scripts.\r\n\r\nThe data files are only resolved in the builder `__init__`. To do so I added DataFilesPatternsList and DataFilesPatternsDict that have `.resolve()` to return resolved DataFilesList and DataFilesDict\r\n<end_of_turn>\n<start_of_turn>model\nLazy data files resolution and offline cache reload<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nworked fine with 3.6.0, and with 4.0.0 `eval_metric = metric.compute()` in HF Accelerate breaks.\n\nThe code that fails is:\nhttps://huggingface.co/spaces/evaluate-metric/glue/blob/v0.4.0/glue.py#L84\n\n```\ndef simple_accuracy(preds, labels):\n    print(preds, labels)\n    print(f\"{preds==labels}\")\n    return float((preds == labels).mean())\n```\ndata:\n```\nColumn([1, 0, 0, 1, 1]) Column([1, 0, 0, 1, 0])\nFalse\n```\n\n```\n[rank0]:     return float((preds == labels).mean())\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: AttributeError: 'bool' object has no attribute 'mean'\n```\n\nSome behavior has changed in this new major release of `datasets` and requires updating HF accelerate and perhaps the glue metric code, all belong to HF.\n\n### Environment info\n\ndatasets=4.0.0<end_of_turn>\n<start_of_turn>model\nmetric glue breaks with 4.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTitle: Add wikipedia-2023-redirects dataset (redirect resolution + pageviews)\n\nSummary\n- New dataset loader: wikipedia_2023_redirects\n- Canonical Wikipedia pages enriched with:\n  - redirects (aliases pointing to the page)\n  - 2023 pageviews (aggregated)\n- Streaming support; robust parsing; license notes included\n- Tests with tiny dummy data (XML + TSVs); covers streaming\n\nMotivation\nRAG/retrieval often benefits from:\n- Query expansion via redirect aliases\n- Popularity prior via pageviews\nThis loader offers a practical, maintenance-light way to access canonical pages alongside their redirect aliases and 2023 pageview totals.\n\nFeatures\n- id: string\n- title: string\n- url: string\n- text: string\n- redirects: list[string]\n- pageviews_2023: int32\n- timestamp: string\n\nLicensing\n- Wikipedia text: CC BY-SA 3.0 (attribution and share-alike apply)\n- Pageviews: public domain\nThe PR docs mention both, and the module docstring cites sources.\n\nNotes\n- The URLs in _get_urls_for_config are wired to dummy files for tests. In production, these would point to Wikimedia dumps:\n  - XML page dumps: https://dumps.wikimedia.org/\n  - Pageviews: https://dumps.wikimedia.org/other/pageviews/\n- The schema is intentionally simple and stable. Pageview aggregation is per-title sum across 2023.\n\nTesting\n- make style && make quality\n- pytest -q tests/test_dataset_wikipedia_2023_redirects.py\n\nExample\n```python\nfrom datasets import load_dataset\nds = load_dataset(\"wikipedia_2023_redirects\", split=\"train\")\nprint(ds[0][\"title\"], ds[0][\"redirects\"][:5], ds[0][\"pageviews_2023\"])\n```\n\nAcknowledgements\n- Wikipedia/Wikimedia Foundation for the source data\n- Hugging Face Datasets for the dataset infrastructure\n<end_of_turn>\n<start_of_turn>model\nAdd wikipedia-2023-redirects dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove the legacy `HfFileSystem` and deprecate `S3FileSystem`\r\n\r\n\r\ncc @philschmid for the SageMaker scripts/notebooks that still use `datasets`' `S3FileSystem`<end_of_turn>\n<start_of_turn>model\nRemove `HfFileSystem` and deprecate `S3FileSystem`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix duplicating default config names.\r\n\r\nCurrently, when calling `push_to_hub(set_default=True` with 2 different config names, both are set as default. \r\n\r\nMoreover, this will generate an error next time we try to push another default config name, raised by `MetadataConfigs.get_default_config_name`:\r\nhttps://github.com/huggingface/datasets/blob/da1db8a5b89fc0badaa0f571b36e122e52ae8c61/src/datasets/arrow_dataset.py#L5757\r\nhttps://github.com/huggingface/datasets/blob/da1db8a5b89fc0badaa0f571b36e122e52ae8c61/src/datasets/utils/metadata.py#L186-L188<end_of_turn>\n<start_of_turn>model\nAvoid multiple default config names<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAvoid parallel redownload in cache by retrying inside the lock if path exists.<end_of_turn>\n<start_of_turn>model\nAvoid parallel redownload in cache<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI noticed that the performance of my dataset preprocessing with `map(...,num_proc=32)` decreases when PyTorch is imported.\r\n\r\n### Steps to reproduce the bug\r\n\r\nI created two example scripts to reproduce this behavior:\r\n\r\n```\r\nimport datasets\r\ndatasets.disable_caching()\r\n\r\nfrom datasets import Dataset\r\nimport time\r\n    \r\nPROC=32\r\n\r\nif __name__ == \"__main__\":\r\n    dataset = [True] * 10000000\r\n    dataset = Dataset.from_dict({'train': dataset})\r\n    \r\n\r\n    start = time.time()\r\n    dataset.map(lambda x: x, num_proc=PROC)\r\n    end = time.time()\r\n    print(end - start)\r\n```\r\nTakes around 4 seconds on my machine.\r\n\r\nWhile the same code, but with an `import torch`:\r\n```\r\nimport datasets\r\ndatasets.disable_caching()\r\n\r\nfrom datasets import Dataset\r\nimport time\r\nimport torch\r\n    \r\nPROC=32\r\n\r\nif __name__ == \"__main__\":\r\n    dataset = [True] * 10000000\r\n    dataset = Dataset.from_dict({'train': dataset})\r\n    \r\n\r\n    start = time.time()\r\n    dataset.map(lambda x: x, num_proc=PROC)\r\n    end = time.time()\r\n    print(end - start)\r\n```\r\ntakes around 22 seconds.\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect that the import of torch to not have such a significant effect on the performance of map using multiprocessing.\r\n\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.15.0-56-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.3\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2\r\n- torch: 2.0.1<end_of_turn>\n<start_of_turn>model\nImporting PyTorch reduces multiprocessing performance for map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTrying to upload a dataset of hundreds of thousands of audio samples (the total volume is not very large, 60 gb) to the hub with push_to_hub, it doesn't work.\r\nFrom time to time one piece of the data (parquet) gets pushed and then I get RemoteDisconnected even though my internet is stable.\r\nPlease help.\r\nI'm trying to upload the dataset for almost a week.\r\nThanks\n\n### Steps to reproduce the bug\n\nnot relevant \n\n### Expected behavior\n\nBe able to upload thedataset\n\n### Environment info\n\npython: 3.9<end_of_turn>\n<start_of_turn>model\nFailing to upload dataset to the hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix typos in PDF and Video documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe [load_dataset](https://huggingface.co/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset) method accepts a `List[str]` as the split parameter at runtime, however, the current type stubs restrict the split parameter to `Union[str, Split, None]`. This causes type checkers like Pylance to raise `reportArgumentType` errors when passing a list of strings, even though it works as intended at runtime.\n\n### Steps to reproduce the bug\n\n1. Use load_dataset with multiple splits e.g.:\n```\nfrom datasets import load_dataset\n\nds_train, ds_val, ds_test = load_dataset(\n    \"Silly-Machine/TuPyE-Dataset\",\n    \"binary\",\n    split=[\"train[:75%]\", \"train[75%:]\", \"test\"]\n)\n```\n2. Observe that code executes correctly at runtime and Pylance raises `Argument of type \"List[str]\" cannot be assigned to parameter \"split\" of type \"str | Split | None\"`\n\n### Expected behavior\n\nThe type stubs for [load_dataset](https://huggingface.co/docs/datasets/v3.6.0/en/package_reference/loading_methods#datasets.load_dataset) should accept `Union[str, Split, List[str], None]` or more specific overloads for the split parameter to correctly represent runtime behavior. \n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.39\n- Python version: 3.12.7\n- `huggingface_hub` version: 0.32.0\n- PyArrow version: 20.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nload_dataset type stubs reject List[str] for split parameter, but runtime supports it<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe following code fails in python 3.11+ \n\n```python\ntokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])  \n```\nError log:\n```bash\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/process.py\", line 315, in _bootstrap\nself.run()\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/process.py\", line 108, in run\nself._target(*self._args, **self._kwargs)\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/pool.py\", line 114, in worker\ntask = get()\n^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/multiprocess/queues.py\", line 371, in get\nreturn _ForkingPickler.loads(res)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 327, in loads\nreturn load(file, ignore, **kwds)\n^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 313, in load\nreturn Unpickler(file, ignore=ignore, **kwds).load()\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 525, in load\nobj = StockUnpickler.load(self)\n^^^^^^^^^^^^^^^^^^^^^^^^^\nFile \"/usr/local/lib/python3.12/dist-packages/dill/_dill.py\", line 659, in _create_code\nif len(args) == 16: return CodeType(*args)\n^^^^^^^^^^^^^^^\nTypeError: code() argument 13 must be str, not int\n```\n\nAfter upgrading dill to the latest 0.4.0 with \"pip install --upgrade dill\", it can pass. So it seems that there is a compatibility issue between dill 0.3.4 and python 3.11+, because python 3.10 works fine. \n\n\nIs the dill deterministic issue mentioned in https://github.com/huggingface/datasets/blob/main/setup.py#L117) still valid? Any plan to unpin? \n<end_of_turn>\n<start_of_turn>model\n`datasets.map(..., num_proc=4)` multi-processing fails<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nIn the examples section of the iterable dataset docs https://huggingface.co/docs/datasets/en/package_reference/main_classes#datasets.IterableDataset\r\nthe ddp example shuffles without seeding\r\n\r\n```python\r\nfrom datasets.distributed import split_dataset_by_node\r\nids = ds.to_iterable_dataset(num_shards=512)\r\nids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\r\nids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating\r\ndataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating\r\nfor example in ids:\r\n    pass\r\n```\r\n\r\nThis code would - I think - raise an error due to the lack of an explicit seed:\r\n\r\nhttps://github.com/huggingface/datasets/blob/2eb4edb97e1a6af2ea62738ec58afbd3812fc66e/src/datasets/iterable_dataset.py#L1707-L1711\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun example code\r\n\r\n### Expected behavior\r\n\r\nAdd explicit seeding to example code\r\n\r\n### Environment info\r\n\r\nlatest datasets<end_of_turn>\n<start_of_turn>model\nSet explicit seed in iterable dataset ddp shuffling example <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDon't ignore results of pattern resolving if `self.data_files` is not None. Otherwise lines 854 and 1037 make no sense.<end_of_turn>\n<start_of_turn>model\nFix checking patterns to infer packaged builder <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSeparate the default patterns that match directories from the ones matching files and ensure directories are checked first (reverts the change from https://github.com/huggingface/datasets/pull/6244, which merged these patterns). Also, ensure that the glob patterns do not overlap to avoid duplicates in the result. \r\n\r\nAdditionally, replace `get_fs_token_paths` with `url_to_fs` to avoid [unnecessary glob calls](https://github.com/fsspec/filesystem_spec/blob/14dce8ca78f7aa509a20edb263bff83a7760c24d/fsspec/core.py#L655-L656).\r\n\r\nfix https://github.com/huggingface/datasets/issues/6259\r\nfix https://github.com/huggingface/datasets/issues/6272<end_of_turn>\n<start_of_turn>model\nImprove default patterns resolution<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nDoes someone know how to return the images from videos?\n\n### Motivation\n\nI am trying to use openpi(https://github.com/Physical-Intelligence/openpi) to finetune my Lerobot dataset(V2.0 and V2.1). I find that although the codedaset is v2.0, they are different. It seems like Lerobot V2.0 has two version, one is data include images infos and another one is separate to data and videos.\n\nDoes someone know how to return the images from videos?\n\n\n\n<end_of_turn>\n<start_of_turn>model\nHow to convert videos to images?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI use it as in the tutorial here: https://huggingface.co/docs/datasets/stream, and it ends up with abort.\n\n### Steps to reproduce the bug\n\n1. `pip install datasets`\n2. \n```\n$ cat main.py \n#!/usr/bin/env python3\n\nfrom datasets import load_dataset\n\ndataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)\nprint(next(iter(dataset)))\n```\n3. `chmod +x main.py`\n```\n$ ./main.py \nREADME.md: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 43.1k/43.1k [00:00<00:00, 7.04MB/s]\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25868/25868 [00:05<00:00, 4859.26it/s]\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25868/25868 [00:00<00:00, 54773.56it/s]\n{'text': \"How AP reported in all formats from tornado-stricken regionsMarch 8, 2012\\nWhen the first serious bout of tornadoes of 2012 blew through middle America in the middle of the night, they touched down in places hours from any AP bureau. Our closest video journalist was Chicago-based Robert Ray, who dropped his plans to travel to Georgia for Super Tuesday, booked several flights to the cities closest to the strikes and headed for the airport. He\u2019d decide once there which flight to take.\\nHe never got on board a plane. Instead, he ended up driving toward Harrisburg, Ill., where initial reports suggested a town was destroyed. That decision turned out to be a lucky break for the AP. Twice.\\nRay was among the first journalists to arrive and he confirmed those reports -- in all formats. He shot powerful video, put victims on the phone with AP Radio and played back sound to an editor who transcribed the interviews and put the material on text wires. He then walked around the devastation with the Central Regional Desk on the line, talking to victims with the phone held so close that editors could transcribe his interviews in real time.\\nRay also made a dramatic image of a young girl who found a man\u2019s prosthetic leg in the rubble, propped it up next to her destroyed home and spray-painted an impromptu sign: \u201cFound leg. Seriously.\u201d\\nThe following day, he was back on the road and headed for Georgia and a Super Tuesday date with Newt Gingrich\u2019s campaign. The drive would take him through a stretch of the South that forecasters expected would suffer another wave of tornadoes.\\nTo prevent running into THAT storm, Ray used his iPhone to monitor Doppler radar, zooming in on extreme cells and using Google maps to direct himself to safe routes. And then the journalist took over again.\\n\u201cWhen weather like that occurs, a reporter must seize the opportunity to get the news out and allow people to see, hear and read the power of nature so that they can take proper shelter,\u201d Ray says.\\nSo Ray now started to use his phone to follow the storms. He attached a small GoPro camera to his steering wheel in case a tornado dropped down in front of the car somewhere, and took video of heavy rain and hail with his iPhone. Soon, he spotted a tornado and the chase was on. He followed an unmarked emergency vehicle to Cleveland, Tenn., where he was first on the scene of the storm's aftermath.\\nAgain, the tornadoes had struck in locations that were hours from the nearest AP bureau. Damage and debris, as well as a wickedly violent storm that made travel dangerous, slowed our efforts to get to the news. That wasn\u2019t a problem in Tennessee, where our customers were well served by an all-formats report that included this text story.\\n\u201cCLEVELAND, Tenn. (AP) _ Fierce wind, hail and rain lashed Tennessee for the second time in three days, and at least 15 people were hospitalized Friday in the Chattanooga area.\u201d\\nThe byline? Robert Ray.\\nFor being adept with technology, chasing after news as it literally dropped from the sky and setting a standard for all-formats reporting that put the AP ahead on the most competitive news story of the day, Ray wins this week\u2019s $300 Best of the States prize.\\n\u00a9 2013 The Associated Press. All rights reserved. Terms and conditions apply. See AP.org for details.\", 'id': '<urn:uuid:d66bc6fe-8477-4adf-b430-f6a558ccc8ff>', 'dump': 'CC-MAIN-2013-20', 'url': 'http://%20jwashington@ap.org/Content/Press-Release/2012/How-AP-reported-in-all-formats-from-tornado-stricken-regions', 'date': '2013-05-18T05:48:54Z', 'file_path': 's3://commoncrawl/crawl-data/CC-MAIN-2013-20/segments/1368696381249/warc/CC-MAIN-20130516092621-00000-ip-10-60-113-184.ec2.internal.warc.gz', 'language': 'en', 'language_score': 0.9721424579620361, 'token_count': 717}\nterminate called without an active exception\nAborted (core dumped)\n```\n\n### Expected behavior\n\nI'm not a proficient Python user, so it might be my own error, but even in that case, the error message should be better.\n\n### Environment info\n\n`Successfully installed datasets-3.6.0 dill-0.3.8 hf-xet-1.1.0 huggingface-hub-0.31.1 multiprocess-0.70.16 requests-2.32.3 xxhash-3.5.0`\n\n```\n$ cat /etc/lsb-release \nDISTRIB_ID=Ubuntu\nDISTRIB_RELEASE=22.04\nDISTRIB_CODENAME=jammy\nDISTRIB_DESCRIPTION=\"Ubuntu 22.04.4 LTS\"\n```<end_of_turn>\n<start_of_turn>model\nterminate called without an active exception; Aborted (core dumped)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThere are code lines specific for pyarrow versions < 15.0.0.\r\n\r\nHowever, we require pyarrow >= 15.0.0 since the merge of PR:\r\n- #6892\r\n\r\nThose code lines are now dead code and should be removed.<end_of_turn>\n<start_of_turn>model\nThere is dead code after we require pyarrow >= 15.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nas reported in https://github.com/huggingface/datasets/issues/6843<end_of_turn>\n<start_of_turn>model\nRetry all requests timeouts<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHello!\r\n\r\nPyright flags any use of `load_dataset` as an error, because the default for `trust_remote_code` is `None`, but the function is typed as `bool`, not `Optional[bool]`. I changed the type and docstrings to reflect this, but no other code was touched.\r\n\r\n<end_of_turn>\n<start_of_turn>model\nfix: None default with bool type on load creates typing error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen trying to run\r\n```\r\nimport datasets\r\nprint(datasets.__version__)\r\n\r\n```\r\nIt generates the following error\r\n\r\n```\r\nTypeError: expected string or bytes-like object\r\n```\r\n\r\nIt looks like It cannot find the valid versions of `fsspec`\r\n\r\nthough fsspec version is fine when I checked Via command\r\n\r\n```\r\nimport fsspec\r\nprint(fsspec.__version__)\r\n\u200b\r\n# output: 2024.3.1\r\n```\r\n\r\nDetailed crash report\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[1], line 1\r\n----> 1 import datasets\r\n      2 print(datasets.__version__)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/__init__.py:18\r\n      1 # ruff: noqa\r\n      2 # Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\r\n      3 #\r\n   (...)\r\n     13 # See the License for the specific language governing permissions and\r\n     14 # limitations under the License.\r\n     16 __version__ = \"2.18.0\"\r\n---> 18 from .arrow_dataset import Dataset\r\n     19 from .arrow_reader import ReadInstruction\r\n     20 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:66\r\n     63 from multiprocess import Pool\r\n     64 from tqdm.contrib.concurrent import thread_map\r\n---> 66 from . import config\r\n     67 from .arrow_reader import ArrowReader\r\n     68 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/config.py:41\r\n     39 # Imports\r\n     40 DILL_VERSION = version.parse(importlib.metadata.version(\"dill\"))\r\n---> 41 FSSPEC_VERSION = version.parse(importlib.metadata.version(\"fsspec\"))\r\n     42 PANDAS_VERSION = version.parse(importlib.metadata.version(\"pandas\"))\r\n     43 PYARROW_VERSION = version.parse(importlib.metadata.version(\"pyarrow\"))\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/packaging/version.py:49, in parse(version)\r\n     43 \"\"\"\r\n     44 Parse the given version string and return either a :class:`Version` object\r\n     45 or a :class:`LegacyVersion` object depending on if the given version is\r\n     46 a valid PEP 440 version or a legacy version.\r\n     47 \"\"\"\r\n     48 try:\r\n---> 49     return Version(version)\r\n     50 except InvalidVersion:\r\n     51     return LegacyVersion(version)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/packaging/version.py:264, in Version.__init__(self, version)\r\n    261 def __init__(self, version: str) -> None:\r\n    262 \r\n    263     # Validate the version and parse it into pieces\r\n--> 264     match = self._regex.search(version)\r\n    265     if not match:\r\n    266         raise InvalidVersion(f\"Invalid version: '{version}'\")\r\n\r\nTypeError: expected string or bytes-like object\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. run `!pip install -U datasets` on kaggle\r\n2. check datasets is installed via\r\n```\r\nimport datasets\r\nprint(datasets.__version__)\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpected to print datasets version, like `2.18.0`\r\n\r\n### Environment info\r\n\r\nRunning on Kaggle, latest enviornment , here is the notebook https://www.kaggle.com/code/jtv199/mistrial-7b-part2<end_of_turn>\n<start_of_turn>model\nType error when importing datasets on Kaggle<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n```python\r\ndef _generate_examples(self, metadata_path, images_dir, conditioning_images_dir):\r\n        with open(metadata_path, 'r') as file:\r\n            metadata = json.load(file)\r\n\r\n        for idx, item in enumerate(metadata):\r\n            image_path = item.get('image_path')\r\n            text_content = item.get('text_content')\r\n            image_data = open(image_path, \"rb\").read()\r\n            yield idx, {\r\n                \"text\": text_content,\r\n                \"image\": {\r\n                    \"path\": image_path,\r\n                    \"bytes\": image_data,\r\n                },\r\n                \"conditioning_image\": {\r\n                    \"path\": image_path,\r\n                    \"bytes\": image_data,\r\n                },\r\n            }\r\n```\r\nHello, \r\n\r\nI use the above function to deal with my local data set, but I am very surprised that the speed at which I generate example is very different. When I start a training task, **sometimes 1000examples/s, sometimes only 10examples/s.**\r\n\r\n![image](https://github.com/huggingface/datasets/assets/46072190/cdc17661-8267-4fd8-b30c-b74d505efd9b)\r\n\r\nI'm not saying that speed is changing all the time. I mean, the reading speed is different in different training, which will cause me to start training over and over again until the speed of this generation of examples is normal.\r\n<end_of_turn>\n<start_of_turn>model\nWhy is the speed difference of gen example so big?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nAdd an option when sharding a dataset to have all shards the same size. Will be good to provide both an option of duplication, and by truncation.\r\n\r\n### Motivation\r\n\r\nCurrently the behavior of sharding is \"If n % i == l, then the first l shards will have length (n // i) + 1, and the remaining shards will have length (n // i).\". However, when using FSDP we want the shards to have the same size. This requires the user to manually handle this situation, but it will be nice if we had an option to shard the dataset into equally sized shards. \r\n\r\n### Your contribution\r\n\r\nFor now just a PR. I can also add code that does what is needed, but probably not efficient.\r\nShard to equal size by duplication:\r\n```\r\nremainder = len(dataset) % num_shards\r\nnum_missing_examples = num_shards - remainder\r\nduplicated = dataset.select(list(range(num_missing_examples)))\r\ndataset = concatenate_datasets([dataset, duplicated])\r\nshard = dataset.shard(num_shards, shard_idx)\r\n```\r\nOr by truncation:\r\n```\r\nshard = dataset.shard(num_shards, shard_idx)\r\nnum_examples_per_shard = len(dataset) // num_shards\r\nshard = shard.select(list(range(num_examples_per_shard)))\r\n```<end_of_turn>\n<start_of_turn>model\nEnable Sharding to Equal Sized Shards<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nStarting from `huggingface_hub` 0.20.0, `list_files_info` will be deprecated in favor of `list_repo_tree` (see https://github.com/huggingface/huggingface_hub/pull/1910)<end_of_turn>\n<start_of_turn>model\nReplace `list_files_info` with `list_repo_tree` in `push_to_hub`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSince the latest release of `datasets` (`2.14.0`), custom filesystem `storage_options` passed to `load_dataset()` do not seem to propagate through all the way - leading to problems if loading data files that need those options to be set.\r\n\r\nI think this is because of the new `_prepare_path_and_storage_options()` (https://github.com/huggingface/datasets/pull/6028), which returns the right `storage_options` to use given a path and a `DownloadConfig` - but which might not be taking into account the extra `storage_options` explicitly provided e.g. through `load_dataset()`\n\n### Steps to reproduce the bug\n\n```python\r\nimport fsspec\r\n\r\nimport pandas as pd\r\nimport datasets\r\n\r\n# Generate mock parquet file\r\ndata_files = \"demo.parquet\"\r\npd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}).to_parquet(data_files)\r\n\r\n_storage_options = {\"x\": 1, \"y\": 2}\r\nfs = fsspec.filesystem(\"file\", **_storage_options)\r\n\r\ndataset = datasets.load_dataset(\r\n    \"parquet\",\r\n    data_files=data_files,\r\n    storage_options=fs.storage_options\r\n)\r\n```\r\n\r\nLooking at the `storage_options` resolved here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/b0177910b32712f28d147879395e511207e39958/src/datasets/data_files.py#L331\r\n\r\n they end up being `{}`, instead of propagating through the `storage_options` that were provided to `load_dataset` (`fs.storage_options`). As these then get used for the filesystem operation a few lines below\r\n\r\nhttps://github.com/huggingface/datasets/blob/b0177910b32712f28d147879395e511207e39958/src/datasets/data_files.py#L339\r\n\r\nthe call will fail if the user-provided `storage_options` were needed.\r\n\r\n---\r\n\r\nA temporary workaround that seemed to work locally to bypass the problem was to bundle a duplicate of the `storage_options` into the `download_config`, so that they make their way all the way to `_prepare_path_and_storage_options()` and get extracted correctly:\r\n\r\n```python\r\ndataset = datasets.load_dataset(\r\n        \"parquet\",\r\n        data_files=data_files,\r\n        storage_options=fs.storage_options,\r\n        download_config=datasets.DownloadConfig(storage_options={fs.protocol: fs.storage_options}),\r\n    )\r\n```\n\n### Expected behavior\n\n`storage_options` provided to `load_dataset` take effect in all backend filesystem operations.\n\n### Environment info\n\ndatasets==2.14.0<end_of_turn>\n<start_of_turn>model\nstorage_options provided to load_dataset not fully piping through since datasets 2.14.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport fsspec 2024.2.\r\n\r\nFirst, we should address:\r\n- #6644<end_of_turn>\n<start_of_turn>model\nSupport fsspec 2024.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nMapped tokenization slows down substantially towards end of dataset.\r\n\r\ntrain set started off very slow, caught up to 20k then tapered off til the end.\r\n\r\nwhat's particularly strange is that the tokenization crashed a few times before due to errors with invalid tokens somewhere or corrupted downloads, and the speed ups/downs consistently happened the same times\r\n\r\n```bash\r\nRunning tokenizer on dataset (num_proc=48):   0%|          | 847000/881416735 [12:18<252:45:45, 967.72 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):   0%|          | 848000/881416735 [12:19<224:16:10, 1090.66 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48):  10%|\u2589         | 84964000/881416735 [3:48:00<11:21:34, 19476.01 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  10%|\u2589         | 84967000/881416735 [3:48:00<12:04:01, 18333.79 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 538631977/881416735 [13:46:40<27:50:04, 3420.84 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 538632977/881416735 [13:46:40<23:48:20, 3999.77 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 881365886/881416735 [38:30:19<04:34, 185.10 examples/s]\r\nRunning tokenizer on dataset (num_proc=48): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 881366886/881416735 [38:30:25<04:36, 180.57 examples/s]\r\n```\r\n\r\nand validation set as well\r\n\r\n```bash\r\nRunning tokenizer on dataset (num_proc=48):  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 41544000/46390354 [28:44<02:37, 30798.76 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589 | 41550000/46390354 [28:44<02:08, 37698.08 examples/s]\r\n\r\nRunning tokenizer on dataset (num_proc=48):  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 44747422/46390354 [2:15:48<12:22:44, 36.87 examples/s]\r\nRunning tokenizer on dataset (num_proc=48):  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 44747422/46390354 [2:16:00<12:22:44, 36.87 examples/s]\r\n\r\n```\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n    using the following kwargs\r\n    \r\n```python\r\nwith accelerator.main_process_first():\r\n        lm_datasets = tokenized_datasets.map(\r\n            group_texts,\r\n            batched=True,\r\n            num_proc=48\r\n            load_from_cache_file=True,\r\n            desc=f\"Grouping texts in chunks of {block_size}\",\r\n        )\r\n```\r\n\r\nrunning through slurm script \r\n```bash\r\n#SBATCH --partition=gpu-nvidia-a100\r\n#SBATCH --nodes=1\r\n#SBATCH --ntasks=1\r\n#SBATCH --gpus-per-task=8\r\n#SBATCH --cpus-per-task=96\r\n```\r\n\r\nusing this dataset https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nConstant speed throughout\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.15.0-1049-aws-x86_64-with-glibc2.10\r\n- Python version: 3.8.18\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nTokenization slows towards end of dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe underlying `SqlDatasetWriter` has `num_proc` as an available argument [here](https://github.com/huggingface/datasets/blob/5dc1a179783dff868b0547c8486268cfaea1ea1f/src/datasets/io/sql.py#L63) , but `Dataset.to_sql()` does not accept it, therefore it is always using one process for the SQL conversion.<end_of_turn>\n<start_of_turn>model\n`Dataset.to_sql` doesn't utilize `num_proc`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nOur CI is broken: 3 failed, 12 errors\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/6069947111/job/16465138041\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/test_load.py::ModuleFactoryTest::test_LocalDatasetModuleFactoryWithoutScript_with_data_dir - AssertionError: assert ({NamedSplit('train'): ['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_LocalDatasetModuleFactory2/data_dir2/subdir1/train.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_LocalDatasetModuleFactory2/data_dir2/subdir1/train.txt'], NamedSplit('test'): ['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_LocalDatasetModuleFactory2/data_dir2/subdir1/test.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_LocalDatasetModuleFactory2/data_dir2/subdir1/test.txt']} is not None and 2 == 1)\r\n +  where 2 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_LocalDatasetModuleFactory2/data_dir2/subdir1/train.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_LocalDatasetModuleFactory2/data_dir2/subdir1/train.txt'])\r\nFAILED tests/test_load.py::test_load_dataset_arrow[False] - AssertionError: assert 20 == 10\r\n +  where 20 = Dataset({\\n    features: ['col_1'],\\n    num_rows: 20\\n}).num_rows\r\nFAILED tests/test_load.py::test_load_dataset_arrow[True] - assert 20 == 10\r\nERROR tests/packaged_modules/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_2/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_2/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_2/audiofolder_data_dir_with_metadata/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_2/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_2/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_2/audiofolder_data_dir_with_metadata/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_3/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_3/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_3/audiofolder_data_dir_with_metadata/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_3/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_3/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_3/audiofolder_data_dir_with_metadata/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[csv-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_4/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_4/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_4/audiofolder_data_dir_with_metadata/train/metadata.csv', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_4/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_4/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_4/audiofolder_data_dir_with_metadata/train/metadata.csv'])\r\nERROR tests/packaged_modules/test_audiofolder.py::test_data_files_with_metadata_and_multiple_splits[csv-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_5/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_5/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_5/audiofolder_data_dir_with_metadata/train/metadata.csv', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_5/audiofolder_data_dir_with_metadata/train/audio_file.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_5/audiofolder_data_dir_with_metadata/train/audio_file2.wav', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_5/audiofolder_data_dir_with_metadata/train/metadata.csv'])\r\nERROR tests/packaged_modules/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[1-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_3/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_3/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_3/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_3/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_3/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_3/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[1-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_4/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_4/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_4/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_4/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_4/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_4/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[2-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_5/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_5/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_5/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_5/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_5/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_5/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_12/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_12/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_12/imagefolder_data_dir_with_metadata_two_splits/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_12/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_12/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_12/imagefolder_data_dir_with_metadata_two_splits/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[jsonl-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_13/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_13/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_13/imagefolder_data_dir_with_metadata_two_splits/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_13/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_13/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_13/imagefolder_data_dir_with_metadata_two_splits/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_folder_based_builder.py::test_data_files_with_metadata_and_splits[2-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_6/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_6/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_6/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_6/autofolder_data_dir_with_metadata_two_splits/train/file.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_6/autofolder_data_dir_with_metadata_two_splits/train/file2.txt', '/tmp/pytest-of-runner/pytest-0/popen-gw0/test_data_files_with_metadata_6/autofolder_data_dir_with_metadata_two_splits/train/metadata.jsonl'])\r\nERROR tests/packaged_modules/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[csv-False] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_14/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_14/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_14/imagefolder_data_dir_with_metadata_two_splits/train/metadata.csv', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_14/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_14/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_14/imagefolder_data_dir_with_metadata_two_splits/train/metadata.csv'])\r\nERROR tests/packaged_modules/test_imagefolder.py::test_data_files_with_metadata_and_multiple_splits[csv-True] - AssertionError: assert 6 == 3\r\n +  where 6 = len(['/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_15/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_15/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_15/imagefolder_data_dir_with_metadata_two_splits/train/metadata.csv', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_15/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_15/imagefolder_data_dir_with_metadata_two_splits/train/image_rgb2.jpg', '/tmp/pytest-of-runner/pytest-0/popen-gw1/test_data_files_with_metadata_15/imagefolder_data_dir_with_metadata_two_splits/train/metadata.csv'])\r\n= 3 failed, 2383 passed, 26 skipped, 9 warnings, 12 errors in 280.79s (0:04:40) =\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken with AssertionError: 3 failed, 12 errors<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nrelated issue: https://github.com/apache/arrow/issues/37057#issue-1841013613\r\n\r\n---\r\n\r\n\r\nHello. I'm trying to interact with HDFS storage from a driver and workers of pyspark YARN cluster. Precisely I'm using **huggingface's `datasets`** ([link](https://github.com/huggingface/datasets)) library that relies on pyarrow to communicate with HDFS. The `from_spark()` ([link](https://huggingface.co/docs/datasets/use_with_spark#load-from-spark)) is what I'm invoking in my script.\r\n\r\nBelow is the error I'm encountering. Note that I've masked sensitive paths. My code is sent to worker containers (docker) from driver container then executed. I confirmed that in both driver and worker images I can connect to HDFS using pyarrow since the envs and required jars are properly set, but strangely that becomes impossible when the same image runs as remote worker process. \r\n\r\nThese are some peculiarities in my environment that might caused this issue. \r\n* **Cluster requires kerberos authentication**\r\n  * But I think the error message implies that's not the problem in this case\r\n* **The user that runs the worker process is different from that built the docker image**\r\n  * To avoid permission-related issues I made all directories that are accessed from the script accessible to everyone\r\n* **Pyspark-part of my code has no problem interacting with HDFS.** \r\n  * Even pyarrow doesn't experience problem when I run the code in interactive session of the same docker images (driver, worker)\r\n  * The problem occurs only when it runs as cluster's worker runtime\r\n\r\nHope I could get some help. Thanks.\r\n\r\n```bash\r\n2023-08-08 18:51:19,638 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n2023-08-08 18:51:20,280 WARN shortcircuit.DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\r\n23/08/08 18:51:22 WARN TaskSetManager: Lost task 0.0 in stage 142.0 (TID 9732) (ac3bax2062.bdp.bdata.ai executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"<MASKED>/application_1682476586273_25865777/container_e143_1682476586273_25865777_01_000003/pyspark.zip/pyspark/worker.py\", line 830, in main\r\n    process()\r\n  File \"<MASKED>/application_1682476586273_25865777/container_e143_1682476586273_25865777_01_000003/pyspark.zip/pyspark/worker.py\", line 820, in process\r\n    out_iter = func(split_index, iterator)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/spark/python/pyspark/rdd.py\", line 5405, in pipeline_func\r\n  File \"/root/spark/python/pyspark/rdd.py\", line 828, in func\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/packaged_modules/spark/spark.py\", line 130, in create_cache_and_write_probe\r\n    open(probe_file, \"a\")\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/streaming.py\", line 74, in wrapper\r\n    return function(*args, download_config=download_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py\", line 496, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 439, in open\r\n    out = open_files(\r\n          ^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 282, in open_files\r\n    fs, fs_token, paths = get_fs_token_paths(\r\n                          ^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 609, in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/registry.py\", line 267, in filesystem\r\n    return cls(**storage_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/spec.py\", line 79, in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 278, in __init__\r\n    fs = HadoopFileSystem(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow/_hdfs.pyx\", line 96, in pyarrow._hdfs.HadoopFileSystem.__init__\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: HDFS connection failed\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n23/08/08 18:51:24 WARN TaskSetManager: Lost task 0.1 in stage 142.0 (TID 9733) (ac3iax2079.bdp.bdata.ai executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"<MASKED>/application_1682476586273_25865777/container_e143_1682476586273_25865777_01_000005/pyspark.zip/pyspark/worker.py\", line 830, in main\r\n    process()\r\n  File \"<MASKED>/application_1682476586273_25865777/container_e143_1682476586273_25865777_01_000005/pyspark.zip/pyspark/worker.py\", line 820, in process\r\n    out_iter = func(split_index, iterator)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/spark/python/pyspark/rdd.py\", line 5405, in pipeline_func\r\n  File \"/root/spark/python/pyspark/rdd.py\", line 828, in func\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/packaged_modules/spark/spark.py\", line 130, in create_cache_and_write_probe\r\n    open(probe_file, \"a\")\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/streaming.py\", line 74, in wrapper\r\n    return function(*args, download_config=download_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py\", line 496, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 439, in open\r\n    out = open_files(\r\n          ^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 282, in open_files\r\n    fs, fs_token, paths = get_fs_token_paths(\r\n                          ^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 609, in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/registry.py\", line 267, in filesystem\r\n    return cls(**storage_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/spec.py\", line 79, in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 278, in __init__\r\n    fs = HadoopFileSystem(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow/_hdfs.pyx\", line 96, in pyarrow._hdfs.HadoopFileSystem.__init__\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: HDFS connection failed\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n\r\n23/08/08 18:51:38 WARN TaskSetManager: Lost task 0.2 in stage 142.0 (TID 9734) (<MASKED> executor 4): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\r\n  File \"<MASKED>/application_1682476586273_25865777/container_e143_1682476586273_25865777_01_000008/pyspark.zip/pyspark/worker.py\", line 830, in main\r\n    process()\r\n  File \"<MASKED>/application_1682476586273_25865777/container_e143_1682476586273_25865777_01_000008/pyspark.zip/pyspark/worker.py\", line 820, in process\r\n    out_iter = func(split_index, iterator)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/root/spark/python/pyspark/rdd.py\", line 5405, in pipeline_func\r\n  File \"/root/spark/python/pyspark/rdd.py\", line 828, in func\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/packaged_modules/spark/spark.py\", line 130, in create_cache_and_write_probe\r\n    open(probe_file, \"a\")\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/streaming.py\", line 74, in wrapper\r\n    return function(*args, download_config=download_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py\", line 496, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 439, in open\r\n    out = open_files(\r\n          ^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 282, in open_files\r\n    fs, fs_token, paths = get_fs_token_paths(\r\n                          ^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/core.py\", line 609, in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/registry.py\", line 267, in filesystem\r\n    return cls(**storage_options)\r\n           ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/spec.py\", line 79, in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/lib/python3.11/site-packages/fsspec/implementations/arrow.py\", line 278, in __init__\r\n    fs = HadoopFileSystem(\r\n         ^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow/_hdfs.pyx\", line 96, in pyarrow._hdfs.HadoopFileSystem.__init__\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 115, in pyarrow.lib.check_status\r\nOSError: HDFS connection failed\r\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:767)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:749)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1019)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\r\n\tat java.lang.Thread.run(Thread.java:745)\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\nUse `from_spark()` function in pyspark YARN setting. I set `cache_dir` to HDFS path.\n\n### Expected behavior\n\nWork as described in document\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\n(`from_spark()`) Unable to connect HDFS in pyspark YARN setting <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee: https://github.com/huggingface/datasets/actions/runs/11046967444/job/30687294297\r\n```\r\nRun uv pip install --system -r additional-tests-requirements.txt --no-deps\r\n  \u00d7 No solution found when resolving dependencies:\r\n  \u2570\u2500\u25b6 Because the current Python version (3.8.18) does not satisfy Python>=3.9\r\n      and torchdata==0.10.0a0+1a98f21 depends on Python>=3.9, we can conclude\r\n      that torchdata==0.10.0a0+1a98f21 cannot be used.\r\n      And because only torchdata==0.10.0a0+1a98f21 is available and\r\n      you require torchdata, we can conclude that your requirements are\r\n      unsatisfiable.\r\nError: Process completed with exit code 1.\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken: No solution found when resolving dependencies<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAllow `load_dataset` to support the Webdataset format.\r\n\r\nIt allows users to download/stream data from local files or from the Hugging Face Hub.\r\n\r\nMoreover it will enable the Dataset Viewer for Webdataset datasets on HF.\r\n\r\n## Implementation details\r\n\r\n- I added a new Webdataset builder\r\n- dataset with TAR files are now read using the Webdataset builder\r\n- Basic decoding from `webdataset` is used by default, except unsafe ones like pickle\r\n- HF authentication support is done with `xopen`\r\n\r\n## TODOS\r\n\r\n- [x] tests\r\n- [x] docs<end_of_turn>\n<start_of_turn>model\nWebdataset dataset builder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNew fix to #7594\r\nThis PR adds support for the columns argument in the JSON dataset builder.\r\nAdded columns parameter to JsonConfig.\r\nApplied column filtering after table creation, filling missing columns with None.\r\n\r\nExtended tests to cover:\r\n- Selecting a subset of columns\r\n- Handling missing requested columns\r\n- Column selection on list-of-strings case<end_of_turn>\n<start_of_turn>model\nAdd columns support to JSON loader<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n#6636 broke `write_examples_on_file` and `write_batch` from the class `ArrowWriter`. I'm undoing these changes. See #6663.\r\n\r\nNote the current implementation doesn't keep the order of the columns and the schema, thus setting a wrong schema for each column.<end_of_turn>\n<start_of_turn>model\nRevert the changes in `arrow_writer.py` from #6636<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI am uploading an image dataset like this:\r\n```\r\ndataset = load_dataset(\r\n    \"json\",\r\n    data_files={\"train\": \"data/custom_dataset/train.json\", \"validation\": \"data/custom_dataset/val.json\"},\r\n)\r\ndataset = dataset.cast_column(\"images\", Sequence(Image()))\r\ndataset.push_to_hub(\"StanfordAIMI/custom_dataset\", max_shard_size=\"1GB\")\r\n```\r\nwhere it takes a long time in the `Map` process. Do you think I can use multi-processing to map all the image data to the memory first? For the `Map()` function, I can set `num_proc`. But for `push_to_hub` and `cast_column`, I can not find it.\r\n\r\nThanks in advance!\r\n\r\nBest,<end_of_turn>\n<start_of_turn>model\nQuestion: Is there any way for uploading a large image dataset?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 3.5.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am using 'load_dataset 'loads a JSON file, but I found a strange bug: an error will be reported when the length of the JSON file exceeds 160000 (uncertain exact number). I have checked the data through the following code and there are no issues. So I cannot determine the true reason for this error. \r\n\r\nThe data is a list containing a dictionary. As follows: \r\n\r\n[\r\n{'input': 'someting...', 'target': 'someting...', 'type': 'someting...', 'history': ['someting...', ...]}, \r\n...\r\n]\n\n### Steps to reproduce the bug\n\n```\r\nimport json\r\nfrom datasets import load_dataset\r\n\r\npath = \"target.json\"\r\ntemp_path = \"temp.json\"\r\n\r\nwith open(path, \"r\") as f:\r\n    data = json.load(f)\r\n    print(f\"\\n-------the JSON file length is: {len(data)}-------\\n\")\r\n\r\nwith open(temp_path, \"w\") as f:\r\n    json.dump(data[:160000], f)\r\ndataset = load_dataset(\"json\", data_files=temp_path)\r\nprint(\"\\n-------This works when the JSON file length is 160000-------\\n\")\r\n\r\nwith open(temp_path, \"w\") as f:\r\n    json.dump(data[160000:], f)\r\ndataset = load_dataset(\"json\", data_files=temp_path)\r\nprint(\"\\n-------This works and eliminates data issues-------\\n\")\r\n\r\nwith open(temp_path, \"w\") as f:\r\n    json.dump(data[:170000], f)\r\ndataset = load_dataset(\"json\", data_files=temp_path)\r\n```\n\n### Expected behavior\n\n```\r\n-------the JSON file length is: 173049-------\r\n\r\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-acf3c7f418c5f4b4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 3328.81it/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 639.47it/s]\r\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-acf3c7f418c5f4b4/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 265.85it/s]\r\n\r\n-------This works when the JSON file length is 160000-------\r\n\r\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-a42f04b263ceea6a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 2038.05it/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 794.83it/s]\r\nDataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-a42f04b263ceea6a/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\r\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 681.00it/s]\r\n\r\n-------This works and eliminates data issues-------\r\n\r\nDownloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-63f391c89599c7b0/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 3682.44it/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 788.70it/s]\r\nGenerating train split: 0 examples [00:00, ? examples/s]Failed to read file '/home/lakala/hjc/code/pycode/glm/temp.json' with error <class 'pyarrow.lib.ArrowInvalid'>: cannot mix list and non-list, non-null values\r\nTraceback (most recent call last):\r\n  File \"/home/lakala/conda/envs/glm/lib/python3.8/site-packages/datasets/builder.py\", line 1858, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"/home/lakala/conda/envs/glm/lib/python3.8/site-packages/datasets/packaged_modules/json/json.py\", line 146, in _generate_tables\r\n    raise ValueError(f\"Not able to read records in the JSON file at {file}.\") from None\r\nValueError: Not able to read records in the JSON file at /home/lakala/hjc/code/pycode/glm/temp.json.\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/lakala/hjc/code/pycode/glm/test.py\", line 22, in <module>\r\n    dataset = load_dataset(\"json\", data_files=temp_path)\r\n  File \"/home/lakala/conda/envs/glm/lib/python3.8/site-packages/datasets/load.py\", line 1797, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/lakala/conda/envs/glm/lib/python3.8/site-packages/datasets/builder.py\", line 890, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/lakala/conda/envs/glm/lib/python3.8/site-packages/datasets/builder.py\", line 985, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/lakala/conda/envs/glm/lib/python3.8/site-packages/datasets/builder.py\", line 1746, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/home/lakala/conda/envs/glm/lib/python3.8/site-packages/datasets/builder.py\", line 1891, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Environment info\n\n```\r\nUbuntu==22.04\r\n\r\npython==3.8\r\n\r\npytorch-transformers==1.2.0\r\ntransformers== 4.27.1\r\ndatasets==2.12.0\r\nnumpy==1.24.3\r\npandas==1.5.3\r\n```<end_of_turn>\n<start_of_turn>model\nStrange bug in loading local JSON files, using load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes 3 authentication issues:\r\n- Fix authentication when passing `token`.\r\n- Fix authentication in `Audio.decode_example` and `Image.decode_example`.\r\n- Fix authentication to resolve `data_files` in repositories without script.\r\n\r\nThis PR also fixes our CI so that we properly test when passing `token` and we do not use the token stored in `HfFolder`.\r\n\r\nFix #6126.\r\n\r\n## Details\r\n\r\n\r\n### Fix authentication when passing `token`\r\n\r\nSee c0a77dc943de68a17f23f141517028c734c78623\r\n\r\nThe root issue was caused when the `token` was set in an already instantiated `DownloadConfig` and thus not propagated to `self._storage_options`:\r\n\r\n  ```python\r\n  download_config.token = token\r\n  ```\r\n\r\nAs this usage pattern is very common, the fix consists in overriding `DownloadConfig.__setattr__`.\r\n\r\nThis fixes authentication issues in the following functions:\r\n- `load_dataset` and `load_dataset_builder`\r\n- `Dataset.push_to_hub` and `Dataset.push_to_hub`\r\n- `inspect.get_dataset_config_info`, `inspect.get_dataset_infos` and `inspect.get_dataset_split_names`\r\n\r\n### Fix authentication in `Audio.decode_example` and `Image.decode_example`. \r\n\r\nSee: 58e62af004b6b8b84dcfd897a4bc71637cfa6c3f\r\n\r\nThe `token` was not set because the `repo_id` was wrongly tried to be parsed from an HTTP URL (`\"http://...\"`), instead of an HFFileSystem URL (`\"hf://\"`)\r\n\r\n### Fix authentication to resolve `data_files` in repositories without script\r\n\r\nSee: e4684fc1032321abf0d494b0c130ea7c82ebda80\r\n\r\nThis is fixed by passing `download_config` to the function `create_builder_configs_from_metadata_configs`<end_of_turn>\n<start_of_turn>model\nFix authentication issues<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\ndatasets version: 2.14.5\r\n\r\nwhen trying to run the following command\r\ntrec = load_dataset('trec', split='train[:1000]', cache_dir='/path/to/my/dir')\r\n\r\nI keep getting error saying the command does not have permission to the default cache directory on my macbook pro machine. \r\n\r\nIt seems the cache_dir parameter cannot change the dataset saving directory from the default \r\n\r\nwhat ever explained in the https://huggingface.co/docs/datasets/cache does not seem to work\r\n\n\n### Steps to reproduce the bug\n\ndatasets version: 2.14.5\r\n\r\nwhen trying to run the following command\r\ntrec = load_dataset('trec', split='train[:1000]', cache_dir='/path/to/my/dir')\r\n\r\nI keep getting error saying the command does not have permission to the default cache directory on my macbook pro machine. \r\n\r\nIt seems the cache_dir parameter cannot change the dataset saving directory from the default \r\n\r\nwhat ever explained in the https://huggingface.co/docs/datasets/cache does not seem to work\n\n### Expected behavior\n\nthe dataset should be saved to the cache_dir points to \n\n### Environment info\n\ndatasets version: 2.14.5\r\nmacos X: Ventura 13.4.1 (c)<end_of_turn>\n<start_of_turn>model\nload_dataset() function's cache_dir does not seems to work<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDatasets 2.18.0 - 3.5.0 has a dependency on dill < 0.3.9. This causes errors with dill >= 0.3.9.\n\nCould you please take a look into it and make it compatible?\n\n### Steps to reproduce the bug\n\n1. Install setuptools >= 2.18.0\n2. Install dill >=0.3.9\n3. Run pip check\n4. Output:\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 2.18.0 requires dill<0.3.9,>=0.3.0, but you have dill 0.3.9 which is incompatible.\n\n### Expected behavior\n\nPip install both libraries without any errors\n\n### Environment info\n\nDatasets version: 2.18 - 3.5\nPython: 3.11<end_of_turn>\n<start_of_turn>model\nIncompatibile dill version (0.3.9) in datasets 2.18.0 - 3.5.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI was not able to open a local saved dataset anymore that was created using an older datasets version after the upgrade yesterday from datasets 3.3.2 to 3.4.0\n\nThe traceback is \n\n```\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/arrow/arrow.py\", line 67, in _generate_tables\n    batches = pa.ipc.open_stream(f)\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 190, in open_stream\n    return RecordBatchStreamReader(source, options=options,\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 52, in __init__\n    self._open(source, options=options, memory_pool=memory_pool)\n  File \"pyarrow/ipc.pxi\", line 1006, in pyarrow.lib._RecordBatchStreamReader._open\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Expected to read 538970747 metadata bytes, but only read 2126\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1855, in _prepare_split_single\n    for _, table in generator:\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/arrow/arrow.py\", line 69, in _generate_tables\n    reader = pa.ipc.open_file(f)\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 234, in open_file\n    return RecordBatchFileReader(\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/ipc.py\", line 110, in __init__\n    self._open(source, footer_offset=footer_offset,\n  File \"pyarrow/ipc.pxi\", line 1090, in pyarrow.lib._RecordBatchFileReader._open\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Not an Arrow file\n```\n\n### Steps to reproduce the bug\n\nLoad a dataset from a local folder with \n\n```\ndataset = load_dataset(\n                args.train_data_dir,\n                cache_dir=args.cache_dir,\n            )\n```\nas it is done for example in the training script for SD3 controlnet.\n\nThis is the minimal script to test it: \n\n```\nfrom datasets import load_dataset\n\ndef main():\n    dataset = load_dataset(\n        \"local_dataset\",  \n    )\n    print(dataset)\n    print(\"Sample data:\", dataset[\"train\"][0])\n\nif __name__ == \"__main__\":\n    main()\n````\n\n### Expected behavior\n\nWork in 3.4.0 like in 3.3.2\n\n### Environment info\n\n- `datasets` version: 3.4.0\n- Platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- `huggingface_hub` version: 0.29.3\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nProblems with local dataset after upgrade from 3.3.2 to 3.4.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhile browsing the dataset at https://huggingface.co/datasets/NeuML/wikipedia-20250123, I noticed that a dataset with nearly 7M entries was estimated to be only 4M in size\u2014almost half the actual amount. According to the post-download loading and the dataset_info (https://huggingface.co/datasets/NeuML/wikipedia-20250123/blob/main/train/dataset_info.json), the true data volume is indeed close to 7M. This significant discrepancy could mislead users when sorting datasets by row count. Why not directly retrieve this information from dataset_info?\n\nNot sure if this is the right place to report this bug, but leaving it here for the team's awareness.<end_of_turn>\n<start_of_turn>model\nFront-end statistical data quantity deviation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport streaming datasets with `pyarrow.parquet.read_table`.\r\n\r\nSee: https://huggingface.co/datasets/uonlp/CulturaX/discussions/2\r\n\r\nCC: @AndreaFrancis <end_of_turn>\n<start_of_turn>model\nSupport streaming datasets with pyarrow.parquet.read_table<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd decorator as explicit test dependency.\r\n\r\nWe use `decorator` library in our CI test since PR:\r\n- #4845\r\n\r\nHowever we did not add it as an explicit test requirement, and we depended on it indirectly through other libraries' dependencies.\r\n\r\nI discovered this while testing Numpy 2.0 and removing incompatible libraries.<end_of_turn>\n<start_of_turn>model\nAdd decorator as explicit test dependency<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6242 <end_of_turn>\n<start_of_turn>model\nFix cast from fixed size list to variable size list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ndelay the import until Video() is instantiated + also import duckdb first (otherwise importing duckdb later causes a segfault)<end_of_turn>\n<start_of_turn>model\nfix decord import<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPR for issue #6782.\r\nMakes `cast_storage` of the `Image` class faster by removing the slow call to `.pylist`.\r\nInstead directly convert each `ListArray` item to either `Array2DExtensionType` or `Array3DExtensionType`.\r\n\r\nThis also preserves the `dtype` removing the warning if the array is already `uint8`.<end_of_turn>\n<start_of_turn>model\nMake Image cast storage faster<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTrain model with my custom dataset stored in HuggingFace and loaded with the loading script requires authentication but I am not sure how ?\r\n\r\n\r\nI am logged in in the terminal, in the browser. I receive this error: \r\n\r\n\r\n/python3.8/site-packages/datasets/utils/file_utils.py\", line 566, in get_from_cache\r\n    raise ConnectionError(f\"Couldn't reach {url} ({repr(head_error)})\")\r\nConnectionError: Couldn't reach https://huggingface.co/datasets/fkov/s/blob/main/data/s/train/labels `(ConnectionError('Unauthorized for URL `https://huggingface.co/datasets/fkov/s/blob/main/data/s/train/labels. Please use the parameter `**`use_auth_token=True`**` after logging in with `**`huggingface-cli login`**`'))\r\n\r\nwhen I added: `use_auth_token=True` and logged in via terminal then I received error:\r\n\r\nor the same error in different format: \r\nraise ConnectionError(f\"`Couldn't reach {url} (error {response.status_code}`)\")\r\nConnectionError: Couldn't reach https://huggingface.co/datasets/fkov/s/blob/main/data/s/train/labels (`error 401`)\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\n1.  cloned transformers library locally:\r\nhttps://huggingface.co/docs/transformers/v4.15.0/examples :\r\n\r\n> git clone https://github.com/huggingface/transformers\r\n> cd transformers\r\n> pip install .\r\n> cd /transformers/examples/pytorch/audio-classification\r\n> pip install -r requirements.txt\r\n\r\n2. created **loading script** \r\n> https://huggingface.co/docs/datasets/dataset_script  added next to dataset:\r\n\r\n3. uploaded **private custom dataset** with loading script to HuggingFace\r\n> https://huggingface.co/docs/datasets/dataset_script\r\n\r\n4. added dataset loading script to **local directory** in the above cloned transformers library:\r\n> cd /transformers/examples/pytorch/audio-classification\r\n\r\n5. logged in to HuggingFace on local terminal with :\r\n> **huggingface-cli login**\r\n\r\n6.  run the model with the custom dataset stored on HuggingFace with code:  https://github.com/huggingface/transformers/blob/main/examples/pytorch/audio-classification/README.md\r\n\r\n     cd /transformers/examples/pytorch/audio-classification\r\n>  python run_audio_classification.py \\\r\n>     --model_name_or_path facebook/wav2vec2-base \\\r\n>     --output_dir l/users/flck/outputs/wav2vec2-base-s \\\r\n>     --overwrite_output_dir \\\r\n>     --dataset_name s \\\r\n>     --dataset_config_name s \\\r\n>     --remove_unused_columns False \\\r\n>     --do_train \\\r\n>     --do_eval \\\r\n>     --fp16 \\\r\n>     --learning_rate 3e-5 \\\r\n>     --max_length_seconds 1 \\\r\n>     --attention_mask False \\\r\n>     --warmup_ratio 0.1 \\\r\n>     --num_train_epochs 5 \\\r\n>     --per_device_train_batch_size 32 \\\r\n>     --gradient_accumulation_steps 4 \\\r\n>     --per_device_eval_batch_size 32 \\\r\n>     --dataloader_num_workers 4 \\\r\n>     --logging_strategy steps \\\r\n>     --logging_steps 10 \\\r\n>     --evaluation_strategy epoch \\\r\n>     --save_strategy epoch \\\r\n>     --load_best_model_at_end True \\\r\n>     --metric_for_best_model accuracy \\\r\n>     --save_total_limit 3 \\\r\n>     --seed 0 \\\r\n>     --push_to_hub \\\r\n>     **--use_auth_token=True**  \r\n\r\n\n\n### Expected behavior\n\nBe able to train a model the https://github.com/huggingface/transformers/blob/main/examples/pytorch/audio-classification/ run_audio_classification.py with private custom dataset stored on HuggingFace.\n\n### Environment info\n\n- datasets version:                2.12.0                  \r\n- `transformers` version: 4.30.0.dev0\r\n- Platform: Linux-5.4.204-ql-generic-12.0-19-x86_64-with-glibc2.17\r\n- Python version: 3.8.12\r\n- Huggingface_hub version: 0.15.1\r\n- Safetensors version: 0.3.1\r\n- PyTorch version (GPU?): 2.0.1+cu117 (True)\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.3\r\n[pip3] torch==2.0.1\r\n[pip3] torchaudio==2.0.2\r\n[conda] numpy                     1.24.3                   pypi_0    pypi\r\n[conda] torch                     2.0.1                    pypi_0    pypi\r\n[conda] torchaudio                2.0.2                    pypi_0    pypi\r\n<end_of_turn>\n<start_of_turn>model\nloading private custom dataset script - authentication error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTrying to read a bunch of JSON files into Dataset class but default approach doesn't work. I don't get why it works when I read it one by one but not when I pass as a list :man_shrugging: \r\n\r\nThe code fails with \r\n\r\n```\r\nArrowInvalid: JSON parse error: Invalid value. in row 0  \r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte \r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Steps to reproduce the bug\n\nThis doesn't work\r\n\r\n```\r\nfrom datasets import Dataset\r\n\r\n# dir contains 100 json files.\r\nDataset.from_json(\"/PUT SOME PATH HERE/*\")\r\n```\r\n\r\nThis works:\r\n```\r\nfrom datasets import concatenate_datasets\r\n\r\nls_ds = []\r\n\r\nfor file in list_of_json_files:\r\n    ls_ds.append(Dataset.from_json(file))\r\n\r\nds = concatenate_datasets(ls_ds)\r\n\r\n```\r\n\r\n\r\n\r\n\n\n### Expected behavior\n\nI expect this to read json files properly as error is not clear\n\n### Environment info\n\n- `datasets` version: 2.17.0\r\n- Platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\nCan't Read List of JSON Files Properly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.0.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs a hotfix for CI, temporarily pin ruff upper version < 0.5.0.\r\n\r\nFix #7006.\r\n\r\nRevert once root cause is fixed.<end_of_turn>\n<start_of_turn>model\nFix CI by temporarily pinning ruff < 0.5.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe underlying `SqlDatasetWriter` has `num_proc` as an available argument [here](https://github.com/huggingface/datasets/blob/5dc1a179783dff868b0547c8486268cfaea1ea1f/src/datasets/io/sql.py#L63) , but `Dataset.to_sql()` does not accept it, therefore it is always using one process for the SQL conversion.<end_of_turn>\n<start_of_turn>model\n`Dataset.to_sql` doesn't utilize `num_proc`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nmy dataset is in form : train- image /n -labels\r\n\r\nand tried the code:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndata_files = {\r\n    \"train\": \"/content/datasets/PotholeDetectionYOLOv8-1/train/\",\r\n      \"validation\": \"/content/datasets/PotholeDetectionYOLOv8-1/valid/\",\r\n      \"test\": \"/content/datasets/PotholeDetectionYOLOv8-1/test/\"\r\n}\r\ndataset = load_dataset(\"imagefolder\", data_dir=data_files)\r\ndataset\r\n```\r\ngot error:\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-29-2ef1926f73d9>](https://localhost:8080/#) in <cell line: 8>()\r\n      6       \"test\": \"/content/datasets/PotholeDetectionYOLOv8-1/test/\"\r\n      7 }\r\n----> 8 dataset = load_dataset(\"imagefolder\", data_dir=data_files)\r\n      9 dataset\r\n\r\n6 frames\r\n[/usr/lib/python3.10/pathlib.py](https://localhost:8080/#) in _parse_args(cls, args)\r\n    576                 parts += a._parts\r\n    577             else:\r\n--> 578                 a = os.fspath(a)\r\n    579                 if isinstance(a, str):\r\n    580                     # Force-cast str subclasses to str (issue #21127)\r\n\r\nTypeError: expected str, bytes or os.PathLike object, not dict\r\n```\n\n### Steps to reproduce the bug\n\nas share above \n\n### Expected behavior\n\nload images and labels , but my dataset only uploads images \r\n- https://huggingface.co/datasets/Andyrasika/potholes-dataset\n\n### Environment info\n\ncolab pro <end_of_turn>\n<start_of_turn>model\nTypeError: expected str, bytes or os.PathLike object, not dict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen I try to push to an arrow repo (can provide the link on Slack), it uploads the files but fails to update the metadata, with\r\n```\r\n  File \"app.py\", line 123, in add_new_eval\r\n    eval_results[level].push_to_hub(my_repo, token=TOKEN, split=SPLIT)\r\n  File \"blabla_my_env_path/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 5501, in push_to_hub\r\n    if not metadata_configs:\r\nUnboundLocalError: local variable 'metadata_configs' referenced before assignment\r\n```\r\n\r\nThis fixes it.<end_of_turn>\n<start_of_turn>model\nFix: Missing a MetadataConfigs init when the repo has a `datasets_info.json` but no README<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\njust run import:\r\n`from datasets import load_dataset`\r\n\r\nand then:\r\n```\r\n\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\__init__.py\", line 22, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\arrow_dataset.py\", line 66, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\arrow_reader.py\", line 30, in <module>\r\n    from .download.download_config import DownloadConfig\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\download\\__init__.py\", line 10, in <module>\r\n    from .streaming_download_manager import StreamingDownloadManager\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\download\\streaming_download_manager.py\", line 21, in <module>\r\n    from ..filesystems import COMPRESSION_FILESYSTEMS\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\datasets\\filesystems\\__init__.py\", line 8, in <module>\r\n    import fsspec.asyn\r\n  File \"C:\\ProgramData\\anaconda3\\envs\\py310\\lib\\site-packages\\fsspec\\asyn.py\", line 157, in <module>\r\n    ResourceEror = resource.error\r\nAttributeError: module 'resource' has no attribute 'error'\r\n\r\nProcess finished with exit code 1\r\n\r\n```\r\n\r\n\r\nand the error codes are:\r\n```\r\ntry:\r\n    import resource\r\nexcept ImportError:\r\n    resource = None\r\n    ResourceError = OSError\r\nelse:\r\n    ResourceEror = resource.error\r\n\r\n```\r\n1. miss spelling : \"ResourceEror \" should be \"ResourceErorr\"\r\n2. module 'resource' has no attribute 'error'\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\nonly one step:\r\n\r\n`from datasets import load_dataset`\n\n### Expected behavior\n\nslove error: module 'resource' has no attribute 'error'\n\n### Environment info\n\npython=3.10\r\n\r\ndatasets==2.14.5\r\n<end_of_turn>\n<start_of_turn>model\nmodule 'resource' has no attribute 'error'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd a new 'Streaming best practices' page with practical patterns and pitfalls for large-scale/production use of IterableDataset. Includes examples for batched map with remove_columns, deterministic shuffling with set_epoch, multi-worker sharding, checkpoint/resume, and persistence to Parquet/Hub. Linked from How-to > General usage, next to Stream.<end_of_turn>\n<start_of_turn>model\ndocs: Streaming best practices<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAllow for anonymized viewing of datasets. For instance, something similar to [Anonymous GitHub](https://anonymous.4open.science/).\n\n### Motivation\n\nWe generally publish our data through Hugging Face. This has worked out very well as it's both our repository and archive (thanks to the DOI feature!). However, we have an increasing challenge when it comes to sharing our datasets for paper (both conference and journal) submissions. Due to the need to share data anonymously, we can't use the Hugging Face URLs, but datasets tend to be too large for inclusion as a zip. Being able to have an anonymous link would be great since we can't be double-publishing the data.\n\n### Your contribution\n\nSorry, I don't have a contribution to make to the implementation of this. Perhaps it would be possible to work off the [Anonymous GitHub](https://github.com/tdurieux/anonymous_github) code to generate something analogous with pointers to the data still on Hugging Face's servers (instead of the duplication of data required for the GitHub version)?<end_of_turn>\n<start_of_turn>model\nOption for Anonymous Dataset link<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n```python\r\n# dataset.py\r\nimport os\r\nimport datasets\r\n\r\nif not os.path.exists('/tmp/test.json'):\r\n  with open('/tmp/test.json', 'w') as file:\r\n    file.write('[{\"text\": \"hello\"}]')\r\n\r\ndef transform(example):\r\n  text = example['text']\r\n  # text += ' world'\r\n  return {'text': text}\r\n\r\ndata = datasets.load_dataset('json', data_files=['/tmp/test.json'], split='train')\r\ndata = data.map(transform)\r\n```\r\n\r\n```python\r\n# test.py\r\nimport dataset\r\nprint(next(iter(dataset.data)))\r\n```\r\n\r\nInitialize cache\r\n```\r\npython3 test.py\r\n# {'text': 'hello'}\r\n```\r\n\r\nEdit dataset.py and uncomment the commented line, run again\r\n```\r\npython3 test.py\r\n# {'text': 'hello'}\r\n# expected: {'text': 'hello world'}\r\n```\r\n\r\nClear cache and run again\r\n```\r\nrm -rf ~/.cache/huggingface/datasets/*\r\npython3 test.py\r\n# {'text': 'hello world'}\r\n```\r\n\r\n\r\nIf instead the two files are combined, then changes to the function are detected correctly. But it's expected when working on any realistic codebase that things will be modularized into separate files.<end_of_turn>\n<start_of_turn>model\nMap cache does not detect function changes in another module<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a bug in the `generate_examples` function where `datasets.Value` features with a `float` dtype were incorrectly generated using `np.random.randint`. This resulted in integer values being cast to float, which is not representative of true floating-point data.\r\n\r\n\r\n**Key changes include:**\r\n\r\n* Added explicit handling for `float` features using `np.random.rand` to generate continuous values.\r\n* Introduced fail-fast type checks for unsupported dtypes to improve robustness.\r\n* Added validation for sequence features to ensure `seq_shapes` is provided.\r\n\r\n### Before Fix\r\n\r\nFloat features were generated incorrectly as integers cast to float:\r\n\r\n```text\r\n- Example 0:\r\n- int_feature: 0\r\n- float_feature: 9.0  <-- Incorrect: An integer disguised as a float\r\n- string_feature: The small grey turtle was surprisingly fast...\r\n- seq_feature: [0.3048 0.4291 0.4283]\r\n```\r\n\r\n### After Fix\r\n\r\nFloat features are now correctly generated as continuous numbers in the range [0, 1):\r\n```text\r\n+ Example 0:\r\n+ int_feature: 0\r\n+ float_feature: 0.0183  <-- Correct: A true random float\r\n+ string_feature: The small grey turtle was surprisingly fast...\r\n+ seq_feature: [0.9237 0.7972 0.8526]\r\n```\r\n#### Note: This PR is a follow-up/fix of the previously closed PR #7769 for clarity and context.\r\n<end_of_turn>\n<start_of_turn>model\nFix: Correct float feature generation in `generate_examples` <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nwhile trying to load custom dataset from jsonl file, I get the error: \"metadata.jsonl doesn't contain any data files\"\n\n### Steps to reproduce the bug\n\nThis is my [metadata_v2.jsonl](https://github.com/user-attachments/files/16016011/metadata_v2.json) file.  I have this file in the folder with all images mentioned in that json(l) file.\r\n\r\nThrough below mentioned command I am trying to load_dataset so that I can upload it as mentioned here on the [official website](https://huggingface.co/docs/datasets/en/image_dataset#upload-dataset-to-the-hub).\r\n\r\n````\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"path/to/jsonl/metadata.jsonl\")\r\n````\r\n\r\nerror:\r\n\r\n````\r\nEmptyDatasetError                         Traceback (most recent call last)\r\nCell In[18], line 3\r\n      1 from datasets import load_dataset\r\n----> 3 dataset = load_dataset(\"imagefolder\", \r\n      4                        data_dir=\"path/to/jsonl/file/metadata.jsonl\")\r\n      5 dataset[0][\"objects\"]\r\n\r\nFile ~/anaconda3/envs/lvis/lib/python3.11/site-packages/datasets/load.py:2594, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2589 verification_mode = VerificationMode(\r\n   2590     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\r\n   2591 )\r\n   2593 # Create a dataset builder\r\n-> 2594 builder_instance = load_dataset_builder(\r\n   2595     path=path,\r\n   2596     name=name,\r\n   2597     data_dir=data_dir,\r\n   2598     data_files=data_files,\r\n   2599     cache_dir=cache_dir,\r\n   2600     features=features,\r\n   2601     download_config=download_config,\r\n   2602     download_mode=download_mode,\r\n   2603     revision=revision,\r\n   2604     token=token,\r\n   2605     storage_options=storage_options,\r\n   2606     trust_remote_code=trust_remote_code,\r\n   2607     _require_default_config_name=name is None,\r\n   2608     **config_kwargs,\r\n   2609 )\r\n   2611 # Return iterable dataset in case of streaming\r\n   2612 if streaming:\r\n\r\nFile ~/anaconda3/envs/lvis/lib/python3.11/site-packages/datasets/load.py:2266, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\r\n   2264     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   2265     download_config.storage_options.update(storage_options)\r\n-> 2266 dataset_module = dataset_module_factory(\r\n   2267     path,\r\n   2268     revision=revision,\r\n   2269     download_config=download_config,\r\n   2270     download_mode=download_mode,\r\n   2271     data_dir=data_dir,\r\n   2272     data_files=data_files,\r\n   2273     cache_dir=cache_dir,\r\n   2274     trust_remote_code=trust_remote_code,\r\n   2275     _require_default_config_name=_require_default_config_name,\r\n   2276     _require_custom_configs=bool(config_kwargs),\r\n   2277 )\r\n   2278 # Get dataset builder class from the processing script\r\n   2279 builder_kwargs = dataset_module.builder_kwargs\r\n\r\nFile ~/anaconda3/envs/lvis/lib/python3.11/site-packages/datasets/load.py:1805, in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\r\n   1782 # We have several ways to get a dataset builder:\r\n   1783 #\r\n   1784 # - if path is the name of a packaged dataset module\r\n   (...)\r\n   1796 \r\n   1797 # Try packaged\r\n   1798 if path in _PACKAGED_DATASETS_MODULES:\r\n   1799     return PackagedDatasetModuleFactory(\r\n   1800         path,\r\n   1801         data_dir=data_dir,\r\n   1802         data_files=data_files,\r\n   1803         download_config=download_config,\r\n   1804         download_mode=download_mode,\r\n-> 1805     ).get_module()\r\n   1806 # Try locally\r\n   1807 elif path.endswith(filename):\r\n\r\nFile ~/anaconda3/envs/lvis/lib/python3.11/site-packages/datasets/load.py:1140, in PackagedDatasetModuleFactory.get_module(self)\r\n   1135 def get_module(self) -> DatasetModule:\r\n   1136     base_path = Path(self.data_dir or \"\").expanduser().resolve().as_posix()\r\n   1137     patterns = (\r\n   1138         sanitize_patterns(self.data_files)\r\n   1139         if self.data_files is not None\r\n-> 1140         else get_data_patterns(base_path, download_config=self.download_config)\r\n   1141     )\r\n   1142     data_files = DataFilesDict.from_patterns(\r\n   1143         patterns,\r\n   1144         download_config=self.download_config,\r\n   1145         base_path=base_path,\r\n   1146     )\r\n   1147     supports_metadata = self.name in _MODULE_SUPPORTS_METADATA\r\n\r\nFile ~/anaconda3/envs/lvis/lib/python3.11/site-packages/datasets/data_files.py:503, in get_data_patterns(base_path, download_config)\r\n    501     return _get_data_files_patterns(resolver)\r\n    502 except FileNotFoundError:\r\n--> 503     raise EmptyDatasetError(f\"The directory at {base_path} doesn't contain any data files\") from None\r\n\r\nEmptyDatasetError: The directory at path/to/jsonl/file/metadata.jsonl doesn't contain any data files`\r\n```\n\n### Expected behavior\n\nIt should be able load the whole file in a format of \"dataset\" inside the dataset variable. But it gives error \"The directory at \"path/to/jsonl/metadata.jsonl\" doesn't contain any data files.\"\n\n### Environment info\n\nI am using conda environment.<end_of_turn>\n<start_of_turn>model\nEmptyDatasetError: The directory at /metadata.jsonl doesn't contain any data files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using `load_dataset`to load [HuggingFaceM4/VQAv2](https://huggingface.co/datasets/HuggingFaceM4/VQAv2), I am getting `FSTimeoutError`. \r\n\r\n### Error\r\n\r\n```\r\nTimeoutError: \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nFSTimeoutError                            Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/fsspec/asyn.py](https://klh9mr78js-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20240924-060116_RC00_678132060#) in sync(loop, func, timeout, *args, **kwargs)\r\n     99     if isinstance(return_result, asyncio.TimeoutError):\r\n    100         # suppress asyncio.TimeoutError, raise FSTimeoutError\r\n--> 101         raise FSTimeoutError from return_result\r\n    102     elif isinstance(return_result, BaseException):\r\n    103         raise return_result\r\n\r\nFSTimeoutError:\r\n```\r\n\r\nIt usually fails around 5-6 GB.\r\n\r\n<img width=\"847\" alt=\"Screenshot 2024-09-26 at 9 10 19\u202fPM\" src=\"https://github.com/user-attachments/assets/ff91995a-fb55-4de6-8214-94025d6c8470\">\r\n\n\n### Steps to reproduce the bug\n\nTo reproduce it, run this in colab notebook:\r\n\r\n```\r\n!pip install -q -U datasets\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset('HuggingFaceM4/VQAv2', split=\"train[:10%]\")\r\n```\r\n\r\n\n\n### Expected behavior\n\nIt should download properly.\n\n### Environment info\n\nUsing Colab Notebook.<end_of_turn>\n<start_of_turn>model\n[FSTimeoutError] load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nEncountering an error while loading the ```liuhaotian/LLaVA-Instruct-150K dataset```.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfw =load_dataset(\"liuhaotian/LLaVA-Instruct-150K\")\r\n```\r\nError:\r\n```\r\nArrowInvalid                              Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json/json.py](https://localhost:8080/#) in _generate_tables(self, files)\r\n    136                                 try:\r\n--> 137                                     pa_table = paj.read_json(\r\n    138                                         io.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\r\n\r\n20 frames\r\nArrowInvalid: JSON parse error: Column() changed from object to array in row 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nArrowTypeError                            Traceback (most recent call last)\r\nArrowTypeError: (\"Expected bytes, got a 'int' object\", 'Conversion failed for column id with type object')\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1895             if isinstance(e, DatasetGenerationError):\r\n   1896                 raise\r\n-> 1897             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1898 \r\n   1899         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n### Expected behavior\r\n\r\nI have tried loading the dataset both on my own server and on Colab, and encountered errors in both instances.\r\n\r\n### Environment info\r\n\r\n```\r\n- `datasets` version: 3.2.0\r\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.26.3\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.9.0\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nArrowInvalid: JSON parse error: Column() changed from object to array in row 0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nit was failing since 3.0 and therefore not updating download counts on HF or in our dashboard<end_of_turn>\n<start_of_turn>model\nfix increase_load_count<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6564 <end_of_turn>\n<start_of_turn>model\nAdd `with_rank` param to `Dataset.filter`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate tqdm >= 4.66.3 to fix vulnerability,<end_of_turn>\n<start_of_turn>model\nUpdate tqdm >= 4.66.3 to fix vulnerability<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am using an iterable dataset in a multi-node setup, trying to do training/inference while filtering the data on the fly. I usually do not use `split_dataset_by_node` but it is very slow using the IterableDatasetShard in `accelerate` and `transformers`. When I filter after applying `split_dataset_by_node`, it results in shards that are not equal sizes due to unequal samples filtered from each one. \r\n\r\nThe distributed process hangs when trying to accomplish this. Is there any way to resolve this or is it impossible to implement?\r\n\r\n\n\n### Steps to reproduce the bug\n\nHere is a toy example of what I am trying to do that reproduces the behavior\r\n\r\n```\r\n# torchrun --nproc-per-node 2 file.py\r\n\r\n\r\nimport os\r\n\r\nimport pandas as pd\r\nimport torch\r\nfrom accelerate import Accelerator\r\nfrom datasets import Features, Value, load_dataset\r\nfrom datasets.distributed import split_dataset_by_node\r\nfrom torch.utils.data import DataLoader\r\n\r\naccelerator = Accelerator(device_placement=True, dispatch_batches=False)\r\nif accelerator.is_main_process:\r\n    if not os.path.exists(\"scratch_data\"):\r\n        os.mkdir(\"scratch_data\")\r\n\r\n    n_shards = 4\r\n    for i in range(n_shards):\r\n        df = pd.DataFrame({\"id\": list(range(10 * i, 10 * (i + 1)))})\r\n        df.to_parquet(f\"scratch_data/shard_{i}.parquet\")\r\n\r\n\r\nworld_size = accelerator.num_processes\r\nlocal_rank = accelerator.process_index\r\n\r\n\r\ndef collate_fn(examples):\r\n    input_ids = []\r\n    for example in examples:\r\n        input_ids.append(example[\"id\"])\r\n    return torch.LongTensor(input_ids)\r\n\r\n\r\ndataset = load_dataset(\r\n    \"parquet\", data_dir=\"scratch_data\", split=\"train\", streaming=True\r\n)\r\ndataset = (\r\n    split_dataset_by_node(dataset, rank=local_rank, world_size=world_size)\r\n    .filter(lambda x: x[\"id\"] < 35)\r\n    .shuffle(seed=42, buffer_size=100)\r\n)\r\n\r\nbatch_size = 2\r\ntrain_dataloader = DataLoader(\r\n    dataset,\r\n    batch_size=batch_size,\r\n    collate_fn=collate_fn,\r\n    num_workers=2\r\n)  \r\n\r\nfor x in train_dataloader:\r\n    x = x.to(accelerator.device)\r\n    print({\"rank\": local_rank, \"id\": x})\r\n   \r\n    y = accelerator.gather_for_metrics(x)\r\n    if accelerator.is_main_process:\r\n        print(\"gathered\", y)\r\n\r\n\r\n```\n\n### Expected behavior\n\nIs there any way to continue training/inference on the GPUs that have remaining data left without waiting for the others? Is it impossible to filter when \n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.10.209-198.812.amzn2.x86_64-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.21.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2023.6.0<end_of_turn>\n<start_of_turn>model\nIs there any way to solve hanging of IterableDataset using split by node + filtering during inference<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRetry the step that (download + update + upload) the README.md using `create_commit(..., parent_commit=...)` if there was a commit in the meantime. This should enable concurrent `push_to_hub()` since it won't overwrite the README.md metadata anymore.\r\n\r\nNote: we fixed an issue server side to make this work:\r\n\r\n<details>\r\n\r\nDO NOT MERGE FOR NOW since it seems there is one bug that prevents this logic from working:\r\n\r\nI'm using parent_commit to enable concurrent push_to_hub() in datasets for a retry mechanism, but for some reason I always run into a weird situation.\r\nSometimes create_commit(.., parent_commit=...) returns error 500 but the commit did happen on the Hub side without respecting parent_commit\r\n\r\ne.g. request id\r\n```\r\nhuggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/lhoestq/tmp/commit/main (Request ID: Root=1-6888d8af-2ce517bc60c69cb378b51526;d1b17993-c5d0-4ccd-9926-060c45f9ed61)\r\n```\r\n\r\nfix coming in [internal](https://github.com/huggingface-internal/moon-landing/pull/14617)\r\n\r\n</details>\r\n\r\nclose https://github.com/huggingface/datasets/issues/7600<end_of_turn>\n<start_of_turn>model\nConcurrent push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs reported by @regisss, an `ExpectedMoreSplits` error is raised when passing `data_dir`:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\r\n    \"lvwerra/stack-exchange-paired\",\r\n    split=\"train\",\r\n    cache_dir=None,\r\n    data_dir=\"data/rl\",\r\n)\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2609, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1027, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1140, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/info_utils.py\", line 92, in verify_splits\r\n    raise ExpectedMoreSplits(str(set(expected_splits) - set(recorded_splits)))\r\ndatasets.utils.info_utils.ExpectedMoreSplits: {'test'}\r\n```<end_of_turn>\n<start_of_turn>model\nExpectedMoreSplits error when using data_dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUsing various version for datasets, I'm no more longer able to load that dataset without a segmentation fault. \r\nSeveral others files are also concerned. \n\n### Steps to reproduce the bug\n\n# Create a new venv \r\npython3 -m venv venv_test \r\nsource venv_test/bin/activate\r\n\r\n# Install the latest version \r\npip install datasets\r\n\r\n# Load that dataset\r\npython3 -q -X faulthandler -c \"from datasets import load_dataset;  load_dataset('EuropeanParliament/Eurovoc', '1998-09')\"\n\n### Expected behavior\n\nData must be loaded \n\n### Environment info\n\ndatasets==2.19.0\r\nPython 3.11.7\r\nDarwin  22.5.0 Darwin Kernel Version 22.5.0: Mon Apr 24 20:51:50 PDT 2023; root:xnu-8796.121.2~5/RELEASE_X86_64 x86_64<end_of_turn>\n<start_of_turn>model\nSegmentation fault<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse more consistent wording in `remove_columns` to explain why it's faster than `map` and update `remove_columns`/`rename_columns` docstrings to fix in-place calls.\r\n\r\n\r\nReported in https://github.com/huggingface/datasets/issues/6700<end_of_turn>\n<start_of_turn>model\n`remove_columns`/`rename_columns` doc fixes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n\r\nuse python -m pip install datasets to install\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\n\n### Expected behavior\n\nit doesn't work\n\n### Environment info\n\ndatasets version==2.15.0\r\npython == 3.10.12\r\nlinux  version I don't know??<end_of_turn>\n<start_of_turn>model\nCannot import name 'load_dataset' from .... module \u2018datasets\u2019<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6366<end_of_turn>\n<start_of_turn>model\nFix python formatting for complex types in `format_table`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn this PR I add support for `.pth` but with `weights_only=True` to disallow the use of pickle<end_of_turn>\n<start_of_turn>model\n[WebDataset] Add `.pth` support for torch tensors<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n# Feel free to give suggestions please..\r\n\r\n### This PR is raised because of issue: https://github.com/huggingface/datasets/issues/7080\r\n\r\n![image](https://github.com/user-attachments/assets/8fbc604f-f0a5-4a59-a63e-aa4c26442c83)\r\n\r\n### This PR gives solution to https://github.com/huggingface/datasets/issues/7080\r\n\r\n1. Checking whether the dataset has splits or subdatasets.\r\n2. Printing the available splits/subdatasets.\r\n3. Asking the user to choose which one to load.\r\n4. Loading only the selected dataset based on the user's input.\r\n\r\n### Key Changes:\r\n\r\n1. Available Splits/Subdatasets: The code checks for available splits/subdatasets using builder_instance.info.splits.keys().\r\n2. User Prompt: If splits are found, it prints them out and prompts the user to select one.\r\n3. Loading Based on User Input: The dataset is loaded based on the user's choice.\r\n\r\nThis way, the dataset loading function will interactively prompt the user to select which subdataset or split they want to load instead of automatically loading all of them.\r\n<end_of_turn>\n<start_of_turn>model\nSolution to issue: #7080 Modified load_dataset function, so that it prompts the user to select a dataset when subdatasets or splits (train, test) are available<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix empty dataset to_parquet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nBelebele is a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. This dataset enables the evaluation of mono- and multi-lingual models in high-, medium-, and low-resource languages. Each question has four multiple-choice answers and is linked to a short passage from the [FLORES-200](https://github.com/facebookresearch/flores/tree/main/flores200) dataset. The human annotation procedure was carefully curated to create questions that discriminate between different levels of generalizable language comprehension and is reinforced by extensive quality checks. While all questions directly relate to the passage, the English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. Belebele opens up new avenues for evaluating and analyzing the multilingual abilities of language models and NLP systems.\r\n\r\nPlease refer to paper for more details, [The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants](https://arxiv.org/abs/2308.16884).\r\n\r\n\r\n## Composition\r\n\r\n- 900 questions per language variant\r\n- 488 distinct passages, there are 1-2 associated questions for each.\r\n- For each question, there is 4 multiple-choice answers, exactly 1 of which is correct.\r\n- 122 language/language variants (including English).\r\n- 900 x 122 = 109,800 total questions.\r\n\n\n### Motivation\n\nofficial repo https://github.com/facebookresearch/belebele\n\n### Your contribution\n\n-<end_of_turn>\n<start_of_turn>model\nAdd Belebele multiple-choice machine reading comprehension (MRC) dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd docs about the CLI.\r\n\r\nClose #6830.\r\n\r\nCC: @severo <end_of_turn>\n<start_of_turn>model\nAdd docs about the CLI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe following code fails because LargeListType is not supported.\r\nThis is especially a problem for .from_polars since polars uses LargeListType.\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport datasets\r\nimport polars as pl\r\n\r\ndf = pl.DataFrame({\"list\": [[]]})\r\ndatasets.Dataset.from_polars(df)\r\n```\r\n\r\n### Expected behavior\r\n\r\nConvert LargeListType to list.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.19.1.dev0\r\n- Platform: Linux-6.8.7-200.fc39.x86_64-x86_64-with-glibc2.38\r\n- Python version: 3.12.2\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 16.0.0\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nlargelisttype not supported (.from_polars())<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nExporting the processed audio inside the table with the dataset.to_parquet function, the object pyarrow {bytes: null, path: \"Some/Path\"}\r\nAt the same time, the same dataset uploaded to the hub has bit arrays\r\n\r\n![Screenshot from 2024-05-27 19-14-49](https://github.com/huggingface/datasets/assets/140120605/ddfba089-426f-4659-9df4-7a634c948b9e)\r\n![Screenshot from 2024-05-27 19-12-51](https://github.com/huggingface/datasets/assets/140120605/4cf8c0a1-650e-491b-86c8-b475c284a021)\r\n\n\n### Steps to reproduce the bug\n\n1.Get dataset from audio and cast it \r\n2.Export and push dataset\r\n3.It\u2019s scary to be indignant at the difference in the uploaded dataset and the fact that it was saved locally\r\n```py\r\nfrom datasets import Dataset, Audio \r\ndf = Dataset.from_csv(\"./datasets.csv\")\r\ndf = df.cast_column(\"audio\", Audio(16000))\r\ndf.to_parquet(\"./datasets.parquet\")\r\ndf.push_to_hub(repo_id=\"************\", token=\"**********************\")\r\n```\r\n\r\nYou can use \"try replicate case\" for this\r\n[replicate_packet.zip](https://github.com/huggingface/datasets/files/15457114/replicate_packet.zip)\r\n\n\n### Expected behavior\n\nTwo parquet tables identical in content. It is obvious?\n\n### Environment info\n\nPython 3.11+ (I try did it in 3.12 and got same result )<end_of_turn>\n<start_of_turn>model\nExport Parquet Tablet Audio-Set is null bytes in Arrow <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR gets multiprocessing to work on mac os:\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\").take(100)\n\nds = ds.map(lambda x: x, num_proc=4)\n\nds.push_to_hub(\"burtenshaw/dataset-test\", num_proc=4)\n```<end_of_turn>\n<start_of_turn>model\n[FIX] force spawning pool for MacOS<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nThe boiler plate code to access a dataset via the hugging face file system is very useful. Please addd \r\n\r\n\r\n## Add Polars (R) option\r\nThe equivailent code works, because the [Polars-R](https://github.com/pola-rs/r-polars) wrapper has hugging faces funcitonaliy as well.\r\n\r\n```r\r\nlibrary(polars)\r\n\r\ndf <- pl$read_parquet(\"hf://datasets/SALURBAL/core__admin_cube_public/core__admin_cube_public.parquet\")\r\n```\r\n\r\n## Polars (python) option\r\n![image](https://github.com/user-attachments/assets/8f1bcd19-e578-4b18-b324-7cc00b80ac0a)\r\n\r\n\r\n## Libraries Currently\r\n\r\n![image](https://github.com/user-attachments/assets/0cf50063-f9db-443c-97b4-3ef0664b6e6e)\r\n\r\n\r\n\r\n\r\n### Motivation\r\n\r\nThere are many data/analysis/research/statistics teams (particularly in academia and pharma) that use R as the default language. R has great integration with most of the newer data techs (arrow, parquet, polars) and having this included could really help in bringing this community into the hugging faces ecosystem.\r\n\r\n**This is a small/low-hanging-fruit front end change but would make a big impact expanding the community**\r\n\r\n### Your contribution\r\n\r\nI am not sure which repositroy this should be in, but I have experience in R, Python and JS and happy to submit a PR in the appropriate repository. <end_of_turn>\n<start_of_turn>model\nAdd R as a How to use from the Polars (R) Library as an option<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\n[MINOR:TYPO] Fix typo in exception text<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe url is: https://huggingface.co/datasets/Confirm-Labs/pile_ngrams_trigrams\r\n\r\nI am able to successfully view the \u201cFiles and versions\u201d tab: [Confirm-Labs/pile_ngrams_trigrams at main](https://huggingface.co/datasets/Confirm-Labs/pile_ngrams_trigrams/tree/main)\r\n\r\nAny help would be appreciated! Thanks! I hope this is the right place to report an issue like this.\r\n<end_of_turn>\n<start_of_turn>model\nViewing dataset card returns \u201c502 Bad Gateway\u201d<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease v3.1.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHere is the code for single-GPU processing: https://pastebin.com/bfmEeK2y\r\nHere is the code for multi-GPU processing: https://pastebin.com/gQ7i5AQy\r\n\r\nHere is the video showing that the multi-GPU mapping does not work as expected (there are so many things wrong here, it's better to watch the 3-minute video than explain here):\r\n\r\nhttps://youtu.be/RNbdPkSppc4\n\n### Steps to reproduce the bug\n\n-\n\n### Expected behavior\n\n-\n\n### Environment info\n\nx2 RTX A4000<end_of_turn>\n<start_of_turn>model\nmulti-GPU map does not work<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nRegardless of what method I use, datasets defaults to zstandard for unpacking my datasets.\r\n\r\nThis is poor behavior, because not only is zstandard not a dependency in the huggingface package (and therefore, your dataset loading will be interrupted while it asks you to install the package), but it happens on datasets that are uploaded in json format too, meaning the dataset loader will attempt to convert the data to a zstandard compatible format, and THEN try to unpackage it.\r\n\r\nMy 4tb drive runs out of room when using zstandard on slimpajama. It loads fine on 1.5tb when using json, however I lack the understanding of the \"magic numbers\" system used to select the unpackaging algorithm, so I can't push a change myself.\r\n\r\nCommenting out this line, in \"/datasets/utils/extract.py\" fixes the issue, and causes SlimPajama to properly extract using rational amounts of storage, however it completely disables zstandard, which is probably undesirable behavior. Someone with an understanding of the \"magic numbers\" system should probably take a pass over this issue.\r\n\r\n```\r\nclass Extractor:\r\n    #  Put zip file to the last, b/c it is possible wrongly detected as zip (I guess it means: as tar or gzip)\r\n    extractors: Dict[str, Type[BaseExtractor]] = {\r\n        \"tar\": TarExtractor,\r\n        \"gzip\": GzipExtractor,\r\n        \"zip\": ZipExtractor,\r\n        \"xz\": XzExtractor,\r\n        #\"zstd\": ZstdExtractor,   # This line needs to go, in order for datasets to work w/o non-dependent packages\r\n        \"rar\": RarExtractor,\r\n        \"bz2\": Bzip2Extractor,\r\n        \"7z\": SevenZipExtractor,  # <Added version=\"2.4.0\"/>\r\n        \"lz4\": Lz4Extractor,  # <Added version=\"2.4.0\"/>\r\n    }\r\n```\n\n### Steps to reproduce the bug\n\n'''\r\nfrom datasaets import load_dataset\r\nload_dataset(path=\"/cerebras/SlimPajama-627B\")\r\n'''\r\n\r\nThis alone should trigger the error on any system that does not have zstandard pip installed.\n\n### Expected behavior\n\nThis repository (which is encoded in json format, not zstandard) should check whether zstandard is installed before defaulting to it. Additionally, using zstandard should not use more than 3x the required space that other extraction mechanisms use.\n\n### Environment info\n\n- `datasets` version: 2.17.1\r\n- Platform: Linux-6.5.0-18-generic-x86_64-with-glibc2.35\r\n- Python version: 3.12.0\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\n.load_dataset() method defaults to zstandard<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen downloading the dataset \"mteb/mtop_domain\", ran into the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/share/project/xzy/test/test_download.py\", line 3, in <module>\r\n    data = load_dataset(\"mteb/mtop_domain\", \"en\", trust_remote_code=True)\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2606, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2277, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1923, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1896, in dataset_module_factory\r\n    ).get_module()\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1507, in get_module\r\n    local_path = self.download_loading_script()\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1467, in download_loading_script\r\n    return cached_path(file_path, download_config=download_config)\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 211, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 689, in get_from_cache\r\n    fsspec_get(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 395, in fsspec_get\r\n    fs.get_file(path, temp_file.name, callback=callback)\r\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 648, in get_file\r\n    http_get(\r\n  File \"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py\", line 578, in http_get\r\n    raise EnvironmentError(\r\nOSError: Consistency check failed: file should be of size 2191 but has size 2190 ((\u2026)ets/mteb/mtop_domain@main/mtop_domain.py).\r\nWe are sorry for the inconvenience. Please retry with `force_download=True`.\r\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.\r\n```\r\nTry to download through HF datasets directly but got the same error as above.\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndata = load_dataset(\"mteb/mtop_domain\", \"en\")\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\n\r\ndata = load_dataset(\"mteb/mtop_domain\", \"en\", force_download=True)\r\n```\r\nWith and without `force_download=True` both ran into the same error.\n\n### Expected behavior\n\nShould download the dataset successfully.\n\n### Environment info\n\n- datasets version: 2.21.0\r\n- huggingface-hub version: 0.24.6<end_of_turn>\n<start_of_turn>model\nBug: Error when downloading mteb/mtop_domain<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport fsspec 2024.6.1.<end_of_turn>\n<start_of_turn>model\nSupport fsspec 2024.6.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nbump fsspec<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-11-9b99bc80ec23>](https://localhost:8080/#) in <cell line: 11>()\r\n      9 import numpy as np\r\n     10 import matplotlib.pyplot as plt\r\n---> 11 from datasets import DatasetDict, Dataset\r\n     12 from transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n     13 from transformers import Trainer, TrainingArguments\r\n\r\n5 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/__init__.py](https://localhost:8080/#) in <module>\r\n     16 __version__ = \"2.17.0\"\r\n     17 \r\n---> 18 from .arrow_dataset import Dataset\r\n     19 from .arrow_reader import ReadInstruction\r\n     20 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in <module>\r\n     65 \r\n     66 from . import config\r\n---> 67 from .arrow_reader import ArrowReader\r\n     68 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n     69 from .data_files import sanitize_patterns\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py](https://localhost:8080/#) in <module>\r\n     27 \r\n     28 import pyarrow as pa\r\n---> 29 import pyarrow.parquet as pq\r\n     30 from tqdm.contrib.concurrent import thread_map\r\n     31 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/__init__.py](https://localhost:8080/#) in <module>\r\n     18 # flake8: noqa\r\n     19 \r\n---> 20 from .core import *\r\n\r\n[/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py](https://localhost:8080/#) in <module>\r\n     34 import pyarrow as pa\r\n     35 import pyarrow.lib as lib\r\n---> 36 import pyarrow._parquet as _parquet\r\n     37 \r\n     38 from pyarrow._parquet import (ParquetReader, Statistics,  # noqa\r\n\r\n/usr/local/lib/python3.10/dist-packages/pyarrow/_parquet.pyx in init pyarrow._parquet()\r\n\r\nValueError: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject\n\n### Steps to reproduce the bug\n\nValueError: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject\n\n### Expected behavior\n\nResolve the binary incompatibility\n\n### Environment info\n\nGoogle Colab Note book<end_of_turn>\n<start_of_turn>model\nValueError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello,\n\nI'm trying to convert an HF dataset into a TFRecord so I'm importing `tensorflow` and `datasets` to do so.\nDepending in what order I'm importing those librairies, my code hangs forever and is unkillable (CTRL+C doesn't work, I need to kill my shell entirely).\n\nThank you for your help\n\ud83d\ude4f \n\n### Steps to reproduce the bug\n\nIf you run the following script, this will hang forever : \n```python\nimport tensorflow as tf\nimport datasets\n\ndataset = datasets.load_dataset(\"imagenet-1k\", split=\"validation\", streaming=True)\nprint(next(iter(dataset)))\n```\n\nhowever running the following will work fine (I just changed the order of the imports) :\n```python\nimport datasets\nimport tensorflow as tf\n\ndataset = datasets.load_dataset(\"imagenet-1k\", split=\"validation\", streaming=True)\nprint(next(iter(dataset)))\n```\n\n### Expected behavior\n\nI'm expecting the script to reach the end and my case print the content of the first item in the dataset \n```\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=408x500 at 0x70C646A03110>, 'label': 91}\n```\n\n### Environment info\n\n```\n$ datasets-cli env\n- `datasets` version: 3.3.2\n- Platform: Linux-6.8.0-1017-aws-x86_64-with-glibc2.35\n- Python version: 3.11.7\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n```\n\nI'm also using `tensorflow==2.18.0`.<end_of_turn>\n<start_of_turn>model\nImport order crashes script execution<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSee the script below\n`drop_last_batch=True` is defined using map() for each dataset. \nThe last batch for each dataset is expected to be dropped, id 21-25.\nThe code behaves as expected when num_workers=0 or 1.\nWhen using num_workers>1, 'a-11', 'b-11', 'a-12', 'b-12' are gone and instead 21 and 22 are sampled.\n\n### Steps to reproduce the bug\n\n```\nfrom datasets import Dataset\nfrom datasets import interleave_datasets\nfrom torch.utils.data import DataLoader\n\ndef convert_to_str(batch, dataset_name):\n    batch['a'] = [f\"{dataset_name}-{e}\" for e in batch['a']]\n    return batch\n\ndef gen1():\n    for ii in range(1, 25):\n        yield {\"a\": ii}\n\ndef gen2():\n    for ii in range(1, 25):\n        yield {\"a\": ii}\n\n# https://github.com/huggingface/datasets/issues/6565\nif __name__ == '__main__':\n    dataset1 = Dataset.from_generator(gen1).to_iterable_dataset(num_shards=2)\n    dataset2 = Dataset.from_generator(gen2).to_iterable_dataset(num_shards=2)\n    dataset1 = dataset1.map(lambda x: convert_to_str(x, dataset_name=\"a\"), batched=True, batch_size=10, drop_last_batch=True)\n    dataset2 = dataset2.map(lambda x: convert_to_str(x, dataset_name=\"b\"), batched=True, batch_size=10, drop_last_batch=True)\n\n    interleaved = interleave_datasets([dataset1, dataset2], stopping_strategy=\"all_exhausted\")\n\n    print(f\"num_workers=0\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=0)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n\n    print('=-' * 20)\n    print(f\"num_workers=1\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=1)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n\n    print('=-' * 20)\n    print(f\"num_workers=2\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=2)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n\n    print('=-' * 20)\n    print(f\"num_workers=3\")\n    loader = DataLoader(interleaved, batch_size=5, num_workers=3)\n    i = 0\n    for b in loader:\n        print(i, b['a'])\n        i += 1\n```\n\n\noutput is:\n```\nnum_workers=0\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n2 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n3 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n4 ['a-11', 'b-11', 'a-12', 'b-12', 'a-13']\n5 ['b-13', 'a-14', 'b-14', 'a-15', 'b-15']\n6 ['a-16', 'b-16', 'a-17', 'b-17', 'a-18']\n7 ['b-18', 'a-19', 'b-19', 'a-20', 'b-20']\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nnum_workers=1\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n2 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n3 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n4 ['a-11', 'b-11', 'a-12', 'b-12', 'a-13']\n5 ['b-13', 'a-14', 'b-14', 'a-15', 'b-15']\n6 ['a-16', 'b-16', 'a-17', 'b-17', 'a-18']\n7 ['b-18', 'a-19', 'b-19', 'a-20', 'b-20']\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nnum_workers=2\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['a-13', 'b-13', 'a-14', 'b-14', 'a-15']\n2 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n3 ['b-15', 'a-16', 'b-16', 'a-17', 'b-17']\n4 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n5 ['a-18', 'b-18', 'a-19', 'b-19', 'a-20']\n6 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n7 ['b-20', 'a-21', 'b-21', 'a-22', 'b-22']\n=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-\nnum_workers=3\nToo many dataloader workers: 3 (max is dataset.num_shards=2). Stopping 1 dataloader workers.\n0 ['a-1', 'b-1', 'a-2', 'b-2', 'a-3']\n1 ['a-13', 'b-13', 'a-14', 'b-14', 'a-15']\n2 ['b-3', 'a-4', 'b-4', 'a-5', 'b-5']\n3 ['b-15', 'a-16', 'b-16', 'a-17', 'b-17']\n4 ['a-6', 'b-6', 'a-7', 'b-7', 'a-8']\n5 ['a-18', 'b-18', 'a-19', 'b-19', 'a-20']\n6 ['b-8', 'a-9', 'b-9', 'a-10', 'b-10']\n7 ['b-20', 'a-21', 'b-21', 'a-22', 'b-22']\n\n\n```\n\n### Expected behavior\n\n`'a-21', 'b-21', 'a-22', 'b-22'` should be dropped\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: Linux-5.15.0-1056-aws-x86_64-with-glibc2.31\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.28.0\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.6.1\n<end_of_turn>\n<start_of_turn>model\n`drop_last_batch` does not drop the last batch using IterableDataset + interleave_datasets + multi_worker<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nfrom datasets import load_dataset\r\nprint(\"11\")\r\ntraindata = load_dataset('ptb_text_only', 'penn_treebank', split='train')\r\nprint(\"22\")\r\nvaldata = load_dataset('ptb_text_only',\r\n                           'penn_treebank',\r\n                           split='validation')\n\n### Steps to reproduce the bug\n\n1\n\n### Expected behavior\n\n1\n\n### Environment info\n\n1<end_of_turn>\n<start_of_turn>model\nConnectionError: Couldn't reach 'allenai/c4' on the Hub (ConnectionError)\u6570\u636e\u96c6\u4e0b\u4e0d\u4e0b\u6765\uff0c\u600e\u4e48\u56de\u4e8b<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis bumps `dill` to 0.3.9 and closes #7510 \r\n\r\nIt turns out the only thing required to make the tests pass was to extend the version checks to include 0.3.9.<end_of_turn>\n<start_of_turn>model\nBump dill to 0.4.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6990.<end_of_turn>\n<start_of_turn>model\nFix incorrect rank value in data splitting<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis work follows #7428. And \"requires-python\" is set in pyproject.toml<end_of_turn>\n<start_of_turn>model\nUse pyupgrade --py39-plus for remaining files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhat I did:\r\n- Fixed a small typo in README to improve clarity\r\n- Fixed repeated word \"frameworks frameworks\"\r\n- Split long paragraphs into shorter sentences for readability\r\n- Added # Example comments before code blocks for clarity\r\n\r\nWhy:\r\n- Helps new users avoid confusion\r\n\r\nHow I tested:\r\n- Checked locally in Markdown preview<end_of_turn>\n<start_of_turn>model\nDocs: fix typo, improve readability, add code comments<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Description\nI encountered consistent 504 Gateway Timeout errors while attempting to upload a large dataset (approximately 500GB) to the Hugging Face Hub. The upload fails during the process with a Gateway Timeout error.\n\nI will continue trying to upload. While it might succeed in future attempts, I wanted to report this issue in the meantime.\n\n### Reproduction\n- I attempted the upload 3 times\n- Each attempt resulted in the same 504 error during the upload process (not at the start, but in the middle of the upload)\n- Using `dataset.push_to_hub()` method\n\n### Environment Information\n\n\n```\n- huggingface_hub version: 0.28.0\n- Platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.39\n- Python version: 3.11.10\n- Running in iPython ?: No\n- Running in notebook ?: No\n- Running in Google Colab ?: No\n- Running in Google Colab Enterprise ?: No\n- Token path ?: /home/hotchpotch/.cache/huggingface/token\n- Has saved token ?: True\n- Who am I ?: hotchpotch\n- Configured git credential helpers: store\n- FastAI: N/A\n- Tensorflow: N/A\n- Torch: 2.5.1\n- Jinja2: 3.1.5\n- Graphviz: N/A\n- keras: N/A\n- Pydot: N/A\n- Pillow: 10.4.0\n- hf_transfer: N/A\n- gradio: N/A\n- tensorboard: N/A\n- numpy: 1.26.4\n- pydantic: 2.10.6\n- aiohttp: 3.11.11\n- ENDPOINT: https://huggingface.co\n- HF_HUB_CACHE: /home/hotchpotch/.cache/huggingface/hub\n- HF_ASSETS_CACHE: /home/hotchpotch/.cache/huggingface/assets\n- HF_TOKEN_PATH: /home/hotchpotch/.cache/huggingface/token\n- HF_STORED_TOKENS_PATH: /home/hotchpotch/.cache/huggingface/stored_tokens\n- HF_HUB_OFFLINE: False\n- HF_HUB_DISABLE_TELEMETRY: False\n- HF_HUB_DISABLE_PROGRESS_BARS: None\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\n- HF_HUB_ENABLE_HF_TRANSFER: False\n- HF_HUB_ETAG_TIMEOUT: 10\n- HF_HUB_DOWNLOAD_TIMEOUT: 10\n\n```\n\n\n\n### Full Error Traceback\n\n```python\nTraceback (most recent call last):\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese.git/info/lfs/objects/batch\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/create_edu_japanese_ds/upload_edu_japanese_ds.py\", line 12, in <module>\n    ds.push_to_hub(\"hotchpotch/fineweb-2-edu-japanese\", private=True)\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/datasets/dataset_dict.py\", line 1665, in push_to_hub\n    split_additions, uploaded_size, dataset_nbytes = self[split]._push_parquet_shards_to_hub(\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 5301, in _push_parquet_shards_to_hub\n    api.preupload_lfs_files(\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/hf_api.py\", line 4215, in preupload_lfs_files\n    _upload_lfs_files(\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/_commit_api.py\", line 395, in _upload_lfs_files\n    batch_actions_chunk, batch_errors_chunk = post_lfs_batch_info(\n                                              ^^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/lfs.py\", line 168, in post_lfs_batch_info\n    hf_raise_for_status(resp)\n  File \"/home/hotchpotch/src/github.com/hotchpotch/fineweb-2-edu-classifier-japanese/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\n    raise _format(HfHubHTTPError, str(e), response) from e\nhuggingface_hub.errors.HfHubHTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/datasets/hotchpotch/fineweb-2-edu-japanese.git/info/lfs/objects/batch\n```\n<end_of_turn>\n<start_of_turn>model\n504 Gateway Timeout when uploading large dataset to Hugging Face Hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes an issue with the deprecation of `use_auth_token` in `DownloadConfig` introduced by:\r\n- #5996\r\n\r\n```python\r\nIn [1]: from datasets import DownloadConfig\r\n\r\nIn [2]: DownloadConfig(use_auth_token=False)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-3-41927b449e72> in <module>\r\n----> 1 DownloadConfig(use_auth_token=False)\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'use_auth_token'\r\n```\r\n\r\n```python\r\nIn [1]: from datasets import get_dataset_config_names\r\nIn [2]: get_dataset_config_names(\"squad\", use_auth_token=False)\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-22-4671992ead50> in <module>\r\n----> 1 get_dataset_config_names(\"squad\", use_auth_token=False)\r\n\r\n~/huggingface/datasets/src/datasets/inspect.py in get_dataset_config_names(path, revision, download_config, download_mode, dynamic_modules_path, data_files, **download_kwargs)\r\n    349     ```\r\n    350     \"\"\"\r\n--> 351     dataset_module = dataset_module_factory(\r\n    352         path,\r\n    353         revision=revision,\r\n\r\n~/huggingface/datasets/src/datasets/load.py in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1374     \"\"\"\r\n   1375     if download_config is None:\r\n-> 1376         download_config = DownloadConfig(**download_kwargs)\r\n   1377     download_mode = DownloadMode(download_mode or DownloadMode.REUSE_DATASET_IF_EXISTS)\r\n   1378     download_config.extract_compressed_file = True\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'use_auth_token'\r\n```<end_of_turn>\n<start_of_turn>model\nFix deprecation of use_auth_token in DownloadConfig<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIn https://github.com/huggingface/datasets/issues/6843 it was noted that the streaming feature of `datasets` is highly susceptible to outages and doesn't back off for long (or even *at all*).\n\nI was training a model while streaming SlimPajama and training crashed with a `FileNotFoundError`. I can only assume that this was due to a momentary outage considering the file in question, `train/chunk9/example_train_3889.jsonl.zst`, [exists like all other files in SlimPajama](https://huggingface.co/datasets/cerebras/SlimPajama-627B/blob/main/train/chunk9/example_train_3889.jsonl.zst).\n```python\n...\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 2226, in __iter__\n    for key, example in ex_iterable:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1499, in __iter__\n    for x in self.ex_iterable:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1067, in __iter__\n    yield from self._iter()\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1231, in _iter\n    for key, transformed_example in iter_outputs():\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1207, in iter_outputs\n    for i, key_example in inputs_iterator:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1111, in iter_inputs\n    for key, example in iterator:\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 371, in __iter__\n    for key, pa_table in self.generate_tables_fn(**gen_kwags):\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/packaged_modules/json/json.py\", line 99, in _generate_tables\n    for file_idx, file in enumerate(itertools.chain.from_iterable(files)):\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/utils/track.py\", line 50, in __iter__\n    for x in self.generator(*self.args):\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/utils/file_utils.py\", line 1378, in _iter_from_urlpaths\n    raise FileNotFoundError(urlpath)\nFileNotFoundError: zstd://example_train_3889.jsonl::hf://datasets/cerebras/SlimPajama-627B@2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/train/chunk9/example_train_3889.jsonl.zst\n```\n\nThat final `raise` is at the bottom of the following snippet:\nhttps://github.com/huggingface/datasets/blob/f693f4e93aabafa878470c80fd42ddb10ec550d6/src/datasets/utils/file_utils.py#L1354-L1379\n\nSo clearly, something choked up in `xisfile`.\n\n### Steps to reproduce the bug\n\nThis happens when streaming a dataset and iterating over it. In my case, that iteration is done in Trainer's `inner_training_loop`, but this is not relevant to the iterator.\n```python\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/accelerate/data_loader.py\", line 835, in __iter__\n    next_batch, next_batch_info = self._fetch_batches(main_iterator)\n```\n\n\n### Expected behavior\n\n\nThis bug and the linked issue have one thing in common: *when streaming fails to retrieve an example, the entire program gives up and crashes*. As users, we cannot even protect ourselves from this: when we are iterating over a dataset, we can't make `datasets` skip over a bad example or wait a little longer to retry the iteration, because when a Python generator/iterator raises an error, it loses all its context. \n\nIn other words: if you have something that looks like `for b in a: for c in b: for d in c:`, errors in the innermost loop can only be caught by a `try ... except` in `c.__iter__()`. There should be such exception handling in `datasets` and it should have a **configurable exponential back-off**: first wait and retry after 1 minute, then 2 minutes, then 4 minutes, then 8 minutes, ... and after a given amount of retries, **skip the bad example**, and **only after** skipping a given amount of examples, give up and crash. This was requested in https://github.com/huggingface/datasets/issues/6843 too, since currently there is only linear backoff *and* it is clearly not applied to `xisfile`.\n\n\n### Environment info\n\n- `datasets` version: 3.3.2 *(the latest version)*\n- Platform: Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28\n- Python version: 3.11.7\n- `huggingface_hub` version: 0.26.5\n- PyArrow version: 15.0.0\n- Pandas version: 2.2.0\n- `fsspec` version: 2024.10.0<end_of_turn>\n<start_of_turn>model\nIterableDataset raises FileNotFoundError instead of retrying<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix this error\r\n\r\n```python\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/datasets/utils/file_utils.py\", line 934, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/fsspec/core.py\", line 147, in open\r\n    return self.__enter__()\r\n           ~~~~~~~~~~~~~~^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/fsspec/core.py\", line 105, in __enter__\r\n    f = self.fs.open(self.path, mode=mode)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/fsspec/spec.py\", line 1338, in open\r\n    f = self._open(\r\n        path,\r\n    ...<4 lines>...\r\n        **kwargs,\r\n    )\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 275, in _open\r\n    return HfFileSystemFile(self, path, mode=mode, revision=revision, block_size=block_size, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 950, in __init__\r\n    self.resolved_path = fs.resolve_path(path, revision=revision)\r\n                         ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 198, in resolve_path\r\n    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)\r\n                                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 125, in _repo_and_revision_exist\r\n    self._api.repo_info(\r\n    ~~~~~~~~~~~~~~~~~~~^\r\n        repo_id, revision=revision, repo_type=repo_type, timeout=constants.HF_HUB_ETAG_TIMEOUT\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    )\r\n    ^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py\", line 2864, in repo_info\r\n    return method(\r\n        repo_id,\r\n    ...<4 lines>...\r\n        files_metadata=files_metadata,\r\n    )\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py\", line 2721, in dataset_info\r\n    r = get_session().get(path, headers=headers, timeout=timeout, params=params)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 602, in get\r\n    return self.request(\"GET\", url, **kwargs)\r\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py\", line 95, in send\r\n    return super().send(request, *args, **kwargs)\r\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/adapters.py\", line 690, in send\r\n    raise ReadTimeout(e, request=request)\r\nrequests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: e7e1ae72-54a0-4ce4-b011-144fb7a3fb06)')\r\n```\r\n\r\nwhich could also be related to\r\n\r\n```python\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/datasets/utils/file_utils.py\", line 1364, in _iter_from_urlpaths\r\n    raise FileNotFoundError(urlpath)\r\nFileNotFoundError: hf://datasets/.../train-00013-of-00031.parquet\r\n```<end_of_turn>\n<start_of_turn>model\nRetry open hf file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsupports #7804\r\nAdd support for the dicom file format. \r\n\r\nThis PR follows PR #7815 and PR #7325 closely.\r\nRemarkable differences:\r\nI made sure that we can load all of pydicom's test data, and encountered the `force=True` parameter that we explicitly support here. This allows to trying to load corrupted dicom files, we explicitly test this!\r\n\r\nThere is one dataset with all of dicom's test data on huggingface which can be loaded using this branch with the following script:\r\n```python\r\nfrom datasets import load_dataset\r\nfrom datasets import Features, ClassLabel\r\nfrom datasets.features import Dicom\r\n\r\nfeatures = Features({\r\n    \"dicom\": Dicom(force=True),  # necessary to be able to load one corrupted file\r\n    \"label\": ClassLabel(num_classes=2)\r\n})\r\n\r\nds = load_dataset(\"TobiasPitters/dicom-sample-dataset\",\r\n                  features=features)\r\n\r\nerror_count = 0\r\n\r\nfor idx, item in enumerate(ds[\"test\"]):\r\n    dicom = item[\"dicom\"]\r\n\r\n    try:\r\n        print(f\"Type: {type(dicom)}\")\r\n        if hasattr(dicom, 'PatientID'):\r\n            print(f\"PatientID: {dicom.PatientID}\")\r\n        if hasattr(dicom, 'StudyInstanceUID'):\r\n            print(f\"StudyInstanceUID: {dicom.StudyInstanceUID}\")\r\n        if hasattr(dicom, 'Modality'):\r\n            print(f\"Modality: {dicom.Modality}\")\r\n    except Exception as e:\r\n        error_count += 1\r\n        print(e)\r\n\r\nprint(f\"Finished processing with {error_count} errors.\")\r\n```\r\n\r\ntodo: \r\n- [ ] add docs (will do so soon)<end_of_turn>\n<start_of_turn>model\nAdd DICOM support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nConsider a WebDataset with multiple images for each item where the number of images may vary: [example](https://huggingface.co/datasets/bigdata-pw/fashion-150k)\r\n\r\nDue to this [code](https://github.com/huggingface/datasets/blob/87f4c2088854ff33e817e724e75179e9975c1b02/src/datasets/packaged_modules/webdataset/webdataset.py#L76-L80) an error is given.\r\n```\r\nThe TAR archives of the dataset should be in WebDataset format, but the files in the archive don't share the same prefix or the same types.\r\n```\r\n\r\nThe purpose of this check is unclear because PyArrow supports different keys.\r\n\r\nRemoving the check allows the dataset to be loaded and there's no issue when iterating through the dataset.\r\n\r\n```\r\n>>> from datasets import load_dataset\r\n>>> path = \"shards/*.tar\"\r\n>>> dataset = load_dataset(\"webdataset\", data_files={\"train\": path}, split=\"train\", streaming=True)\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 152/152 [00:00<00:00, 56458.93it/s]\r\n>>> dataset\r\nIterableDataset({\r\n    features: ['__key__', '__url__', '1.jpg', '2.jpg', '3.jpg', '4.jpg', 'json'],\r\n    n_shards: 152\r\n})\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\nload_dataset(\"bigdata-pw/fashion-150k\")\r\n```\n\n### Expected behavior\n\nDataset loads without error\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-5.14.0-467.el9.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.9.19\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\nWebDataset with different prefixes are unsupported<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\nTraceback (most recent call last):\r\n  File \"train_controlnet_sdxl.py\", line 1252, in <module>\r\n    main(args)\r\n  File \"train_controlnet_sdxl.py\", line 1013, in main\r\n    train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3093, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3489, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 557, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow/array.pxi\", line 248, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 113, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/arrow_writer.py\", line 191, in __arrow_array__\r\n    out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/features/features.py\", line 447, in cast_to_python_objects\r\n    return _cast_to_python_objects(\r\n  File \"/home/miniconda3/envs/mhh_df/lib/python3.8/site-packages/datasets/features/features.py\", line 324, in _cast_to_python_objects\r\n    for x in obj.detach().cpu().numpy()\r\nTypeError: Got unsupported ScalarType BFloat16\r\n```\n\n### Steps to reproduce the bug\n\nHere is my train script I use BF16 type\uff0cI use diffusers train my model\r\n```\r\nexport MODEL_DIR=\"/home/mhh/sd_models/stable-diffusion-xl-base-1.0\"\r\nexport OUTPUT_DIR=\"./control_net\"\r\nexport VAE_NAME=\"/home/mhh/sd_models/sdxl-vae-fp16-fix\"\r\n\r\naccelerate launch train_controlnet_sdxl.py \\\r\n --pretrained_model_name_or_path=$MODEL_DIR \\\r\n --output_dir=$OUTPUT_DIR \\\r\n --pretrained_vae_model_name_or_path=$VAE_NAME \\\r\n --dataset_name=/home/mhh/sd_datasets/fusing/fill50k \\\r\n --mixed_precision=\"bf16\" \\\r\n --resolution=1024 \\\r\n --learning_rate=1e-5 \\\r\n --max_train_steps=200 \\\r\n --validation_image \"/home/mhh/sd_datasets/controlnet_image/conditioning_image_1.png\" \"/home/mhh/sd_datasets/controlnet_image/conditioning_image_2.png\" \\\r\n --validation_prompt \"red circle with blue background\" \"cyan circle with brown floral background\" \\\r\n --validation_steps=50 \\\r\n --train_batch_size=1 \\\r\n --gradient_accumulation_steps=4 \\\r\n --report_to=\"wandb\" \\\r\n --seed=42 \\\r\n```\n\n### Expected behavior\n\nWhen I changed the data type to fp16, it worked.\n\n### Environment info\n\ndatasets                2.16.1\r\nnumpy                   1.24.4<end_of_turn>\n<start_of_turn>model\nI train controlnet_sdxl in bf16 datatype, got unsupported ERROR in datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR implements support for robust image decoding in the `Image` feature, as discussed in issue #7612.\r\n\r\n## \ud83d\udd27 What was added\r\n- A new boolean field: `ignore_decode_errors` (default: `False`)\r\n- If set to `True`, any exceptions during decoding will be caught, and `None` will be returned instead of raising an error\r\n\r\n```python\r\nfeatures = Features({\r\n    \"image\": Image(decode=True, ignore_decode_errors=True),\r\n})\r\n````\r\n\r\nThis enables robust iteration over potentially corrupted datasets \u2014 especially useful when streaming datasets like WebDataset or image-heavy public sets where sample corruption is common.\r\n\r\n## \ud83e\uddea Behavior\r\n\r\n* If `ignore_decode_errors=False` (default), decoding behaves exactly as before\r\n* If `True`, decoding errors are caught, and a warning is emitted:\r\n\r\n  ```\r\n  [Image.decode_example] Skipped corrupted image: ...\r\n  ```\r\n\r\n## \ud83e\uddf5 Linked issue\r\n\r\nCloses #7612\r\n\r\nLet me know if you'd like a follow-up test PR. Happy to write one!<end_of_turn>\n<start_of_turn>model\nAdd ignore_decode_errors option to Image feature for robust decoding #7612<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfixes #7637\r\n\r\nThis PR introduces subset_name as a user-facing alias for the name (previously `config_name`) argument in load_dataset. It aligns terminology with the Hugging Face Hub UI (which shows \u201cSubset\u201d), reducing confusion for new users.\r\n\r\nSupports `subset_name` in `load_dataset()`\r\n\r\nAdds `.subset_name` property to DatasetBuilder\r\n\r\nMaintains full backward compatibility\r\n\r\nRaises clear error if name and `subset_name` conflict<end_of_turn>\n<start_of_turn>model\nfeat: add subset_name as alias for name in load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDisable implicit token in CI.\r\n\r\nThis PR allows running CI tests locally without implicitly using the local user HF token. For example, run locally the tests in:\r\n- #7124<end_of_turn>\n<start_of_turn>model\nDisable implicit token in CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.13.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.18.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #1774, fix #5875 \r\n\r\n<end_of_turn>\n<start_of_turn>model\nMake split slicing consistent with list slicing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nEnable the `load_dataset` function to load local datasets with symbolic links. \r\n\r\nE.g, this dataset can be loaded:\r\n\u251c\u2500\u2500 example_dataset/\r\n\u2502   \u251c\u2500\u2500 data/\r\n\u2502   \u2502   \u251c\u2500\u2500 train/\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file0\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file1\r\n\u2502   \u2502   \u251c\u2500\u2500 dev/\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file2\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 file3\r\n\u2502   \u251c\u2500\u2500 metadata.csv\r\n\r\nwhile this dataset can't:\r\n\u251c\u2500\u2500 example_dataset_symlink/\r\n\u2502   \u251c\u2500\u2500 data/\r\n\u2502   \u2502   \u251c\u2500\u2500 train/\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sym0 -> file0\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sym1 -> file1\r\n\u2502   \u2502   \u251c\u2500\u2500 dev/\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sym2 -> file2\r\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 sym3 -> file3\r\n\u2502   \u251c\u2500\u2500 metadata.csv\r\n\r\nI have created an example dataset in order to reproduce the problem:\r\n\r\n1. Unzip `example_dataset.zip`.\r\n2. Run `no_symlink.sh`. Training should start without issues. \r\n3. Run `symlink.sh`. You will see that all four examples will be in train split, instead of having two examples in train and two examples in dev. The script won't load the correct audio files.\r\n\r\n[example_dataset.zip](https://github.com/huggingface/datasets/files/14807053/example_dataset.zip)\r\n\r\n### Motivation\r\n\r\nI have a very large dataset locally. Instead of initiating training on the entire dataset, I need to start training on smaller subsets of the data. Due to the purpose of the experiments I am running, I will need to create many smaller datasets with overlapping data. Instead of copying the all the files for each subset, I would prefer copying symbolic links of the data. This way, the memory usage would not significantly increase beyond the initial dataset size.\r\n\r\nAdvantages of this approach:\r\n\r\n- It would leave a smaller memory footprint on the hard drive\r\n- Creating smaller datasets would be much faster\r\n\r\n### Your contribution\r\n\r\nI would gladly contribute, if this is something useful to the community. It seems like a simple change of code, something like `file_path = os.path.realpath(file_path)` should be added before loading the files. If anyone has insights on how to incorporate this functionality, I would greatly appreciate your knowledge and input.<end_of_turn>\n<start_of_turn>model\nload_dataset can't work with symbolic links<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen I load a dataset from a number of arrow files, as in:\r\n\r\n```\r\nrandom_dataset = load_dataset(\r\n    \"arrow\",\r\n    data_files={split: shard_filepaths},\r\n    streaming=True,\r\n    split=split,\r\n)\r\n```\r\n\r\nI'm able to get fast iteration speeds when iterating over the dataset without shuffling. \r\n\r\nWhen I shuffle the dataset, the iteration speed is reduced by ~1000x.\r\n\r\nIt's very possible the way I'm loading dataset shards is not appropriate; if so please advise!\r\n\r\nThanks for the help\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere's full code to reproduce the issue:\r\n\r\n- Generate a random dataset\r\n- Create shards of data independently using Dataset.save_to_disk()\r\n- The below will generate 16 shards (arrow files), of 512 examples each\r\n\r\n\r\n```\r\nimport time\r\nfrom pathlib import Path\r\nfrom multiprocessing import Pool, cpu_count\r\n\r\nimport torch\r\nfrom datasets import Dataset, load_dataset\r\n\r\nsplit = \"train\"\r\nsplit_save_dir = \"/tmp/random_split\"\r\n\r\ndef generate_random_example():\r\n    return {\r\n        'inputs': torch.randn(128).tolist(),\r\n        'indices': torch.randint(0, 10000, (2, 20000)).tolist(),\r\n        'values': torch.randn(20000).tolist(), \r\n    }\r\n\r\ndef generate_shard_dataset(examples_per_shard: int = 512):\r\n    dataset_dict = {\r\n        'inputs': [],\r\n        'indices': [],\r\n        'values': []\r\n    }\r\n\r\n    for _ in range(examples_per_shard):\r\n        example = generate_random_example()\r\n        dataset_dict['inputs'].append(example['inputs'])\r\n        dataset_dict['indices'].append(example['indices'])\r\n        dataset_dict['values'].append(example['values'])\r\n\r\n    return Dataset.from_dict(dataset_dict)\r\n\r\ndef save_shard(shard_idx, save_dir, examples_per_shard):\r\n    shard_dataset = generate_shard_dataset(examples_per_shard)\r\n    shard_write_path = Path(save_dir) / f\"shard_{shard_idx}\"\r\n    shard_dataset.save_to_disk(shard_write_path)\r\n    return str(Path(shard_write_path) / \"data-00000-of-00001.arrow\")\r\n\r\ndef generate_split_shards(save_dir, num_shards: int = 16, examples_per_shard: int = 512):\r\n    with Pool(cpu_count()) as pool:\r\n        args = [(m, save_dir, examples_per_shard) for m in range(num_shards)]\r\n        shard_filepaths = pool.starmap(save_shard, args)\r\n    \r\n    return shard_filepaths\r\n\r\nshard_filepaths = generate_split_shards(split_save_dir)\r\n```\r\n\r\nLoad the dataset as IterableDataset:\r\n\r\n\r\n```\r\nrandom_dataset = load_dataset(\r\n    \"arrow\",\r\n    data_files={split: shard_filepaths},\r\n    streaming=True,\r\n    split=split,\r\n)\r\nrandom_dataset = random_dataset.with_format(\"numpy\")\r\n```\r\n\r\nObserve the iterations/second when iterating over the dataset directly, and applying shuffling before iterating:\r\n\r\nWithout shuffling, this gives ~1500 iterations/second\r\n\r\n```\r\nstart_time = time.time()\r\nfor count, item in enumerate(random_dataset):\r\n    if count > 0 and count % 100 == 0:\r\n        elapsed_time = time.time() - start_time\r\n        iterations_per_second = count / elapsed_time\r\n        print(f\"Processed {count} items at an average of {iterations_per_second:.2f} iterations/second\")\r\n```\r\n\r\n```\r\nProcessed 100 items at an average of 705.74 iterations/second\r\nProcessed 200 items at an average of 1169.68 iterations/second\r\nProcessed 300 items at an average of 1497.97 iterations/second\r\nProcessed 400 items at an average of 1739.62 iterations/second\r\nProcessed 500 items at an average of 1931.11 iterations/second`\r\n```\r\n\r\nWhen shuffling, this gives ~3 iterations/second:\r\n\r\n\r\n```\r\n\r\nrandom_dataset = random_dataset.shuffle(buffer_size=100,seed=42)\r\n\r\nstart_time = time.time()\r\nfor count, item in enumerate(random_dataset):\r\n    if count > 0 and count % 100 == 0:\r\n        elapsed_time = time.time() - start_time\r\n        iterations_per_second = count / elapsed_time\r\n        print(f\"Processed {count} items at an average of {iterations_per_second:.2f} iterations/second\")\r\n```\r\n\r\n\r\n```\r\nProcessed 100 items at an average of 3.75 iterations/second\r\nProcessed 200 items at an average of 3.93 iterations/second\r\n```\r\n\r\n### Expected behavior\r\n\r\nIterations per second should be barely affected by shuffling, especially with a small buffer size\r\n\r\n### Environment info\r\n\r\nDatasets version: 2.21.0\r\nPython 3.10\r\nUbuntu 22.04<end_of_turn>\n<start_of_turn>model\nSlow iteration speeds when using IterableDataset.shuffle with load_dataset(data_files=..., streaming=True)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRaise an error in  `DatasetBuilder.as_dataset` when `file_format != \"arrow\"` (and fix the docstring)\r\n\r\nFix #5874 <end_of_turn>\n<start_of_turn>model\nRaise error in `DatasetBuilder.as_dataset` when `file_format` is not `\"arrow\"`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixed typos in ReadMe and added punctuation marks\r\n\r\nTensorflow --> TensorFlow\r\n<end_of_turn>\n<start_of_turn>model\nUpdate README.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf you `import` a module that runs `datasets.map(f, num_proc=N)` at the top-level, Python hangs.\n\n\n### Steps to reproduce the bug\n\n1. Create a file that runs datasets.map at the top-level:\n\n```bash\ncat <<EOF > import_me.py\nimport datasets\n\nthe_dataset = datasets.load_dataset(\"openai/openai_humaneval\")\nthe_dataset = the_dataset.map(lambda item: item, num_proc=2)\nEOF\n```\n\n2. Start Python REPL:\n\n```bash\nuv run --python 3.12.3 --with \"datasets==4.0.0\"  python3\nPython 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n```\n\n3. Import the file:\n\n```python\nimport import_me\n````\n\nObserve hang.\n\n### Expected behavior\n\nIdeally would not hang, or would fallback to num_proc=1 with a warning.\n\n### Environment info\n\n\n- `datasets` version: 4.0.0\n- Platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- `huggingface_hub` version: 0.34.4\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.2\n- `fsspec` version: 2025.3.0\n<end_of_turn>\n<start_of_turn>model\ndatasets.map(f, num_proc=N) hangs with N>1 when run on import<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm currently trying to stream data using dataset since the dataset is too big but it hangs indefinitely without loading the first batch. I use AIMOS which is a supercomputer that uses proxy to connect to the internet. I assume it has to do with the network configurations. I've already set up both HTTP_PROXY and HTTPS_PROXY. streaming = False works fine.\n\n### Steps to reproduce the bug\n\nuse load_dataset with streaming = True in AIMOS\n\n### Expected behavior\n\ndoes not hang indefinitely and loads batches to start training run\n\n### Environment info\n\n_libgcc_mutex             0.1                 conda_forge    conda-forge\r\n_openmp_mutex             4.5                       2_gnu    conda-forge\r\n_pytorch_select           2.0                      cuda_2    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nabseil-cpp                20220623.0           h9888cd1_6    conda-forge\r\nabsl-py                   1.0.0           py311h399429b_0    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\naiofiles                  23.2.1             pyhd8ed1ab_0    conda-forge\r\naiohttp                   3.8.6           py311hf118e41_0\r\naiosignal                 1.2.0              pyhd3eb1b0_0\r\narchspec                  0.2.3              pyhd8ed1ab_0    conda-forge\r\narrow-cpp                 11.0.0           ha3edaa6_5_cpu    conda-forge\r\nasync-timeout             4.0.2           py311h6ffa863_0\r\nattrs                     23.1.0          py311h6ffa863_0\r\nav                        10.0.0          py311he6153ed_2    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\naws-c-auth                0.6.24               hb81f6d7_5    conda-forge\r\naws-c-cal                 0.5.20               h3c2b4d9_6    conda-forge\r\naws-c-common              0.8.11               h4194056_0    conda-forge\r\naws-c-compression         0.2.16               ha19333d_3    conda-forge\r\naws-c-event-stream        0.2.18               h12a9399_6    conda-forge\r\naws-c-http                0.7.4                ha2cde00_2    conda-forge\r\naws-c-io                  0.13.17              h9189062_2    conda-forge\r\naws-c-mqtt                0.8.6                h40d1a04_6    conda-forge\r\naws-c-s3                  0.2.4                hbdbe4f0_3    conda-forge\r\naws-c-sdkutils            0.1.7                ha19333d_3    conda-forge\r\naws-checksums             0.1.14               ha19333d_3    conda-forge\r\naws-crt-cpp               0.19.7               hd018011_7    conda-forge\r\naws-sdk-cpp               1.10.57              hb9575ba_4    conda-forge\r\nblas                      1.0                    openblas\r\nblinker                   1.8.2              pyhd8ed1ab_0    conda-forge\r\nboltons                   23.0.0          py311h6ffa863_0\r\nboost-cpp                 1.82.0               h25e6d66_2\r\nbottleneck                1.3.5           py311h34f6284_0\r\nbrotli                    1.0.9                hf118e41_7\r\nbrotli-bin                1.0.9                hf118e41_7\r\nbrotli-python             1.0.9           py311h4a02239_7\r\nbzip2                     1.0.8                h7b6447c_0\r\nc-ares                    1.19.1               hf118e41_0\r\nca-certificates           2024.6.2             h0f6029e_0    conda-forge\r\ncachetools                5.3.3              pyhd8ed1ab_0    conda-forge\r\ncertifi                   2024.6.2           pyhd8ed1ab_0    conda-forge\r\ncffi                      1.15.1          py311hf118e41_3\r\ncharset-normalizer        2.0.4              pyhd3eb1b0_0\r\nclick                     8.1.7           unix_pyh707e725_0    conda-forge\r\nconda                     24.5.0          py311h1af927a_0    conda-forge\r\nconda-content-trust       0.2.0           py311h6ffa863_0\r\nconda-libmamba-solver     23.11.1         py311h6ffa863_0\r\nconda-package-handling    2.2.0           py311h6ffa863_0\r\nconda-package-streaming   0.9.0           py311h6ffa863_0\r\ncontourpy                 1.0.5           py311h25e6d66_0\r\ncryptography              41.0.3          py311hb0e80e7_0\r\ncudatoolkit               11.8.0              hedcfb66_13    conda-forge\r\ncudnn                     8.9.2_11.8           h9ceb136_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\ncycler                    0.11.0             pyhd3eb1b0_0\r\ndatasets                  2.12.0          py311h6ffa863_0\r\ndill                      0.3.6           py311h6ffa863_0\r\ndistro                    1.9.0              pyhd8ed1ab_0    conda-forge\r\nffmpeg                    4.2.2                  opence_0    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nfilelock                  3.9.0           py311h6ffa863_0\r\nfmt                       9.1.0                h25e6d66_0\r\nfonttools                 4.25.0             pyhd3eb1b0_0\r\nfreetype                  2.12.1               hd23a775_0\r\nfrozendict                2.4.4           py311hb02d432_0    conda-forge\r\nfrozenlist                1.4.0           py311hf118e41_0\r\nfsspec                    2023.9.2        py311h6ffa863_0\r\ngflags                    2.2.2                he6710b0_0\r\ngiflib                    5.2.1                hf118e41_3\r\nglog                      0.6.0                hbe088e0_0    conda-forge\r\ngmp                       6.3.0                h46f38da_0    conda-forge\r\ngmpy2                     2.1.5           py311h2758da7_1    conda-forge\r\ngoogle-auth               2.30.0             pyhff2d567_0    conda-forge\r\ngoogle-auth-oauthlib      0.5.3              pyhd8ed1ab_0    conda-forge\r\ngrpc-cpp                  1.51.1               h8ba971d_1    conda-forge\r\ngrpcio                    1.54.3          py311h414e0d3_0    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nhuggingface_hub           0.17.3          py311h6ffa863_0\r\nicu                       73.1                 h4a02239_0\r\nidna                      3.4             py311h6ffa863_0\r\nimportlib-metadata        6.0.0           py311h6ffa863_0\r\njinja2                    3.1.4              pyhd8ed1ab_0    conda-forge\r\njpeg                      9e                   hf118e41_1\r\njsonpatch                 1.32               pyhd3eb1b0_0\r\njsonpointer               2.1                pyhd3eb1b0_0\r\nkiwisolver                1.4.4           py311h4a02239_0\r\nkrb5                      1.20.1               hc019ccd_1\r\nlame                      3.100             hb283c62_1003    conda-forge\r\nlcms2                     2.12                 h2045e0b_0\r\nld_impl_linux-ppc64le     2.38                 hec883e6_1\r\nlerc                      3.0                  h29c3540_0\r\nleveldb                   1.23                 h24532b4_1    conda-forge\r\nlibabseil                 20220623.0      cxx17_h9235812_6    conda-forge\r\nlibarchive                3.6.2                hd8ab008_2\r\nlibarrow                  11.0.0           h837770b_5_cpu    conda-forge\r\nlibboost                  1.82.0               haf51a6a_2\r\nlibbrotlicommon           1.0.9                hf118e41_7\r\nlibbrotlidec              1.0.9                hf118e41_7\r\nlibbrotlienc              1.0.9                hf118e41_7\r\nlibcrc32c                 1.1.2                h3b9df90_0    conda-forge\r\nlibcurl                   8.4.0                h4d62439_0\r\nlibdeflate                1.17                 hf118e41_1\r\nlibedit                   3.1.20221030         hf118e41_0\r\nlibev                     4.33                 h140841e_1\r\nlibevent                  2.1.10               h19c23f1_4    conda-forge\r\nlibexpat                  2.6.2                h46f38da_0    conda-forge\r\nlibffi                    3.4.4                h4a02239_0\r\nlibgcc-ng                 13.2.0              h31e42bb_10    conda-forge\r\nlibgfortran-ng            11.2.0               hb3889a9_1\r\nlibgfortran5              11.2.0               h1234567_1\r\nlibgomp                   13.2.0              h31e42bb_10    conda-forge\r\nlibgoogle-cloud           2.7.0                h11140b6_1    conda-forge\r\nlibgrpc                   1.51.1               h4d29a31_1    conda-forge\r\nlibmamba                  1.5.3                h7c6fafd_0\r\nlibmambapy                1.5.3           py311h828bf7b_0\r\nlibnghttp2                1.57.0               h44e5816_0\r\nlibnsl                    2.0.1                ha17a0cc_0    conda-forge\r\nlibopenblas               0.3.23               hc5a31fb_2    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nlibopus                   1.3.1                h4e0d66e_1    conda-forge\r\nlibpng                    1.6.39               hf118e41_0\r\nlibprotobuf               3.21.12              h1776448_0    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nlibsolv                   0.7.24               h0f529ac_0\r\nlibsqlite                 3.45.3               hd4bbf49_0    conda-forge\r\nlibssh2                   1.10.0               h50fa78f_2\r\nlibstdcxx-ng              13.2.0              h262982c_10    conda-forge\r\nlibthrift                 0.18.0               h82f1162_0    conda-forge\r\nlibtiff                   4.5.1                h4a02239_0\r\nlibutf8proc               2.8.0                hb283c62_0    conda-forge\r\nlibuuid                   2.38.1               h4194056_0    conda-forge\r\nlibvpx                    1.13.1               h46f38da_0    conda-forge\r\nlibwebp                   1.3.2                h0f96ee2_0\r\nlibwebp-base              1.3.2                hf118e41_0\r\nlibxcrypt                 4.4.36               ha17a0cc_1    conda-forge\r\nlibxml2                   2.10.4               h18e3229_1\r\nlibzlib                   1.2.13               h1f2b957_6    conda-forge\r\nllvm-openmp               14.0.6               hc028133_0    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nlmdb                      0.9.31               ha17a0cc_1    conda-forge\r\nlz4-c                     1.9.4                h4a02239_0\r\nmarkdown                  3.4.4              pyhd8ed1ab_0    conda-forge\r\nmarkupsafe                2.1.5           py311h32d8acf_0    conda-forge\r\nmatplotlib                3.8.0           py311h6ffa863_0\r\nmatplotlib-base           3.8.0           py311h52e1fcc_0\r\nmenuinst                  2.1.1           py311h1af927a_0    conda-forge\r\nmpc                       1.3.1                heaf1863_0    conda-forge\r\nmpfr                      4.2.1                haad2271_1    conda-forge\r\nmpmath                    1.3.0              pyhd8ed1ab_0    conda-forge\r\nmultidict                 6.0.2           py311hf118e41_0\r\nmultiprocess              0.70.14         py311h6ffa863_0\r\nmunkres                   1.1.4                      py_0\r\nmypy_extensions           1.0.0              pyha770c72_0    conda-forge\r\nnccl                      2.18.3               cuda11.8_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nncurses                   6.4                  h4a02239_0\r\nnest-asyncio              1.6.0              pyhd8ed1ab_0    conda-forge\r\nnetworkx                  2.8.8              pyhd8ed1ab_0    conda-forge\r\nnomkl                     3.0                           0    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nnumactl                   2.0.16               hba61f60_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nnumexpr                   2.8.7           py311hc46fc55_0\r\nnumpy                     1.24.3          py311h148a09e_0\r\nnumpy-base                1.24.3          py311h06b82f6_0\r\noauthlib                  3.2.2              pyhd8ed1ab_0    conda-forge\r\nopenjpeg                  2.4.0                hfe35807_0\r\nopenssl                   3.3.1                h1f2b957_0    conda-forge\r\norc                       1.8.2                h341c9a4_2    conda-forge\r\npackaging                 23.1            py311h6ffa863_0\r\npandas                    2.1.1           py311h52e1fcc_0\r\npcre2                     10.42                h280155c_0\r\npillow                    10.0.1          py311he33076b_0\r\npip                       23.3            py311h6ffa863_0\r\nplatformdirs              4.2.2              pyhd8ed1ab_0    conda-forge\r\npluggy                    1.0.0           py311h6ffa863_1\r\npooch                     1.8.2              pyhd8ed1ab_0    conda-forge\r\nprotobuf                  4.21.12         py311ha7baec7_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\npsutil                    5.9.8           py311hd26027c_0    conda-forge\r\npyarrow                   11.0.0          py311h04a18d5_1\r\npyasn1                    0.6.0              pyhd8ed1ab_0    conda-forge\r\npyasn1-modules            0.4.0              pyhd8ed1ab_0    conda-forge\r\npybind11-abi              4                    hd3eb1b0_1\r\npycosat                   0.6.6           py311hf118e41_0\r\npycparser                 2.21               pyhd3eb1b0_0\r\npyjwt                     2.8.0              pyhd8ed1ab_1    conda-forge\r\npyopenssl                 23.2.0          py311h6ffa863_0\r\npyparsing                 3.0.9           py311h6ffa863_0\r\npyre-extensions           0.0.30             pyhd8ed1ab_0    conda-forge\r\npysocks                   1.7.1           py311h6ffa863_0\r\npython                    3.11.8          h3332dee_0_cpython    conda-forge\r\npython-dateutil           2.8.2              pyhd3eb1b0_0\r\npython-tzdata             2023.3             pyhd3eb1b0_0\r\npython-xxhash             2.0.2           py311hf118e41_1\r\npython_abi                3.11                    4_cp311    conda-forge\r\npytorch                   2.0.1           cuda11.8_py311_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\npytorch-base              2.0.1           cuda11.8_py311_pb4.21.12_4    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\npytz                      2023.3.post1    py311h6ffa863_0\r\npyu2f                     0.1.5              pyhd8ed1ab_0    conda-forge\r\npyyaml                    6.0.1           py311hf118e41_0\r\nre2                       2023.02.01           h883269e_0    conda-forge\r\nreadline                  8.2                  hf118e41_0\r\nregex                     2023.10.3       py311hf118e41_0\r\nreproc                    14.2.4               h29c3540_1\r\nreproc-cpp                14.2.4               h29c3540_1\r\nrequests                  2.31.0          py311h6ffa863_0\r\nrequests-oauthlib         2.0.0              pyhd8ed1ab_0    conda-forge\r\nresponses                 0.13.3             pyhd3eb1b0_0\r\nrsa                       4.9                pyhd8ed1ab_0    conda-forge\r\nruamel.yaml               0.17.21         py311hf118e41_0\r\ns2n                       1.3.37               h5e47323_0    conda-forge\r\nsafetensors               0.4.0           py311hda16d9e_0\r\nscipy                     1.11.1          py311hd69e9bb_0    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nsentencepiece             0.1.97          h1e74c73_py311_pb4.21.12_2    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\nsetuptools                68.0.0          py311h6ffa863_0\r\nsix                       1.16.0             pyhd3eb1b0_1\r\nsnappy                    1.1.9                h29c3540_0\r\nsqlite                    3.41.2               hf118e41_0\r\nsympy                     1.12.1          pypyh2585a3b_103    conda-forge\r\ntabulate                  0.8.10             pyhd8ed1ab_0    conda-forge\r\ntensorboard               2.13.0          pyhab0730d_pb4.21.12_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\ntensorboard-data-server   0.7.0              pyh6f84499_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\ntensorboard-plugin-wit    1.6.0              pyh9f0ad1d_0    conda-forge\r\ntk                        8.6.13               hd4bbf49_0    conda-forge\r\ntokenizers                0.13.3          py311h3d4f45a_0\r\ntorchdata                 0.6.0                   py311_2    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\ntorchsnapshot             0.1.0              pyhd8ed1ab_0    conda-forge\r\ntorchtext-base            0.15.2          cuda11.8_py311_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\ntorchtnt                  0.2.4              pyhd8ed1ab_0    conda-forge\r\ntorchvision-base          0.15.2          cuda11.8_py311_1    https://ftp.osuosl.org/pub/open-ce/1.10.0\r\ntornado                   6.3.3           py311hf118e41_0\r\ntqdm                      4.65.0          py311h7837921_0\r\ntransformers              4.32.1          py311h6ffa863_0\r\ntruststore                0.8.0           py311h6ffa863_0\r\ntyping-extensions         4.7.1           py311h6ffa863_0\r\ntyping_extensions         4.7.1           py311h6ffa863_0\r\ntyping_inspect            0.9.0              pyhd8ed1ab_0    conda-forge\r\ntzdata                    2023c                h04d1e81_0\r\nurllib3                   1.26.18         py311h6ffa863_0\r\nutf8proc                  2.6.1                h140841e_0\r\nwerkzeug                  2.3.8              pyhd8ed1ab_0    conda-forge\r\nwheel                     0.41.2          py311h6ffa863_0\r\nxxhash                    0.8.0                h140841e_3\r\nxz                        5.4.2                hf118e41_0\r\nyaml                      0.2.5                h7b6447c_0\r\nyaml-cpp                  0.8.0                h4a02239_0\r\nyarl                      1.8.1           py311hf118e41_0\r\nzipp                      3.11.0          py311h6ffa863_0\r\nzlib                      1.2.13               h1f2b957_6    conda-forge\r\nzstandard                 0.19.0          py311hf118e41_0\r\nzstd                      1.5.5                h57e4825_0<end_of_turn>\n<start_of_turn>model\nDataset with streaming doesn't work with proxy<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTo have Splits, Configurations and Builder parameters at the same doc level<end_of_turn>\n<start_of_turn>model\nFlatten repository_structure docs on yaml<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ndon't require `trust_remote_code` in (deprecated) `inspect_dataset` (it defeats its purpose)\r\n\r\n(not super important but we might as well keep it until the next major release)\r\n\r\nthis is needed to fix the tests in https://github.com/huggingface/datasets/pull/6448<end_of_turn>\n<start_of_turn>model\nDon't require trust_remote_code in inspect_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUsing `Dataset.map(fn, batched=True)` allows resizing the dataset by returning a dict of lists, all of which must be the same size. If they are not the same size, an error like `pyarrow.lib.ArrowInvalid: Column 1 named x expected length 1 but got length 0` is raised.\r\n\r\nThis is not the case if the function returns an empty list for an existing column in the dataset. In that case, the dataset is silently resized to 0 rows.\n\n### Steps to reproduce the bug\n\nMWE:\r\n\r\n```\r\nimport datasets\r\ndata = datasets.Dataset.from_dict({\"test\": [1]})\r\n\r\ndef mapping_fn(examples):\r\n    return {\"test\": [], \"y\": [1]}\r\n\r\ndata = data.map(mapping_fn, batched=True)\r\nprint(len(data))\r\n```\r\n\r\nNote  that when returning `\"x\": []`, the error is raised correctly, also when returning `\"test\": [1,2]`.\n\n### Expected behavior\n\nExpected an exception: `pyarrow.lib.ArrowInvalid: Column 1 named test expected length 1 but got length 0` or `pyarrow.lib.ArrowInvalid: Column 2 named y expected length 0 but got length 1`.\r\n\r\nAny exception would be acceptable.\n\n### Environment info\n\n- `datasets` version: 2.19.1\r\n- Platform: Linux-5.4.0-153-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.8\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nBatched mapping does not raise an error if values for an existing column are empty<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR contributes a minor refactor, in a small function in `src/datasets/formatting/formatting.py`. No change in logic. \r\n\r\nIn the original code, there are separate if-conditionals for `isinstance(key, range)` and `isinstance(key, Iterable)`, with essentially the same logic. \r\n\r\nThis PR combines these two using a single if statement.\r\n\r\n**Considerations**\r\n\r\n1. Although range in python is guaranteed to have integers, internally calling `int()` on an object that is already an int is negligible. (In python it returns the original object. It doesn't create a new integer object or perform any actual conversion)\r\n\r\n2. Technically a range is already an Iterable, and we could just do `isinstance(key, Iterable)` but I explicitly did `isinstance(key, (range, Iterable))` just to be super obvious and consistent that both cases are handled because I see `slice, range, Iterable` everywhere in this `formatting.py`\r\n\r\n3. This PR removes the `if len(key)>0` conditional. I think it is cleaner to have it this way for three reasons.\r\n- There was originally no else statement and the code would have failed silently anyway.  \r\n- The if len(key)>0 should be caught much earlier, rather than in `formatting.py`. \r\n- There are actually multiple cases where this would fail, if len(key)>0, if key is non numeric or float, or if key is a list of lists. It's clunky to state all this and the error be thrown during max or indexing. \r\n\r\n\r\n**Previous PR and Issues Checks**\r\n\r\n1. No known PR or Issues (both closed or open) in hf datasets repository \r\n\r\n**Tests**\r\n\r\n1. Tested using Dataset (load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")), Pytorch DataLoader, with a Pytorch BatchSampler (list of indexes returned instead of single index). \r\n<end_of_turn>\n<start_of_turn>model\n(refactor) remove redundant logic in _check_valid_index_key<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\n[WebDataset] Support compressed files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWill unexpected get keys with `None` value in the parsed json dict.\n\n### Steps to reproduce the bug\n\n```jsonl test.jsonl\r\n{\"id\": 0, \"indexs\": {\"-1\": [0, 10]}}\r\n{\"id\": 1, \"indexs\": {\"-1\": [0, 10]}}\r\n```\r\n\r\n```python\r\ndataset = Dataset.from_json('.test.jsonl')\r\nprint(dataset[0])\r\n```\r\n\r\nResult:\r\n```\r\n{'id': 0, 'indexs': {'-1': [...], '-2': None, '-3': None, '-4': None, '-5': None, '-6': None, '-7': None, '-8': None, '-9': None, ...}}\r\n```\r\n\r\nThose keys with `None` value will unexpected appear in the dict.\r\n\n\n### Expected behavior\n\nResult should be\r\n```\r\n{'id': 0, 'indexs': {'-1': [0, 10]}}\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-6.5.0-14-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.6\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\n`Dataset` unexpected changed dict data and may cause error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n![Image](https://github.com/user-attachments/assets/6e247f4a-d436-4428-a682-fe18ebdc73a9)\n\n## Xet is out !\n\nOver the past few weeks, Hugging Face\u2019s [Xet Team](https://huggingface.co/xet-team) took a major step forward by [migrating the first Model and Dataset repositories off LFS and to Xet storage](https://huggingface.co/posts/jsulz/911431940353906).\n\nSee more information on the HF blog: https://huggingface.co/blog/xet-on-the-hub\n\nYou can already enable Xet on Hugging Face account to benefit from faster downloads and uploads :)\n\nWe finalized an official integration with the `huggingface_hub` library that means you get the benefits of Xet without any significant changes to your current workflow.\n\n\n## Previous versions of `datasets`\n\nFor older versions of `datasets` you might see this warning in `push_to_hub()`:\n\n```\nUploading files as bytes or binary IO objects is not supported by Xet Storage.\n```\n\nThis means the `huggingface_hub` + Xet integration isn't enabled for your version of `datasets`.\n\nYou can fix this by updating to `datasets>=3.6.0` and `huggingface_hub>=0.31.0`\n\n```\npip install -U datasets huggingface_hub\n```\n\n## The future\n\nStay tuned for more Xet optimizations, especially on [Xet-optimized Parquet](https://huggingface.co/blog/improve_parquet_dedupe)\n\n<end_of_turn>\n<start_of_turn>model\nFaster downloads/uploads with Xet storage<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nand better handling of LocalEntryNotfoundError cc @Wauplin \r\n\r\nfollow up to #7234 <end_of_turn>\n<start_of_turn>model\nuse huggingface_hub offline mode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix link to quickstart docs in README.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Related Issue\r\n\r\nFixes #7503  \r\nPartially addresses #5044 by allowing `load_dataset()` to auto-detect and gracefully delegate to `load_from_disk()` for locally saved datasets.\r\n\r\n---\r\n\r\n### What does this PR do?\r\n\r\nThis PR introduces a minimal fallback mechanism in `load_dataset()` that detects when the provided `path` points to a dataset saved using `save_to_disk()`, and automatically redirects to `load_from_disk()`.\r\n\r\n#### \ud83d\udc1b Before (unexpected metadata-only rows):\r\n\r\n```python\r\nds = load_dataset(\"/path/to/saved_dataset\")\r\n# \u2192 returns rows with only internal metadata (_data_files, _fingerprint, etc.)\r\n````\r\n\r\n#### \u2705 After (graceful fallback):\r\n\r\n```python\r\nds = load_dataset(\"/path/to/saved_dataset\")\r\n# \u2192 logs a warning and internally switches to load_from_disk()\r\n```\r\n\r\n---\r\n\r\n### Why is this useful?\r\n\r\n* Prevents confusion when reloading local datasets saved via `save_to_disk()`.\r\n* Enables smoother compatibility with frameworks (e.g., TRL, `lighteval`) that rely on `load_dataset()` calls.\r\n* Fully backward-compatible \u2014 hub-based loading, custom builders, and streaming remain untouched.\r\n<end_of_turn>\n<start_of_turn>model\nfeat(load): fallback to `load_from_disk()` when loading a saved dataset directory<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI am not sure if this is a new feature, but I wanted to post this problem here, and hear if others have ways of optimizing and speeding up this process.\r\n\r\nLet's say I have a really large dataset that I cannot load into memory. At this point, I am only aware of `streaming=True` to load the dataset. Now, the dataset consists of many tables. Ideally, I would want to have some simple filtering criterion, such that I only see the \"good\" tables. Here is an example of what the code might look like:\r\n\r\n```\r\ndataset = load_dataset(\r\n    \"really-large-dataset\",\r\n    streaming=True\r\n)\r\n# And let's say we process the dataset bit by bit because we want intermediate results\r\ndataset = islice(dataset, 10000)\r\n\r\n# Define a function to filter the data\r\ndef filter_function(table):\r\n    if some_condition:\r\n        return True\r\n    else:\r\n        return False\r\n\r\n# Use the filter function on your dataset\r\nfiltered_dataset = (ex for ex in dataset if filter_function(ex))\r\n```\r\n\r\nAnd then I work on the processed dataset, which would be magnitudes faster than working on the original. I would love to hear if the problem setup + solution makes sense to people, and if anyone has suggestions!\n\n### Motivation\n\nSee description above\n\n### Your contribution\n\nHappy to make PR if this is a new feature<end_of_turn>\n<start_of_turn>model\nFilter Large Dataset Entry by Entry<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nsupport fsspec 2025.10.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am encountering an `IndexError` when trying to access data from a DataLoader which wraps around a dataset I've loaded using the `datasets` library. The error suggests that the dataset size is `0`, but when I check the length and print the dataset, it's clear that it has `1166` entries.\n\n### Steps to reproduce the bug\n\n1. Load a dataset with `1166` entries.\r\n2. Create a DataLoader using this dataset.\r\n3. Try iterating over the DataLoader.\r\n\r\ncode:\r\n```python\r\ndef get_train_dataloader(self) -> DataLoader:\r\n        if self.train_dataset is None:\r\n            raise ValueError(\"Trainer: training requires a train_dataset.\")\r\n\r\n        train_dataset = self.train_dataset\r\n        data_collator = self.data_collator\r\n        print(len(train_dataset))\r\n        print(train_dataset)\r\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\r\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\r\n        else:\r\n            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\r\n        train_sampler = self._get_train_sampler()\r\n        dl = DataLoader(\r\n            train_dataset,\r\n            batch_size=self._train_batch_size,\r\n            sampler=train_sampler,\r\n            collate_fn=data_collator,\r\n            drop_last=self.args.dataloader_drop_last,\r\n            num_workers=self.args.dataloader_num_workers,\r\n            pin_memory=self.args.dataloader_pin_memory,\r\n            worker_init_fn=seed_worker,\r\n        )\r\n        print(dl)\r\n        print(len(dl))\r\n        for i in dl:\r\n            print(i)\r\n            break\r\n        return dl\r\n```\r\n\r\noutput : \r\n```\r\n1166\r\nDataset({\r\n    features: ['input_ids', 'special_tokens_mask'],\r\n    num_rows: 1166\r\n})\r\n<torch.utils.data.dataloader.DataLoader object ...>\r\n146\r\n```\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/dl/zym/llamaJP/TestUseContinuePretrainLlama.py\", line 266, in <module>\r\n    train()\r\n  File \"/home/dl/zym/llamaJP/TestUseContinuePretrainLlama.py\", line 260, in train\r\n    trainer.train()\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/transformers/trainer.py\", line 1506, in train\r\n    return inner_training_loop(\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/transformers/trainer.py\", line 1520, in _inner_training_loop\r\n    train_dataloader = self.get_train_dataloader()\r\n  File \"/home/dl/zym/llamaJP/TestUseContinuePretrainLlama.py\", line 80, in get_train_dataloader\r\n    for i in dl:\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 630, in __next__\r\n    data = self._next_data()\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\r\n    data = self.dataset.__getitems__(possibly_batched_index)\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2807, in __getitems__\r\n    batch = self.__getitem__(keys)\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\r\n    return self._getitem(key)\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2787, in _getitem\r\n    pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 583, in query_table\r\n    _check_valid_index_key(key, size)\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 536, in _check_valid_index_key\r\n    _check_valid_index_key(int(max(key)), size=size)\r\n  File \"/root/miniconda3/envs/LLM/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 526, in _check_valid_index_key\r\n    raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\nIndexError: Invalid key: 1116 is out of bounds for size 0\r\n```\n\n### Expected behavior\n\nI expect to be able to iterate over the DataLoader without encountering an IndexError since the dataset is populated.\n\n### Environment info\n\n- `datasets` library version: [2.14.5]\r\n- Platform: [Linux] \r\n- Python version: 3.10\r\n- Other libraries involved: HuggingFace Transformers<end_of_turn>\n<start_of_turn>model\nIndexError: Invalid key is out of bounds for size 0 despite having a populated dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a 30TB dataset. When I perform add_column and cast_column operations on it and then execute a multiprocessing map, it results in an OOM (Out of Memory) error. However, if I skip the add_column and cast_column steps and directly run the map, there is no OOM. After debugging step by step, I found that the OOM is caused at this point, and I suspect it\u2019s because the add_column and cast_column operations are not cached, which causes the entire dataset to be loaded in each subprocess, leading to the OOM. The critical line of code is: https://github.com/huggingface/datasets/blob/e71b0b19d79c7531f9b9bea7c09916b5f6157f42/src/datasets/utils/py_utils.py#L607\n\n\nNote num_process=1 would not cause OOM. I'm confused.\n\n### Steps to reproduce the bug\n\nFor reproduce, you can load  dataset and set cache_dir (for caching): amphion/Emilia-Dataset which is a veru large datasets that RAM can not fits.\nAnd apply the map with multiprocessing after a transform operation  (e.g. add_column, cast_column).\nAs long as num_process>1, it must cause OOM.\n\n### Expected behavior\n\nIt should not cause OOM.\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-5.10.134-16.101.al8.x86_64-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- `huggingface_hub` version: 0.33.1\n- PyArrow version: 20.0.0\n- Pandas version: 2.3.0\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nApplying map after transform with multiprocessing will cause OOM<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix iterate nested field<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n\n\nWhen I run the following python script using datasets I get a segfault.\n```python\nfrom datasets import load_dataset\nfrom tqdm import tqdm\nprogress_bar = tqdm(total=(1000), unit='cols', desc='cols ')\nprogress_bar.update(1)\n```\n\n```\n% lldb -- python3 crashmin.py \n(lldb) target create \"python3\"\nCurrent executable set to '/Users/ian/bug/venv/bin/python3' (arm64).\n(lldb) settings set -- target.run-args  \"crashmin.py\"\n(lldb) r\nProcess 8095 launched: '/Users/ian/bug/venv/bin/python3' (arm64)\nProcess 8095 stopped\n* thread #2, stop reason = exec\n    frame #0: 0x0000000100014b30 dyld`_dyld_start\ndyld`_dyld_start:\n->  0x100014b30 <+0>: mov    x0, sp\n    0x100014b34 <+4>: and    sp, x0, #0xfffffffffffffff0\n    0x100014b38 <+8>: mov    x29, #0x0 ; =0 \nTarget 0: (Python) stopped.\n(lldb) c\nProcess 8095 resuming\ncols :   0% 0/1000 [00:00<?, ?cols/s]Process 8095 stopped\n* thread #2, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x10)\n    frame #0: 0x0000000101783454 _datetime.cpython-313-darwin.so`delta_new + 188\n_datetime.cpython-313-darwin.so`delta_new:\n->  0x101783454 <+188>: ldr    x3, [x20, #0x10]\n    0x101783458 <+192>: adrp   x0, 10\n    0x10178345c <+196>: add    x0, x0, #0x6fc ; \"seconds\"\nTarget 0: (Python) stopped.\n(lldb) bt\n* thread #2, queue = 'com.apple.main-thread', stop reason = EXC_BAD_ACCESS (code=1, address=0x10)\n  * frame #0: 0x0000000101783454 _datetime.cpython-313-darwin.so`delta_new + 188\n    frame #1: 0x0000000100704b60 Python`type_call + 96\n    frame #2: 0x000000010067ba34 Python`_PyObject_MakeTpCall + 120\n    frame #3: 0x00000001007aae3c Python`_PyEval_EvalFrameDefault + 30236\n    frame #4: 0x000000010067c900 Python`PyObject_CallOneArg + 112\n    frame #5: 0x000000010070f0a0 Python`slot_tp_finalize + 116\n    frame #6: 0x000000010070c3b4 Python`subtype_dealloc + 788\n    frame #7: 0x00000001006c378c Python`insertdict + 756\n    frame #8: 0x00000001006db2b0 Python`_PyModule_ClearDict + 660\n    frame #9: 0x000000010080a9a8 Python`finalize_modules + 1772\n    frame #10: 0x0000000100809a44 Python`_Py_Finalize + 264\n    frame #11: 0x0000000100837630 Python`Py_RunMain + 252\n    frame #12: 0x0000000100837ef8 Python`pymain_main + 304\n    frame #13: 0x0000000100837f98 Python`Py_BytesMain + 40\n    frame #14: 0x000000019cfcc274 dyld`start + 2840\n(lldb) register read x20\n     x20 = 0x0000000000000000\n(lldb) \n```\n\n### Steps to reproduce the bug\n\nRun the script above, and observe the segfault.\n\n### Expected behavior\n\nNo segfault\n\n### Environment info\n```\n% pip freeze datasets | grep -i datasets\ndatasets==4.2.0\n(venv) 0 ~/bug 14:58:06 \n% pip freeze tqdm | grep -i tqdm\ntqdm==4.67.1\n(venv) 0 ~/bug 14:58:16 \n% python --version\nPython 3.13.7\n```<end_of_turn>\n<start_of_turn>model\nSIGSEGV when Python exits due to near null deref<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd missing `embed_storage` method in Pdf feature (Same as in Audio and Image)<end_of_turn>\n<start_of_turn>model\nfix: Add embed_storage in Pdf feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nBackground: Digital images are often represented as a  (Height, Width, Channel) tensor. This is the same for huggingface datasets that contain images. These images are loaded in Pillow containers which offer, for example, the `.convert` method.\r\n\r\nI can convert an image from a (H,W,3) shape to a grayscale (H,W) image and I have no problems with this. But when attempting to return a (H,W,1) shaped matrix from a map function, it never completes and sometimes even results in an OOM from the OS.\r\n\r\n\r\nI've used various methods to expand a (H,W) shaped array to a (H,W,1) array. But they all resulted in extremely long map operations consuming a lot of CPU and RAM.\r\n\r\n### Steps to reproduce the bug\r\n\r\nBelow is a minimal example using two methods to get the desired output. Both of which don't work\r\n```py\r\nimport tensorflow as tf\r\nimport datasets\r\nimport numpy as np\r\n\r\nds = datasets.load_dataset(\"project-sloth/captcha-images\")\r\nto_gray_pillow = lambda sample: {'image': np.expand_dims(sample['image'].convert(\"L\"), axis=-1)}\r\nds_gray = ds.map(to_gray_pillow)\r\n\r\n# Alternatively\r\nds = datasets.load_dataset(\"project-sloth/captcha-images\").with_format(\"tensorflow\")\r\nto_gray_tf = lambda sample: {'image': tf.expand_dims(tf.image.rgb_to_grayscale(sample['image']), axis=-1)}\r\nds_gray = ds.map(to_gray_tf)\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expect the map operation to complete and return a new dataset containing grayscale images in a (H,W,1) shape.\r\n\r\n### Environment info\r\n\r\ndatasets 2.21.0\r\npython tested with both 3.11 and 3.12\r\nhost os : linux<end_of_turn>\n<start_of_turn>model\nAttempting to return a rank 3 grayscale image from dataset.map results in extreme slowdown <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi,\r\n\r\nI am trying to load a local csv dataset(similar to explodinggradients_fiqa) using load_dataset. When I try to pass features, I am facing the mentioned issue.\r\n\r\nCSV Data sample(golden_dataset.csv):\r\nQuestion        |          Context                      |         answer      |        groundtruth\r\n\"what is abc?\"   | \"abc is this and that\"     |    \"abc is this \"  |    \"abc is this and that\"\r\n\r\n```\r\nimport csv \r\n\r\n# built it based on https://huggingface.co/datasets/explodinggradients/fiqa/viewer/ragas_eval?row=0\r\nmydict = [\r\n{'question' : \"what is abc?\", 'contexts': [\"abc is this and that\"], 'answer': \"abc is this \" , 'groundtruth':  [\"abc is this and that\"]},\r\n{'question' : \"what is abc?\", 'contexts': [\"abc is this and that\"], 'answer': \"abc is this \" , 'groundtruth':  [\"abc is this and that\"]},\r\n{'question' : \"what is abc?\", 'contexts': [\"abc is this and that\"], 'answer': \"abc is this \" , 'groundtruth':  [\"abc is this and that\"]}\r\n]\r\n         \r\nfields = ['question', 'contexts', 'answer', 'ground_truths'] \r\n\r\nwith open('golden_dataset.csv', 'w', newline='\\n') as file: \r\n    writer = csv.DictWriter(file, fieldnames = fields)\r\n    \r\n    writer.writeheader() \r\n    for row in mydict:\r\n        writer.writerow(row)\r\n```\r\n\r\nRetrieved dataset:\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['question', 'contexts', 'answer', 'ground_truths'],\r\n        num_rows: 1\r\n    })\r\n})\r\n\r\n\r\nCode to reproduce issue:\r\n\r\n\r\n```\r\nfrom datasets import load_dataset, Features, Sequence, Value\r\n\r\nencode_features = Features(\r\n    {\r\n        \"question\": Value(dtype='string', id=0),\r\n        \"contexts\": Sequence(feature=Value(dtype='string', id=1)),\r\n        \"answer\": Value(dtype='string', id=2),\r\n        \"ground_truths\": Sequence(feature=Value(dtype='string',id=3)),\r\n    }\r\n)\r\n\r\neval_dataset = load_dataset('csv', data_files='/golden_dataset.csv', features = encode_features )\r\n```\r\n\r\n\r\nError trace:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nArrowNotImplementedError                  Traceback (most recent call last)\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1925, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1924 _time = time.time()\r\n-> 1925 for _, table in generator:\r\n   1926     if max_shard_size is not None and writer._num_bytes > max_shard_size:\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:192, in Csv._generate_tables(self, files)\r\n    189         # Uncomment for debugging (will print the Arrow table size and elements)\r\n    190         # logger.warning(f\"pa_table: {pa_table} num rows: {pa_table.num_rows}\")\r\n    191         # logger.warning('\\n'.join(str(pa_table.slice(i, 1).to_pydict()) for i in range(pa_table.num_rows)))\r\n--> 192         yield (file_idx, batch_idx), self._cast_table(pa_table)\r\n    193 except ValueError as e:\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/packaged_modules/csv/csv.py:167, in Csv._cast_table(self, pa_table)\r\n    165 if all(not require_storage_cast(feature) for feature in self.config.features.values()):\r\n    166     # cheaper cast\r\n--> 167     pa_table = pa.Table.from_arrays([pa_table[field.name] for field in schema], schema=schema)\r\n    168 else:\r\n    169     # more expensive cast; allows str <-> int/float or str to Audio for example\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/table.pxi:3781, in pyarrow.lib.Table.from_arrays()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/table.pxi:1449, in pyarrow.lib._sanitize_arrays()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/array.pxi:354, in pyarrow.lib.asarray()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/table.pxi:551, in pyarrow.lib.ChunkedArray.cast()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/compute.py:400, in cast(arr, target_type, safe, options, memory_pool)\r\n    399         options = CastOptions.safe(target_type)\r\n--> 400 return call_function(\"cast\", [arr], options, memory_pool)\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/_compute.pyx:572, in pyarrow._compute.call_function()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/_compute.pyx:367, in pyarrow._compute.Function.call()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/pyarrow/error.pxi:121, in pyarrow.lib.check_status()\r\n\r\nArrowNotImplementedError: Unsupported cast from string to list using function cast_list\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\nCell In[57], line 1\r\n----> 1 eval_dataset = load_dataset('csv', data_files='/golden_dataset.csv', features = encode_features )\r\n     \r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/load.py:2153, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2150 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   2152 # Download and prepare data\r\n-> 2153 builder_instance.download_and_prepare(\r\n   2154     download_config=download_config,\r\n   2155     download_mode=download_mode,\r\n   2156     verification_mode=verification_mode,\r\n   2157     try_from_hf_gcs=try_from_hf_gcs,\r\n   2158     num_proc=num_proc,\r\n   2159     storage_options=storage_options,\r\n   2160 )\r\n   2162 # Build dataset for splits\r\n   2163 keep_in_memory = (\r\n   2164     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2165 )\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:954, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952     if num_proc is not None:\r\n    953         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954     self._download_and_prepare(\r\n    955         dl_manager=dl_manager,\r\n    956         verification_mode=verification_mode,\r\n    957         **prepare_split_kwargs,\r\n    958         **download_and_prepare_kwargs,\r\n    959     )\r\n    960 # Sync info\r\n    961 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1049, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1045 split_dict.add(split_generator.split_info)\r\n   1047 try:\r\n   1048     # Prepare split will record examples associated to the split\r\n-> 1049     self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1050 except OSError as e:\r\n   1051     raise OSError(\r\n   1052         \"Cannot find data file. \"\r\n   1053         + (self.manual_download_instructions or \"\")\r\n   1054         + \"\\nOriginal error:\\n\"\r\n   1055         + str(e)\r\n   1056     ) from None\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1813, in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1811 job_id = 0\r\n   1812 with pbar:\r\n-> 1813     for job_id, done, content in self._prepare_split_single(\r\n   1814         gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1815     ):\r\n   1816         if done:\r\n   1817             result = content\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1958, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1956     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1957         e = e.__context__\r\n-> 1958     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1960 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset \r\n```\r\nEnvironment Info:\r\ndatasets version: 2.14.5\r\nPython version: 3.10.8\r\nPyArrow version: 12.0.1\r\nPandas version: 2.0.3\r\n\r\nI have also tried to load dataset first and then use cast_column, or save_to_disk and load_from_disk.<end_of_turn>\n<start_of_turn>model\nArrowNotImplementedError: Unsupported cast from string to list using function cast_list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a dataset with an embedding column, when I try to map that dataset I get the following exception:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/jmif/.virtualenvs/llm-training/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3189, in map\r\n    for rank, done, content in iflatmap_unordered(\r\n  File \"/Users/jmif/.virtualenvs/llm-training/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1387, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"/Users/jmif/.virtualenvs/llm-training/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1387, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"/Users/jmif/.virtualenvs/llm-training/lib/python3.10/site-packages/multiprocess/pool.py\", line 774, in get\r\n    raise self._value\r\nTypeError: Couldn't cast array of type\r\nfixed_size_list<item: float>[2]\r\nto\r\nSequence(feature=Value(dtype='float32', id=None), length=2, id=None)\r\n```\n\n### Steps to reproduce the bug\n\nHere's a simple repro script:\r\n\r\n```\r\nfrom datasets import Features, Value, Sequence, ClassLabel, Dataset\r\n\r\ndataset_features = Features({\r\n    'text': Value('string'),\r\n    'embedding': Sequence(Value('double'), length=2),\r\n    'categories': Sequence(ClassLabel(names=sorted([\r\n        'one',\r\n        'two',\r\n        'three'\r\n    ]))),\r\n})\r\n\r\ndataset = Dataset.from_dict(\r\n    {\r\n        'text': ['A'] * 10000,\r\n        'embedding': [[0.0, 0.1]] * 10000,\r\n        'categories': [[0]] * 10000,\r\n    },\r\n    features=dataset_features\r\n)\r\n\r\ndef test_mapper(r):\r\n    r['text'] = list(map(lambda t: t + ' b', r['text']))\r\n    return r\r\n\r\n\r\ndataset = dataset.map(test_mapper, batched=True, batch_size=10, features=dataset_features, num_proc=2)\r\n```\r\n\r\nRemoving the embedding column fixes the issue!\n\n### Expected behavior\n\nThe mapping completes successfully.\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: macOS-14.0-arm64-arm-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.1\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nCouldn't cast array of type fixed_size_list to Sequence(Value(float64))<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nnewly uploaded datasets, since yesterday, yields an error.\r\n\r\nold datasets, works fine.\r\n\r\nSeems like the datasets api server returns a 500\r\n\r\nI'm getting the same error, when I invoke `load_dataset` with my dataset.\r\n\r\nLong discussion about it here, but I'm not sure anyone from huggingface have seen it.\r\nhttps://discuss.huggingface.co/t/hfhubhttperror-500-server-error-internal-server-error-for-url/99580/1\n\n### Steps to reproduce the bug\n\nthis api url: \r\n\r\nhttps://huggingface.co/api/datasets/neoneye/simon-arc-shape-v4-rev3\r\n\r\n\r\nrespond with:\r\n\r\n```\r\n{\"error\":\"Internal Error - We're working hard to fix this as soon as possible!\"}\r\n```\r\n\n\n### Expected behavior\n\nreturn no error with newer datasets.\r\n\r\nWith older datasets I can load the datasets fine.\n\n### Environment info\n\n# Browser\r\nWhen I access the api in the browser:\r\nhttps://huggingface.co/api/datasets/neoneye/simon-arc-shape-v4-rev3\r\n\r\n```\r\n{\"error\":\"Internal Error - We're working hard to fix this as soon as possible!\"}\r\n```\r\n\r\n### Request headers\r\n\r\n```\r\nAccept\ttext/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\r\nAccept-Encoding\tgzip, deflate, br, zstd\r\nAccept-Language\ten-US,en;q=0.5\r\nConnection\tkeep-alive\r\nHost\thuggingface.co\r\nPriority\tu=1\r\nSec-Fetch-Dest\tdocument\r\nSec-Fetch-Mode\tnavigate\r\nSec-Fetch-Site\tcross-site\r\nUpgrade-Insecure-Requests\t1\r\nUser-Agent\tMozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:127.0) Gecko/20100101 Firefox/127.0\r\n```\r\n\r\n### Response headers\r\n\r\n```\r\nX-Firefox-Spdy\th2\r\naccess-control-allow-origin\thttps://huggingface.co\r\naccess-control-expose-headers\tX-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range\r\ncontent-length\t80\r\ncontent-type\tapplication/json; charset=utf-8\r\ncross-origin-opener-policy\tsame-origin\r\ndate\tFri, 26 Jul 2024 19:09:45 GMT\r\netag\tW/\"50-9qrwU+BNI4SD0Fe32p/nofkmv0c\"\r\nreferrer-policy\tstrict-origin-when-cross-origin\r\nvary\tOrigin\r\nvia\t1.1 1624c79cd07e6098196697a6a7907e4a.cloudfront.net (CloudFront)\r\nx-amz-cf-id\tSP8E7n5qRaP6i9c9G83dNAiOzJBU4GXSrDRAcVNTomY895K35H0nJQ==\r\nx-amz-cf-pop\tCPH50-C1\r\nx-cache\tError from cloudfront\r\nx-error-message\tInternal Error - We're working hard to fix this as soon as possible!\r\nx-powered-by\thuggingface-moon\r\nx-request-id\tRoot=1-66a3f479-026417465ef42f49349fdca1\r\n```<end_of_turn>\n<start_of_turn>model\nHfHubHTTPError: 500 Server Error: Internal Server Error for url:<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6214 <end_of_turn>\n<start_of_turn>model\nAdd support for `fsspec>=2023.9.0`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6554.<end_of_turn>\n<start_of_turn>model\nDo not use Parquet exports if revision is passed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIf a split is empty, then the JSON split info should mention num_bytes = 0 and num_examples = 0.\r\n\r\nUntil now they were omited because the JSON dumps ignore the fields that are equal to the default values.\r\n\r\nThis is needed in datasets-server since we parse this information to the viewer<end_of_turn>\n<start_of_turn>model\nFix empty splitinfo json<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7066\r\nThis PR introduces automatic **subset-level grouping** for folder-based dataset builders by:\r\n\r\n1. Adding a utility function `group_files_by_subset()` that clusters files by root name (ignoring digits and shard suffixes).\r\n2. Integrating this logic into `FolderBasedBuilder._split_generators()` to yield one split per subset.\r\n3. Adding unit tests for the grouping function.\r\n4. Updating the documentation to describe this new behavior under `docs/source/repository_structure.mdx`.\r\n\r\n---\r\n\r\n### Motivation\r\n\r\nDatasets with files like:\r\n```\r\n\r\ntrain0.jsonl\r\ntrain1.jsonl\r\nanimals.jsonl\r\nmetadata.jsonl\r\n\r\n```\r\n\r\nwill now be **automatically grouped** as:\r\n- `\"train\"` subset \u2192 `train0.jsonl`, `train1.jsonl`\r\n- `\"animals\"` subset \u2192 `animals.jsonl`\r\n- `\"metadata\"` subset \u2192 `metadata.jsonl`\r\n\r\nThis enables structured multi-subset loading even when the dataset doesn't follow traditional `train/validation/test` split conventions.\r\n\r\n---\r\n\r\n### Files Changed\r\n\r\n- `src/datasets/data_files.py`: added `group_files_by_subset()` utility\r\n- `src/datasets/packaged_modules/folder_based_builder/folder_based_builder.py`: grouped files before yielding splits\r\n- `tests/test_data_files.py`: added unit test `test_group_files_by_subset`\r\n- `docs/source/repository_structure.mdx`: documented subset grouping for maintainers and users\r\n\r\n---\r\n\r\n### Benefits\r\n\r\n- More flexible and robust dataset split logic\r\n- Enables logical grouping of user-uploaded files without nested folder structure\r\n- Backward-compatible with all existing folder-based configs\r\n\r\n---\r\n\r\nReady for review \u2705<end_of_turn>\n<start_of_turn>model\nIntroduces automatic subset-level grouping for folder-based dataset builders #7066<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n\r\nPackage version: datasets-2.14.4\r\n\r\nWhen I run the codes:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"glue\", \"ax\")\r\n```\r\n\r\n\r\nI got the following errors:\r\n\r\n\r\n---------------------------------------------------------------------------\r\nSchemaInferenceError                      Traceback (most recent call last)\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1949, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1948 num_shards = shard_id + 1\r\n-> 1949 num_examples, num_bytes = writer.finalize()\r\n   1950 writer.close()\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/arrow_writer.py:598, in ArrowWriter.finalize(self, close_stream)\r\n    597         self.stream.close()\r\n--> 598     raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\n    599 logger.debug(\r\n    600     f\"Done writing {self._num_examples} {self.unit} in {self._num_bytes} bytes {self._path if self._path else ''}.\"\r\n    601 )\r\n\r\nSchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\nCell In[5], line 3\r\n      1 from datasets import load_dataset\r\n----> 3 dataset = load_dataset(\"glue\", \"ax\")\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/load.py:2136, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2133 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   2135 # Download and prepare data\r\n-> 2136 builder_instance.download_and_prepare(\r\n   2137     download_config=download_config,\r\n   2138     download_mode=download_mode,\r\n   2139     verification_mode=verification_mode,\r\n   2140     try_from_hf_gcs=try_from_hf_gcs,\r\n   2141     num_proc=num_proc,\r\n   2142     storage_options=storage_options,\r\n   2143 )\r\n   2145 # Build dataset for splits\r\n   2146 keep_in_memory = (\r\n   2147     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2148 )\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:954, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952     if num_proc is not None:\r\n    953         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954     self._download_and_prepare(\r\n    955         dl_manager=dl_manager,\r\n    956         verification_mode=verification_mode,\r\n    957         **prepare_split_kwargs,\r\n    958         **download_and_prepare_kwargs,\r\n    959     )\r\n    960 # Sync info\r\n    961 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1049, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1045 split_dict.add(split_generator.split_info)\r\n   1047 try:\r\n   1048     # Prepare split will record examples associated to the split\r\n-> 1049     self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1050 except OSError as e:\r\n   1051     raise OSError(\r\n   1052         \"Cannot find data file. \"\r\n   1053         + (self.manual_download_instructions or \"\")\r\n   1054         + \"\\nOriginal error:\\n\"\r\n   1055         + str(e)\r\n   1056     ) from None\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1813, in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1811 job_id = 0\r\n   1812 with pbar:\r\n-> 1813     for job_id, done, content in self._prepare_split_single(\r\n   1814         gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1815     ):\r\n   1816         if done:\r\n   1817             result = content\r\n\r\nFile ~/anaconda3/envs/python3/lib/python3.10/site-packages/datasets/builder.py:1958, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1956     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1957         e = e.__context__\r\n-> 1958     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1960 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"glue\", \"ax\")\n\n### Expected behavior\n\nWhen generating the train split:\r\nGenerating train split:\r\n0/0 [00:00<?, ? examples/s]\r\n\r\nIt raise the error:\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n\n\n### Environment info\n\ndatasets-2.14.4.\r\nPython 3.10<end_of_turn>\n<start_of_turn>model\nDatasetGenerationError when load glue benchmark datasets from `load_dataset` <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs a hotfix for CI, temporarily pin `huggingface-hub` upper version\r\n\r\nFix #6860.\r\n\r\nRevert once root cause is fixed, see:\r\n- https://github.com/huggingface/transformers/issues/30618<end_of_turn>\n<start_of_turn>model\nFix CI by temporarily pinning huggingface-hub < 0.23.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDocument that `Dataset.to_json` defaults to JSON Lines, by adding explanation in the corresponding docstring.\r\n\r\nFix #6894.<end_of_turn>\n<start_of_turn>model\nDocument that to_json defaults to JSON Lines<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nAfter concatenate datasets on an iterable dataset, the shuffling state is destroyed, similar to #7156 \r\n\r\nThis means concatenation cant be used for resolving uneven numbers of samples across devices when using iterable datasets in a distributed setting as discussed in #6623 \r\n\r\nI also noticed that the number of shards is the same after concatenation, which I found surprising, but I don't understand the internals well enough to know whether this is actually surprising or not\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport datasets\r\nimport torch.utils.data\r\n\r\n\r\ndef gen(shards):\r\n    yield {\"shards\": shards}\r\n\r\n\r\ndef main():\r\n    dataset1 = datasets.IterableDataset.from_generator(\r\n        gen, gen_kwargs={\"shards\": list(range(25))}  # TODO: how to understand this?\r\n    )\r\n    dataset2 = datasets.IterableDataset.from_generator(\r\n        gen, gen_kwargs={\"shards\": list(range(25, 50))}  # TODO: how to understand this?\r\n    )\r\n    dataset1 = dataset1.shuffle(buffer_size=1)\r\n    dataset2 = dataset2.shuffle(buffer_size=1)\r\n    print(dataset1.n_shards)\r\n    print(dataset2.n_shards)\r\n\r\n    dataset = datasets.concatenate_datasets(\r\n        [dataset1, dataset2]\r\n    )\r\n    print(dataset.n_shards)\r\n    # dataset = dataset1\r\n\r\n    dataloader = torch.utils.data.DataLoader(\r\n        dataset,\r\n        batch_size=8,\r\n        num_workers=0,\r\n    )\r\n\r\n    for i, batch in enumerate(dataloader):\r\n        print(batch)\r\n    print(\"\\nNew epoch\")\r\n\r\n    dataset = dataset.set_epoch(1)\r\n\r\n    for i, batch in enumerate(dataloader):\r\n        print(batch)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\n### Expected behavior\r\n\r\nShuffling state should be preserved\r\n\r\n### Environment info\r\n\r\nLatest datasets<end_of_turn>\n<start_of_turn>model\nconcatenate_datasets does not preserve shuffling state<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nhttps://github.com/huggingface/datasets/blob/3f149204a2a5948287adcade5e90707aa5207a92/src/datasets/load.py#L1240C1-L1240C69\r\nmetadata_patterns = get_metadata_patterns(base_path, download_config=self.download_config)<end_of_turn>\n<start_of_turn>model\nBug get_metadata_patterns arg error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, just a couple of small issues I ran into while reading the docs for [loading pdf data](https://huggingface.co/docs/datasets/main/en/document_load):\n\n1. The link for the [`Create a pdf dataset`](https://huggingface.co/docs/datasets/main/en/document_load#pdffolder) points to https://huggingface.co/docs/datasets/main/en/pdf_dataset instead of https://huggingface.co/docs/datasets/main/en/document_dataset and hence gives a 404 error.\n\n2. At the top of the page, it's mentioned that to work with pdf datasets we need to have the `pdfplumber` package installed but the link to its installation guide points to `pytorch/vision` [installation instructions](https://github.com/pytorch/vision#installation) instead of `pdfplumber`'s [guide](https://github.com/jsvine/pdfplumber#installation)\n\nI love the work on enabling pdf dataset support and these small tweaks would help everyone navigate the docs better. Thanks!\n\n### Steps to reproduce the bug\n\nThe issue is on the [Load Document Data](https://huggingface.co/docs/datasets/main/en/document_load) page of the datasets docs.\n\n### Expected behavior\n\n1. For solving the first issue, I went through the [source .mdx code](https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx?plain=1#L188) of the datasets docs and found that the link is pointing to `./pdf_dataset` instead of `./document_dataset`\n\n2. For the second issue, I went through the [source .mdx code](https://github.com/huggingface/datasets/blob/main/docs/source/document_load.mdx?plain=1#L13) of the datasets docs and found that the link is `pytorch/vision` [installation instructions](https://github.com/pytorch/vision#installation) instead of `pdfplumber`'s [guide](https://github.com/jsvine/pdfplumber#installation)\n\nJust replacing these two links should fix the bugs\n\n### Environment info\n\ndatasets v3.5.0 (main at the time of writing)<end_of_turn>\n<start_of_turn>model\nBroken links in pdf loading documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfollowing the addition of List in #7634 \r\n\r\nbefore:\r\n\r\n```python\r\nIn [3]: ds.features\r\nOut[3]: \r\n{'json': {'id': Value(dtype='string', id=None),\r\n  'metadata:transcript': [{'end': Value(dtype='float64', id=None),\r\n    'start': Value(dtype='float64', id=None),\r\n    'transcript': Value(dtype='string', id=None),\r\n    'words': [{'end': Value(dtype='float64', id=None),\r\n      'score': Value(dtype='float64', id=None),\r\n      'start': Value(dtype='float64', id=None),\r\n      'word': Value(dtype='string', id=None)}]}],\r\n  'metadata:vad': [{'end': Value(dtype='float64', id=None),\r\n    'start': Value(dtype='float64', id=None)}]},\r\n 'mp4': Value(dtype='binary', id=None),\r\n 'npz': {'boxes_and_keypoints:box': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'boxes_and_keypoints:is_valid_box': Sequence(feature=Value(dtype='bool', id=None), length=-1, id=None),\r\n  'boxes_and_keypoints:keypoints': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:EmotionArousalToken': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:EmotionValenceToken': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:FAUToken': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:FAUValue': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:alignment_head_rotation': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:alignment_translation': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:emotion_arousal': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:emotion_scores': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:emotion_valence': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:expression': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:frame_latent': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:gaze_encodings': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:head_encodings': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:hypernet_features': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'movement:is_valid': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'smplh:body_pose': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\r\n  'smplh:global_orient': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None),\r\n  'smplh:is_valid': Sequence(feature=Value(dtype='bool', id=None), length=-1, id=None),\r\n  'smplh:left_hand_pose': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\r\n  'smplh:right_hand_pose': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),\r\n  'smplh:translation': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None)},\r\n 'wav': Audio(sampling_rate=None, mono=True, decode=True, id=None),\r\n '__key__': Value(dtype='string', id=None),\r\n '__url__': Value(dtype='string', id=None)}\r\n```\r\n\r\nafter:\r\n\r\n```python\r\nIn [3]: ds.features\r\nOut[3]: \r\n{'json': {'id': Value('string'),\r\n  'metadata:transcript': List({'end': Value('float64'), 'start': Value('float64'), 'transcript': Value('string'), 'words': List({'end': Value('float64'), 'score': Value('float64'), 'start': Value('float64'), 'word': Value('string')})}),\r\n  'metadata:vad': List({'end': Value('float64'), 'start': Value('float64')})},\r\n 'mp4': Value('binary'),\r\n 'npz': {'boxes_and_keypoints:box': List(List(Value('float32'))),\r\n  'boxes_and_keypoints:is_valid_box': List(Value('bool')),\r\n  'boxes_and_keypoints:keypoints': List(List(List(Value('float32')))),\r\n  'movement:EmotionArousalToken': List(List(Value('float32'))),\r\n  'movement:EmotionValenceToken': List(List(Value('float32'))),\r\n  'movement:FAUToken': List(List(Value('float32'))),\r\n  'movement:FAUValue': List(List(Value('float32'))),\r\n  'movement:alignment_head_rotation': List(List(Value('float32'))),\r\n  'movement:alignment_translation': List(List(List(Value('float32')))),\r\n  'movement:emotion_arousal': List(List(Value('float32'))),\r\n  'movement:emotion_scores': List(List(Value('float32'))),\r\n  'movement:emotion_valence': List(List(Value('float32'))),\r\n  'movement:expression': List(List(Value('float32'))),\r\n  'movement:frame_latent': List(List(Value('float32'))),\r\n  'movement:gaze_encodings': List(List(Value('float32'))),\r\n  'movement:head_encodings': List(List(Value('float32'))),\r\n  'movement:hypernet_features': List(List(Value('float32'))),\r\n  'movement:is_valid': List(List(Value('float32'))),\r\n  'smplh:body_pose': List(List(List(Value('float32')))),\r\n  'smplh:global_orient': List(List(Value('float32'))),\r\n  'smplh:is_valid': List(Value('bool')),\r\n  'smplh:left_hand_pose': List(List(List(Value('float32')))),\r\n  'smplh:right_hand_pose': List(List(List(Value('float32')))),\r\n  'smplh:translation': List(List(Value('float32')))},\r\n 'wav': Audio(sampling_rate=None, decode=True, stream_index=None),\r\n '__key__': Value('string'),\r\n '__url__': Value('string')}\r\n```<end_of_turn>\n<start_of_turn>model\nbetter features repr<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`Datasets==2.18.0` is not compatible with `fsspec==2023.12.2`.\r\n\r\nI have to downgrade fsspec to `fsspec==2023.10.0` to make `Datasets==2.18.0` work properly.\n\n### Steps to reproduce the bug\n\nTo reproduce the bug:\r\n1. Make sure that `Datasets==2.18.0` and `fsspec==2023.12.2`.\r\n2. Run the following code:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"trec\")\r\n```\r\n3. Then one will get the following error message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2556, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2265, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/builder.py\", line 371, in __init__\r\n    self.config, self.config_id = self._create_builder_config(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/builder.py\", line 620, in _create_builder_config\r\n    builder_config._resolve_data_files(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/builder.py\", line 211, in _resolve_data_files\r\n    self.data_files = self.data_files.resolve(base_path, download_config)\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 799, in resolve\r\n    out[key] = data_files_patterns_list.resolve(base_path, download_config)\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 752, in resolve\r\n    resolve_pattern(\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 393, in resolve_pattern\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find 'hf://datasets/trec@65752bf53af25bc935a0dce92fb5b6c930728450/default/train/0000.parquet' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.geoparquet', '.gpq', '.arrow', '.txt', '.tar', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']\r\n```\r\n4. Similar issue also found for the following code:\r\n```\r\ndataset = load_dataset(\"sst\", \"default\")\r\n```\n\n### Expected behavior\n\nIf the dataset is loaded correctly, one will have:\r\n```\r\n>>> print(dataset)\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text', 'coarse_label', 'fine_label'],\r\n        num_rows: 5452\r\n    })\r\n    test: Dataset({\r\n        features: ['text', 'coarse_label', 'fine_label'],\r\n        num_rows: 500\r\n    })\r\n})\r\n>>>\r\n```\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-6.2.0-35-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.1\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\n[Bug Report] `datasets==2.18.0` is not compatible with `fsspec==2023.12.2`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`Node.js` 16 GitHub Actions are deprecated. See: https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/\r\n\r\nWe should update them to Node 20.\r\n\r\nSee warnings in our CI, e.g.: https://github.com/huggingface/datasets/actions/runs/7957295009?pr=6678\r\n> Node.js 16 actions are deprecated. Please update the following actions to use Node.js 20: actions/checkout@v3, actions/setup-python@v4. For more information see: https://github.blog/changelog/2023-09-22-github-actions-transitioning-from-node-16-to-node-20/.\r\n<end_of_turn>\n<start_of_turn>model\nNode.js 16 GitHub Actions are deprecated<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6022 <end_of_turn>\n<start_of_turn>model\nFix `ClassLabel` min max check for `None` values<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThis is the error I got, it seems numba package does not support python 3.13\r\n\r\nPS C:\\Users\\sergi\\Documents> pip install datasets[audio]\r\nDefaulting to user installation because normal site-packages is not writeable\r\nCollecting datasets[audio]\r\n  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\r\n... (OTHER PACKAGES)\r\nCollecting numba>=0.51.0 (from librosa->datasets[audio])\r\n  Downloading numba-0.60.0.tar.gz (2.7 MB)\r\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 2.7/2.7 MB 44.1 MB/s eta 0:00:00\r\n  Installing build dependencies ... done\r\n  Getting requirements to build wheel ... error\r\n  error: subprocess-exited-with-error\r\n\r\n  \u00d7 Getting requirements to build wheel did not run successfully.\r\n  \u2502 exit code: 1\r\n  \u2570\u2500> [24 lines of output]\r\n      Traceback (most recent call last):\r\n        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.496.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\r\n          main()\r\n          ~~~~^^\r\n        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.496.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\r\n          json_out['return_val'] = hook(**hook_input['kwargs'])\r\n                                   ~~~~^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_3.13.496.0_x64__qbz5n2kfra8p0\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\r\n          return hook(config_settings)\r\n        File \"C:\\Users\\sergi\\AppData\\Local\\Temp\\pip-build-env-yauns_qh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 334, in get_requires_for_build_wheel\r\n          return self._get_build_requires(config_settings, requirements=[])\r\n                 ~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n        File \"C:\\Users\\sergi\\AppData\\Local\\Temp\\pip-build-env-yauns_qh\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\", line 304, in _get_build_requires\r\n          self.run_setup()\r\n          ~~~~~~~~~~~~~~^^\r\n      RuntimeError: Cannot install on Python version 3.13.1; only versions >=3.9,<3.13 are supported.\r\n      [end of output]\r\n\r\n  note: This error originates from a subprocess, and is likely not a problem with pip.\r\nerror: subprocess-exited-with-error\r\n\r\n\u00d7 Getting requirements to build wheel did not run successfully.\r\n\u2502 exit code: 1\r\n\u2570\u2500> See above for output.\n\n### Steps to reproduce the bug\n\n1. install python >=3.13\r\n2. !pip install datasets[audio]\n\n### Expected behavior\n\nI needed datasets[audio] in the python 3.13\n\n### Environment info\n\npython 3.13.1<end_of_turn>\n<start_of_turn>model\nNot available datasets[audio] on python 3.13<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nHost the MedImg dataset (similar to Imagenet but for biomedical images). \r\n\r\n### Motivation\r\n\r\nThere is a clear need for biomedical image foundation models and large scale biomedical datasets that are easily streamable. This would be an excellent tool for the biomedical community.\r\n\r\n### Your contribution\r\n\r\nMedImg can be found [here](https://www.cuilab.cn/medimg/#).<end_of_turn>\n<start_of_turn>model\nAdd MedImg for streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nthis should fix the CI failures on `main`\r\n\r\nclose https://github.com/huggingface/datasets/issues/5477<end_of_turn>\n<start_of_turn>model\nFix CI: pyarrow 15, pandas 2.2 and sqlachemy<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHello!\r\n\r\n## Pull Request overview\r\n* Move dataset card creation to method for easier overriding\r\n\r\n## Details\r\nIt's common for me to fully automatically download, reformat, and upload a dataset (e.g. see https://huggingface.co/datasets?other=sentence-transformers), but one aspect that I cannot easily automate is the dataset card generation. This is because during `push_to_hub`, the dataset card is created in 3 lines of code in a much larger method. To automatically generate a dataset card, I need to either:\r\n1. Subclass `Dataset`/`DatasetDict`, copy the entire `push_to_hub` method to override the ~3 lines used to generate the dataset card. This is not viable as the method is likely to change over time.\r\n2. Use `push_to_hub` normally, then separately download the pushed (but empty) dataset card, update it, and reupload the modified dataset. This works fine, but prevents me from being able to return a `Dataset` to my users which will automatically use a nice dataset card.\r\n\r\nSo, in this PR I'm proposing to move the dataset generation into another method so that it can be overridden more easily. For example, imagine the following use case:\r\n````python\r\n\r\nimport json\r\nfrom typing import Any, Dict, Optional\r\nfrom datasets import Dataset, load_dataset\r\nfrom datasets.info import DatasetInfosDict, DatasetInfo\r\nfrom datasets.utils.metadata import MetadataConfigs\r\nfrom huggingface_hub import DatasetCardData, DatasetCard\r\n\r\nTEMPLATE = r\"\"\"---\r\n{dataset_card_data}\r\n---\r\n\r\n# Dataset Card for {source_dataset_name} with mined hard negatives\r\n\r\nThis dataset is a collection of {column_one}-{column_two}-negative triplets from the {source_dataset_name} dataset. See [{source_dataset_name}](https://huggingface.co/datasets/{source_dataset_id}) for additional information. This dataset can be used directly with Sentence Transformers to train embedding models.\r\n\r\n## Mining Parameters\r\nThe negative samples have been mined using the following parameters:\r\n- `range_min`: {range_min}, i.e. we skip the {range_min} most similar samples\r\n- `range_max`: {range_max}, i.e. we only look at the top {range_max} most similar samples\r\n- `margin`: {margin}, i.e. we require negative similarity + margin < positive similarity, so negative samples can't be more similar than the known true answer\r\n- `sampling_strategy`: {sampling_strategy}, i.e. whether to randomly sample from the candidate negatives or take the \"top\" negatives\r\n- `num_negatives`: {num_negatives}, i.e. we mine {num_negatives} negatives per question-answer pair\r\n\r\n## Dataset Format\r\n- Columns: {column_one}, {column_two}, negative\r\n- Column types: str, str, str\r\n- Example:\r\n```python\r\n{example}\r\n```\r\n\"\"\"\r\n\r\nclass HNMDataset(Dataset):\r\n    @classmethod\r\n    def from_dict(cls, *args, mining_kwargs: Dict[str, Any], **kwargs) -> \"HNMDataset\":\r\n        dataset = super().from_dict(*args, **kwargs)\r\n        dataset.mining_kwargs = mining_kwargs\r\n        return dataset\r\n    \r\n    def _create_dataset_card(\r\n        self,\r\n        dataset_card_data: DatasetCardData,\r\n        dataset_card: Optional[DatasetCard],\r\n        config_name: str,\r\n        info_to_dump: DatasetInfo,\r\n        metadata_config_to_dump: MetadataConfigs,\r\n    ) -> DatasetCard:\r\n        if dataset_card:\r\n            return dataset_card\r\n\r\n        DatasetInfosDict({config_name: info_to_dump}).to_dataset_card_data(dataset_card_data)\r\n        MetadataConfigs({config_name: metadata_config_to_dump}).to_dataset_card_data(dataset_card_data)\r\n        dataset_card_data.tags = [\"sentence-transformers\"]\r\n        dataset_name = self.mining_kwargs[\"source_dataset\"].info.dataset_name\r\n        # Very messy, just as an example:\r\n        dataset_id = list(self.mining_kwargs[\"source_dataset\"].info.download_checksums.keys())[0].removeprefix(\"hf://datasets/\").split(\"@\")[0]\r\n        content = TEMPLATE.format(**{\r\n            \"dataset_card_data\": str(dataset_card_data),\r\n            \"source_dataset_name\": dataset_name,\r\n            \"source_dataset_id\": dataset_id,\r\n            \"range_min\": self.mining_kwargs[\"range_min\"],\r\n            \"range_max\": self.mining_kwargs[\"range_max\"],\r\n            \"margin\": self.mining_kwargs[\"margin\"],\r\n            \"sampling_strategy\": self.mining_kwargs[\"sampling_strategy\"],\r\n            \"num_negatives\": self.mining_kwargs[\"num_negatives\"],\r\n            \"column_one\": self.column_names[0],\r\n            \"column_two\": self.column_names[1],\r\n            \"example\": json.dumps(self[0], indent=4),\r\n        })\r\n        return DatasetCard(content)\r\n\r\nsource_dataset = load_dataset(\"sentence-transformers/gooaq\", split=\"train[:100]\")\r\ndataset = HNMDataset.from_dict({\r\n    \"query\": source_dataset[\"question\"],\r\n    \"answer\": source_dataset[\"answer\"],\r\n    # \"negative\": ... <- In my case, this column would be 'mined' automatically with these parameters\r\n}, mining_kwargs={\r\n    \"range_min\": 10,\r\n    \"range_max\": 20,\r\n    \"max_score\": 0.9,\r\n    \"margin\": 0.1,\r\n    \"sampling_strategy\": \"random\",\r\n    \"num_negatives\": 3,\r\n    \"source_dataset\": source_dataset,\r\n})\r\ndataset.push_to_hub(\"tomaarsen/mining_demo\", private=True)\r\n````\r\n\r\nIn this script, I've created a subclass which stores some additional information about how the dataset was generated. It's a bit hacky (e.g. setting a `mining_kwargs` parameter in `from_dict` that wasn't created in `__init__`, but that's just a consequence of how the `from_...` methods don't accept kwargs), but it allows me to create a \"hard negatives mining\" function that returns a dataset which people can use locally like normal, but if they choose to upload it, then it'll automatically include some information, e.g.: https://huggingface.co/datasets/tomaarsen/mining_demo\r\n\r\nThis allows others to actually find this dataset (e.g. via the `sentence-transformers` tag) and get an idea of the quality, source, etc. by looking at the model card.\r\n\r\n## Note\r\nI'm not fixed on this solution whatsoever: I am also completely fine with other solutions, e.g. a `dataset.set_dataset_card_creator` method that allows me to provide a function without even having to subclass anything. I'm open to all ideas :)\r\n\r\ncc @albertvillanova @lhoestq \r\ncc @LysandreJik \r\n\r\n- Tom Aarsen<end_of_turn>\n<start_of_turn>model\n[`feat`] Move dataset card creation to method for easier overriding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7504\r\n\r\nThis PR resolves a compatibility error when loading datasets via `load_dataset()` using outdated arguments like `use_auth_token`.\r\n\r\n**What was happening:**\r\nUsers passing `use_auth_token` in `load_dataset(..., use_auth_token=...)` encountered a `ValueError`: BuilderConfig ParquetConfig(...) doesn't have a 'use_auth_token' key.\r\n\r\n**Why:**\r\n`use_auth_token` has been deprecated and removed from config definitions (replaced by `token`), but the `load_dataset()` function still forwarded it via `**config_kwargs` to BuilderConfigs, leading to unrecognized key errors.\r\n\r\n**Fix:**\r\nWe now intercept and strip `use_auth_token` from `config_kwargs` inside `load_dataset`, replacing it with a warning:\r\n```python\r\nif \"use_auth_token\" in config_kwargs:\r\n    logger.warning(\"The 'use_auth_token' argument is deprecated. Please use 'token' instead.\")\r\n    config_kwargs.pop(\"use_auth_token\")\r\n\r\n```\r\nThis ensures legacy compatibility while guiding users to switch to the token argument.\r\n\r\nLet me know if you'd prefer a deprecation error instead of a warning. Thanks!<end_of_turn>\n<start_of_turn>model\nfix(load): strip deprecated use_auth_token from config_kwargs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs reported by @Wauplin, when loading a community dataset with script, there is a bug in the example of usage of the error message if the dataset has multiple configs (and no default config) and the user does not pass any config. For example:\r\n```python\r\n>>> ds = load_dataset(\"google/fleurs\")\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['af_za', 'am_et', 'ar_eg', 'as_in', 'ast_es', 'az_az', 'be_by', 'bg_bg', 'bn_in', 'bs_ba', 'ca_es', 'ceb_ph', 'ckb_iq', 'cmn_hans_cn', 'cs_cz', 'cy_gb', 'da_dk', 'de_de', 'el_gr', 'en_us', 'es_419', 'et_ee', 'fa_ir', 'ff_sn', 'fi_fi', 'fil_ph', 'fr_fr', 'ga_ie', 'gl_es', 'gu_in', 'ha_ng', 'he_il', 'hi_in', 'hr_hr', 'hu_hu', 'hy_am', 'id_id', 'ig_ng', 'is_is', 'it_it', 'ja_jp', 'jv_id', 'ka_ge', 'kam_ke', 'kea_cv', 'kk_kz', 'km_kh', 'kn_in', 'ko_kr', 'ky_kg', 'lb_lu', 'lg_ug', 'ln_cd', 'lo_la', 'lt_lt', 'luo_ke', 'lv_lv', 'mi_nz', 'mk_mk', 'ml_in', 'mn_mn', 'mr_in', 'ms_my', 'mt_mt', 'my_mm', 'nb_no', 'ne_np', 'nl_nl', 'nso_za', 'ny_mw', 'oc_fr', 'om_et', 'or_in', 'pa_in', 'pl_pl', 'ps_af', 'pt_br', 'ro_ro', 'ru_ru', 'sd_in', 'sk_sk', 'sl_si', 'sn_zw', 'so_so', 'sr_rs', 'sv_se', 'sw_ke', 'ta_in', 'te_in', 'tg_tj', 'th_th', 'tr_tr', 'uk_ua', 'umb_ao', 'ur_pk', 'uz_uz', 'vi_vn', 'wo_sn', 'xh_za', 'yo_ng', 'yue_hant_hk', 'zu_za', 'all']\r\nExample of usage:\r\n\t`load_dataset('fleurs', 'af_za')`\r\n```\r\n\r\nNote the example of usage in the error message suggests loading \"fleurs\" instead of \"google/fleurs\".<end_of_turn>\n<start_of_turn>model\nWrong example of usage when config name is missing for community script-datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Summary\r\nFixes #7818 \r\n\r\nThis PR resolves the `ValueError: Unable to avoid copy while creating an array` error that occurs when using `train_test_split` with `stratify_by_column` parameter in NumPy 2.0+.\r\n\r\n## Changes\r\n- Wrapped the stratify column array access with `np.asarray()` in `arrow_dataset.py`\r\n- This allows NumPy 2.0 to make a copy when the Arrow array is non-contiguous in memory\r\n\r\n## Testing\r\n- \u2705 Tested with NumPy 2.3.4 - stratified splits work correctly\r\n- \u2705 Tested with NumPy 1.26.4 - backward compatibility maintained\r\n- \u2705 Verified class balance is preserved in stratified splits\r\n- \u2705 Non-stratified splits continue to work as expected\r\n\r\n<end_of_turn>\n<start_of_turn>model\nresolves the ValueError: Unable to avoid copy while creating an array<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...<end_of_turn>\n<start_of_turn>model\ndeleted<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nA first step towards https://github.com/huggingface/datasets/issues/6529<end_of_turn>\n<start_of_turn>model\nRun download_and_prepare if missing splits<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...<end_of_turn>\n<start_of_turn>model\n...<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nThis feature proposes adding a new dataset builder called bookfolder to the datasets library. This builder would allow users to easily load datasets consisting of various digital book formats, including: AZW, AZW3, CB7, CBR, CBT, CBZ, EPUB, MOBI, and PDF.\n\n### Motivation\n\nCurrently, loading datasets of these digital book files requires manual effort. This would also lower the barrier to entry for working with these formats, enabling more diverse and interesting datasets to be used within the Hugging Face ecosystem.\n\n### Your contribution\n\nThis feature is rather simple as it will be based on the folder-based builder, similar to imagefolder. I'm willing to contribute to this feature by submitting a PR<end_of_turn>\n<start_of_turn>model\nAdd bookfolder Dataset Builder for Digital Book Formats<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSince the Hub returns dataset info for gated datasets and unauthenticated users, there is dead code: https://github.com/huggingface/datasets/blob/98fdc9e78e6d057ca66e58a37f49d6618aab8130/src/datasets/load.py#L1846-L1852\r\n\r\nWe should remove the dead code and properly handle this case: currently we are raising a `ConnectionError` instead of a `DatasetNotFoundError` (as before).\r\n\r\nSee:\r\n- https://github.com/huggingface/dataset-viewer/issues/3025\r\n- https://github.com/huggingface/huggingface_hub/issues/2457<end_of_turn>\n<start_of_turn>model\nConnectionError for gated datasets and unauthenticated users<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes dataset loading from disk by ensuring that memory maps and streams are properly closed. \r\n\r\n\r\nFor more details, see https://github.com/huggingface/datasets/issues/7373.<end_of_turn>\n<start_of_turn>model\nfix: loading of datasets from Disk(#7373)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7538 and https://github.com/huggingface/datasets/issues/7475<end_of_turn>\n<start_of_turn>model\nRebatch arrow iterables before formatted iterable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRetry intermediate commits too<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen preprocessing a multi-label dataset, I introduced a step to convert int labels to float labels as [BCEWithLogitsLoss](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) expects float labels and forward function of models in transformers package internally use `BCEWithLogitsLoss`\n\nHowever, the casting was canceled after `.map` process and the label values still use int values, which leads to an error\n```\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1711, in forward\n    loss = loss_fct(logits, labels)\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/modules/loss.py\", line 819, in forward\n    return F.binary_cross_entropy_with_logits(\n  File \"/home/yoshitomo/anaconda3/envs/torchdistill/lib/python3.10/site-packages/torch/nn/functional.py\", line 3628, in binary_cross_entropy_with_logits\n    return torch.binary_cross_entropy_with_logits(\nRuntimeError: result type Float can't be cast to the desired output type Long\n```\n\nThis seems like happening only when the original labels are int values (see examples below)\n\n### Steps to reproduce the bug\n\nIf the original dataset uses a list of int labels, it will cancel the int->float casting\n\n```python\nfrom datasets import Dataset\n\ndata = {\n    'text': ['text1', 'text2', 'text3', 'text4'],\n    'labels': [[0, 1, 2], [3], [3, 4], [3]]\n}\n\ndataset = Dataset.from_dict(data)\n\nlabel_set = set([label for labels in data['labels'] for label in labels])\nlabel2idx = {label: idx for idx, label in enumerate(sorted(label_set))}\n\n\ndef multi_labels_to_ids(labels):\n    ids = [0.0] * len(label2idx)\n    for label in labels:\n        ids[label2idx[label]] = 1.0\n    return ids\n\ndef preprocess(examples):\n    result = {'sentence': [[0, 3, 4] for _ in range(len(examples['labels']))]}\n    print('\"labels\" are int', examples['labels'])\n    result['labels'] = [multi_labels_to_ids(l) for l in examples['labels']]\n    print('\"labels\" were converted to multi-label format with float values', result['labels'])\n    return result\n\n\npreprocessed_dataset = dataset.map(preprocess, batched=True, remove_columns=['labels', 'text'])\nprint(preprocessed_dataset[0]['labels'])\n# Output: \"[1, 1, 1, 0, 0]\"\n# Expected: \"[1.0, 1.0, 1.0, 0.0, 0.0]\"\n```\n\nIf the original dataset uses non-int labels, it works as expected.\n\n```python\nfrom datasets import Dataset\n\ndata = {\n    'text': ['text1', 'text2', 'text3', 'text4'],\n    'labels': [['label1', 'label2', 'label3'], ['label4'], ['label4', 'label5'], ['label4']]\n}\n\ndataset = Dataset.from_dict(data)\n\nlabel_set = set([label for labels in data['labels'] for label in labels])\nlabel2idx = {label: idx for idx, label in enumerate(sorted(label_set))}\n\n\ndef multi_labels_to_ids(labels):\n    ids = [0.0] * len(label2idx)\n    for label in labels:\n        ids[label2idx[label]] = 1.0\n    return ids\n\ndef preprocess(examples):\n    result = {'sentence': [[0, 3, 4] for _ in range(len(examples['labels']))]}\n    print('\"labels\" are int', examples['labels'])\n    result['labels'] = [multi_labels_to_ids(l) for l in examples['labels']]\n    print('\"labels\" were converted to multi-label format with float values', result['labels'])\n    return result\n\n\npreprocessed_dataset = dataset.map(preprocess, batched=True, remove_columns=['labels', 'text'])\nprint(preprocessed_dataset[0]['labels'])\n# Output: \"[1.0, 1.0, 1.0, 0.0, 0.0]\"\n# Expected: \"[1.0, 1.0, 1.0, 0.0, 0.0]\"\n```\n\nNote that the only difference between these two examples is\n>    'labels': [[0, 1, 2], [3], [3, 4], [3]]\n\nv.s\n\n>    'labels': [['label1', 'label2', 'label3'], ['label4'], ['label4', 'label5'], ['label4']]\n\n\n\n### Expected behavior\n\nEven if the original dataset uses a list of int labels, the int->float casting during `.map` process should not be canceled as shown in the above example\n\n### Environment info\n\nOS Ubuntu 22.04 LTS\nPython 3.10.11\ndatasets v3.4.1<end_of_turn>\n<start_of_turn>model\nLabel casting during `map` process is canceled after the `map` process<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThere is a typo in the section of the documentation dedicated to creating an audio dataset. The Dataset is incorrectly suffixed with a `Config`\r\n\r\n\r\nhttps://huggingface.co/datasets/indonesian-nlp/librivox-indonesia/blob/main/librivox-indonesia.py#L59<end_of_turn>\n<start_of_turn>model\nfix typo in Audio dataset documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDocumentation link: https://huggingface.co/docs/datasets/en/package_reference/main_classes\n\nLink to method (docstring present): https://github.com/huggingface/datasets/blob/6f2502c5a026caa89839713f6f7c8b958e5e83eb/src/datasets/arrow_dataset.py#L1029\n\nThe docstring is present for the function, but seems missing from the official documentation for the `Dataset` class on HuggingFace.\n\nThe method in question:\n```python\n    @classmethod\n    def from_dict(\n        cls,\n        mapping: dict,\n        features: Optional[Features] = None,\n        info: Optional[DatasetInfo] = None,\n        split: Optional[NamedSplit] = None,\n    ) -> \"Dataset\":\n        \"\"\"\n        Convert `dict` to a `pyarrow.Table` to create a [`Dataset`].\n\n        Important: a dataset created with from_dict() lives in memory\n        and therefore doesn't have an associated cache directory.\n        This may change in the future, but in the meantime if you\n        want to reduce memory usage you should write it back on disk\n        and reload using e.g. save_to_disk / load_from_disk.\n\n        Args:\n            mapping (`Mapping`):\n                Mapping of strings to Arrays or Python lists.\n            features ([`Features`], *optional*):\n                Dataset features.\n            info (`DatasetInfo`, *optional*):\n                Dataset information, like description, citation, etc.\n            split (`NamedSplit`, *optional*):\n                Name of the dataset split.\n\n        Returns:\n            [`Dataset`]\n        \"\"\"\n```<end_of_turn>\n<start_of_turn>model\n[Docs] Missing documentation for `Dataset.from_dict`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nOnce a file is uploaded and the commit is made, I am unable to delete individual files without completely deleting the whole dataset via the website UI.\n\n### Motivation\n\nWould be a useful addition\n\n### Your contribution\n\nWould love to help out with some guidance<end_of_turn>\n<start_of_turn>model\nDelete uploaded files from the UI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to:\r\n- #5850<end_of_turn>\n<start_of_turn>model\nRefactor extensions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nbefore the release, to make sure everything works fine<end_of_turn>\n<start_of_turn>model\nupdate torchcodec in ci<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 3.4.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi @mariosasko @lhoestq  @katielink \r\n\r\nThanks for your contribution and hard work.\r\n\r\n### Feature request\r\n\r\nFirst, I download a dataset as normal by:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('username/data_name', cache_dir='data')\r\n```\r\n\r\nThe dataset format in `data` directory will be:\r\n```\r\n-data\r\n  |-data_name\r\n    |-test-00000-of-00001-bf4c733542e35fcb.parquet\r\n    |-train-00000-of-00001-2a1df75c6bce91ab.parquet\r\n```\r\n\r\n\r\nThen I use SCP to clone this dataset into another machine, and then try:\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('data/data_name')  # load from local path\r\n```\r\n\r\nThis leads to re-generating training and validation split for each time, and the disk quota will be duplicated occupation.\r\n\r\nHow can I just load the dataset without generating and saving these splits again?\r\n\r\n### Motivation\r\n\r\nI do not want to download the same dataset in two machines, scp is much faster and better than HuggingFace API. I hope we can directly load the downloaded datasets (.parquest)\r\n\r\n### Your contribution\r\n\r\nPlease refer to the feature<end_of_turn>\n<start_of_turn>model\nHow to load existing downloaded dataset ?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi! When I load this dataset, it fails with a pyarrow error. I'm using datasets 4.1.1, though I also see this with datasets 4.1.2\n\nTo reproduce:\n\n```\nimport datasets\n\nds = datasets.load_dataset(path=\"metr-evals/malt-public\", name=\"irrelevant_detail\")\n```\n\nError:\n```\nTraceback (most recent call last):\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 1815, in _prepare_split_single\n    for _, table in generator:\n                    ^^^^^^^^^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 93, in _generate_tables\n    for batch_idx, record_batch in enumerate(\n                                   ~~~~~~~~~^\n        parquet_fragment.to_batches(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n        )\n        ^\n    ):\n    ^\n  File \"pyarrow/_dataset.pyx\", line 3904, in _iterator\n  File \"pyarrow/_dataset.pyx\", line 3494, in pyarrow._dataset.TaggedRecordBatchIterator.__next__\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/neev/scratch/test_hf.py\", line 3, in <module>\n    ds = datasets.load_dataset(path=\"metr-evals/malt-public\", name=\"irrelevant_detail\")\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/load.py\", line 1412, in load_dataset\n    builder_instance.download_and_prepare(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        download_config=download_config,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        storage_options=storage_options,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 894, in download_and_prepare\n    self._download_and_prepare(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dl_manager=dl_manager,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        **download_and_prepare_kwargs,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 970, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 1702, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ):\n    ^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 1858, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n```\n\n### Steps to reproduce the bug\n\nTo reproduce:\n\n```\nimport datasets\n\nds = datasets.load_dataset(path=\"metr-evals/malt-public\", name=\"irrelevant_detail\")\n```\n\n### Expected behavior\n\nThe dataset loads\n\n### Environment info\n\nDatasets: 4.1.1\nPython: 3.13\nPlatform: Macos<end_of_turn>\n<start_of_turn>model\nCannot load dataset, fails with nested data conversions not implemented for chunked array outputs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nIt is possible to register custom features using datasets.features.features.register_feature (https://github.com/huggingface/datasets/pull/6727)\r\n\r\nHowever such features are not compatible with Features.encode_example/decode_example if they require special encoding / decoding logic because encode_nested_example / decode_nested_example checks whether the feature is in a fixed list of encodable types:\r\n\r\nhttps://github.com/huggingface/datasets/blob/16a121d7821a7691815a966270f577e2c503473f/src/datasets/features/features.py#L1349\r\n\r\nThis prevents the extensibility of features to complex cases\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nclass ListOfStrs:\r\n    def encode_example(self, value):\r\n        if isinstance(value, str):\r\n            return [str]\r\n        else:\r\n            return value\r\nfeats = Features(strlist=ListOfStrs())\r\nassert feats.encode_example({\"strlist\": \"a\"})[\"strlist\"] = feats[\"strlist\"].encode_example(\"a\")}\r\n```\r\n\r\n### Expected behavior\r\n\r\nRegistered feature types should be encoded based on some property of the feature (e.g. requires_encoding)?\r\n\r\n### Environment info\r\n\r\n3.0.2<end_of_turn>\n<start_of_turn>model\nCustom features not compatible with special encoding/decoding logic<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate process.mdx: Minor Code Listings Updates and Fixes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- Add audio support\r\n- Fix an issue where user-provided features with additional fields are not taken into account\r\n\r\nClose https://github.com/huggingface/datasets/issues/6569<end_of_turn>\n<start_of_turn>model\n[WebDataset] Audio support and bug fixes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #5876<end_of_turn>\n<start_of_turn>model\nForce overwrite existing filesystem protocol<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported in https://discuss.huggingface.co/t/speeding-up-streaming-of-large-datasets-fineweb/90714/6  when training using a streaming a dataloader\r\n\r\ncc @Wauplin it looks like the retries from `hfh` are not always enough. In this PR I let `datasets` do additional retries (that users can configure in `datasets.config`) since I couldn't find an easy way to increase the max_retries for `hfh` users in general.<end_of_turn>\n<start_of_turn>model\n[Streaming] retry on requests errors<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello,\n\nI would like to upload a video dataset (some .mp4 files and some segments within them), i.e. rows correspond to subsequences from videos. Videos might be referenced by several rows.\n\nI created a dataset locally and it references the videos and the video readers can read them correctly. I use push_to_hub() to upload the dataset to the hub.\n\nExpectation: A user uses `load_dataset` and can load the videos.\n\nHowever, the videos seem to be just referenced via paths on the computer and not uploaded to the hub. Therefore a target user cannot load the videos in the dataset.\n\n### Steps to reproduce the bug\n\n1. create a video dataset with paths e.g. { [\"videos\"]: [path1, path2, ...]}\n2. dataset.push_to_hub\n3. on a different computer (or same pc if relative paths are used in a different folder):\n```\ndataset = load_dataset(\"siplab/egosim\", split=\"train\")\nvideo = dataset[0][\"video_head\"]\n```\n\n3. will fail\n\n### Expected behavior\n\nExpectation: A user uses `load_dataset` and can load the videos.\n\n### Environment info\n\ndatasets                       3.1.0\nPython 3.8.18<end_of_turn>\n<start_of_turn>model\npush_to_hub does not upload videos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n[<ipython-input-48-6a7b3e847019>](https://localhost:8080/#) in <cell line: 7>()\r\n      5 }\r\n      6 \r\n----> 7 csv_datasets_reloaded = load_dataset(\"tsv\", data_files=data_files)\r\n      8 csv_datasets_reloaded\r\n\r\n2 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1489                     raise e1 from None\r\n   1490                 if isinstance(e1, FileNotFoundError):\r\n-> 1491                     raise FileNotFoundError(\r\n   1492                         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1493                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n\r\nFileNotFoundError: Couldn't find a dataset script at /content/tsv/tsv.py or any data file in the same directory. Couldn't find 'tsv' on the Hugging Face Hub either: FileNotFoundError: Dataset 'tsv' doesn't exist on the Hub\r\n```\n\n### Steps to reproduce the bug\n\n```\r\ndata_files = {\r\n    \"train\": \"/content/PUBHEALTH/train.tsv\",\r\n    \"validation\": \"/content/PUBHEALTH/dev.tsv\",\r\n    \"test\": \"/content/PUBHEALTH/test.tsv\",\r\n}\r\n\r\ntsv_datasets_reloaded = load_dataset(\"tsv\", data_files=data_files)\r\ntsv_datasets_reloaded\r\n```\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-48-6a7b3e847019> in <cell line: 7>()\r\n      5 }\r\n      6 \r\n----> 7 csv_datasets_reloaded = load_dataset(\"tsv\", data_files=data_files)\r\n      8 csv_datasets_reloaded\r\n\r\n2 frames\r\n/usr/local/lib/python3.10/dist-packages/datasets/load.py in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1489                     raise e1 from None\r\n   1490                 if isinstance(e1, FileNotFoundError):\r\n-> 1491                     raise FileNotFoundError(\r\n   1492                         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1493                         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n\r\nFileNotFoundError: Couldn't find a dataset script at /content/tsv/tsv.py or any data file in the same directory. Couldn't find 'tsv' on the Hugging Face Hub either: FileNotFoundError: Dataset 'tsv' doesn't exist on the Hub\r\n```\r\n\n\n### Expected behavior\n\nload the data, push to hub\n\n### Environment info\n\njupyter notebook RTX 3090<end_of_turn>\n<start_of_turn>model\nCouldn't find a dataset script at /content/tsv/tsv.py or any data file in the same directory<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using `datasets.DatasetDict.push_to_hub` an `HfHubHTTPError: 413 Client Error: Payload Too Large for url` is raised if the dataset contains a large `ClassLabel` feature. Even if the total size of the dataset is small.\n\n### Steps to reproduce the bug\n\n``` python\nimport random\nimport sys\n\nimport datasets\n\nrandom.seed(42)\n\n\ndef random_str(sz):\n    return \"\".join(chr(random.randint(ord(\"a\"), ord(\"z\"))) for _ in range(sz))\n\n\ndata = datasets.DatasetDict(\n    {\n        str(i): datasets.Dataset.from_dict(\n            {\n                \"label\": [list(range(3)) for _ in range(10)],\n                \"abstract\": [random_str(10_000) for _ in range(10)],\n            },\n        )\n        for i in range(3)\n    }\n)\nfeatures = data[\"1\"].features.copy()\nfeatures[\"label\"] = datasets.Sequence(\n    datasets.ClassLabel(names=[str(i) for i in range(50_000)])\n)\ndata = data.map(lambda examples: {}, features=features)\n\nfeat_size = sys.getsizeof(data[\"1\"].features[\"label\"].feature.names)\nprint(f\"Size of ClassLabel names: {feat_size}\")\n# Size of ClassLabel names: 444376\n\n\ndata.push_to_hub(\"dconnell/pubtator3_test\")\n```\n\nNote that this succeeds if `ClassLabel` has fewer names or if `ClassLabel` is replaced with `Value(\"int64\")`\n\n\n### Expected behavior\n\nShould push the dataset to hub.\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\n\n- `datasets` version: 3.2.0\n- Platform: Linux-5.15.0-126-generic-x86_64-with-glibc2.35\n- Python version: 3.12.8\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0\n<end_of_turn>\n<start_of_turn>model\npush_to_hub payload too large error when using large ClassLabel feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReported in https://github.com/huggingface/datasets/pull/5902#issuecomment-1649885621 (cc @alvarobartt)<end_of_turn>\n<start_of_turn>model\nFix Quickstart notebook link<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe [tutorial](https://huggingface.co/docs/datasets/main/en/object_detection) mentions the necessity of conversion the input image from BGR to RGB\r\n\r\n> albumentations expects the image to be in BGR format, not RGB, so you\u2019ll have to convert the image before applying the transform.\r\n\r\n[Link to tutorial](https://github.com/huggingface/datasets/blob/0a068dbf3b446417ffd89d32857608394ec699e6/docs/source/object_detection.mdx#L77)\r\n\r\nHowever, relevant albumentations' tutorials [on channels conversion](https://albumentations.ai/docs/examples/example/#read-the-image-from-the-disk-and-convert-it-from-the-bgr-color-space-to-the-rgb-color-space) and  [on boxes](https://albumentations.ai/docs/examples/example_bboxes/) imply that it's not really true no more.\r\n\r\nI suggest removing this outdated conversion from the tutorial.<end_of_turn>\n<start_of_turn>model\nConversion from RGB to BGR in Object Detection tutorial<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported in https://discuss.huggingface.co/t/datasetdict-save-to-disk-with-num-proc-1-seems-to-hang-with-error/75595<end_of_turn>\n<start_of_turn>model\nfix CastError pickling<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI've spent a significant amount of time trying to locate the split object inside my _split_generators() custom function.\r\nThen after diving [in the code](https://github.com/huggingface/datasets/blob/5ff3670c18ed34fa8ddfa70a9aa403ae6cc9ad54/src/datasets/load.py#L2558) I realized that `download_and_prepare` is executed before! split is passed to the dataset builder in `as_dataset`.\r\n\r\nIf I'm not missing something, this seems like bad design, for the following use case:\r\n\r\n> Imagine there is a huge dataset that has an evaluation test set and you want to just download and run just to compare your method.\r\n\r\nIs there a current workaround that can help me achieve the same result?\r\n\r\nThank you,<end_of_turn>\n<start_of_turn>model\nImpossible to only download a test split<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen trying to cast a list array to fixed size list, an AttributeError is raised:\r\n> AttributeError: 'pyarrow.lib.FixedSizeListType' object has no attribute 'length'\r\n\r\nSteps to reproduce the bug:\r\n```python\r\nimport pyarrow as pa\r\nfrom datasets.table import array_cast\r\n\r\narr = pa.array([[0, 1]])\r\narray_cast(arr, pa.list_(pa.int64(), 2))\r\n```\r\n\r\nStack trace:\r\n```\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-12-6cb90a1d8216> in <module>\r\n      3 \r\n      4 arr = pa.array([[0, 1]])\r\n----> 5 array_cast(arr, pa.list_(pa.int64(), 2))\r\n\r\n~/huggingface/datasets/src/datasets/table.py in wrapper(array, *args, **kwargs)\r\n   1802             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1803         else:\r\n-> 1804             return func(array, *args, **kwargs)\r\n   1805 \r\n   1806     return wrapper\r\n\r\n~/huggingface/datasets/src/datasets/table.py in array_cast(array, pa_type, allow_primitive_to_str, allow_decimal_to_str)\r\n   1920                 else:\r\n   1921                     array_values = array.values[\r\n-> 1922                         array.offset * pa_type.length : (array.offset + len(array)) * pa_type.length\r\n   1923                     ]\r\n   1924                     return pa.FixedSizeListArray.from_arrays(_c(array_values, pa_type.value_type), pa_type.list_size)\r\n\r\nAttributeError: 'pyarrow.lib.FixedSizeListType' object has no attribute 'length'\r\n```<end_of_turn>\n<start_of_turn>model\nCasting list array to fixed size list raises error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to https://github.com/huggingface/huggingface_hub/issues/3340.\r\n\r\nThis PR adapts `datasets` to be compatible with both huggingface_hub v0.x and v1.x. \r\n\r\nIn practice nothing else should change (I've checked the codebase). The `HfHubHTTPError` is a base error defined in `huggingface_hub` that inherits from `requests.HTTPError` in v0.x and will inherit from `httpx.HTTPError` in v1.x. It has been introduced ~2 years ago so it's fine to use it right now (i.e. no need to wait for v1.x release or bump minimal version).\r\n\r\nMost of the changes have been around the test suite to make sure that tests are passing with both `requests` and `httpx` backends. Mid-term it would be good to completely remove the `requests` dependency from `datasets` but that's an orthogonal topic. <end_of_turn>\n<start_of_turn>model\nSupport huggingface_hub v0.x and v1.x<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nSuppose I have a two iterable datasets, one with the features: \r\n* `{\"audio\", \"text\", \"column_a\"}`\r\n\r\nAnd the other with the features:\r\n* `{\"audio\", \"sentence\", \"column_b\"}`\r\n\r\nI want to combine both datasets using `interleave_datasets`, which requires me to unify the column names. I would typically do this by:\r\n1. Renaming the common columns to the same name (e.g. `\"text\"` -> `\"sentence\"`)\r\n2. Removing the unwanted columns (e.g. `\"column_a\"`, `\"column_b\"`)\r\n\r\nHowever, the process of renaming and removing columns in an iterable dataset doesn't work, since we need to preserve the original text column, meaning we can't combine the datasets.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\n# load LS in streaming mode\r\ndataset = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", streaming=True)\r\n\r\n# check original features\r\ndataset_features = dataset.features.keys()\r\nprint(\"Original features: \", dataset_features)\r\n\r\n#\u00a0rename \"text\" -> \"sentence\"\r\ndataset = dataset.rename_column(\"text\", \"sentence\")\r\n\r\n# remove unwanted columns\r\nCOLUMNS_TO_KEEP = {\"audio\", \"sentence\"}\r\ndataset = dataset.remove_columns(set(dataset_features - COLUMNS_TO_KEEP))\r\n\r\n# stream first sample, should return \"audio\" and \"sentence\" columns\r\nprint(next(iter(dataset)))\r\n```\r\nTraceback:\r\n```python\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\nCell In[5], line 17\r\n     14 COLUMNS_TO_KEEP = {\"audio\", \"sentence\"}\r\n     15 dataset = dataset.remove_columns(set(dataset_features - COLUMNS_TO_KEEP))\r\n---> 17 print(next(iter(dataset)))\r\n\r\nFile ~/datasets/src/datasets/iterable_dataset.py:1353, in IterableDataset.__iter__(self)\r\n   1350         yield formatter.format_row(pa_table)\r\n   1351     return\r\n-> 1353 for key, example in ex_iterable:\r\n   1354     if self.features:\r\n   1355         # `IterableDataset` automatically fills missing columns with None.\r\n   1356         # This is done with `_apply_feature_types_on_example`.\r\n   1357         example = _apply_feature_types_on_example(\r\n   1358             example, self.features, token_per_repo_id=self._token_per_repo_id\r\n   1359         )\r\n\r\nFile ~/datasets/src/datasets/iterable_dataset.py:652, in MappedExamplesIterable.__iter__(self)\r\n    650     yield from ArrowExamplesIterable(self._iter_arrow, {})\r\n    651 else:\r\n--> 652     yield from self._iter()\r\n\r\nFile ~/datasets/src/datasets/iterable_dataset.py:729, in MappedExamplesIterable._iter(self)\r\n    727 if self.remove_columns:\r\n    728     for c in self.remove_columns:\r\n--> 729         del transformed_example[c]\r\n    730 yield key, transformed_example\r\n    731 current_idx += 1\r\n\r\nKeyError: 'text'\r\n```\r\n=> we see that `datasets` is looking for the column \"text\", even though we've renamed this to \"sentence\" and then removed the un-wanted \"text\" column from our dataset.\r\n\r\n### Expected behavior\r\n\r\nShould be able to rename and remove columns from iterable dataset.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.1.dev0\r\n- Platform: macOS-13.5.1-arm64-arm-64bit\r\n- Python version: 3.11.6\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.9.2<end_of_turn>\n<start_of_turn>model\nIterable Dataset: rename column clashes with remove column<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nminor release step improvement<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nin <cell line: 1>:1                                                                              \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1537 in train                    \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1534 \u2502   \u2502   inner_training_loop = find_executable_batch_size(                                 \u2502\r\n\u2502   1535 \u2502   \u2502   \u2502   self._inner_training_loop, self._train_batch_size, args.auto_find_batch_size  \u2502\r\n\u2502   1536 \u2502   \u2502   )                                                                                 \u2502\r\n\u2502 \u2771 1537 \u2502   \u2502   return inner_training_loop(                                                       \u2502\r\n\u2502   1538 \u2502   \u2502   \u2502   args=args,                                                                    \u2502\r\n\u2502   1539 \u2502   \u2502   \u2502   resume_from_checkpoint=resume_from_checkpoint,                                \u2502\r\n\u2502   1540 \u2502   \u2502   \u2502   trial=trial,                                                                  \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1789 in _inner_training_loop     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1786 \u2502   \u2502   \u2502   \u2502   rng_to_sync = True                                                        \u2502\r\n\u2502   1787 \u2502   \u2502   \u2502                                                                                 \u2502\r\n\u2502   1788 \u2502   \u2502   \u2502   step = -1                                                                     \u2502\r\n\u2502 \u2771 1789 \u2502   \u2502   \u2502   for step, inputs in enumerate(epoch_iterator):                                \u2502\r\n\u2502   1790 \u2502   \u2502   \u2502   \u2502   total_batched_samples += 1                                                \u2502\r\n\u2502   1791 \u2502   \u2502   \u2502   \u2502   if rng_to_sync:                                                           \u2502\r\n\u2502   1792 \u2502   \u2502   \u2502   \u2502   \u2502   self._load_rng_state(resume_from_checkpoint)                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py:377 in __iter__                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   374 \u2502   \u2502   dataloader_iter = super().__iter__()                                               \u2502\r\n\u2502   375 \u2502   \u2502   # We iterate one batch ahead to check when we are at the end                       \u2502\r\n\u2502   376 \u2502   \u2502   try:                                                                               \u2502\r\n\u2502 \u2771 377 \u2502   \u2502   \u2502   current_batch = next(dataloader_iter)                                          \u2502\r\n\u2502   378 \u2502   \u2502   except StopIteration:                                                              \u2502\r\n\u2502   379 \u2502   \u2502   \u2502   yield                                                                          \u2502\r\n\u2502   380                                                                                            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:633 in __next__           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    630 \u2502   \u2502   \u2502   if self._sampler_iter is None:                                                \u2502\r\n\u2502    631 \u2502   \u2502   \u2502   \u2502   # TODO(https://github.com/pytorch/pytorch/issues/76750)                   \u2502\r\n\u2502    632 \u2502   \u2502   \u2502   \u2502   self._reset()  # type: ignore[call-arg]                                   \u2502\r\n\u2502 \u2771  633 \u2502   \u2502   \u2502   data = self._next_data()                                                      \u2502\r\n\u2502    634 \u2502   \u2502   \u2502   self._num_yielded += 1                                                        \u2502\r\n\u2502    635 \u2502   \u2502   \u2502   if self._dataset_kind == _DatasetKind.Iterable and \\                          \u2502\r\n\u2502    636 \u2502   \u2502   \u2502   \u2502   \u2502   self._IterableDataset_len_called is not None and \\                    \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:677 in _next_data         \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502    674 \u2502                                                                                         \u2502\r\n\u2502    675 \u2502   def _next_data(self):                                                                 \u2502\r\n\u2502    676 \u2502   \u2502   index = self._next_index()  # may raise StopIteration                             \u2502\r\n\u2502 \u2771  677 \u2502   \u2502   data = self._dataset_fetcher.fetch(index)  # may raise StopIteration              \u2502\r\n\u2502    678 \u2502   \u2502   if self._pin_memory:                                                              \u2502\r\n\u2502    679 \u2502   \u2502   \u2502   data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)            \u2502\r\n\u2502    680 \u2502   \u2502   return data                                                                       \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py:49 in fetch             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   46 \u2502   def fetch(self, possibly_batched_index):                                                \u2502\r\n\u2502   47 \u2502   \u2502   if self.auto_collation:                                                             \u2502\r\n\u2502   48 \u2502   \u2502   \u2502   if hasattr(self.dataset, \"__getitems__\") and self.dataset.__getitems__:         \u2502\r\n\u2502 \u2771 49 \u2502   \u2502   \u2502   \u2502   data = self.dataset.__getitems__(possibly_batched_index)                    \u2502\r\n\u2502   50 \u2502   \u2502   \u2502   else:                                                                           \u2502\r\n\u2502   51 \u2502   \u2502   \u2502   \u2502   data = [self.dataset[idx] for idx in possibly_batched_index]                \u2502\r\n\u2502   52 \u2502   \u2502   else:                                                                               \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:2782 in __getitems__           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2779 \u2502                                                                                         \u2502\r\n\u2502   2780 \u2502   def __getitems__(self, keys: List) -> List:                                           \u2502\r\n\u2502   2781 \u2502   \u2502   \"\"\"Can be used to get a batch using a list of integers indices.\"\"\"                \u2502\r\n\u2502 \u2771 2782 \u2502   \u2502   batch = self.__getitem__(keys)                                                    \u2502\r\n\u2502   2783 \u2502   \u2502   n_examples = len(batch[next(iter(batch))])                                        \u2502\r\n\u2502   2784 \u2502   \u2502   return [{col: array[i] for col, array in batch.items()} for i in range(n_example  \u2502\r\n\u2502   2785                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:2778 in __getitem__            \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2775 \u2502                                                                                         \u2502\r\n\u2502   2776 \u2502   def __getitem__(self, key):  # noqa: F811                                             \u2502\r\n\u2502   2777 \u2502   \u2502   \"\"\"Can be used to index columns (by string names) or rows (by integer index or i  \u2502\r\n\u2502 \u2771 2778 \u2502   \u2502   return self._getitem(key)                                                         \u2502\r\n\u2502   2779 \u2502                                                                                         \u2502\r\n\u2502   2780 \u2502   def __getitems__(self, keys: List) -> List:                                           \u2502\r\n\u2502   2781 \u2502   \u2502   \"\"\"Can be used to get a batch using a list of integers indices.\"\"\"                \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:2762 in _getitem               \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   2759 \u2502   \u2502   format_kwargs = kwargs[\"format_kwargs\"] if \"format_kwargs\" in kwargs else self._  \u2502\r\n\u2502   2760 \u2502   \u2502   format_kwargs = format_kwargs if format_kwargs is not None else {}                \u2502\r\n\u2502   2761 \u2502   \u2502   formatter = get_formatter(format_type, features=self._info.features, **format_kw  \u2502\r\n\u2502 \u2771 2762 \u2502   \u2502   pa_subtable = query_table(self._data, key, indices=self._indices if self._indice  \u2502\r\n\u2502   2763 \u2502   \u2502   formatted_output = format_table(                                                  \u2502\r\n\u2502   2764 \u2502   \u2502   \u2502   pa_subtable, key, formatter=formatter, format_columns=format_columns, output  \u2502\r\n\u2502   2765 \u2502   \u2502   )                                                                                 \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:578 in query_table     \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   575 \u2502   \u2502   _check_valid_column_key(key, table.column_names)                                   \u2502\r\n\u2502   576 \u2502   else:                                                                                  \u2502\r\n\u2502   577 \u2502   \u2502   size = indices.num_rows if indices is not None else table.num_rows                 \u2502\r\n\u2502 \u2771 578 \u2502   \u2502   _check_valid_index_key(key, size)                                                  \u2502\r\n\u2502   579 \u2502   # Query the main table                                                                 \u2502\r\n\u2502   580 \u2502   if indices is None:                                                                    \u2502\r\n\u2502   581 \u2502   \u2502   pa_subtable = _query_table(table, key)                                             \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:531 in                 \u2502\r\n\u2502 _check_valid_index_key                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   528 \u2502   \u2502   \u2502   _check_valid_index_key(min(key), size=size)                                    \u2502\r\n\u2502   529 \u2502   elif isinstance(key, Iterable):                                                        \u2502\r\n\u2502   530 \u2502   \u2502   if len(key) > 0:                                                                   \u2502\r\n\u2502 \u2771 531 \u2502   \u2502   \u2502   _check_valid_index_key(int(max(key)), size=size)                               \u2502\r\n\u2502   532 \u2502   \u2502   \u2502   _check_valid_index_key(int(min(key)), size=size)                               \u2502\r\n\u2502   533 \u2502   else:                                                                                  \u2502\r\n\u2502   534 \u2502   \u2502   _raise_bad_key_type(key)                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:521 in                 \u2502\r\n\u2502 _check_valid_index_key                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   518 def _check_valid_index_key(key: Union[int, slice, range, Iterable], size: int) -> None:    \u2502\r\n\u2502   519 \u2502   if isinstance(key, int):                                                               \u2502\r\n\u2502   520 \u2502   \u2502   if (key < 0 and key + size < 0) or (key >= size):                                  \u2502\r\n\u2502 \u2771 521 \u2502   \u2502   \u2502   raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")       \u2502\r\n\u2502   522 \u2502   \u2502   return                                                                             \u2502\r\n\u2502   523 \u2502   elif isinstance(key, slice):                                                           \u2502\r\n\u2502   524 \u2502   \u2502   pass                                             \n\n### Steps to reproduce the bug\n\n``\r\nimport json\r\nimport os\r\nfrom pprint import pprint\r\n\r\nimport bitsandbytes as bnb\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nimport transformers\r\nfrom datasets import Dataset,load_dataset\r\n\r\nfrom peft import (\r\n    LoraConfig,\r\n    PeftConfig,\r\n    PeftModel,\r\n    get_peft_model,\r\n    prepare_model_for_kbit_training\r\n)\r\n\r\nfrom transformers import (\r\n    AutoConfig,\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n\r\n)\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n\r\ndef print_trainable_parameters(model):\r\n    \"\"\"\r\n    Prints the number of trainable parameters in the model.\r\n    \"\"\"\r\n    trainable_params = 0\r\n    all_param = 0\r\n    for _, param in model.named_parameters():\r\n        all_param += param.numel()\r\n        if param.requires_grad:\r\n            trainable_params += param.numel()\r\n    print(\r\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\r\n    )\r\n\r\n\r\nMODEL_NAME = \"tiiuae/falcon-7b\"\r\n\r\nbnb_config = BitsAndBytesConfig(\r\n    load_in_4bit = True,\r\n    bnb_4bit_use_double_quant=True,\r\n    bnb_4bit_quant_type=\"nf4\",\r\n    bnb_4bit_compute_dtype=torch.bfloat16,\r\n)\r\n\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    MODEL_NAME,\r\n    device_map = \"auto\",\r\n    trust_remote_code = True,\r\n    quantization_config = bnb_config\r\n)\r\n\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\r\ntokenizer.pad_token = tokenizer.eos_token\r\n\r\nmodel.gradient_checkpointing_enable()\r\nmodel = prepare_model_for_kbit_training(model)\r\n\r\n\r\nconfig = LoraConfig(\r\n    r = 16,\r\n    lora_alpha = 32,\r\n    target_modules = [\"query_key_value\"],\r\n    lora_dropout = 0.05,\r\n    bias = \"none\",\r\n    task_type = \"CASUAL_LM\"\r\n)\r\n\r\nmodel = get_peft_model(model,config)\r\nprint_trainable_parameters(model)\r\n\r\ndef generate_prompt(data_point):\r\n  return f\"\"\"\r\n<human>: {data_point[\"question\"]}\r\n<assistant>: {data_point[\"answer\"]}  \r\n\"\"\".strip()\r\n\r\ndef generate_and_tokenize_prompt(data_point):\r\n    full_prompt = generate_prompt(data_point)\r\n    tokenized_full_prompt = tokenizer(full_prompt, padding = True, truncation = True,return_tensors = None)\r\n    return dict({\r\n        \"input_ids\" : tokenized_full_prompt[\"input_ids\"],\r\n        \"attention_mask\" : tokenized_full_prompt[\"attention_mask\"]\r\n\r\n    })\r\n\r\n\r\ndata = data[\"train\"].shuffle().map(generate_and_tokenize_prompt, batched = False)  \r\n\r\nOUTPUT_DIR = \"experiments\"\r\n\r\ntrainings_args = transformers.TrainingArguments(\r\n    per_device_train_batch_size = 1,\r\n    gradient_accumulation_steps = 4,\r\n    num_train_epochs = 1,\r\n    learning_rate = 2e-4,\r\n    fp16 = True,\r\n    save_total_limit = 3,\r\n    logging_steps = 1,\r\n    output_dir = OUTPUT_DIR,\r\n    max_steps = 80,\r\n    optim = \"paged_adamw_8bit\",\r\n    lr_scheduler_type = \"cosine\",\r\n    warmup_ratio = 0.05,\r\n    #remove_unused_columns=True\r\n)\r\n\r\ntrainer = transformers.Trainer(\r\n    model = model,\r\n    train_dataset = data,\r\n    args = trainings_args, \r\n    data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\r\n\r\n)\r\n\r\nmodel.config.use_cache = False\r\n\r\ntrainer.train()\r\n\r\n\r\nIndexError: Invalid key: 32 is out of bounds for size 0\r\n\r\nDataSet Format is like : \r\n[{\"question\": \"How can I create an account?\", \"answer\": \"To create an account, click on the 'Sign Up' button on the top right corner of our website and follow the instructions to complete the registration process.\"}, .... ]\n\n### Expected behavior\n\n-\n\n### Environment info\n\n\r\n!pip install -q pip \r\n!pip install -q bitsandbytes==0.39.0 \r\n!pip install -q torch==2.0.1\r\n\r\n!pip install -q git+https://github.com/huggingface/transformers.git \r\n!pip install -q git+https://github.com/huggingface/peft.git \r\n!pip install -q git+https://github.com/huggingface/accelerate.git \r\n\r\n!pip install -q datasets \r\n!pip install -q loralib==0.1.1 \r\n!pip install -q einops==0.6.1 \r\n\r\n\r\nimport json\r\nimport os\r\nfrom pprint import pprint\r\n\r\nimport bitsandbytes as bnb\r\nimport pandas as pd\r\nimport torch\r\nimport torch.nn as nn\r\nimport transformers\r\nfrom datasets import Dataset,load_dataset\r\n\r\nfrom peft import (\r\n    LoraConfig,\r\n    PeftConfig,\r\n    PeftModel,\r\n    get_peft_model,\r\n    prepare_model_for_kbit_training\r\n)\r\n\r\nfrom transformers import (\r\n    AutoConfig,\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n\r\n)\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\r\n<end_of_turn>\n<start_of_turn>model\nIndexError Not Solving -> IndexError: Invalid key: ?? is out of bounds for size 0 or ??<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported in https://github.com/huggingface/evaluate/issues/542\r\n\r\ncc @stas00 @williamberrios\r\n\r\nclose https://github.com/huggingface/datasets/issues/6589<end_of_turn>\n<start_of_turn>model\nFix filelock: use current umask for filelock >= 3.10<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ndocs: update cache.mdx to include HF_DATASETS_CACHE documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI noticed that some of my images loaded using PIL have some metadata related to exif that can rotate them when loading.\r\nSince the dataset.features.Image uses PIL for loading, the loaded image may be rotated (width and height will be inverted) thus for tasks as object detection and layoutLM this can create some inconsistencies (between input bboxes and input images). \r\n\r\nFor now there is no option in datasets.features.Image to specify that. We need to do the following when preparing examples (when preparing images for training, test or inference): \r\n```\r\nfrom PIL import Image, ImageOps \r\npil = ImageOps.exif_transpose(pil)\r\n```\r\n\r\nreference: https://stackoverflow.com/a/63950647/5720150 \r\n\r\nIs it possible to add this by default to the datasets.feature.Image ? or to add the option to do the ImageOps.exif_transpose?\r\n\r\nThank you\n\n### Motivation\n\nPrevent having inverted data related to exif metadata that may affect object detection tasks\n\n### Your contribution\n\nChanging in datasets.featrues.Image I can help with that. <end_of_turn>\n<start_of_turn>model\nexif_transpose not done to Image (PIL problem)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nuseful to still get `isinstance(Sequence(Value(\"int64\")), Sequence)`for downstream libs like evaluate<end_of_turn>\n<start_of_turn>model\nBackward compat sequence instance<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\n`datasets.load_dataset()` triggers a new download of the **whole dataset** when the README.md file has been touched on huggingface hub, even if data files / parquet files are the exact same.\r\n\r\nI think the current behaviour of the load_dataset function is triggered whenever a change of the hash of latest commit on huggingface hub, but  is there a clever way to only download again the dataset **if and only if** data is modified ?  \r\n\r\n### Motivation\r\n\r\nThe current behaviour is a waste of network bandwidth / disk space / research time.\r\n\r\n### Your contribution\r\n\r\nI don't have time to submit a PR, but I hope a simple solution will emerge from this issue ! <end_of_turn>\n<start_of_turn>model\nAvoid downloading the whole dataset when only README.me has been touched on hub.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I used load_dataset to download this data set, the following error occurred. The main problem was that the target data did not exist.\n\n### Steps to reproduce the bug\n\n1.I tried downloading directly.\r\n```python\r\nwiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\")\r\n```\r\nAn exception occurred\r\n```\r\nMissingBeamOptions: Trying to generate a dataset using Apache Beam, yet no Beam Runner or PipelineOptions() has been provided in `load_dataset` or in the builder arguments. For big datasets it has to run on large-scale data processing tools like Dataflow, Spark, etc. More information about Apache Beam runners at https://beam.apache.org/documentation/runners/capability-matrix/\r\nIf you really want to run it locally because you feel like the Dataset is small enough, you can use the local beam runner called `DirectRunner` (you may run out of memory). \r\nExample of usage: \r\n\t`load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')`\r\n```\r\n2.I modified the code as prompted.\r\n```python\r\nwiki_dataset = load_dataset('wikipedia', '20220301.en', beam_runner='DirectRunner')\r\n```\r\nAn exception occurred:\r\n```\r\nFileNotFoundError: Couldn't find file at https://dumps.wikimedia.org/enwiki/20220301/dumpstatus.json\r\n```\r\n\n\n### Expected behavior\n\nI searched in the parent directory of the corresponding URL, but there was no corresponding \"20220301\" directory.\r\nI really need this data set and hope to provide a download method.\n\n### Environment info\n\npython 3.8\r\ndatasets 2.16.0\r\napache-beam 2.52.0\r\ndill 0.3.7\r\n<end_of_turn>\n<start_of_turn>model\nDatasets : wikipedia 20220301.en error <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://github.com/huggingface/datasets-server/pull/2089 for reference\r\n\r\n```\r\nfrom datasets import (Array2D, Dataset, Features)\r\nfeature_type = Array2D(shape=(2, 2), dtype=\"float32\")\r\ncontent = [[0.0, 0.0], [0.0, 0.0]]\r\nfeatures = Features({\"col\": feature_type})\r\ndataset = Dataset.from_dict({\"col\": [content]}, features=features)\r\n```\r\n\r\ngenerates\r\n\r\n```\r\n/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/features/features.py:648: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\r\n  pa.PyExtensionType.__init__(self, self.storage_dtype)\r\n/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/features/features.py:1661: RuntimeWarning: pickle-based deserialization of pyarrow.PyExtensionType subclasses is disabled by default; if you only ingest trusted data files, you may re-enable this using `pyarrow.PyExtensionType.set_auto_load(True)`.\r\nIn the future, Python-defined extension subclasses should derive from pyarrow.ExtensionType (not pyarrow.PyExtensionType) and implement their own serialization mechanism.\r\n\r\n  obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/features/features.py:1661: FutureWarning: pyarrow.PyExtensionType is deprecated and will refuse deserialization by default. Instead, please derive from pyarrow.ExtensionType and implement your own serialization mechanism.\r\n  obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 924, in from_dict\r\n    return cls(pa_table, info=info, split=split)\r\n  File \"/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 693, in __init__\r\n    inferred_features = Features.from_arrow_schema(arrow_table.schema)\r\n  File \"/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1661, in from_arrow_schema\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \"/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1661, in <dictcomp>\r\n    obj = {field.name: generate_from_arrow_type(field.type) for field in pa_schema}\r\n  File \"/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1381, in generate_from_arrow_type\r\n    return Value(dtype=_arrow_to_datasets_dtype(pa_type))\r\n  File \"/home/slesage/hf/datasets-server/libs/libcommon/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 111, in _arrow_to_datasets_dtype\r\n    raise ValueError(f\"Arrow type {arrow_type} does not have a datasets dtype equivalent.\")\r\nValueError: Arrow type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> does not have a datasets dtype equivalent.\r\n```<end_of_turn>\n<start_of_turn>model\nIssue with pyarrow 14.0.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI encountered a TypeError when batch processing a dataset with Sequence features in datasets package version 2.16.1. The error arises from a mismatch in handling fixed-size list arrays during the map function execution. Debugging pinpoints the issue to an if-statement in datasets/table.py, line 2093, failing to correctly process sequence lengths.\n\n### Steps to reproduce the bug\n\nCreate virtual environment and activate\r\n```\r\nvirtualenv venv\r\nsource venv/bin/activate\r\n```\r\n\r\nThen install the datasets package (I'm using the latest version)\r\n```\r\npip install datasets==2.16.1\r\n```\r\n\r\nThen run\r\n```python\r\n# bug.py\r\nfrom datasets import Dataset\r\nfrom datasets.features import Features, Sequence, Value\r\n\r\ndata = {\r\n    \"num\": [[1, 2], [3, 4]],\r\n}\r\nfeatures = Features({'num': Sequence(feature=Value(dtype='int32'), length=2)})\r\ndataset = Dataset.from_dict(data, features=features)\r\ndataset.map(lambda x: x, batched=True, batch_size=1)\r\n```\n\n### Expected behavior\n\nI get the following stack trace\r\n\r\n```\r\nMap:  50%|\u2588\u2588\u2588\u2588\u2588     | 1/2 [00:00<00:00, 423.92 examples/s]\r\nTraceback (most recent call last):\r\n  File \"/PATH/TO/BUG_PORT/bug.py\", line 9, in <module>\r\n    dataset.map(lambda x: x, batched=True, batch_size=1)\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3093, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3489, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 551, in write_batch\r\n    array = cast_array_to_feature(col_values, col_type) if col_type is not None else col_values\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/table.py\", line 1797, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/table.py\", line 1797, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/PATH/TO/BUG_PORT/venv/lib/python3.9/site-packages/datasets/table.py\", line 2111, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\nTypeError: Couldn't cast array of type\r\nfixed_size_list<item: int32>[2]\r\nto\r\nSequence(feature=Value(dtype='int32', id=None), length=2, id=None)\r\n\r\n```\r\n\r\nAfter some debugging, I found that the if-statement that is actually failing is line 2093 in `datasets/table.py`\r\n```python\r\n# datasets/table.py\r\n                ...\r\n2093                if feature.length * len(array) == len(array_values):\r\n2094                    return pa.FixedSizeListArray.from_arrays(_c(array_values, feature.feature), feature.length)\r\n                ...\r\n```\n\n### Environment info\n\nPlatform: MacOS\r\nDatasets version: datasets==2.16.1\r\nPython version: 3.9.6<end_of_turn>\n<start_of_turn>model\nBatched dataset map throws exception that cannot cast fixed length array to Sequence<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nEnables streaming from local symlinks #7083 \r\n\r\n@lhoestq<end_of_turn>\n<start_of_turn>model\nremove filecheck to enable symlinks<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nDocs: Use Image(mode=\"F\") for PNG/JPEG depth maps <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes example in #7208 - I'm not sure what other checks I should do? @lhoestq \r\n\r\nI also haven't thought hard about the concatenate / interleaving example iterables but think this might work assuming that features are either all identical or None?<end_of_turn>\n<start_of_turn>model\nPreserve features in iterable dataset.filter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi,\n\nI\u2019m new to HF dataset and I tried to create datasets based on data versioned in **lakeFS** _(**MinIO** S3 bucket as storage backend)_\n\nHere I\u2019m using \u00b130000 PIL image from MNIST data however it is taking around 12min to execute, which is a lot!\n\nFrom what I understand, it is loading the images into cache then building the dataset.\n\u2013 Please find bellow the execution screenshot \u2013\n\nIs there a way to optimize this or am I doing something wrong?\n\nThanks!\n\n![Image](https://github.com/user-attachments/assets/c79257c8-f023-42a9-9e6f-0898b3ea93fe)<end_of_turn>\n<start_of_turn>model\nCreating a HF Dataset from lakeFS with S3 storage takes too much time!<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs suggested by @Wauplin, see: https://github.com/huggingface/datasets/pull/6831#discussion_r1579492603\r\n> I would not advertise the --token arg in the example as this shouldn't be the recommended way (best to login with env variable or huggingface-cli login)<end_of_turn>\n<start_of_turn>model\nRemove token arg from CLI examples<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n-<end_of_turn>\n<start_of_turn>model\n-<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes part of issue #6046\r\n\r\n### Problem\r\nThe `user-agent` defined in `DownloadConfig` was not passed down to fsspec-based filesystems like `HfFileSystem`, which prevents proper identification/tracking of client requests.\r\n\r\n### Solution\r\nAdded support for injecting the `user-agent` into `storage_options[\"headers\"]` within `_prepare_single_hop_path_and_storage_options()` based on the `protocol`.\r\n\r\nNow, when using `hf://`, `http://`, or `https://`, the custom user-agent is passed automatically.\r\n\r\n### Code Location\r\nModified:\r\n- `src/datasets/utils/file_utils.py`\r\n\r\nUsed `get_datasets_user_agent(...)` to ensure proper formatting and fallback logic.<end_of_turn>\n<start_of_turn>model\nPass user-agent from DownloadConfig into fsspec storage_options<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe github actions step to publish the release 2.17.0 to conda channel has failed due to expired token. Can some one please update the anaconda token rerun the failed action? @albertvillanova ?\r\n \r\n![image](https://github.com/huggingface/datasets/assets/7138162/1b56ad3d-7643-4778-9cce-4bf531717700)\r\n\n\n### Steps to reproduce the bug\n\nPlease see this actions [link](https://github.com/huggingface/datasets/actions/runs/7842473662)\n\n### Expected behavior\n\nThe action runs successfully and the latest release is pushed to HuggingFace conda channel\n\n### Environment info\n\nNot applicable.<end_of_turn>\n<start_of_turn>model\nRelease not pushed to conda channel<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIssue\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nexamples = load_dataset('facebook/winoground', use_auth_token=<YOUR USER ACCESS TOKEN>)\r\n```\r\n\r\nGives error\r\n\r\n```\r\nTypeError: DatasetInfo.__init__() got an unexpected keyword argument 'task_templates'\r\n```\r\n\r\nA simple downgrade to lower `datasets v 2.21.0` solves it.\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. `pip install datsets`\r\n2.\r\n```python\r\nfrom datasets import load_dataset\r\nexamples = load_dataset('facebook/winoground', use_auth_token=<YOUR USER ACCESS TOKEN>)\r\n```\r\n\r\n\n\n### Expected behavior\n\nShould load the dataset correctly.\n\n### Environment info\n\n- Datasets version  `3.0.0`\r\n- `transformers` version: 4.45.0.dev0\r\n- Platform: Linux-6.8.0-40-generic-x86_64-with-glibc2.35\r\n- Python version: 3.12.4\r\n- Huggingface_hub version: 0.24.6\r\n- Safetensors version: 0.4.5\r\n- Accelerate version: 0.35.0.dev0\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.4.1+cu121 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: Yes\r\n<end_of_turn>\n<start_of_turn>model\nDatasets Unknown Keyword Argument Error - task_templates<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix error from https://github.com/huggingface/datasets/pull/7643<end_of_turn>\n<start_of_turn>model\nfix sequence ci<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.14.5<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhile investigating an issue on a Hub dataset, I have discovered the no-script datasets containing ZIP files do not load.\r\n\r\nFor example, that no-script dataset containing ZIP files, raises NonMatchingSplitsSizesError:\r\n```python\r\nIn [2]: ds = load_dataset(\"sidovic/LearningQ-qg\")\r\n\r\nNonMatchingSplitsSizesError: [\r\n  {\r\n    'expected': SplitInfo(name='train', num_bytes=0, num_examples=188660, shard_lengths=None, dataset_name=None), \r\n    'recorded': SplitInfo(name='train', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='learning_q-qg')\r\n  }, {\r\n    'expected': SplitInfo(name='validation', num_bytes=0, num_examples=20630, shard_lengths=None, dataset_name=None), \r\n    'recorded': SplitInfo(name='validation', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='learning_q-qg')\r\n  }, {\r\n    'expected': SplitInfo(name='test', num_bytes=0, num_examples=18227, shard_lengths=None, dataset_name=None), \r\n    'recorded': SplitInfo(name='test', num_bytes=0, num_examples=0, shard_lengths=None, dataset_name='learning_q-qg')\r\n  }\r\n]\r\n```\r\n\r\nAs another example, a no-script dataset containing just a (CSV)-ZIP file, raises a DatasetGenerationError:\r\n```\r\n>               num_examples, num_bytes = writer.finalize()\r\n\r\nsrc/datasets/builder.py:1949: \r\n\r\n>           raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\nE           datasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\nsrc/datasets/arrow_writer.py:598: SchemaInferenceError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nsrc/datasets/load.py:2143: in load_dataset\r\n    builder_instance.download_and_prepare(\r\nsrc/datasets/builder.py:954: in download_and_prepare\r\n    self._download_and_prepare(\r\nsrc/datasets/builder.py:1049: in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\nsrc/datasets/builder.py:1813: in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n\r\n>           raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\nE           datasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n\r\nsrc/datasets/builder.py:1958: DatasetGenerationError\r\n```\r\n\r\nAfter investigating, I think this bug was introduced in this PR:\r\n- #5972\r\n\r\nRelated to:\r\n- https://huggingface.co/datasets/sidovic/LearningQ-qg/discussions/1\r\n\r\nCC: @lhoestq <end_of_turn>\n<start_of_turn>model\nNo-script datasets with ZIP files do not load<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Description\n\nWhen loading a dataset with a field declared as a list of structs using `Sequence(Features(...))`, `load_dataset` incorrectly infers the field as a plain `struct<...>` instead of a `list<struct<...>>`. This leads to the following error:\n\n```\nArrowNotImplementedError: Unsupported cast from list<item: struct<id: string, data: string>> to struct using function cast_struct\n```\n\nThis occurs even when the `features` schema is explicitly provided and the dataset format supports nested structures natively (e.g., JSON, JSONL).\n\n---\n\n###  Minimal Reproduction\n\n[Colab Link.](https://colab.research.google.com/drive/1FZPQy6TP3jVd4B3mYKyfQaWNuOAvljUq?usp=sharing)\n\n#### Dataset\n\n```python\ndata = [\n    {\n        \"list\": [\n            {\"id\": \"example1\", \"data\": \"text\"},\n        ]\n    },\n]\n```\n\n#### Schema\n\n```python\nfrom datasets import Features, Sequence, Value\n\nitem = Features({\n    \"id\":   Value(\"string\"),\n    \"data\": Value(\"string\"),\n})\n\nfeatures = Features({\n    \"list\": Sequence(item),\n})\n```\n\n---\n\n###  Tested File Formats\n\nThe same schema was tested across different formats:\n\n| Format    | Method                      | Result              |\n| --------- | --------------------------- | ------------------- |\n| JSONL     | `load_dataset(\"json\", ...)` |  Arrow cast error  |\n| JSON      | `load_dataset(\"json\", ...)` |  Arrow cast error  |\n| In-memory | `Dataset.from_list(...)`    | Works as expected |\n\nThe issue seems not to be in the schema or the data, but in how `load_dataset()` handles the `Sequence(Features(...))` pattern when parsing from files (specifically JSON and JSONL).\n\n---\n\n### Expected Behavior\n\nIf `features` is explicitly defined as:\n\n```python\nFeatures({\"list\": Sequence(Features({...}))})\n```\n\nThen the data should load correctly across all backends \u2014 including from JSON and JSONL \u2014 without any Arrow casting errors. This works correctly when loading from memory via `Dataset.from_list`.\n\n---\n\n###  Environment\n\n* `datasets`: 3.6.0\n* `pyarrow`: 20.0.0\n* Python: 3.12.10\n* OS: Ubuntu 24.04.2 LTS\n* Notebook: \\[Colab test notebook available]\n\n---\n<end_of_turn>\n<start_of_turn>model\n`Sequence(Features(...))` causes PyArrow cast error in `load_dataset` despite correct schema.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ne.g. when running `load_dataset(\"parquet\", data_files=\"doesnt_exist.parquet\")`<end_of_turn>\n<start_of_turn>model\nRaise FileNotFoundError when passing data_files that don't exist<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to https://github.com/huggingface/datasets/issues/6130<end_of_turn>\n<start_of_turn>model\nDocument BUILDER_CONFIG_CLASS<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n\r\nstep 'Generating train split' in load_dataset is too slow\uff1a\r\n![image](https://github.com/huggingface/datasets/assets/19569322/d9b08eee-95fe-4741-a346-b70416c948f8)\r\n\n\n### Steps to reproduce the bug\n\nData\uff1a own data\uff0c16K16B Mono wav\r\nOficial Script:[ run_speech_recognition_seq2seq.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_seq2seq.py)\r\nAdd Code\uff1a\r\n    if data_args.data_path is not None:\r\n        print(data_args.data_path)\r\n        raw_datasets = load_dataset(\"audiofolder\", data_dir=data_args.data_path, cache_dir=model_args.cache_dir)\r\n        raw_datasets = raw_datasets.cast_column(\"audio\", Audio(sampling_rate=16000))\r\n        raw_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.005, shuffle=True)\r\n \uff08change cache_dir to other path \uff0cex:/DATA/cache\uff09\n\n### Expected behavior\n\nload data fast,at least 1000+\r\n`Generating train split: 387875 examples [32:24:45, 1154.83 examples/s]`\n\n### Environment info\n\n\r\n- `transformers` version: 4.28.0.dev0\r\n- Platform: Linux-5.4.0-149-generic-x86_64-with-debian-bullseye-sid\r\n- Python version: 3.7.16\r\n- Huggingface_hub version: 0.13.2\r\n- PyTorch version (GPU?): 1.13.1+cu116 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: <fill in>\r\n- Using distributed or parallel set-up in script?: <fill in><end_of_turn>\n<start_of_turn>model\nLoad Data Sets Too Slow In Train Seq2seq Model<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`data`\r\n> DatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: 4500\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: 500\r\n    })\r\n})\r\n\r\n**# input (in-correct sentence)**\r\n`data['train'][0]['input']`\r\n**>>** 'We are meet sunday 10am12pmET in Crown Heights Brooklyn New York'\r\n**# output (correct sentence)**\r\n`data['train'][0]['output']`\r\n**>>** 'We meet Sundays 10am-12pmET in Crown Heights, Brooklyn, New York.'\r\n\r\n**I Want to align the output tokens with input**\r\n\r\n```\r\n`# tokenize both inputs and targets\r\ndef tokenize_fn(batch):\r\n    # tokenize the input sequence first\r\n    # this populates input_ids, attention_mask, etc.\r\n\r\n    tokenized_inputs = tokenizer(\r\n    batch['input']\r\n    )\r\n    \r\n    labels_batch = tokenizer.tokenize(batch['output']) # original targets\r\n\r\n    aligned_labels_batch = []\r\n    for i, labels in enumerate(labels_batch):\r\n        word_ids = tokenized_inputs[i].word_ids()\r\n        aligned_labels_batch.append(align_targets(labels, word_ids))  # align_targets is another user defined function which is been called here\r\n\r\n    # recall: the 'target' must be stored in key called 'labels'\r\n    tokenized_inputs['labels'] = aligned_labels_batch\r\n\r\n    return tokenized_inputs`\r\n```\r\n```\r\ndata.map(\r\n  tokenize_fn,\r\n  batched=True,\r\n  remove_columns=data['train'].column_names,\r\n)\r\n```\r\n\r\nWhen this user defined function is mapped to every records of train and test batch am getting following error:\r\n\r\n**1.** **raise DatasetTransformationNotAllowedError(\r\n   3457         \"Using `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn't create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\"**\r\n\r\n**2.** **TypeError: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]**<end_of_turn>\n<start_of_turn>model\nToken Alignment for input and output data over train and test batch/dataset.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### What\r\nFix the \"WebDataset documentation\" link on the Create a video dataset page to point\r\nto the correct section on the video load guide.\r\n\r\n### Why\r\nThe link currently points to an external repo, but the Hugging Face docs\r\nhave an internal \"WebDataset\" section under video_load.\r\n\r\n### How\r\n- docs/source/video_dataset.mdx: updated link to\r\n  `https://huggingface.co/docs/datasets/main/en/video_load#webdataset`\r\n\r\n### Issue\r\nFixes #7699\r\n<end_of_turn>\n<start_of_turn>model\n[docs] Fix broken WebDataset link on \u201cCreate a video dataset\u201d page<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix typo in image dataset docs.\r\n\r\nTypo reported by @datavistics.<end_of_turn>\n<start_of_turn>model\nFix typo in image dataset docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR introduces a new `batch` method to the `Dataset` class, aligning its functionality with the `IterableDataset.batch()` method (implemented in #7054). The implementation uses as well the existing `map` method for efficient batching of examples.\r\n\r\nKey changes:\r\n- Add `batch` method to `Dataset` class in `arrow_dataset.py`\r\n- Utilize `map` method for batching\r\n\r\nCloses #7063\r\n\r\nOnce the approach is approved, i will create the tests and update the documentation.<end_of_turn>\n<start_of_turn>model\nAdd `batch` method to `Dataset` class<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nworkaround to fix an issue with transformers CI\r\n\r\nhttps://github.com/huggingface/transformers/pull/24867#discussion_r1266519626<end_of_turn>\n<start_of_turn>model\nBump dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nto_iterable_dataset is missing in document\n\n### Steps to reproduce the bug\n\nto_iterable_dataset is missing in document\n\n### Expected behavior\n\ndocument enhancement\n\n### Environment info\n\nunrelated<end_of_turn>\n<start_of_turn>model\nto_iterable_dataset is missing in document<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nmention av in video docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhile [the docs](https://huggingface.co/docs/datasets/upload_dataset#upload-dataset) seem to state that `.jsonl` is a supported extension for `datasets`, loading the dataset results in a `JSONDecodeError`.\n\n### Steps to reproduce the bug\n\nCode:\r\n```\r\nfrom datasets import load_dataset\r\ndset = load_dataset('slotreck/pickle')\r\n```\r\n\r\nTraceback:\r\n```\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 925/925 [00:00<00:00, 3.11MB/s]\r\nDownloading and preparing dataset json/slotreck--pickle to /mnt/home/lotrecks/.cache/huggingface/datasets/slotreck___json/slotreck--pickle-0c311f36ed032b04/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 589k/589k [00:00<00:00, 18.9MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 104k/104k [00:00<00:00, 4.61MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 170k/170k [00:00<00:00, 7.71MB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00,  3.77it/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 523.92it/s]\r\nGenerating train split: 0 examples [00:00, ? examples/s]Failed to read file '/mnt/home/lotrecks/.cache/huggingface/datasets/downloads/6ec07bb2f279c9377036af6948532513fa8f48244c672d2644a2d7018ee5c9cb' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Column(/ner/[]/[]/[]) changed from number to string in row 0\r\nTraceback (most recent call last):                      \r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py\", line 144, in _generate_tables\r\n    dataset = json.load(f)\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/json/__init__.py\", line 296, in load\r\n    parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/json/__init__.py\", line 348, in loads\r\n    return _default_decoder.decode(s)\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/json/decoder.py\", line 340, in decode\r\n    raise JSONDecodeError(\"Extra data\", s, end)\r\njson.decoder.JSONDecodeError: Extra data: line 2 column 1 (char 3086)\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/builder.py\", line 1879, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py\", line 147, in _generate_tables\r\n    raise e\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/packaged_modules/json/json.py\", line 122, in _generate_tables\r\n    io.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\r\n  File \"pyarrow/_json.pyx\", line 259, in pyarrow._json.read_json\r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: JSON parse error: Column(/ner/[]/[]/[]) changed from number to string in row 0\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/load.py\", line 1815, in load_dataset\r\n    storage_options=storage_options,\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/builder.py\", line 913, in download_and_prepare\r\n    **download_and_prepare_kwargs,\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/builder.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/builder.py\", line 1768, in _prepare_split\r\n    gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n  File \"/mnt/home/lotrecks/anaconda3/envs/pickle/lib/python3.7/site-packages/datasets/builder.py\", line 1912, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Expected behavior\n\nFor the dataset to be loaded without error.\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-centos-7.9.2009-Core\r\n- Python version: 3.7.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.3.5<end_of_turn>\n<start_of_turn>model\njsonlines files don't load with `load_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI receive this error message when using `load_dataset` with \"csv\" path and `dataset_files=s3://...`:\r\n```\r\nTypeError: Session.__init__() got an unexpected keyword argument 'hf'\r\n```\r\n\r\nI found a similar issue here: https://stackoverflow.com/questions/77596258/aws-issue-load-dataset-from-s3-fails-with-unexpected-keyword-argument-error-in\r\n\r\nFull stacktrace:\r\n```\r\n.../site-packages/datasets/load.py:2549: in load_dataset\r\n    builder_instance.download_and_prepare(\r\n.../site-packages/datasets/builder.py:1005: in download_and_prepare\r\n    self._download_and_prepare(\r\n.../site-packages/datasets/builder.py:1078: in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n.../site-packages/datasets/packaged_modules/csv/csv.py:147: in _split_generators\r\n    data_files = dl_manager.download_and_extract(self.config.data_files)\r\n.../site-packages/datasets/download/download_manager.py:562: in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n.../site-packages/datasets/download/download_manager.py:426: in download\r\n    downloaded_path_or_paths = map_nested(\r\n.../site-packages/datasets/utils/py_utils.py:466: in map_nested\r\n    mapped = [\r\n.../site-packages/datasets/utils/py_utils.py:467: in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n.../site-packages/datasets/utils/py_utils.py:387: in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n.../site-packages/datasets/utils/py_utils.py:387: in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n.../site-packages/datasets/utils/py_utils.py:370: in _single_map_nested\r\n    return function(data_struct)\r\n.../site-packages/datasets/download/download_manager.py:451: in _download\r\n    out = cached_path(url_or_filename, download_config=download_config)\r\n.../site-packages/datasets/utils/file_utils.py:188: in cached_path\r\n    output_path = get_from_cache(\r\n...1/site-packages/datasets/utils/file_utils.py:511: in get_from_cache\r\n    response = fsspec_head(url, storage_options=storage_options)\r\n.../site-packages/datasets/utils/file_utils.py:316: in fsspec_head\r\n    fs, _, paths = fsspec.get_fs_token_paths(url, storage_options=storage_options)\r\n.../site-packages/fsspec/core.py:622: in get_fs_token_paths\r\n    fs = filesystem(protocol, **inkwargs)\r\n.../site-packages/fsspec/registry.py:290: in filesystem\r\n    return cls(**storage_options)\r\n.../site-packages/fsspec/spec.py:79: in __call__\r\n    obj = super().__call__(*args, **kwargs)\r\n.../site-packages/s3fs/core.py:187: in __init__\r\n    self.s3 = self.connect()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <s3fs.core.S3FileSystem object at 0x1500a1310>, refresh = True\r\n\r\n    def connect(self, refresh=True):\r\n        \"\"\"\r\n        Establish S3 connection object.\r\n    \r\n        Parameters\r\n        ----------\r\n        refresh : bool\r\n            Whether to create new session/client, even if a previous one with\r\n            the same parameters already exists. If False (default), an\r\n            existing one will be used if possible\r\n        \"\"\"\r\n        if refresh is False:\r\n            # back compat: we store whole FS instance now\r\n            return self.s3\r\n        anon, key, secret, kwargs, ckwargs, token, ssl = (\r\n            self.anon, self.key, self.secret, self.kwargs,\r\n            self.client_kwargs, self.token, self.use_ssl)\r\n    \r\n        if not self.passed_in_session:\r\n>           self.session = botocore.session.Session(**self.kwargs)\r\nE           TypeError: Session.__init__() got an unexpected keyword argument 'hf'\r\n```\n\n### Steps to reproduce the bug\n\n1. Assuming a valid CSV file located at `s3://bucket/data.csv`\r\n2. Run the below code:\r\n```\r\nstorage_options = {\r\n    \"key\": \"...\",\r\n    \"secret\": \"...\",\r\n    \"client_kwargs\": {\r\n        \"endpoint_url\": \"...\",\r\n    }\r\n}\r\nload_dataset(\"csv\", data_files=\"s3://bucket/data.csv\", storage_options=storage_options)\r\n```\r\n\r\nEncountered in version `2.16.1` but also reproduced in `2.16.0` and `2.15.0`.\r\n\r\nNote: I encountered this in a unit test using a `moto` mock for S3, however since the error occurs before the session is instantiated, it should not be the issue.\n\n### Expected behavior\n\nNo exception is raised, the boto3 session is created successfully, and the CSV file is downloaded successfully and returned as a dataset.\r\n\r\n===\r\n\r\nAfter some research I found that `DownloadConfig` has a `__post_init__` method that always forces this value to be set in its `storage_options`, even though in case of an S3 location the storage options get passed on to the S3 Session which does not expect this parameter. I assume this parameter is needed when reading from the huggingface hub and should not be set in this context.\r\n\r\nUnfortunately there is nothing the user can do to work around it. Even if you manually do something like:\r\n```\r\ndownload_config = DownloadConfig()\r\ndel download_config.storage_options[\"hf\"]\r\nload_dataset(\"csv\", data_files=\"s3://bucket/data.csv\", download_config=download_config)\r\n```\r\nthe library will still reinsert this parameter when `download_config = self.download_config.copy()` in line 418 of `download_manager.py` (`DownloadManager.download`).\r\n\r\nTherefore `load_dataset` currently cannot be used to read a dataset in CSV format from an S3 location.\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: macOS-14.2.1-arm64-arm-64bit\r\n- Python version: 3.11.7\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\nUnexpected keyword argument 'hf' when downloading CSV dataset from S3<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`datasets` should load the JSON data in https://huggingface.co/datasets/facebook/natural_reasoning, not the PDF<end_of_turn>\n<start_of_turn>model\nPriotitize json<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFinally fix https://github.com/huggingface/datasets/issues/5537<end_of_turn>\n<start_of_turn>model\nDon't expand_info in HF glob<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.17.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`IterableDataset.from_generator` crashes when using BFloat16:\r\n\r\n```\r\n  File \"/usr/local/lib/python3.11/site-packages/datasets/utils/_dill.py\", line 169, in _save_torchTensor\r\n    args = (obj.detach().cpu().numpy(),)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n\r\nTypeError: Got unsupported ScalarType BFloat16\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nimport torch\r\nfrom datasets import IterableDataset\r\n\r\n\r\ndef demo(x):\r\n    yield {\"x\": x}\r\n\r\n\r\nx = torch.tensor([1.], dtype=torch.bfloat16)\r\n\r\ndataset = IterableDataset.from_generator(\r\n    demo,\r\n    gen_kwargs=dict(x=x),\r\n)\r\n\r\nexample = next(iter(dataset))\r\nprint(example)\r\n```\n\n### Expected behavior\n\nCode sample should print:\r\n```python\r\n{'x': tensor([1.], dtype=torch.bfloat16)}\r\n```\n\n### Environment info\n\n```\r\ndatasets==2.20.0\r\ntorch==2.2.2\r\n```<end_of_turn>\n<start_of_turn>model\nIterableDataset: Unsupported ScalarType BFloat16<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollow-up to https://github.com/huggingface/datasets/pull/6795. Useful for https://github.com/huggingface/dataset-viewer/issues/2742. cc @albertvillanova <end_of_turn>\n<start_of_turn>model\nAdd a doc page for the convert_to_parquet CLI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 4.3.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6039 <end_of_turn>\n<start_of_turn>model\nCheck if column names match in Parquet loader only when config `features` are specified<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently load_dataset() is forcing users to specify a subset. Example\r\n\r\n`from datasets import load_dataset\r\ndataset = load_dataset(\"m-a-p/COIG-CQIA\")`\r\n\r\n```---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[<ipython-input-10-c0cb49385da6>](https://localhost:8080/#) in <cell line: 2>()\r\n      1 from datasets import load_dataset\r\n----> 2 dataset = load_dataset(\"m-a-p/COIG-CQIA\")\r\n\r\n3 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _create_builder_config(self, config_name, custom_features, **config_kwargs)\r\n    582                     if not config_kwargs:\r\n    583                         example_of_usage = f\"load_dataset('{self.dataset_name}', '{self.BUILDER_CONFIGS[0].name}')\"\r\n--> 584                         raise ValueError(\r\n    585                             \"Config name is missing.\"\r\n    586                             f\"\\nPlease pick one among the available configs: {list(self.builder_configs.keys())}\"\r\n\r\nValueError: Config name is missing.\r\nPlease pick one among the available configs: ['chinese_traditional', 'coig_pc', 'exam', 'finance', 'douban', 'human_value', 'logi_qa', 'ruozhiba', 'segmentfault', 'wiki', 'wikihow', 'xhs', 'zhihu']\r\nExample of usage:\r\n\t`load_dataset('coig-cqia', 'chinese_traditional')`\r\n```\r\nThis means a dataset cannot contain all the subsets at the same time. I guess one workaround is to manually specify the subset files like in [here](https://huggingface.co/datasets/m-a-p/COIG-CQIA/discussions/1#658698b44bb41498f75c5622), which is clumsy.\r\n\r\n\r\n\n\n### Motivation\n\nIdeally, if not subset is specified, the API should just try to load all subsets. This makes it much easier to handle datasets w/ subsets.\n\n### Your contribution\n\nNot sure since I'm not familiar w/ the lib src.<end_of_turn>\n<start_of_turn>model\nload_dataset() should load all subsets, if no specific subset is specified<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, I want to use the rotten tomatoes dataset but for a task other than classification, but when I interleave the dataset, it throws ```'ValueError: Column label is not present in features.'```. It seems that the label_col must be there in the dataset for some reason? \r\n\r\nHere is the full stacktrace\r\n\r\n```\r\n  File \"/home/suryahari/Vornoi/tryage-handoff-other-datasets.py\", line 276, in create_dataloaders                                                                                                    \r\n    dataset = interleave_datasets(dsfold, stopping_strategy=\"all_exhausted\")                                                                                                                         \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/combine.py\", line 134, in interleave_datasets                                                                   \r\n    return _interleave_iterable_datasets(                                                                                                                                                            \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1833, in _interleave_iterable_datasets                                               \r\n    info = DatasetInfo.from_merge([d.info for d in datasets])                                                                                                                                        \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/info.py\", line 275, in from_merge                                                                               \r\n    dataset_infos = [dset_info.copy() for dset_info in dataset_infos if dset_info is not None]                                                                                                       \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/info.py\", line 275, in <listcomp>                                                                               \r\n    dataset_infos = [dset_info.copy() for dset_info in dataset_infos if dset_info is not None]                                                                                                       \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/info.py\", line 378, in copy                                                                                     \r\n    return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})                                                                                                                 \r\n  File \"<string>\", line 20, in __init__                                                                                                                                                              \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/info.py\", line 208, in __post_init__                                                                            \r\n    self.task_templates = [                                                                                                                                                                          \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/info.py\", line 209, in <listcomp>                                                                               \r\n    template.align_with_features(self.features) for template in (self.task_templates)                                                                                                                \r\n  File \"/home/suryahari/miniconda3/envs/vornoi/lib/python3.10/site-packages/datasets/tasks/text_classification.py\", line 20, in align_with_features                                                  \r\n    raise ValueError(f\"Column {self.label_column} is not present in features.\")                                                                                                                      \r\nValueError: Column label is not present in features.    \r\n```\n\n### Steps to reproduce the bug\n\nDelete the column `labels` from the `rotten_tomatoes` dataset. Try to interleave it with other datasets.\n\n### Expected behavior\n\nShould let me use the dataset with just the `text` field\n\n### Environment info\n\nlatest datasets library? I don't think this was an issue in earlier versions.<end_of_turn>\n<start_of_turn>model\nUsing a dataset for a use other than it was intended for.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThis happens with datasets-2.18.0; I downgraded the version to 2.14.6 fixing this temporarily.\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/xxx/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py\", line 2556, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/xxx/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py\", line 2228, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/xxx/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py\", line 1879, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/home/xxx/miniconda3/envs/py310/lib/python3.10/site-packages/datasets/load.py\", line 1831, in dataset_module_factory\r\n    can_load_config_from_parquet_export = \"DEFAULT_CONFIG_NAME\" not in f.read()\r\n  File \"/home/xxx/miniconda3/envs/py310/lib/python3.10/codecs.py\", line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Using Python3.10/3.11\r\n2. Install datasets-2.18.0\r\n3. test with \r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"codeparrot/apps\")\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nNormally it should manage to download and load the dataset without such error.\r\n\r\n### Environment info\r\n\r\nUbuntu, Python3.10/3.11<end_of_turn>\n<start_of_turn>model\nLoad codeparrot/apps raising UnicodeDecodeError in datasets-2.18.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen we execute code \n```\nfrequencies = (\n    train_df[\"condition\"]\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n)\nfrequencies.head()\n```\nanswer should be like this\n\ncondition      | frequency\nbirth control | 27655\ndepression    | 8023\nacne              | 5209\nanxiety          | 4991\npain               | 4744\n\nbut he is different\n\nfrequency      | count\nbirth control  | 27655\ndepression    | 8023\nacne              | 5209\nanxiety          | 4991\npain               | 4744\n\nthis is not correct, correct code\n```\nfrequencies = (\n    train_df[\"condition\"]\n    .value_counts()\n    .to_frame()\n    .reset_index()\n    .rename(columns={\"index\": \"condition\", \"count\": \"frequency\"})\n)\n````\n\n### Steps to reproduce the bug\n\n```\nfrequencies = (\ntrain_df[\"condition\"]\n.value_counts()\n.to_frame()\n.reset_index()\n.rename(columns={\"index\": \"condition\", \"condition\": \"frequency\"})\n)\nfrequencies.head()\n```\n\n### Expected behavior\n\n\ncondition      | frequency\nbirth control | 27655\ndepression    | 8023\nacne              | 5209\nanxiety          | 4991\npain               | 4744\n\n### Environment info\n\nGoogle Colab<end_of_turn>\n<start_of_turn>model\nError in code \"Time to slice and dice\" from course \"NLP Course\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nCurrently, distributed training with `IterableDataset` needs to pass fixed seed to shuffle to keep each node use the same seed to avoid overlapping.\r\nhttps://github.com/huggingface/datasets/blob/a7f8d9019e7cb104eac4106bdc6ec0292f0dc61a/src/datasets/iterable_dataset.py#L1174-L1177\r\n\r\nMy question is why not directly use `self._epoch` which is set by `set_epoch` as seed? It's almost the same across nodes.\r\nhttps://github.com/huggingface/datasets/blob/a7f8d9019e7cb104eac4106bdc6ec0292f0dc61a/src/datasets/iterable_dataset.py#L1790-L1801\r\n\r\nIf not using `self._epoch` as shuffling seed, what does this method do to prepare an epoch seeded generator?\r\nhttps://github.com/huggingface/datasets/blob/a7f8d9019e7cb104eac4106bdc6ec0292f0dc61a/src/datasets/iterable_dataset.py#L1206\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nAs mentioned above.\r\n\r\n### Expected behavior\r\n\r\nAs mentioned above.\r\n\r\n### Environment info\r\n\r\nNot related<end_of_turn>\n<start_of_turn>model\nWhy not use self._epoch as seed to shuffle in distributed training with IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n... to make working with object detection datasets easier. Currently, `Sequence(int_or_float, length=4)` can be used to represent this feature optimally (in the storage backend), so I only see this feature being useful if we make it work with the viewer. Also, bounding boxes usually come in 4 different formats (explained [here](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/)), so we need to decide which one to support (or maybe all of them).\r\n\r\ncc @NielsRogge @severo<end_of_turn>\n<start_of_turn>model\nAdd `BoundingBox` feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to do model pruning on sentence-transformers/all-mini-L6-v2 for the code-search-net/code_search_net dataset using INCTrainer class \nHowever I am getting below error\n\n```\nraise IndexError(f\"Invalid Key: {key is our of bounds for size {size}\")\nIndexError: Invalid key: 1840208 is out of bounds for size 0\n```\n\n### Steps to reproduce the bug\n\nModel pruning on the above dataset using the below guide \n\nhttps://huggingface.co/docs/optimum/en/intel/neural_compressor/optimization#pruning\n### Expected behavior\n\nThe modsl should be successfully pruned \n\n### Environment info\n\nTorch version: 2.4.1\nPython version: 3.8.10<end_of_turn>\n<start_of_turn>model\nIndex Error Invalid Ket is out of bounds for size 0 for code-search-net/code_search_net dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nGiven that Arrow is the preferred backend for a Dataset, what is a user supposed to do if they want concurrent reads, concurrent writes AND on-disk compression for a larger dataset?\n\nParquet would be the obvious answer except that there is no native support for writing sharded, parquet datasets concurrently [[1](https://github.com/huggingface/datasets/issues/7047)].\n\nAm I missing something?  \n\nAnd if so, why is this not the standard/default way that `Dataset`'s work as they do in Xarray, Ray Data, Composer, etc.?<end_of_turn>\n<start_of_turn>model\nWhat is the canonical way to compress a Dataset?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nIn  #3627 and  #5543, you tried to fix the host URL in The Pile datasets. But both URLs are not working now:\r\n\r\n`HTTPError: 404 Client Error: Not Found for URL: https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst`\r\nAnd\r\n`ConnectTimeout: HTTPSConnectionPool(host='mystic.the-eye.eu', port=443): Max retries exceeded with url: /public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst (Caused by ConnectTimeoutError(, 'Connection to mystic.the-eye.eu timed out. (connect timeout=10.0)'))`\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\n# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\r\ndata_files = \"https://mystic.the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\r\npubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\r\npubmed_dataset\r\n```\r\n\r\nResult:\r\n`ConnectTimeout: HTTPSConnectionPool(host='mystic.the-eye.eu', port=443): Max retries exceeded with url: /public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst (Caused by ConnectTimeoutError(, 'Connection to mystic.the-eye.eu timed out. (connect timeout=10.0)'))`\r\n\r\nAnd\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\n# This takes a few minutes to run, so go grab a tea or coffee while you wait :)\r\ndata_files = \"https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst\"\r\npubmed_dataset = load_dataset(\"json\", data_files=data_files, split=\"train\")\r\npubmed_dataset\r\n```\r\n\r\nResult:\r\n`HTTPError: 404 Client Error: Not Found for URL: https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst`\r\n\r\n### Expected behavior\r\n\r\nDownloading as normal.\r\n\r\n### Environment info\r\n\r\nEnvironment info\r\n\r\n    `datasets` version: 2.9.0\r\n    Platform: Windows\r\n    Python version: 3.9.13\r\n<end_of_turn>\n<start_of_turn>model\nFix host URL in The Pile datasets <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix tensorflow min version depending on Python version.\r\n\r\nRelated to:\r\n- #6991<end_of_turn>\n<start_of_turn>model\nFix tensorflow min version depending on Python version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nHi!\r\nI am using `datasets` in library `xtuner` and am highly interested in the features introduced since v2.15.0.\r\n\r\nTo avoid installation from source in our pypi wheels, we are eagerly waiting for the new release. So, Does your team have a new release plan for v2.15.1 and could you please share it with us?\r\n\r\nThanks very much!\r\n\r\n\r\n### Motivation\r\n\r\n.\r\n\r\n### Your contribution\r\n\r\n.<end_of_turn>\n<start_of_turn>model\nNew version release request<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAttempting to save a dataset as a compressed csv file, the compression kwargs provided to `.to_csv()` that get piped to panda's `pandas.DataFrame.to_csv` do not have any effect - resulting in the dataset not getting compressed.\r\n\r\nA warning is raised if explicitly providing a `compression` kwarg, but no warnings are raised if relying on the defaults. This can lead to datasets secretly not getting compressed for users expecting the behaviour to match panda's `.to_csv()`, where the compression format is automatically inferred from the destination path suffix.\n\n### Steps to reproduce the bug\n\n```python\r\n# dataset is not compressed (but at least a warning is emitted)\r\n\r\nimport datasets\r\n\r\ndataset = datasets.load_dataset(\"rotten_tomatoes\", split=\"train\")\r\n\r\ndataset.to_csv(\"uncompressed.csv\")\r\nprint(os.path.getsize(\"uncompressed.csv\"))  # 1008607\r\n\r\ndataset.to_csv(\"compressed.csv.gz\",  compression={'method': 'gzip', 'compresslevel': 1, 'mtime': 1})\r\nprint(os.path.getsize(\"compressed.csv.gz\"))  # 1008607\r\n```\r\n\r\n```shell\r\n>>>\r\nRuntimeWarning: compression has no effect when passing a non-binary object as input.\r\n  csv_str = batch.to_pandas().to_csv(\r\n```\r\n\r\n```python\r\n# dataset is not compressed and no warnings are emitted\r\n\r\ndataset.to_csv(\"compressed.csv.gz\")\r\nprint(os.path.getsize(\"compressed.csv.gz\"))  # 1008607\r\n\r\n# compare with\r\ndataset.to_pandas().to_csv(\"pandas.csv.gz\")\r\nprint(os.path.getsize(\"pandas.csv.gz\"))  # 418561\r\n```\r\n\r\n---\r\n\r\nI think that this is because behind the scenes `pandas.DataFrame.to_csv` is always called with a buf-like `path_or_buf`, but users that are providing a path-like to `datasets.Dataset.to_csv` are likely not to expect / know that - leading to a mismatch in their understanding of the expected behaviour of the `compression` kwarg.\n\n### Expected behavior\n\nThe dataset to be saved as a compressed csv file when providing a `compression` kwarg, or when relying on the default `compression='infer'`\n\n### Environment info\n\n`datasets == 2.13.1`\r\n<end_of_turn>\n<start_of_turn>model\nCompression kwargs have no effect when saving datasets as csv<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\npin fsspec before it switches to glob.glob<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6291 <end_of_turn>\n<start_of_turn>model\nFix ArrayXD cast<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #5998 <end_of_turn>\n<start_of_turn>model\nAlign `column_names` type check with type hint in `sort`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf the user provides a `Features` type value to `datasets.Dataset` with members having `Array2D` with a value for `dtype`, it is not respected during `with_format(\"numpy\")` which should return a `np.array` with `dtype` that the user provided for `Array2D`. It seems for floats, it will be set to `float32` and for ints it will be set to `int64`\n\n### Steps to reproduce the bug\n\n```python\r\nimport numpy as np\r\nimport datasets\r\nfrom datasets import Dataset, Features, Array2D\r\n\r\nprint(f\"datasets version: {datasets.__version__}\")\r\n\r\ndata_info = {\r\n    \"arr_float\" : \"float64\",\r\n    \"arr_int\" : \"int32\"\r\n}\r\n\r\nsample = {key : [np.zeros([4, 5], dtype=dtype)] for key, dtype in data_info.items()}\r\n\r\nfeatures = {key : Array2D(shape=(None, 5), dtype=dtype) for key, dtype in data_info.items()}\r\nfeatures = Features(features)\r\n\r\ndataset = Dataset.from_dict(sample, features=features)\r\n\r\nds = dataset.with_format(\"numpy\")\r\nfor key in features:\r\n    print(f\"{key} feature dtype: \", ds.features[key].dtype)\r\n    print(f\"{key} dtype:\", ds[key].dtype)\r\n```\r\n\r\nOutput:\r\n```bash\r\ndatasets version: 3.0.2\r\narr_float feature dtype:  float64\r\narr_float dtype: float32\r\narr_int feature dtype:  int32\r\narr_int dtype: int64\r\n```\n\n### Expected behavior\n\nIt should return a `np.array` with `dtype` that the user provided for the corresponding member in the `Features` type value\n\n### Environment info\n\n- `datasets` version: 3.0.2\r\n- Platform: Linux-6.11.5-arch1-1-x86_64-with-glibc2.40\r\n- Python version: 3.12.7\r\n- `huggingface_hub` version: 0.26.1\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\nmismatch for datatypes when providing `Features` with `Array2D` and user specified `dtype` and using with_format(\"numpy\")<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nIn the sort method\uff0chere's a piece of code\r\n\r\n```python\r\n# column_names: Union[str, Sequence_[str]]\r\n\r\n# Check proper format of and for duplicates in column_names\r\nif not isinstance(column_names, list):\r\n    column_names = [column_names]\r\n```\r\n\r\nI get an error when I pass in a tuple based on the column_names type annotation, it will raise an errror.As in the example below, while the type annotation implies that a tuple can be passed.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('glue', 'ax')['test']\r\ndataset.sort(column_names=('premise', 'hypothesis'))\r\n# Raise ValueError: Column '('premise', 'hypothesis')' not found in the dataset.\r\n```\r\n\r\nOf course, after I modified the tuple into a list, everything worked fine\r\n\r\nChange the code to the following so there will be no problem\r\n\r\n```python\r\n# Check proper format of and for duplicates in column_names\r\nif not isinstance(column_names, list):\r\n    if isinstance(column_names, str):\r\n        column_names = [column_names]\r\n    else:\r\n        column_names = list(column_names)\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('glue', 'ax')['test']\r\ndataset.sort(column_names=('premise', 'hypothesis'))\r\n# Raise ValueError: Column '('premise', 'hypothesis')' not found in the dataset.\r\n```\r\n\r\n### Expected behavior\r\n\r\nPassing tuple into column_names should be equivalent to passing list\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: macOS-13.1-arm64-arm-64bit\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2<end_of_turn>\n<start_of_turn>model\nThe current implementation has a potential bug in the sort method<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn the doc, I found the following example: https://github.com/huggingface/datasets/blob/611f5a592359ebac6f858f515c776aa7d99838b2/docs/source/stream.mdx?plain=1#L65-L78\n\nI am confused, \n1. If we have already loaded the dataset, why doing `to_iterable_dataset`?  Does it go through the dataset faster than map-style dataset?\n2. `load_dataset(streaming=True)` is useful for huge dataset, but the speed is slow. How to make it comparable to `to_iterable_dataset` without loading the whole dataset into RAM?<end_of_turn>\n<start_of_turn>model\nQuestion about iterable dataset and streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn `src/datasets/load.py`, we can use unpacking rather than concatenating two lists for improved time and memory performance. It\u2019s a small improvement in absolute terms, but a consistent and measurable one:\r\n\r\n```diff\r\n- ALL_ALLOWED_EXTENSIONS = list(_EXTENSION_TO_MODULE.keys()) + [\".zip\"]\r\n+ ALL_ALLOWED_EXTENSIONS = [*_EXTENSION_TO_MODULE.keys(), \".zip\"]\r\n```\r\n\r\nBenchmarking shows approximately 32.3% time improvement and 30.6% memory improvement.\r\n\r\nExample benchmarking script:\r\n```python\r\n#!/usr/bin/env python3\r\n\"\"\"\r\nBenchmark script to test performance of list(_EXTENSION_TO_MODULE.keys()) vs [*_EXTENSION_TO_MODULE.keys()]\r\n\"\"\"\r\nimport time\r\nimport tracemalloc\r\nfrom statistics import mean, stdev\r\n\r\n# Simulate _EXTENSION_TO_MODULE - based on actual size from datasets\r\n_EXTENSION_TO_MODULE = {\r\n    f\".ext{i}\": f\"module{i}\" for i in range(20)  # Realistic size\r\n}\r\n\r\ndef method_old():\r\n    \"\"\"Current implementation using list()\"\"\"\r\n    return list(_EXTENSION_TO_MODULE.keys()) + [\".zip\"]\r\n\r\ndef method_new():\r\n    \"\"\"Proposed implementation using unpacking\"\"\"\r\n    return [*_EXTENSION_TO_MODULE.keys(), \".zip\"]\r\n\r\ndef benchmark_time(func, iterations=100000):\r\n    \"\"\"Benchmark execution time\"\"\"\r\n    times = []\r\n    for _ in range(10):  # Multiple runs for accuracy\r\n        start = time.perf_counter()\r\n        for _ in range(iterations):\r\n            func()\r\n        end = time.perf_counter()\r\n        times.append((end - start) / iterations * 1_000_000)  # microseconds\r\n    \r\n    return mean(times), stdev(times)\r\n\r\ndef benchmark_memory(func):\r\n    \"\"\"Benchmark peak memory usage\"\"\"\r\n    tracemalloc.start()\r\n    func()\r\n    current, peak = tracemalloc.get_traced_memory()\r\n    tracemalloc.stop()\r\n    return peak\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Benchmarking list() vs unpacking performance...\\n\")\r\n    \r\n    # Time benchmarks\r\n    old_time, old_std = benchmark_time(method_old)\r\n    new_time, new_std = benchmark_time(method_new)\r\n    \r\n    print(f\"Time Performance (\u00b5s per operation):\")\r\n    print(f\"  list() approach:     {old_time:.3f} \u00b1 {old_std:.3f}\")\r\n    print(f\"  unpacking approach:  {new_time:.3f} \u00b1 {new_std:.3f}\")\r\n    print(f\"  Improvement:         {((old_time - new_time) / old_time * 100):.1f}% faster\")\r\n    \r\n    # Memory benchmarks\r\n    old_mem = benchmark_memory(method_old)\r\n    new_mem = benchmark_memory(method_new)\r\n    \r\n    print(f\"\\nMemory Usage (bytes):\")\r\n    print(f\"  list() approach:     {old_mem}\")\r\n    print(f\"  unpacking approach:  {new_mem}\")\r\n    print(f\"  Reduction:           {old_mem - new_mem} bytes ({((old_mem - new_mem) / old_mem * 100):.1f}% less)\")\r\n    \r\n    # Verify identical results\r\n    assert method_old() == method_new(), \"Results should be identical!\"\r\n    print(f\"\\n\u2713 Both methods produce identical results\")\r\n```\r\n\r\nResults:\r\n```\r\nBenchmarking list() vs unpacking performance...\r\n\r\nTime Performance (\u00b5s per operation):\r\n  list() approach:     0.213 \u00b1 0.020\r\n  unpacking approach:  0.144 \u00b1 0.002\r\n  Improvement:         32.3% faster\r\n\r\nMemory Usage (bytes):\r\n  list() approach:     392\r\n  unpacking approach:  272\r\n  Reduction:           120 bytes (30.6% less)\r\n\r\n\u2713 Both methods produce identical results\r\n```<end_of_turn>\n<start_of_turn>model\nRefactor: use unpacking in load.py for time and memory improvement<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nYep its not [there](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/utils/__init__.py) anymore.\r\n\r\n```text\r\n+ python /home/trainer/sft_train.py --model_name cognitivecomputations/dolphin-2.2.1-mistral-7b --dataset_name wasertech/OneOS --load_in_4bit --use_peft --batch_size 4 --num_train_epochs 1 --learning_rate 1.41e-5 --gradient_accumulation_steps 8 --seq_length 4096 --output_dir output --log_with wandb\r\nTraceback (most recent call last):\r\n  File \"/home/trainer/sft_train.py\", line 22, in <module>\r\n    from datasets import load_dataset\r\n  File \"/home/trainer/llm-train/lib/python3.8/site-packages/datasets/__init__.py\", line 22, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"/home/trainer/llm-train/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 66, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"/home/trainer/llm-train/lib/python3.8/site-packages/datasets/arrow_reader.py\", line 30, in <module>\r\n    from .download.download_config import DownloadConfig\r\n  File \"/home/trainer/llm-train/lib/python3.8/site-packages/datasets/download/__init__.py\", line 9, in <module>\r\n    from .download_manager import DownloadManager, DownloadMode\r\n  File \"/home/trainer/llm-train/lib/python3.8/site-packages/datasets/download/download_manager.py\", line 31, in <module>\r\n    from ..utils import tqdm as hf_tqdm\r\n  File \"/home/trainer/llm-train/lib/python3.8/site-packages/datasets/utils/__init__.py\", line 19, in <module>\r\n    from .info_utils import VerificationMode\r\n  File \"/home/trainer/llm-train/lib/python3.8/site-packages/datasets/utils/info_utils.py\", line 5, in <module>\r\n    from huggingface_hub.utils import insecure_hashlib\r\nImportError: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (/home/trainer/llm-train/lib/python3.8/site-packages/huggingface_hub/utils/__init__.py)\r\n```\n\n### Steps to reproduce the bug\n\nUsing `datasets==2.16.1` and `huggingface_hub== 0.17.3`, load a dataset with `load_dataset`.\n\n### Expected behavior\n\nThe dataset should be (downloaded - if needed - and) returned.\n\n### Environment info\n\n```text\r\ntrainer@a311ae86939e:/mnt$ pip show datasets\r\nName: datasets\r\nVersion: 2.16.1\r\nSummary: HuggingFace community-driven open-source library of datasets\r\nHome-page: https://github.com/huggingface/datasets\r\nAuthor: HuggingFace Inc.\r\nAuthor-email: thomas@huggingface.co\r\nLicense: Apache 2.0\r\nLocation: /home/trainer/llm-train/lib/python3.8/site-packages\r\nRequires: packaging, pyyaml, multiprocess, pyarrow-hotfix, pandas, pyarrow, xxhash, dill, numpy, aiohttp, tqdm, fsspec, requests, filelock, huggingface-hub\r\nRequired-by: trl, lm-eval, evaluate\r\n\r\ntrainer@a311ae86939e:/mnt$ pip show huggingface_hub\r\nName: huggingface-hub\r\nVersion: 0.17.3\r\nSummary: Client library to download and publish models, datasets and other repos on the huggingface.co hub\r\nHome-page: https://github.com/huggingface/huggingface_hub\r\nAuthor: Hugging Face, Inc.\r\nAuthor-email: julien@huggingface.co\r\nLicense: Apache\r\nLocation: /home/trainer/llm-train/lib/python3.8/site-packages\r\nRequires: requests, pyyaml, packaging, typing-extensions, tqdm, filelock, fsspec\r\nRequired-by: transformers, tokenizers, peft, evaluate, datasets, accelerate\r\n\r\ntrainer@a311ae86939e:/mnt$ huggingface-cli env\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- huggingface_hub version: 0.17.3\r\n- Platform: Linux-6.5.13-7-MANJARO-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: /home/trainer/.cache/huggingface/token\r\n- Has saved token ?: True\r\n- Who am I ?: wasertech\r\n- Configured git credential helpers: \r\n- FastAI: N/A\r\n- Tensorflow: N/A\r\n- Torch: 2.1.2\r\n- Jinja2: 3.1.2\r\n- Graphviz: N/A\r\n- Pydot: N/A\r\n- Pillow: 10.2.0\r\n- hf_transfer: N/A\r\n- gradio: N/A\r\n- tensorboard: N/A\r\n- numpy: 1.24.4\r\n- pydantic: N/A\r\n- aiohttp: 3.9.1\r\n- ENDPOINT: https://huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: /home/trainer/.cache/huggingface/hub\r\n- HUGGINGFACE_ASSETS_CACHE: /home/trainer/.cache/huggingface/assets\r\n- HF_TOKEN_PATH: /home/trainer/.cache/huggingface/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\r\n\r\n\r\n```<end_of_turn>\n<start_of_turn>model\n`ImportError`: cannot import name 'insecure_hashlib' from 'huggingface_hub.utils' (.../huggingface_hub/utils/__init__.py)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd `torchdata` as a regular test dependency.\r\n\r\nNote that previously, `torchdata` was installed from their repo and current main branch (0.10.0.dev) requires Python>=3.9.\r\n\r\nAlso note they made a recent release: 0.8.0 on Jul 31, 2024.\r\n\r\nFix #7171.<end_of_turn>\n<start_of_turn>model\nAdd torchdata as a regular test dependency<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n@lhoestq \r\nProposed fix for #7423. Added a couple simple tests as requested. I had some test failures related to Java and pyspark even when installing with dev but these don't seem to be related to the changes here and fail for me even on clean main.\r\n\r\nThe typeerror raised when using the wrong type is: \"Wrong key type: '{key}' of type '{type(key)}'. Expected one of int, slice, range, str or Iterable.\" I think that is fine. But I could modify the int part to something more generic (although I'm not sure what) if wanted.<end_of_turn>\n<start_of_turn>model\nAllow dataset row indexing with np.int types (#7423)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://huggingface.co/datasets/allenai/objaverse for example<end_of_turn>\n<start_of_turn>model\nAdd support for 3D datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen calling load_dataset, raise error\r\n```\r\nTraceback (most recent call last):                                                                             \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py\", line 1694, in _pre\r\npare_split_single                                                                                              \r\n    writer.write(example, key)                                                                                 \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 490, in \r\nwrite                                                                                                          \r\n    self.write_examples_on_file()                                                                              \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 448, in \r\nwrite_examples_on_file                                                                                         \r\n    self.write_batch(batch_examples=batch_examples)                                                            \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 559, in \r\nwrite_batch \r\n    self.write_table(pa_table, writer_batch_size)      \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 571, in \r\nwrite_table                \r\n    pa_table = pa_table.combine_chunks()               \r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^               \r\n  File \"pyarrow/table.pxi\", line 3439, in pyarrow.lib.Table.combine_chunks                                     \r\n  File \"pyarrow/error.pxi\", line 144, in pyarrow.lib.pyarrow_internal_check_status                             \r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status                                              \r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\nThe above exception was the direct cause of the following exception:                                           \r\n                                                                                                               \r\nTraceback (most recent call last):\r\ndataset = load_dataset(                            \r\n              ^^^^^^^^^^^^^                            \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/load.py\", line 2133, in load_da\r\ntaset                      \r\n    builder_instance.download_and_prepare(             \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py\", line 954, in downl\r\noad_and_prepare            \r\n    self._download_and_prepare(                        \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py\", line 1717, in _dow\r\nnload_and_prepare          \r\n    super()._download_and_prepare(                     \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py\", line 1049, in _dow\r\nnload_and_prepare          \r\n    self._prepare_split(split_generator, **prepare_split_kwargs)                                               \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py\", line 1555, in _pre\r\npare_split                 \r\n    for job_id, done, content in self._prepare_split_single(                                                   \r\n  File \"/home/aihao/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py\", line 1712, in _pre\r\npare_split_single          \r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e                      \r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset                        \r\nSetting num_proc from 8 back to 1 for the train split to disable multiprocessing as it only contains one shard.\r\n09/04/2023 12:02:04 - WARNING - datasets.builder - Setting num_proc from 8 back to 1 for the train split to dis\r\nable multiprocessing as it only contains one shard.\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nCall load_dataset with the large image as feature\r\n\r\n### Expected behavior\r\n\r\nno error\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.3\r\n- Platform: Linux-6.2.0-31-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nWhen calling load_dataset, raise error: pyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi\nI am having an issue when loading a dataset.\nI have about 200 json files each about 1GB (total about 215GB). each row has a few features which are a list of ints.\nI am trying to load the dataset using `load_dataset`.\nThe dataset is about 1.5M samples\nI use `num_proc=32` and a node with 378GB of memory. \nAbout a third of the way there I get an OOM.\nI also saw an old bug with a similar issue, which says to set `writer_batch_size`. I tried to lower it to 10, but it still crashed.\nI also tried to lower the `num_proc` to 16 and even 8, but still the same issue.\n\n### Steps to reproduce the bug\n\n`dataset = load_dataset(\"json\", data_dir=data_config.train_path, num_proc=data_config.num_proc, writer_batch_size=50)[\"train\"]`\n\n\n### Expected behavior\n\nLoading a dataset with more than 100GB to spare should not cause an OOM error.\nmaybe i am missing something but I would love some help.\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-6.6.20-aufs-1-x86_64-with-glibc2.36\n- Python version: 3.11.2\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nDataset uses excessive memory when loading files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemoves the warning about requiring to write a dataset loading script to define multiple configurations, as the README YAML can be used instead (for simple cases). Also, deletes the section about using the `BatchSampler` in `torch<=1.12.1` to speed up loading, as `torch 1.12.1` is over a year old (and `torch 2.0` has been out for a while).<end_of_turn>\n<start_of_turn>model\nMisc doc improvements<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRight now we can only download multiple separate archives or a single file archive, but not multipart archives, such as those produced by `tar --multi-volume`. This PR allows for downloading and extraction of archives split into multiple parts.\r\n\r\nWith the new `multi_part` field of the `DownloadConfig` set, the downloader will first retrieve all the files and attempt to concatenate them before starting extraction. This will obviously fail if files retrieved are actually multiple separate archives, so the option is set to `False` by default.\r\n\r\nTests and docs incoming. <end_of_turn>\n<start_of_turn>model\nAdding option for multipart achive download<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\npyarrow \u5c1d\u8bd5\u4e86\u82e5\u5e72\u4e2a\u7248\u672c\u90fd\u4e0d\u53ef\u4ee5<end_of_turn>\n<start_of_turn>model\nAttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #6466 \r\n\r\nThe idea is to do a recursive check for structs. PyArrow handles it well enough.\r\n\r\n\r\nFor a demo you can do:\r\n\r\n```python\r\nfrom datasets import Dataset, concatenate_datasets\r\n\r\nds = Dataset.from_dict({'speaker': [{'name': 'Ben', 'email': None}]})\r\nds2 = Dataset.from_dict({'speaker': [{'name': 'Fred', 'email': 'abc@aol.com'}]})\r\nprint(concatenate_datasets([ds, ds2]).features)\r\nprint(concatenate_datasets([ds, ds2]).to_dict())\r\n```\r\n\r\nNow both the features and the rows are fixed.\r\n\r\nI note that Sequence suffers from the same problem, so I can fix that in a future PR once this one is merged.<end_of_turn>\n<start_of_turn>model\nAllow concatenation of datasets with mixed structs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7000<end_of_turn>\n<start_of_turn>model\nFix dump of bfloat16 torch tensor<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.3.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe current example isn't working because it can't find `labels` inside the Dataset object. So I've added an extra step to the process. Tested and working in Colab.<end_of_turn>\n<start_of_turn>model\nFix use_dataset.mdx<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ngetting this error:\r\n```\r\n/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\r\n  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\r\n```\r\n\r\nSince datasets supports arrow greater than 8.0.0, we need to handle both cases.\r\n\r\n[Arrow v14 docs](https://arrow.apache.org/docs/python/generated/pyarrow.concat_tables.html)\r\n[Arrow v13 docs](https://arrow.apache.org/docs/13.0/python/generated/pyarrow.concat_tables.html)<end_of_turn>\n<start_of_turn>model\nhandle future deprecation argument<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\n--------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-6-80c6086794e8>](https://localhost:8080/#) in <cell line: 10>()\r\n      8 \t\t  report_to=\"wandb\")\r\n      9 \r\n---> 10 trainer = Trainer(\r\n     11     model=model,\r\n     12     args=training_args,\r\n\r\n1 frames\r\n[/usr/local/lib/python3.10/dist-packages/transformers/trainer.py](https://localhost:8080/#) in _move_model_to_device(self, model, device)\r\n    688 \r\n    689     def _move_model_to_device(self, model, device):\r\n--> 690         model = model.to(device)\r\n    691         # Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\r\n    692         if self.args.parallel_mode == ParallelMode.TPU and hasattr(model, \"tie_weights\"):\r\n\r\nAttributeError: 'str' object has no attribute 'to'\r\n```\n\n### Steps to reproduce the bug\n\nhere is the notebook:\r\n```\r\nhttps://colab.research.google.com/drive/10JDBNsLlYrQdnI2FWfDK3F5M8wvVUDXG?usp=sharing\r\n\r\n```\r\n\n\n### Expected behavior\n\nrun the Training \n\n### Environment info\n\nColab Notebook , T4 <end_of_turn>\n<start_of_turn>model\nAttributeError: 'str' object has no attribute 'to'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a large dataset that I shared into 1024 shards and save on the disk during pre-processing. During training, I load the dataset using load_from_disk() and convert it into an iterable dataset, shuffle it and split the shards to different DDP nodes using the recommended method.\n\nHowever, when the training is resumed mid-epoch, I get thousands of identical warning messages:\n\n```\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\n```\n\n\n\n### Steps to reproduce the bug\n\n1. Run a multi-node training job using the following python script and interrupt the training after a few seconds to save a mid-epoch checkpoint.\n\n```python\n#!/usr/bin/env python\nimport os\nimport time\nfrom typing import Dict, List\nimport torch\nimport lightning as pl\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\nfrom datasets.distributed import split_dataset_by_node\nimport datasets\nfrom transformers import AutoTokenizer\nfrom more_itertools import flatten, chunked\nfrom torchdata.stateful_dataloader import StatefulDataLoader\nfrom lightning.pytorch.callbacks.on_exception_checkpoint import (\n    OnExceptionCheckpoint,\n)\n\ndatasets.logging.set_verbosity_debug()\n\n\ndef dummy_generator():\n    # Generate 60 examples: integers from $0$ to $59$\n    # 64 sequences of different lengths\n    dataset = [\n        list(range(3, 10)),\n        list(range(10, 15)),\n        list(range(15, 21)),\n        list(range(21, 27)),\n        list(range(27, 31)),\n        list(range(31, 36)),\n        list(range(36, 45)),\n        list(range(45, 50)),\n    ]\n    for i in range(8):\n        for j, ids in enumerate(dataset):\n            yield {\"token_ids\": [idx + i * 50 for idx in ids]}\n\n\ndef group_texts(\n    examples: Dict[str, List[List[int]]],\n    block_size: int,\n    eos_token_id: int,\n    bos_token_id: int,\n    pad_token_id: int,\n) -> Dict[str, List[List[int]]]:\n    real_block_size = block_size - 2  # make space for bos and eos\n    # colapse the sequences into a single list of tokens and then create blocks of real_block_size\n    input_ids = []\n    attention_mask = []\n    for block in chunked(flatten(examples[\"token_ids\"]), real_block_size):\n        s = [bos_token_id] + list(block) + [eos_token_id]\n        ls = len(s)\n        attn = [True] * ls\n        s += [pad_token_id] * (block_size - ls)\n        attn += [False] * (block_size - ls)\n        input_ids.append(s)\n        attention_mask.append(attn)\n\n    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n\n\ndef collate_fn(batch):\n    return {\n        \"input_ids\": torch.tensor(\n            [item[\"input_ids\"] for item in batch], dtype=torch.long\n        ),\n        \"attention_mask\": torch.tensor(\n            [item[\"attention_mask\"] for item in batch], dtype=torch.long\n        ),\n    }\n\n\nclass DummyModule(pl.LightningModule):\n\n    def __init__(self):\n        super().__init__()\n        # A dummy linear layer (not used for actual computation)\n        self.layer = torch.nn.Linear(1, 1)\n        self.ds = None\n        self.prepare_data_per_node = False\n\n    def on_train_start(self):\n        # This hook is called once training begins on each process.\n        print(f\"[Rank {self.global_rank}] Training started.\", flush=True)\n        self.data_file = open(f\"data_{self.global_rank}.txt\", \"w\")\n\n    def on_train_end(self):\n        self.data_file.close()\n\n    def training_step(self, batch, batch_idx):\n        # Print batch information to verify data loading.\n        time.sleep(5)\n        # print(\"batch\", batch, flush=True)\n        print(\n            f\"\\n[Rank {self.global_rank}] Training step, epoch {self.trainer.current_epoch}, batch {batch_idx}: {batch['input_ids']}\",\n            flush=True,\n        )\n        self.data_file.write(\n            f\"[Rank {self.global_rank}] Training step, epoch {self.trainer.current_epoch}, batch {batch_idx}: {batch['input_ids']}\\n\"\n        )\n        # Compute a dummy loss (here, simply a constant tensor)\n        loss = torch.tensor(0.0, requires_grad=True)\n        return loss\n\n    def on_train_epoch_start(self):\n        epoch = self.trainer.current_epoch\n        print(\n            f\"[Rank {self.global_rank}] Training epoch {epoch} started.\",\n            flush=True,\n        )\n        self.data_file.write(\n            f\"[Rank {self.global_rank}] Training epoch {epoch} started.\\n\"\n        )\n\n    def configure_optimizers(self):\n        # Return a dummy optimizer.\n        return torch.optim.SGD(self.parameters(), lr=0.001)\n\n\nclass DM(pl.LightningDataModule):\n    def __init__(self):\n        super().__init__()\n        self.ds = None\n        self.prepare_data_per_node = False\n\n    def set_epoch(self, epoch: int):\n        self.ds.set_epoch(epoch)\n\n    def prepare_data(self):\n        # download the dataset\n        dataset = Dataset.from_generator(dummy_generator)\n        # save the dataset\n        dataset.save_to_disk(\"dataset\", num_shards=4)\n\n    def setup(self, stage: str):\n        # load the dataset\n        ds = datasets.load_from_disk(\"dataset\").to_iterable_dataset(\n            num_shards=4\n        )\n        ds = ds.map(\n            group_texts,\n            batched=True,\n            batch_size=5,\n            fn_kwargs={\n                \"block_size\": 5,\n                \"eos_token_id\": 1,\n                \"bos_token_id\": 0,\n                \"pad_token_id\": 2,\n            },\n            remove_columns=[\"token_ids\"],\n        ).shuffle(seed=42, buffer_size=8)\n        ds = split_dataset_by_node(\n            ds,\n            rank=self.trainer.global_rank,\n            world_size=self.trainer.world_size,\n        )\n        self.ds = ds\n\n    def train_dataloader(self):\n        print(\n            f\"[Rank {self.trainer.global_rank}] Preparing train_dataloader...\",\n            flush=True,\n        )\n        rank = self.trainer.global_rank\n        print(\n            f\"[Rank {rank}] Global rank: {self.trainer.global_rank}\",\n            flush=True,\n        )\n        world_size = self.trainer.world_size\n        print(f\"[Rank {rank}] World size: {world_size}\", flush=True)\n        return StatefulDataLoader(\n            self.ds,\n            batch_size=2,\n            num_workers=2,\n            collate_fn=collate_fn,\n            drop_last=True,\n            persistent_workers=True,\n        )\n\n\nif __name__ == \"__main__\":\n    print(\"Starting Lightning training\", flush=True)\n    # Optionally, print some SLURM environment info for debugging.\n    print(f\"SLURM_NNODES: {os.environ.get('SLURM_NNODES', '1')}\", flush=True)\n\n    # Determine the number of nodes from SLURM (defaulting to 1 if not set)\n    num_nodes = int(os.environ.get(\"SLURM_NNODES\", \"1\"))\n    model = DummyModule()\n    dm = DM()\n    on_exception = OnExceptionCheckpoint(\n        dirpath=\"checkpoints\",\n        filename=\"on_exception\",\n    )\n\n    # Configure the Trainer to use distributed data parallel (DDP).\n    trainer = pl.Trainer(\n        accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n        devices=1,\n        strategy=(\n            \"ddp\" if num_nodes > 1 else \"auto\"\n        ),  # Use DDP strategy for multi-node training.\n        num_nodes=num_nodes,\n        max_epochs=2,\n        logger=False,\n        enable_checkpointing=True,\n        num_sanity_val_steps=0,\n        enable_progress_bar=False,\n        callbacks=[on_exception],\n    )\n\n    # resume (uncomment to resume)\n    # trainer.fit(model, datamodule=dm, ckpt_path=\"checkpoints/on_exception.ckpt\")\n    # train\n\n    trainer.fit(model, datamodule=dm)\n```\n\n\n\n```bash\n#!/bin/bash\n#SBATCH --job-name=pl_ddp_test\n#SBATCH --nodes=2                     # Adjust number of nodes as needed\n#SBATCH --ntasks-per-node=1           # One GPU (process) per node\n#SBATCH --cpus-per-task=3             # At least as many dataloader workers as required\n#SBATCH --gres=gpu:1                  # Request one GPU per node\n#SBATCH --time=00:10:00               # Job runtime (adjust as needed)\n#SBATCH --partition=gpu-preempt               # Partition or queue name\n#SBATCH -o script.out\n\n# Disable Python output buffering.\nexport PYTHONUNBUFFERED=1\n\necho \"SLURM job starting on $(date)\"\necho \"Running on nodes: $SLURM_NODELIST\"\necho \"Current directory: $(pwd)\"\nls -l\n\n# Launch the script using srun so that each process starts the Lightning module.\nsrun script.py\n```\n2. Uncomment the \"resume\" line (second to last) and comment the original `trainer.fit` call (last line). \nIt will produce the following log.\n```\n[Rank 0] Preparing train_dataloader...\n[Rank 0] Global rank: 0\n[Rank 0] World size: 2\n[Rank 1] Preparing train_dataloader...\n[Rank 1] Global rank: 1\n[Rank 1] World size: 2\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nAssigning 2 shards (or data sources) of the dataset to each node.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#0 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#0 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#0 dataloader worker#1, ': Finished iterating over 1/1 shards.\nnode#0 dataloader worker#0, ': Finished iterating over 1/1 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\n[Rank 0] Training started.\n[Rank 0] Training epoch 0 started.\n[Rank 0] Training epoch 1 started.\nAssigning 2 shards (or data sources) of the dataset to each node.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#0 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nnode#0 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#1 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#1 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#0 dataloader worker#1, ': Finished iterating over 1/1 shards.\nnode#0 dataloader worker#0, ': Finished iterating over 1/1 shards.\n`Trainer.fit` stopped: `max_epochs=2` reached.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#1 dataloader worker#1, ': Finished iterating over 1/1 shards.\nnode#1 dataloader worker#0, ': Finished iterating over 1/1 shards.\n[Rank 1] Training started.\n[Rank 1] Training epoch 0 started.\n[Rank 1] Training epoch 1 started.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nnode#1 dataloader worker#1, ': Starting to iterate over 1/2 shards.\nnode#1 dataloader worker#0, ': Starting to iterate over 1/2 shards.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nLoading a state dict of a shuffle buffer of a dataset without the buffer content.The shuffle buffer will be refilled before starting to yield new examples.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nSet __getitem__(key) output type to arrow for no columns  (when key is int or slice) and don't output other (un-formatted) columns.\nnode#1 dataloader worker#0, ': Finished iterating over 1/1 shards.\nnode#1 dataloader worker#1, ': Finished iterating over 1/1 shards.\n```\n\nI'm also attaching the relevant state_dict to make sure that the state is being checkpointed as expected.\n\n```\n{'_iterator_finished': True,\n                                     '_snapshot': {'_last_yielded_worker_id': 1,\n                                                   '_main_snapshot': {'_IterableDataset_len_called': None,\n                                                                      '_base_seed': 3992758080362545099,\n                                                                      '_index_sampler_state': {'samples_yielded': 64},\n                                                                      '_num_workers': 2,\n                                                                      '_sampler_iter_state': None,\n                                                                      '_sampler_iter_yielded': 32,\n                                                                      '_shared_seed': None},\n                                                   '_snapshot_step': 32,\n                                                   '_worker_snapshots': {'worker_0': {'dataset_state': {'ex_iterable': {'shard_example_idx': 0,\n                                                                                                                        'shard_idx': 1},\n                                                                                                        'num_examples_since_previous_state': 0,\n                                                                                                        'previous_state': {'shard_example_idx': 0,\n                                                                                                                           'shard_idx': 1},\n                                                                                                        'previous_state_example_idx': 33},\n                                                                                      'fetcher_state': {'dataset_iter_state': None,\n                                                                                                        'fetcher_ended': False},\n                                                                                      'worker_id': 0},\n                                                                         'worker_1': {'dataset_state': {'ex_iterable': {'shard_example_idx': 0,\n                                                                                                                        'shard_idx': 1},\n                                                                                                        'num_examples_since_previous_state': 0,\n                                                                                                        'previous_state': {'shard_example_idx': 0,\n                                                                                                                           'shard_idx': 1},\n                                                                                                        'previous_state_example_idx': 33},\n                                                                                      'fetcher_state': {'dataset_iter_state': None,\n                                                                                                        'fetcher_ended': False},\n                                                                                      'worker_id': 1}}},\n                                     '_steps_since_snapshot': 0}\n```\n\n### Expected behavior\n\nSince I'm following all the recommended steps, I don't expect to see any warning when resuming. Am I doing something wrong? Also, can someone explain why I'm seeing 20 identical messages in the log in this reproduction setting? I'm trying to understand why I see thousands of these messages with the actual dataset. \n\nOne more surprising thing I noticed in the logs is the change in a number of shards per worker. In the following messages, the denominator changes from 2 to 1.  \n\n```\nnode#1 dataloader worker#1, ': Starting to iterate over 1/2 shards.\n...\nnode#1 dataloader worker#1, ': Finished iterating over 1/1 shards.\n```\n\n### Environment info\n\npython: 3.11.10\ndatasets: 3.3.2\nlightning: 2.3.1\n\n<end_of_turn>\n<start_of_turn>model\nExcessive warnings when resuming an IterableDataset+buffered shuffle+DDP.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7630\r\n\r\n### Problem\r\nWhen calling `.map()` on an `IterableDataset`, resuming from a checkpoint skips a large number of samples. This is because `MappedExamplesIterable` did not implement `state_dict()` or `load_state_dict()`, so checkpointing was not properly delegated to the underlying iterable.\r\n\r\n### What This PR Does\r\nThis patch adds:\r\n```python\r\ndef state_dict(self):\r\n    return self.ex_iterable.state_dict()\r\n\r\ndef load_state_dict(self, state):\r\n    self.ex_iterable.load_state_dict(state)\r\n```\r\n\r\nto MappedExamplesIterable, so the wrapped base iterable's state can be saved and restored as expected.\r\n\r\nResult\r\nUsing .map() no longer causes sample skipping after checkpoint resume.\r\n\r\nLet me know if a dedicated test case is required \u2014 happy to add one!<end_of_turn>\n<start_of_turn>model\nfix(iterable): ensure MappedExamplesIterable supports state_dict for resume<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTesting `ds = load_dataset(\"audiofolder\", data_files=\"s3://datasets.huggingface.co/SpeechCommands/v0.01/v0.01_test.tar.gz\", storage_options={\"anon\": True})` and trying to fix the issues raised by `fsspec` ...\r\n\r\nTODO: fix\r\n```\r\n    self.session = aiobotocore.session.AioSession(**self.kwargs)\r\nTypeError: __init__() got an unexpected keyword argument 'hf'\r\n```\r\nby \"preparing `storage_options`\" for the `fsspec` head/get<end_of_turn>\n<start_of_turn>model\nFix `fsspec` download<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello,\r\nI am currently working on a project where both [DataLab](https://github.com/ExpressAI/DataLab) and [datasets](https://github.com/huggingface/datasets) are subdependencies.\r\nI noticed that I cannot import both libraries, as they both register FileSystems in `fsspec`, expecting the FileSystems not being registered before.\r\n\r\nWhen running the code below, I get the following error:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\__init__.py\", line 28, in <module>\r\n    from datalabs.arrow_dataset import concatenate_datasets, Dataset\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\arrow_dataset.py\", line 60, in <module>\r\n    from datalabs.arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\arrow_writer.py\", line 28, in <module>\r\n    from datalabs.features import (\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\features\\__init__.py\", line 2, in <module>\r\n    from datalabs.features.audio import Audio\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\features\\audio.py\", line 21, in <module>\r\n    from datalabs.utils.streaming_download_manager import xopen\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\utils\\streaming_download_manager.py\", line 16, in <module>\r\n    from datalabs.filesystems import COMPRESSION_FILESYSTEMS\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\datalabs\\filesystems\\__init__.py\", line 37, in <module>\r\n    fsspec.register_implementation(fs_class.protocol, fs_class)\r\n  File \"C:\\Users\\Bened\\anaconda3\\envs\\ner-eval-dashboard2\\lib\\site-packages\\fsspec\\registry.py\", line 51, in register_implementation\r\n    raise ValueError(\r\nValueError: Name (bz2) already in the registry and clobber is False\r\n```\r\n\r\nI think as simple solution would be to just set `clobber=True` in https://github.com/huggingface/datasets/blob/main/src/datasets/filesystems/__init__.py#L28. This allows the register to discard previous registrations. This should work, as the datalabs FileSystems are copies of the datasets FileSystems. However, I don't know if it is guaranteed to be compatible with other libraries that might use the same protocols.\r\n\r\n\r\nI am linking the symmetric issue on [DataLab](https://github.com/ExpressAI/DataLab/issues/425) as ideally the issue is solved in both libraries the same way. Otherwise, it could lead to different behaviors depending on which library gets imported first.\r\n\n\n### Steps to reproduce the bug\n\n1. Run `pip install datalabs==0.4.15 datasets==2.12.0`\r\n2. Run the following python code:\r\n   ```\r\n      import datalabs\r\n      import datasets\r\n   ```\n\n### Expected behavior\n\nIt should be possible to import both libraries without getting a Value Error\n\n### Environment info\n\ndatalabs==0.4.15\r\ndatasets==2.12.0\r\n<end_of_turn>\n<start_of_turn>model\nIncompatibility with DataLab<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- use `orch.cuda.set_device` instead of `CUDA_VISIBLE_DEVICES `\r\n- add `if __name__ == \"__main__\"`\r\n\r\nfix https://github.com/huggingface/datasets/issues/6186<end_of_turn>\n<start_of_turn>model\nFix multi gpu map example<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nHuggingface has quite a comprehensive set of guides for [audio datasets](https://huggingface.co/docs/datasets/en/audio_dataset). It seems, however, all these guides assume the audio array data to be decoded/inserted into a HF dataset always originates from individual files. The [Audio-dataclass](https://github.com/huggingface/datasets/blob/3.0.1/src/datasets/features/audio.py#L20) appears designed with this assumption in mind. Looking at its source code it returns a dictionary with the keys `path`, `array` and `sampling_rate`. \r\n\r\nHowever, sometimes users may have different pipelines where they themselves decode the audio array. This feature request has to do with wishing some clarification in guides on whether it is possible, and in such case how users can insert already decoded audio array data into datasets (pandas DataFrame, HF dataset or whatever) that are later saved as parquet, and still get a functioning audio preview in the dataset viewer. \r\n\r\nDo I perhaps need to write a tempfile of my audio array slice to wav and capture the bytes object with `io.BytesIO` and pass that to `Audio()`? \r\n\r\n### Motivation\r\n\r\nI'm working with large audio datasets, and my pipeline reads (decodes) audio from larger files, and slices the relevant portions of audio from that larger file based on metadata I have available. \r\n\r\nThe pipeline is designed this way to avoid having to store multiple copies of data, and to avoid having to store tens of millions of small files. \r\n\r\nI tried [test-uploading parquet files](https://huggingface.co/datasets/Lauler/riksdagen_test) where I store the audio array data of decoded slices of audio in an `audio` column with a dictionary with the keys `path`, `array` and `sampling_rate`. But I don't know the secret sauce of what the Huggingface Hub expects and requires to be able to display audio previews correctly. \r\n\r\n### Your contribution\r\n\r\nI could contribute a tool agnostic guide of creating HF audio datasets directly as parquet to the HF documentation if there is an interest. Provided you help me figure out the secret sauce of what the dataset viewer expects to display the preview correctly.<end_of_turn>\n<start_of_turn>model\nAudio preview in dataset viewer for audio array data without a path/filename<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix `_prepare_single_hop_path_and_storage_options`:\r\n- Do not pass HF authentication headers and HF user-agent to non-HF HTTP URLs\r\n- Do not overwrite passed `storage_options` nested values:\r\n  - Before, when passed \r\n    ```DownloadConfig(storage_options={\"https\": {\"client_kwargs\": {\"raise_for_status\": True}}})```, \r\n    it was overwritten to \r\n    ```{\"https\": {\"client_kwargs\": {\"trust_env\": True}}}```\r\n  - Now, the result combines both: \r\n    ```{\"https\": {\"client_kwargs\": {\"trust_env\": True, \"raise_for_status\": True}}}```<end_of_turn>\n<start_of_turn>model\nFix prepare_single_hop_path_and_storage_options<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\ndataset = load_dataset(\"Doraemon-AI/text-to-neo4j-cypher-chinese\")\r\n\r\ngot exception:\r\nGenerating train split: 1834 examples [00:00, 5227.98 examples/s]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 2011, in _prepare_split_single\r\n    writer.write_table(table)\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\", line 585, in write_table\r\n    pa_table = table_cast(pa_table, self._schema)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 2295, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 2254, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 2254, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 1802, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 1802, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 2018, in cast_array_to_feature\r\n    casted_array_values = _c(array.values, feature[0])\r\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 1804, in wrapper\r\n    return func(array, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/table.py\", line 2115, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\nTypeError: Couldn't cast array of type\r\nstruct<m.name: string, x.name: string, p.name: string, n.name: string, h.name: string, name: string, c: int64, collect(r.name): list<item: string>, q.name: string, rel.name: string, count(p): int64, 1: int64, p.location: string, max(n.name): null, mn.name: string, p.time: int64, min(q.name): string>\r\nto\r\n{'q.name': Value(dtype='string', id=None), 'mn.name': Value(dtype='string', id=None), 'x.name': Value(dtype='string', id=None), 'p.name': Value(dtype='string', id=None), 'n.name': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None), 'm.name': Value(dtype='string', id=None), 'h.name': Value(dtype='string', id=None), 'count(p)': Value(dtype='int64', id=None), 'rel.name': Value(dtype='string', id=None), 'c': Value(dtype='int64', id=None), 'collect(r.name)': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), '1': Value(dtype='int64', id=None), 'p.location': Value(dtype='string', id=None), 'substring(h.name,0,5)': Value(dtype='string', id=None), 'p.time': Value(dtype='int64', id=None)}\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/llm/train-2.py\", line 150, in <module>\r\n    dataset = load_dataset(\"Doraemon-AI/text-to-neo4j-cypher-chinese\")\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 2609, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1027, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1122, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 1882, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 2038, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n\n### Steps to reproduce the bug\n\ndataset = load_dataset(\"Doraemon-AI/text-to-neo4j-cypher-chinese\")\r\n\n\n### Expected behavior\n\nno exception\n\n### Environment info\n\npython 3.11\r\ndatasets 2.19.0<end_of_turn>\n<start_of_turn>model\nload_dataset doesn't support list column<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSuddenly, I started getting this error message saying it was an internal error.\n\n`Error creating/pushing dataset: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/ll4ma-lab/grasp-dataset/commit/main (Request ID: Root=1-6787f0b7-66d5bd45413e481c4c2fb22d;670d04ff-65f5-4741-a353-2eacc47a3928)\n\nInternal Error - We're working hard to fix this as soon as possible!\nTraceback (most recent call last):\n  File \"/uufs/chpc.utah.edu/common/home/hermans-group1/martin/software/pkg/miniforge3/envs/myenv2/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 406, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/uufs/chpc.utah.edu/common/home/hermans-group1/martin/software/pkg/miniforge3/envs/myenv2/lib/python3.10/site-packages/requests/models.py\", line 1024, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/ll4ma-lab/grasp-dataset/commit/main\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/uufs/chpc.utah.edu/common/home/u1295595/grasp_dataset_converter/src/grasp_dataset_converter/main.py\", line 142, in main\n    subset_train.push_to_hub(dataset_name, split='train')\n  File \"/uufs/chpc.utah.edu/common/home/hermans-group1/martin/software/pkg/miniforge3/envs/myenv2/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 5624, in push_to_hub\n    commit_info = api.create_commit(\n  File \"/uufs/chpc.utah.edu/common/home/hermans-group1/martin/software/pkg/miniforge3/envs/myenv2/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n  File \"/uufs/chpc.utah.edu/common/home/hermans-group1/martin/software/pkg/miniforge3/envs/myenv2/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 1518, in _inner\n    return fn(self, *args, **kwargs)\n  File \"/uufs/chpc.utah.edu/common/home/hermans-group1/martin/software/pkg/miniforge3/envs/myenv2/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 4087, in create_commit\n    hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\n  File \"/uufs/chpc.utah.edu/common/home/hermans-group1/martin/software/pkg/miniforge3/envs/myenv2/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\n    raise _format(HfHubHTTPError, str(e), response) from e\nhuggingface_hub.errors.HfHubHTTPError: 500 Server Error: Internal Server Error for url: https://huggingface.co/api/datasets/ll4ma-lab/grasp-dataset/commit/main (Request ID: Root=1-6787f0b7-66d5bd45413e481c4c2fb22d;670d04ff-65f5-4741-a353-2eacc47a3928)\n\nInternal Error - We're working hard to fix this as soon as possible!`\n\n### Steps to reproduce the bug\n\nI am pushing a Dataset in a loop via push_to_hub API\n\n### Expected behavior\n\nIt worked fine until it stopped working suddenly.\nExpected behavior: It should start working again\n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-4.18.0-477.15.1.el8_8.x86_64-x86_64-with-glibc2.28\n- Python version: 3.10.0\n- `huggingface_hub` version: 0.27.1\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0\n<end_of_turn>\n<start_of_turn>model\n500 Server error with pushing a dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCreate `DatasetNotFoundError` and `DataFilesNotFoundError`.\r\n\r\nFix #6397.\r\n\r\nCC: @severo <end_of_turn>\n<start_of_turn>model\nCreate DatasetNotFoundError and DataFilesNotFoundError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf I want to get the first 10 samples of my dataset, I can do :\r\n\r\n```\r\nds = datasets.load_dataset('mnist', split='train[:10]')\r\n```\r\n\r\nBut if I exceed the number of samples in the dataset, an exception is raised : \r\n\r\n```\r\nds = datasets.load_dataset('mnist', split='train[:999999999]')\r\n```\r\n\r\n> ValueError: Requested slice [:999999999] incompatible with 60000 examples.\n\n### Steps to reproduce the bug\n\n```\r\nds = datasets.load_dataset('mnist', split='train[:999999999]')\r\n```\n\n### Expected behavior\n\nI would expect it to behave like python lists (no exception raised, the whole list is kept) : \r\n\r\n```\r\nd = list(range(1000))[:999999]\r\nprint(len(d))  # > 1000\r\n```\n\n### Environment info\n\n- `datasets` version: 2.9.0\r\n- Platform: macOS-12.6-arm64-arm-64bit\r\n- Python version: 3.9.12\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nWhy split slicing doesn't behave like list slicing ?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfixed a few typos<end_of_turn>\n<start_of_turn>model\nUpdate README.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Before:\r\n<img width=\"935\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/35881688/93666e72-059b-4180-9e1d-ff176a3d9dac\">\r\n\r\n## After:\r\n<img width=\"956\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/35881688/75df7c3e-f473-44f0-a872-eeecf6a85fe2\">\r\n<end_of_turn>\n<start_of_turn>model\nRemove underlines between badges<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nto is not an inplace function.\r\n\r\nBut i am not sure about this code anyway, i think this is modifying the global variable `model` everytime the function is called? Which is on every batch? So it is juggling the same model on every gpu right? Isnt that very inefficient? <end_of_turn>\n<start_of_turn>model\nFix multi gpu process example<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDataset is 10X slower when applying trivial transforms:\r\n\r\n```\r\nimport time\r\nimport numpy as np\r\nfrom datasets import Dataset, Features, Array2D\r\n\r\na = np.zeros((800, 800))\r\na = np.stack([a] * 1000)\r\nfeatures = Features({\"a\": Array2D(shape=(800, 800), dtype=\"uint8\")})\r\n\r\nds1 = Dataset.from_dict({\"a\": a}, features=features).with_format('numpy')\r\n\r\ndef transform(batch):\r\n\treturn batch\r\n\r\nds2 = ds1.with_transform(transform)\r\n\r\n%time sum(1 for _ in ds1)\r\n%time sum(1 for _ in ds2)\r\n```\r\n\r\n```\r\nCPU times: user 472 ms, sys: 319 ms, total: 791 ms\r\nWall time: 794 ms\r\nCPU times: user 9.32 s, sys: 443 ms, total: 9.76 s\r\nWall time: 9.78 s\r\n```\r\n\r\nIn my real code I'm using set_transform to apply some post-processing on-the-fly for the 2d array, but it significantly slows down the dataset even if the transform itself is trivial.\r\n\r\nRelated issue: https://github.com/huggingface/datasets/issues/5841\n\n### Steps to reproduce the bug\n\nUse code in the description to reproduce.\n\n### Expected behavior\n\nTrivial custom transform in the example should not slowdown the dataset iteration.\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.15.0-79-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.4\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\nSuper slow iteration with trivial custom transform<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 3.6.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses #6116 <end_of_turn>\n<start_of_turn>model\n[Docs] Add description of `select_columns` to guide<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nUploading datasets is straightforward thanks to the ability to push Audio to hub. However, it would be nice if the data (text and audio) could be segmented when being pushed (if not possible already).\n\n### Motivation\n\nIt's easy to create a vtt file from an audio file. If there could be auto-segmenting, this would make the creation of datasets much faster.\n\n### Your contribution\n\nI have made a custom script to do this but it's not all that clean - uses librosa and pydub.<end_of_turn>\n<start_of_turn>model\nEasy way to segment into 30s snippets given an m4a file and a vtt file <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI am on a school project and the initial type for feature annotations are `Array2D(shape=(None, 4))`. I am trying to cast this type to a `float64` and pyarrow gives me this error : \r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/alan/dev/ClassezDesImagesAvecDesAlgorithmesDeDeeplearning/src/sdd/data/dataset.py\", line 141, in <module>\r\n    dataset = StanfordDogsDataset(size, 5).original(True).demo()\r\n  File \"<attrs generated init __main__.StanfordDogsDataset>\", line 4, in __init__\r\n  File \"/home/alan/dev/ClassezDesImagesAvecDesAlgorithmesDeDeeplearning/src/sdd/data/dataset.py\", line 33, in __attrs_post_init__\r\n    self.dataset = self.dataset.cast_column(\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/fingerprint.py\", line 511, in wrapper\r\n    out = func(dataset, *args, **kwargs)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2110, in cast_column\r\n    return self.cast(features)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2055, in cast\r\n    dataset = dataset.map(\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3097, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3474, in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 2328, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 2287, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 2287, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 1831, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 1831, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 2143, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home/alan/.cache/pypoetry/virtualenvs/sdd-2XWLAjSi-py3.10/lib/python3.10/site-packages/datasets/table.py\", line 1967, in array_cast\r\n    return pa_type.wrap_array(array)\r\n  File \"pyarrow/types.pxi\", line 1369, in pyarrow.lib.BaseExtensionType.wrap_array\r\nTypeError: Incompatible storage type for extension<arrow.py_extension_type<Array2DExtensionType>>: expected list<item: list<item: double>>, got list<item: list<item: int32>>\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\ndataset = datasets.load_dataset(\"Alanox/stanford-dogs\", split=\"full\")\r\ndataset = dataset.cast_column(\"annotations\", Array2D((None, 4), \"float64\"))\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt should simply cast the column feature type to a `float64` without error\r\n\r\n### Environment info\r\n\r\ndatasets == 2.14.5<end_of_turn>\n<start_of_turn>model\nCasting type from Array2D int to Array2D float crashes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nMy data repository was first called `BramVanroy/hplt-mono-v1-2` but I then renamed to use underscores instead of dashes. However, it seems that `datasets` retrieves the old repo name when it checks whether the repo contains data loading scripts in this line.\r\n\r\nhttps://github.com/huggingface/datasets/blob/6fb6c834f008996c994b0a86c3808d0a33d44525/src/datasets/load.py#L1845\r\n\r\nWhen I print `filename` it returns `hplt-mono-v1-2.py` but the files in the repo are of course `['.gitattributes', 'README.md', 'hplt_mono_v1_2.py']`. So the `filename` is the original reponame instead of the renamed one.\r\n\r\nI am not sure if this is a caching issue or not or how I can resolve it.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\n\r\nds = load_dataset(\r\n    \"BramVanroy/hplt-mono-v1-2\",\r\n    \"ky\",\r\n    trust_remote_code=True\r\n)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThat the most recent repo name is used when `filename` is generated.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.1\r\n- Platform: Linux-5.14.0-284.25.1.el9_2.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\nDataset with loading script does not work in renamed repos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have 127 elements in my input dataset.  When I do a len on the dataset after loaded, it is only 124 elements.\n\n### Steps to reproduce the bug\n\n    train_dataset = nlp.load_dataset(data_args.dataset_path, name=data_args.qg_format, split=nlp.Split.TRAIN)\r\n    valid_dataset = nlp.load_dataset(data_args.dataset_path, name=data_args.qg_format, split=nlp.Split.VALIDATION)\r\n    logger.info(len(train_dataset))\r\n    logger.info(len(valid_dataset))\r\n\r\nBoth train and valid input are 127 items.  However, they both only load 124 items.  The input format is in json.  At the end of the day, I am trying to create .pt files.  \n\n### Expected behavior\n\nI see all 127 elements in my dataset when performing len\n\n### Environment info\n\nPython 3.10.  CentOS operating system.  nlp==0.40, datasets==2.14.5, transformers==4.26.1<end_of_turn>\n<start_of_turn>model\nload_dataset does not load all of the data in my input file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\n[rank0]:   File \"/home/qinghao/miniconda3/envs/doremi/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 238, in __iter__\r\n[rank0]:     for key_example in islice(self.generate_examples_fn(**gen_kwags), shard_example_idx_start, None):\r\n[rank0]:   File \"/home/qinghao/miniconda3/envs/doremi/lib/python3.10/site-packages/datasets/packaged_modules/generator/generator.py\", line 32, in _generate_examples\r\n[rank0]:     for idx, ex in enumerate(self.config.generator(**gen_kwargs)):\r\n[rank0]:   File \"/home/qinghao/workdir/doremi/doremi/dataloader.py\", line 337, in take_data_generator\r\n[rank0]:     for ex in ds:\r\n[rank0]:   File \"/home/qinghao/miniconda3/envs/doremi/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1791, in __iter__\r\n[rank0]:     yield from self._iter_pytorch()\r\n[rank0]:   File \"/home/qinghao/miniconda3/envs/doremi/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1704, in _iter_pytorch\r\n[rank0]:     ex_iterable = ex_iterable.shard_data_sources(worker_id=worker_info.id, num_workers=worker_info.num_workers)\r\n[rank0]: TypeError: UpdatableRandomlyCyclingMultiSourcesExamplesIterable.shard_data_sources() got an unexpected keyword argument 'worker_id'\r\n```\n\n### Steps to reproduce the bug\n\nIterableDataset cannot use\n\n### Expected behavior\n\ncan work on datasets==2.10,  but will raise error for later versions.\n\n### Environment info\n\ndatasets==3.0.1<end_of_turn>\n<start_of_turn>model\nshard_data_sources() got an unexpected keyword argument 'worker_id'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSince we switched to the new HfFileSystem we no longer apply user's proxy and user-agent.\r\n\r\nUsing the HTTP_PROXY and HTTPS_PROXY environment variables works though since we use aiohttp to call the HF Hub.\r\n\r\nThis can be implemented in `_prepare_single_hop_path_and_storage_options`.\r\n\r\nThough ideally the `HfFileSystem` could support passing at least the proxies<end_of_turn>\n<start_of_turn>model\nSupport proxy and user-agent in fsspec calls<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport fsspec 2024.5.0.<end_of_turn>\n<start_of_turn>model\nSupport fsspec 2024.5.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nWhile I was creating the dataset which contained all documents from the Lushootseed Wikipedia, the dataset card asked me to enter which language the dataset was in. Since Lushootseed is a critically endangered language, it was not available as one of the options. Is it possible to allow entering languages that aren't available in the options?\n\n### Motivation\n\nI'd like to add more information about my dataset in the dataset card, and the language is one of the most important pieces of information, since the entire dataset is primarily concerned collecting Lushootseed documents.\n\n### Your contribution\n\nI can submit a pull request<end_of_turn>\n<start_of_turn>model\nUnable to create dataset card for Lushootseed language<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nhello, when I run the command \"from datasets.distributed import split_dataset_by_node\", I always met the bug \"No module named 'datasets.distributed\" in different version like 4.0.0, 2.21.0 and so on. How can I solve this?\n\n### Steps to reproduce the bug\n\n1. pip install datasets\n2. from datasets.distributed import split_dataset_by_node\n\n### Expected behavior\n\nexpecting the command \"from datasets.distributed import split_dataset_by_node\" can be ran successfully\n\n### Environment info\n\npython: 3.12<end_of_turn>\n<start_of_turn>model\nNo module named \"distributed\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ncc @kashif <end_of_turn>\n<start_of_turn>model\nFix infer list of images<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\u2026h str like that of load_from_disk<end_of_turn>\n<start_of_turn>model\nreplaced PathLike as a variable for save_to_disk for dataset_path wit\u2026<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe [map() documentation](https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.map) reads:\r\n`\r\nds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)`\r\n\r\nI have been trying to reproduce it in my code as:\r\n\r\n`tokenizedDataset = dataset.map(lambda x: tokenizer(x['text']), batched=True)`\r\n\r\nBut it doesn't work as it throws the error:\r\n\r\n> KeyError: 'text'\r\n\r\nCan you please guide me on how to fix it?\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. `from datasets import load_dataset\r\n\r\ndataset = load_dataset(\"amazon_reviews_multi\")`\r\n\r\n2. Then this code: `from transformers import AutoTokenizer\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")`\r\n3. The line I quoted above (which I have been trying)\n\n### Expected behavior\n\nAs mentioned in the documentation, it should run without any error and map the tokenization on the whole dataset.\n\n### Environment info\n\nPython 3.10.2<end_of_turn>\n<start_of_turn>model\nmap() not recognizing \"text\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a json named temp.json.\r\n```json\r\n{\"ref1\": \"ABC\", \"ref2\": \"DEF\", \"cuts\":[{\"cut1\": 3, \"cut2\": 5}]}\r\n```\r\nI want to load it.\r\n```python\r\nds = datasets.load_dataset('json', data_files=\"./temp.json\", features=datasets.Features({\r\n    'ref1': datasets.Value('string'),\r\n    'ref2': datasets.Value('string'),\r\n    'cuts': datasets.Sequence({\r\n        \"cut1\": datasets.Value(\"uint16\"),\r\n        \"cut2\": datasets.Value(\"uint16\")\r\n    })\r\n}))\r\n```\r\nThe above code does not work. However, I can load it without giving features.\r\n```python\r\nds = datasets.load_dataset('json', data_files=\"./temp.json\")\r\n```\r\nIs it possible to load integers as uint16 to save some memory?\n\n### Steps to reproduce the bug\n\nAs in the bug description.\n\n### Expected behavior\n\nThe data are loaded and integers are uint16.\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.21.0\r\n- Platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.9\r\n- `huggingface_hub` version: 0.24.5\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\ndatasets cannot handle nested json if features is given.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR remove conditions for Python < 3.9.<end_of_turn>\n<start_of_turn>model\nRemove conditions for Python < 3.9<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\n`drop_duplicates` method for huggingface datasets (similiar in simplicity to the `pandas` one)\n\n### Motivation\n\nEase of use\n\n### Your contribution\n\nI don't think i am good enough to help<end_of_turn>\n<start_of_turn>model\n`drop_duplicates` method<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`dtype: ClassLabel` in the README.md yaml metadata is parsed incorrectly and causes the data viewer to fail.\n\nThis yaml in my metadata ([source](https://huggingface.co/datasets/BrentLab/yeast_genome_resources/blob/main/README.md), though i changed `ClassLabel` to `string` to using different dtype in order to avoid the error):\n\n```yaml\nlicense: mit\npretty_name: BrentLab Yeast Genome Resources\nsize_categories:\n  - 1K<n<10K\nlanguage:\n  - en\ndataset_info:\n  features:\n    - name: start\n      dtype: int32\n      description: Start coordinate (1-based, **inclusive**)\n    - name: end\n      dtype: int32\n      description: End coordinate (1-based, **inclusive**)\n    - name: strand\n      dtype: ClassLabel\n...\n```\n\nis producing the following error in the data viewer:\n\n```\nError code:   ConfigNamesError\nException:    ValueError\nMessage:      Feature type 'Classlabel' not found. Available feature types: ['Value', 'ClassLabel', 'Translation', 'TranslationVariableLanguages', 'LargeList', 'List', 'Array2D', 'Array3D', 'Array4D', 'Array5D', 'Audio', 'Image', 'Video', 'Pdf']\nTraceback:    Traceback (most recent call last):\n                File \"/src/services/worker/src/worker/job_runners/dataset/config_names.py\", line 66, in compute_config_names_response\n                  config_names = get_dataset_config_names(\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 161, in get_dataset_config_names\n                  dataset_module = dataset_module_factory(\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1031, in dataset_module_factory\n                  raise e1 from None\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 996, in dataset_module_factory\n                  return HubDatasetModuleFactory(\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 605, in get_module\n                  dataset_infos = DatasetInfosDict.from_dataset_card_data(dataset_card_data)\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/info.py\", line 386, in from_dataset_card_data\n                  dataset_info = DatasetInfo._from_yaml_dict(dataset_card_data[\"dataset_info\"])\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/info.py\", line 317, in _from_yaml_dict\n                  yaml_data[\"features\"] = Features._from_yaml_list(yaml_data[\"features\"])\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 2027, in _from_yaml_list\n                  return cls.from_dict(from_yaml_inner(yaml_data))\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1872, in from_dict\n                  obj = generate_from_dict(dic)\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1459, in generate_from_dict\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1459, in <dictcomp>\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1465, in generate_from_dict\n                  raise ValueError(f\"Feature type '{_type}' not found. Available feature types: {list(_FEATURE_TYPES.keys())}\")\n              ValueError: Feature type 'Classlabel' not found. Available feature types: ['Value', 'ClassLabel', 'Translation', 'TranslationVariableLanguages', 'LargeList', 'List', 'Array2D', 'Array3D', 'Array4D', 'Array5D', 'Audio', 'Image', 'Video', 'Pdf']\n```\n\nI think that this is caused by this line\n\nhttps://github.com/huggingface/datasets/blob/896616c6cb03d92a33248c3529b0796cda27e955/src/datasets/features/features.py#L2013\n\nReproducible example from [naming.py](https://github.com/huggingface/datasets/blob/896616c6cb03d92a33248c3529b0796cda27e955/src/datasets/naming.py)\n\n```python\nimport itertools\nimport os\nimport re\n\n\n_uppercase_uppercase_re = re.compile(r\"([A-Z]+)([A-Z][a-z])\")\n_lowercase_uppercase_re = re.compile(r\"([a-z\\d])([A-Z])\")\n\n_single_underscore_re = re.compile(r\"(?<!_)_(?!_)\")\n_multiple_underscores_re = re.compile(r\"(_{2,})\")\n\n_split_re = r\"^\\w+(\\.\\w+)*$\"\n\ndef snakecase_to_camelcase(name):\n    \"\"\"Convert snake-case string to camel-case string.\"\"\"\n    name = _single_underscore_re.split(name)\n    name = [_multiple_underscores_re.split(n) for n in name]\n    return \"\".join(n.capitalize() for n in itertools.chain.from_iterable(name) if n != \"\")\n\nsnakecase_to_camelcase(\"ClassLabel\")\n```\n\nResult:\n\n```raw\n'Classlabel'\n```<end_of_turn>\n<start_of_turn>model\ndtype: ClassLabel is not parsed correctly in `features.py`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nSupport for slice splits in `datasets.load_from_disk`, similar to how it's already supported for `datasets.load_dataset`.\r\n\r\n### Motivation\r\n\r\nSlice splits are convienient in a numer of cases - adding support to `datasets.load_from_disk` would make working with local datasets easier and homogenize the APIs of load_from_disk and load_dataset. \r\n\r\n### Your contribution\r\n\r\nSure, if the devs think the feature request is sensible. <end_of_turn>\n<start_of_turn>model\nSlice splits support for datasets.load_from_disk<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI don't know if it is a bug or an expected behavior, but the tensor type seems to be ignored after applying map. For example, mapping over to tokenize text with a transformers' tokenizer always returns lists and it ignore the `return_tensors` argument.\r\n\r\nIf this is an expected behaviour (e.g., for caching/Arrow compatibility/etc.) it should be clearly documented. For example, current documentation (see [here](https://huggingface.co/docs/datasets/v2.17.1/en/nlp_process#map)) clearly state to \"set  `return_tensors=\"np\"` when you tokenize your text\" to have Numpy arrays.\n\n### Steps to reproduce the bug\n\n```py\r\n# %%%\r\n\r\nimport datasets\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport torch\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\n# %%\r\nds = datasets.load_dataset(\"cnn_dailymail\", \"1.0.0\", split=\"train[:1%]\")\r\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\r\n\r\n#%%\r\nfor return_tensors in [None, \"np\", \"pt\", \"tf\", \"jax\"]:\r\n  print(f\"********** no map, return_tensors={return_tensors} **********\")\r\n  _ds = tokenizer(ds[\"article\"], return_tensors=return_tensors, truncation=True, padding=True)\r\n  print('Type <input_ids>:', type(_ds[\"input_ids\"]))\r\n\r\n# %%\r\nfor return_tensors in [None, \"np\", \"pt\", \"tf\", \"jax\"]:\r\n  print(f\"********** map, return_tensors={return_tensors} **********\")\r\n  _ds = ds.map(\r\n    lambda examples: tokenizer(examples[\"article\"], return_tensors=return_tensors, truncation=True, padding=True),\r\n    batched=True,\r\n    remove_columns=[\"article\"],\r\n  )\r\n\r\n  print('Type <input_ids>:', type(_ds[0][\"input_ids\"]))\r\n```\r\n\n\n### Expected behavior\n\nThe output from the script above. I would expect the second half to be the same.\r\n\r\n```\r\n********** no map, return_tensors=None **********\r\nType <input_ids>: <class 'list'>\r\n********** no map, return_tensors=np **********\r\nType <input_ids>: <class 'numpy.ndarray'>\r\n********** no map, return_tensors=pt **********\r\nType <input_ids>: <class 'torch.Tensor'>\r\n********** no map, return_tensors=tf **********\r\nType <input_ids>: <class 'tensorflow.python.framework.ops.EagerTensor'>\r\n********** no map, return_tensors=jax **********\r\nType <input_ids>: <class 'jaxlib.xla_extension.ArrayImpl'>\r\n\r\n\r\n********** map, return_tensors=None **********\r\nType <input_ids>: <class 'list'>\r\n********** map, return_tensors=np **********\r\nType <input_ids>: <class 'list'>\r\n********** map, return_tensors=pt **********\r\nType <input_ids>: <class 'list'>\r\n********** map, return_tensors=tf **********\r\nType <input_ids>: <class 'list'>\r\n********** map, return_tensors=jax **********\r\nType <input_ids>: <class 'list'>\r\n```\n\n### Environment info\n\n- `datasets` version: 2.17.1\r\n- Platform: Redacted (linux)\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nTensor type (e.g. from `return_tensors`) ignored in map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nWhen using `Dataset.from_generator`, the generator is hashed when building the fingerprint. Similar to `.map`, it would be interesting to let the user bypass this hashing by accepting a `fingerprint` argument to `.from_generator`.\r\n\r\n### Motivation\r\n\r\nUsing the `.from_generator` constructor with a non-picklable generator fails. By accepting a `fingerprint` argument to `.from_generator`, the user would have the opportunity to manually fingerprint the dataset and thus bypass the crash.\r\n\r\n### Your contribution\r\n\r\nIf validated, I can try to submit a PR for this.<end_of_turn>\n<start_of_turn>model\nSupport custom fingerprinting with `Dataset.from_generator`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix\r\n\r\n```\r\nTypeError: ArrayExtensionArray.to_pylist() got an unexpected keyword argument 'maps_as_pydicts'\r\n```<end_of_turn>\n<start_of_turn>model\nsupport pyarrow 20<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ncc @kashif <end_of_turn>\n<start_of_turn>model\nBackward compat list feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## medical data\r\n\r\n**Description:**\r\nThis dataset, named \"medical data,\" is a collection of text data from various sources, carefully curated and cleaned for use in natural language processing (NLP) tasks. It consists of a diverse range of text, including articles, books, and online content, covering topics from science to literature.\r\n\r\n**Citation:**\r\nIf applicable, please include a citation for this dataset to give credit to the original sources or contributors.\r\n\r\n**Key Features:**\r\n- Language: The text is primarily in English, but it may include content in other languages as well.\r\n- Use Cases: This dataset is suitable for text classification, language modeling, sentiment analysis, and other NLP tasks.\r\n\r\n**Usage:**\r\nTo access this dataset, use the `load_your_dataset` function provided in the `your_dataset.py` script within this repository. You can specify the dataset split you need, such as \"train,\" \"test,\" or \"validation,\" to get the data for your specific task.\r\n\r\n**Contributors:**\r\n- [Keyur Chaudhari]\r\n\r\n**Contact:**\r\nIf you have any questions or need assistance regarding this dataset, please feel free to contact [keyurchaudhari536@gmail.com].\r\n\r\nPlease note that this dataset is shared under a specific license, which can be found in the [LICENSE](link to your dataset's license) file. Make sure to review and adhere to the terms of the license when using this dataset for your projects.\r\n<end_of_turn>\n<start_of_turn>model\nAdd my dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nPython 3.14<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhenever I try to create a dataset which contains images using `Dataset.from_generator`, it freezes around 996 rows. I suppose it has something to do with memory consumption, but there's more memory available. I\r\n\r\nSomehow it worked a few times but mostly this makes the datasets library much more cumbersome to work with because generators are the easiest way to turn an existing dataset into a Hugging Face dataset.\r\n\r\nI've let it run in the frozen state for way longer than it can possibly take to load the actual dataset.\r\n\r\nLet me know if you have ideas how to resolve it!\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset\r\nimport numpy as np\r\n\r\ndef gen():\r\n    for row in range(10000):\r\n        yield {\"i\": np.random.rand(512, 512, 3)}\r\n        \r\nDataset.from_generator(gen)\r\n# -> 90% of the time gets stuck around 1000 rows\r\n```\n\n### Expected behavior\n\nShould continue and go through all the examples yielded by the generator, or at least throw an error or somehow communicate what's going on.\n\n### Environment info\n\n- `datasets` version: 2.8.0\r\n- Platform: Linux-5.15.0-52-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.1\r\n<end_of_turn>\n<start_of_turn>model\nDataset.from_generator consistently freezes at ~1000 rows<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse shared memory for the IterableDataset epoch.\r\n\r\nThis way calling `ds.set_epoch()` in the main process will update the epoch in the DataLoader workers as well.\r\nThis is useful especially because the epoch is used to compute the `effective_seed` used for shuffling.\r\n\r\nI used torch's shared memory in case users want to send dataset copies without shared memory using pickle. I also find it easier to use than using `multiprocessing.shared_memory` than requires unlinking only in the main process, or `mp.Value` that is not picklable.\r\n\r\nclose https://github.com/huggingface/datasets/issues/6673\r\n\r\ncc @rwightman <end_of_turn>\n<start_of_turn>model\nPersist IterableDataset epoch in workers<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen sharding `IterableDataset`s, a temporary list is created that is then indexed. There is no need to create a temporary list of a potentially very large step/world size, with standard `islice` functionality, so we avoid it.\r\n\r\n```shell\r\npytest tests/test_distributed.py -k iterable\r\n```\r\nRuns successfully.<end_of_turn>\n<start_of_turn>model\nDo not consume unnecessary memory during sharding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nEither update the documentation, or re-introduce the flag (and corresponding logic to convert the audio to mono)\n\n### Steps to reproduce the bug\n\nAudio(sampling_rate=16000, mono=True) raises the error \nTypeError: Audio.__init__() got an unexpected keyword argument 'mono'\n\nHowever, in the class documentation, is says:\nArgs:\n        sampling_rate (`int`, *optional*):\n            Target sampling rate. If `None`, the native sampling rate is used.\n        mono (`bool`, defaults to `True`):\n            Whether to convert the audio signal to mono by averaging samples across\n            channels.\n        [...]\n\n### Expected behavior\n\nThe above call should either work, or the documentation within the Audio class should be updated\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.34.4\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.2\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nAudio mono argument no longer supported, despite class documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi there,\n\nI uploaded a dataset here https://huggingface.co/datasets/V-STaR-Bench/V-STaR, but I found that Data Studio incorrectly convert the \"bboxes\" value for the whole dataset. Therefore, anyone who downloaded the dataset via the API would get the wrong \"bboxes\" value in the data file.\n\nCould you help me address the issue?\n\nMany thanks,\n\n\n\n### Steps to reproduce the bug\n\nThe JSONL file of [V_STaR_test_release.jsonl](https://huggingface.co/datasets/V-STaR-Bench/V-STaR/blob/main/V_STaR_test_release.jsonl) has the correct values of every \"bboxes\" for each sample.\n\nBut in the Data Studio, we can see that the values of \"bboxes\" have changed, and load the dataset via API will also get the wrong values.\n\n### Expected behavior\n\nFix the bug to correctly download my dataset.\n\n### Environment info\n\n- `datasets` version: 2.16.1\n- Platform: Linux-5.14.0-427.22.1.el9_4.x86_64-x86_64-with-glibc2.34\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.29.3\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nData Studio Error: Convert JSONL incorrectly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nSupport the GeoParquet format\n\n### Motivation\n\nGeoParquet (https://geoparquet.org/) is a common format for sharing vectorial geospatial data on the cloud, along with \"traditional\" data columns.\r\n\r\nIt would be nice to be able to load this format with datasets, and more generally, in the Datasets Hub (see https://huggingface.co/datasets/joshuasundance/govgis_nov2023-slim-spatial/discussions/1).\n\n### Your contribution\n\nI would be happy to help work on a PR (but I don't think I can do one on my own).\r\n\r\nAlso, we have to define what we want to support:\r\n- load all the columns, but get the \"geospatial\" column in text-only mode for now\r\n- or, fully support the spatial features, maybe taking inspiration from (or depending upon) https://geopandas.org/en/stable/index.html (which itself depends on https://fiona.readthedocs.io/en/stable/, which requires a local install of https://gdal.org/)<end_of_turn>\n<start_of_turn>model\nSupport GeoParquet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nShouldn't this kinda work ?\r\n```\r\nDataset.from_json(\"hf://datasets/HuggingFaceTB/eval_data/resolve/main/eval_data_context_and_answers.json\")\r\n```\r\nI got an error\r\n```\r\nFile ~/miniconda3/envs/datatrove/lib/python3.10/site-packages/datasets/data_files.py:380, in resolve_pattern(pattern, base_path, allowed_extensions, download_config)\r\n    378     if allowed_extensions is not None:\r\n    379         error_msg += f\" with any supported extension {list(allowed_extensions)}\"\r\n--> 380     raise FileNotFoundError(error_msg)\r\n    381 return out\r\n\r\nFileNotFoundError: Unable to find 'hf://datasets/HuggingFaceTB/eval_data/resolve/main/eval_data_context_and_answers.json'\r\n(I'm logged in)\r\n```\r\n\r\nFix: the correct path is\r\n```\r\nhf://datasets/HuggingFaceTB/eval_data/eval_data_context_and_answers.json\r\n```\r\n\r\nProposal: raise a clearer error\n\n### Motivation\n\nClearer error message\n\n### Your contribution\n\nCan open a PR<end_of_turn>\n<start_of_turn>model\nLoading from hf hub with clearer error message<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently if you change the `dataset_info` of a dataset (e.g. in the YAML part of the README.md), the cache ignores this change.\r\n\r\nThis is problematic because you want to regenerate a dataset if you change the features or the split sizes for example (e.g. after push_to_hub)\r\n\r\nIdeally we should take the resolved files into account as well but this will be for another PR<end_of_turn>\n<start_of_turn>model\nUpdate builder hash with info<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSkip faiss tests on Windows to avoid running CI for 360 minutes.\r\n\r\nFix #7013.\r\n\r\nRevert once the underlying issue is fixed.<end_of_turn>\n<start_of_turn>model\nSkip faiss tests on Windows to avoid running CI for 360 minutes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI've been using Dataset.map() with `num_proc=os.cpu_count()` to leverage multicore processing for my datasets, but from time to time I get stuck processes waiting forever. Apparently, when one of the subprocesses is abruptly killed (OOM killer, segfault, SIGKILL, etc), the main process keeps waiting for the async task sent to that child process to finish.\r\n\r\nIt seems to be easy to reproduce the issue with the following script:\r\n\r\n```\r\nimport os\r\nfrom datasets import Dataset, Features, Value\r\n\r\n\r\ndef do_stuck(item):\r\n    os.kill(os.getpid(), 9)\r\n\r\ndata = {\r\n    \"col1\": list(range(5)),\r\n    \"col2\": list(range(5)),\r\n}\r\n\r\nds = Dataset.from_dict(\r\n    data,\r\n    features=Features({\r\n        \"col1\": Value(\"int64\"),\r\n        \"col2\": Value(\"int64\"),\r\n    }),\r\n)\r\n\r\nprint(ds.map(do_stuck, num_proc=4))\r\n```\r\n\r\nThis is an old behavior in Python, which apparently was fixed a few years ago in `concurrent.futures.ProcessPoolExecutor` ([ref](https://bugs.python.org/issue9205)), but not in `multiprocessing.pool.Pool` / `multiprocess.pool.Pool`, which is used by `Dataset.map` ([ref](https://bugs.python.org/issue22393)).\r\n\r\nThis PR is an idea to try to detect when a child process gets killed, and raises a `RuntimeError` warning the dataset.map() caller.\r\n\r\nEDIT: Related proposal for future improvement: https://github.com/huggingface/datasets/discussions/5977<end_of_turn>\n<start_of_turn>model\nAvoid stuck map operation when subprocesses crashes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n```python\r\n>>> Features({\"a\": Sequence(Sequence({\"c\": Value(\"int64\")}))})\r\n{'a': List({'c': List(Value('int64'))})}\r\n```\r\n\r\ninstead of `{'a': {'c': List(List(Value('int64')))}}`<end_of_turn>\n<start_of_turn>model\nFix double sequence<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFirst implementation of the Pdf feature to support pdfs (#7318) . Using [pdfplumber](https://github.com/jsvine/pdfplumber?tab=readme-ov-file#python-library) as the default library to work with pdfs. \r\n@lhoestq and @AndreaFrancis <end_of_turn>\n<start_of_turn>model\nIntroduce pdf support (#7318)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFor consistency with the JSON builder and Pandas<end_of_turn>\n<start_of_turn>model\nDeprecate `errors` param in favor of `encoding_errors` in text builder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI understand `dataset` provides a [`map`](https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_dataset.py#L2849) function. This function in turn takes in a callable that is used to tokenize the text on which a model is trained. Frequently this text will not fit within a models's context window. In this case it would be useful to wrap around the text into multiple rows with each row fitting the model's context window. I tried to do it using this code as example which in turn I have borrowed from [here](https://stackoverflow.com/a/76343993/147530):\r\n\r\n```\r\ndata = data.map(lambda samples: tokenizer(samples[\"text\"], max_length=tokenizer.model_max_length, truncation=True, stride=4, return_overflowing_tokens=True), batched=True)\r\n```\r\n\r\nbut running the code gives me this error:\r\n\r\n```\r\nFile \"/llm/fine-tune.py\", line 117, in <module>\r\n    data = data.map(lambda samples: tokenizer(samples[\"text\"], max_length=tokenizer.model_max_length, truncation=True, stride=4, return_overflowing_tokens=True), batched=True)\r\n  File \"/llm/.env/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 580, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/llm/.env/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 545, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/llm/.env/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3087, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/llm/.env/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3480, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"/llm/.env/lib/python3.9/site-packages/datasets/arrow_writer.py\", line 556, in write_batch\r\n    pa_table = pa.Table.from_arrays(arrays, schema=schema)\r\n  File \"pyarrow/table.pxi\", line 3798, in pyarrow.lib.Table.from_arrays\r\n  File \"pyarrow/table.pxi\", line 2962, in pyarrow.lib.Table.validate\r\n  File \"pyarrow/error.pxi\", line 100, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Column 1 named input_ids expected length 394 but got length 447\r\n```\r\n\r\nThe lambda function I have provided is correctly chopping up long text so it wraps around (and because of this 394 samples become 447 after wrap around) but the dataset `map` function does not like it.\n\n### Motivation\n\nplease see above\n\n### Your contribution\n\nI'm afraid I don't have much knowledge to help<end_of_turn>\n<start_of_turn>model\nextend the map function so it can wrap around long text that does not fit in the context window<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix getting the info for script-based datasets with Parquet export with a single config not named \"default\".\r\n\r\nE.g.\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\n\r\nb = load_dataset_builder(\"bookcorpus\")\r\nprint(b.info.features)\r\n# should print {'text': Value(dtype='string', id=None)}\r\n```\r\n\r\nI fixed this by setting the default config name when there is only one config.<end_of_turn>\n<start_of_turn>model\nFix missing info when loading some datasets from Parquet export<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7583<end_of_turn>\n<start_of_turn>model\nload_dataset splits typing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7163<end_of_turn>\n<start_of_turn>model\nfix docstring code example for distributed shuffle<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelease 2.14.7.<end_of_turn>\n<start_of_turn>model\nRelease: 2.14.7<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ncloses #7567\r\n\r\nAdd `shift_rngs` method to `ExamplesIterable` that is called directly after sharding. If a generator is available (not the case for all subclasses) we update the seed of the generator by shifting by the worker_id.\r\n\r\n~This is just the fix for `shuffle`, in the corresponding issue `interleave_datasets` is mentioned as well, which won't be fixed with this approach.~\r\n\r\nEDIT: This is a fix for `shuffle` and `interleave_datasets`. Adding recursivity to `shift_rngs` solved `interleave_datasets` as well. Not sure though if this is completely safe or if we could destroy something with that. I don't think so but could be wrong and appreciate some guidance from the maintainers. I also checked, on a single_worker we are always handing over `index=0` so that case preserves the seed the user specified. <end_of_turn>\n<start_of_turn>model\nFix random seed on shuffle and interleave_datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nStructured data types (graphs etc.) might often be most efficiently stored as multiple columns, which then need to be combined during feature decoding\r\n\r\nAlthough it is currently possible to nest features as structs, my impression is that in particular when dealing with e.g. a feature composed of multiple numpy array / ArrayXD's, it would be more efficient to store each ArrayXD as a separate column (though I'm not sure by how much)\r\n\r\nPerhaps specification / implementation could be supported by something like:\r\n\r\n```\r\nfeatures=Features(**{(\"feature0\", \"feature1\")=Features(feature0=Array2D((None,10), dtype=\"float32\"), feature1=Array2D((None,10), dtype=\"float32\"))\r\n```\r\n\r\n### Motivation\r\n\r\nDefining efficient composite feature types based on numpy arrays for representing data such as graphs with multiple node and edge attributes is currently challenging.\r\n\r\n### Your contribution\r\n\r\nPossibly able to contribute<end_of_turn>\n<start_of_turn>model\nComposite (multi-column) features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7447<end_of_turn>\n<start_of_turn>model\nFix resuming after `ds.set_epoch(new_epoch)`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDear huggingfacers,\r\n\r\nI'm trying to use a subset of the-stack dataset. When I run the command the first time\r\n\r\n```\r\ndataset = load_dataset(\r\n    path='bigcode/the-stack',\r\n    data_dir='data/fortran',\r\n    split='train' )\r\n```\r\n\r\nIt downloads the files and caches them normally.\r\n\r\nNevertheless, since my compute nodes are not online (`HF_DATASETS_OFFLINE=1`) . Whenever I try to run the command again, the library is passing the wrong cache path:\r\n\r\n\r\n`Cache directory for the-stack doesn't exist at /Users/user/.cache/huggingface/datasets/bigcode___the-stack/default-data_dir=data%2Ffortran-data_dir=data%2Ffortran`\r\n\r\nwhen the right path is:\r\n\r\n`'/Users/user/.cache/huggingface/datasets/bigcode___the-stack/default-data_dir=data\\%2Ffortran`\r\n\r\nNot sure why those redundancies are included in the path.  If I try adding the correct path through the the cache_dir argument it throws an error:\r\n\r\nConnectionError: Couldn't reach the Hugging Face Hub for dataset 'bigcode/the-stack': Offline mode is enabled.\r\n\r\n\r\nYour help with this issue is greatly appreciated. Thanks a lot for the great work.\r\n\n\n### Steps to reproduce the bug\n\n1: \r\n\r\n`dataset = load_dataset(\r\n    path='bigcode/the-stack',\r\n    data_dir='data/fortran',\r\n    split='train' )`\r\n\r\n\r\n2: \r\n\r\n`HF_DATASETS_OFFLINE=1`\r\n\r\n3:\r\n\r\n`dataset = load_dataset(\r\n    path='bigcode/the-stack',\r\n    data_dir='data/fortran',\r\n    split='train' )`\r\n\n\n### Expected behavior\n\nbeing able to use the cached data\n\n### Environment info\n\nseveral different systems<end_of_turn>\n<start_of_turn>model\nWrong path for cache directory in offline mode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nit seems to be an issue with datasets not passing the token to embed_table_storage when generating a dataset\r\n\r\nSee https://github.com/huggingface/datasets-server/issues/2010<end_of_turn>\n<start_of_turn>model\nParquet stream-conversion fails to embed images/audio files from gated repos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using the huggingface datasets.Audio feature to decode a local or remote (public HF dataset) audio file inside Google Colab, the notebook kernel crashes with std::bad_alloc (C++ memory allocation failure).\nThe crash happens even with a minimal code example and valid .wav file that can be read successfully using soundfile.\n\nHere is a sample Collab notebook to reproduce the problem.\nhttps://colab.research.google.com/drive/1nnb-GC5748Tux3xcYRussCGp2x-zM9Id?usp=sharing\n\ncode sample:\n```\n...\naudio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n# Accessing the first element crashes the Colab kernel\nprint(audio_dataset[0][\"audio\"])\n```\n\nError log \n```\nWARNING what(): std::bad_alloc\nterminate called after throwing an instance of 'std::bad_alloc'\n```\n\nEnvironment\n\nPlatform: Google Colab (Python 3.12.12)\ndatasets Version: 4.3.0\nsoundfile Version: 0.13.1\ntorchaudio Version: 2.8.0+cu126\n\nThanks in advance to help me on this error I get approx two weeks now after it was working before.\n\nRegards\n\n### Steps to reproduce the bug\n\nhttps://colab.research.google.com/drive/1nnb-GC5748Tux3xcYRussCGp2x-zM9Id?usp=sharing\n\n### Expected behavior\n\nLoading the audio and decode it.\nIt should safely return:\n\n{\n  \"path\": \"path/filaname.wav\",\n  \"array\": np.ndarray([...]),\n  \"sampling_rate\": 16000\n}\n\n### Environment info\n\nEnvironment\n\nPlatform: Google Colab (Python 3.12.12)\ndatasets Version: 4.3.0\nsoundfile Version: 0.13.1\ntorchaudio Version: 2.8.0+cu126<end_of_turn>\n<start_of_turn>model\nAudio.cast_column() or Audio.decode_example() causes Colab kernel crash (std::bad_alloc)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'd expect the following code to download just the validation split but instead I get all data on my disk (train, test and validation splits)\r\n\r\n`\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"imagenet-1k\", split=\"validation\", trust_remote_code=True)\r\n`\r\n\r\nIs it expected to work like that? \n\n### Steps to reproduce the bug\n\n1. Install the required libraries (python, datasets, huggingface_hub)\r\n2. Login using huggingface cli\r\n2. Run the code in the description\n\n### Expected behavior\n\nJust a single (validation) split should be downloaded.\n\n### Environment info\n\npython: 3.12.2\r\ndatasets: 2.18.0\r\nhuggingface_hub: 0.22.2<end_of_turn>\n<start_of_turn>model\nLoading just one particular split is not possible for imagenet-1k<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHuggingface datasets use memory-mapped file to map large datasets in memory for fast access.\r\nHowever, it seems like huggingface will occupy all the memory for memory-mapped files, which makes a troublesome situation since we cluster will distribute a small portion of memory to me (once it's over the limit, memory cannot be allocated), however, when the dataset checks the total memory, all of the memory will be taken into account which makes huggingface dataset try to allocate more memory than allowed. \r\nSo is there a way to explicitly limit the size of memory mapped file?\n\n### Steps to reproduce the bug\n\npython\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"c4\", \"en\", streaming=True)\n\n### Expected behavior\n\nIn a normal environment, this will not have any problem.\r\nHowever, when the system allocates a portion of the memory to the program and when the dataset checks the total memory, all of the memory will be taken into account which makes huggingface dataset try to allocate more memory than allowed. \n\n### Environment info\n\nlinux cluster with SGE\uff08Sun Grid Engine\uff09<end_of_turn>\n<start_of_turn>model\nhow to limit the size of memory mapped file?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7494<end_of_turn>\n<start_of_turn>model\npdf docs fixes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes issue #7745.\r\n\r\nWho can review:\r\n@lhoestq <end_of_turn>\n<start_of_turn>model\nFix Audio docstring by removing unsupported mono argument<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am working on a retrieval project and encountering I have encountered two issues in the hugging face faiss integration:\r\n\r\n1. I am trying to pass in a dataset with a faiss index to the Huggingface trainer. The code works for a cpu faiss index, but doesn't for a gpu one, getting error:\r\n\r\n```\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/transformers/trainer.py\", line 1543, in train\r\n    return inner_training_loop(\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/transformers/trainer.py\", line 1555, in _inner_training_loop\r\n    train_dataloader = self.get_train_dataloader()\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/transformers/trainer.py\", line 831, in get_train_dataloader\r\n    train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/transformers/trainer.py\", line 725, in _remove_unused_columns\r\n    return dataset.remove_columns(ignored_columns)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/datasets/fingerprint.py\", line 481, in wrapper\r\n    out = func(dataset, *args, **kwargs)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2146, in remove_columns\r\n    dataset = copy.deepcopy(self)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 271, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 231, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/copy.py\", line 161, in deepcopy\r\n    rv = reductor(4)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/faiss/__init__.py\", line 556, in index_getstate\r\n    return {\"this\": serialize_index(self).tobytes()}\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/faiss/__init__.py\", line 1607, in serialize_index\r\n    write_index(index, writer)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/faiss/swigfaiss.py\", line 9843, in write_index\r\n    return _swigfaiss.write_index(*args)\r\nRuntimeError: Error in void faiss::write_index(const faiss::Index*, faiss::IOWriter*) at /project/faiss/faiss/impl/index_write.cpp:590: don't know how to serialize this type of index\r\n```\r\nThe index was created with the add_faiss_index method            \r\n```\r\ntrain_dataset.add_faiss_index(\r\n                column='embeddings',\r\n                index_name='embeddings',\r\n                string_factory=faiss_index_string,\r\n                train_size=config.faiss_train_size,\r\n                device=0,  # Use -1 for CPU, or specify GPU device ID\r\n                faiss_verbose=True\r\n            )\r\n\r\n```\r\n2. Athough faiss is written to be compatible on the gpu for searching [https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU](https://github.com/facebookresearch/faiss/wiki/Faiss-on-the-GPU) I am getting error when trying to use the hugggingface code to do the search on gpu. This seems to be caused by this line https://github.com/huggingface/datasets/blob/f9975f636542df7f95c27065ea93147440d690b7/src/datasets/search.py#L376 producing error \r\n\r\n```\r\n    total_scores, total_examples = self.dataset.get_nearest_examples_batch('embeddings', embeddings, k=self.k)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/datasets/search.py\", line 773, in get_nearest_examples_batch\r\n    total_scores, total_indices = self.search_batch(index_name, queries, k, **kwargs)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/datasets/search.py\", line 727, in search_batch\r\n    return self._indexes[index_name].search_batch(queries, k, **kwargs)\r\n  File \"/users/rubman/.conda/envs/protein_npt_env/lib/python3.10/site-packages/datasets/search.py\", line 376, in search_batch\r\n    if not queries.flags.c_contiguous:\r\nAttributeError: 'Tensor' object has no attribute 'flags'\r\n```\r\n\n\n### Steps to reproduce the bug\n\n```\r\ntrain_dataset.add_faiss_index(\r\n                column='embeddings',\r\n                index_name='embeddings',\r\n                string_factory=faiss_index_string,\r\n                train_size=config.faiss_train_size,\r\n                device=0,  # Use -1 for CPU, or specify GPU device ID\r\n                faiss_verbose=True\r\n            )\r\nTrainer(\r\n            model=model,\r\n            args=args,\r\n            train_dataset=train_dataset,\r\n            eval_dataset=eval_dataset,\r\n            data_collator=data_collator,\r\n            tokenizer=tokenizer\r\n        )\r\ntrain_dataset.get_nearest_examples_batch('embeddings', embeddings, k=self.k)\r\n```\n\n### Expected behavior\n\nI would expect the faiss database code to be gpu compatible \n\n### Environment info\n\nhuggingface Version: 2.16.1<end_of_turn>\n<start_of_turn>model\nFaiss GPU index cannot be serialised when passed to trainer <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen attempting to load a large dataset (like tiiuae/falcon-refinedweb or allenai/c4) using streaming=True in a standard Google Colab notebook, the process fails with a NotImplementedError: Loading a streaming dataset cached in a LocalFileSystem is not supported yet. This issue persists even after upgrading datasets and huggingface_hub and restarting the session.\n\n### Steps to reproduce the bug\n\nOpen a new Google Colab notebook.\n\n(Optional but recommended) Run !pip install --upgrade datasets huggingface_hub and restart the runtime.\n\nRun the following code:\n\nPython\n\nfrom datasets import load_dataset\n\ntry:\n    print(\"Attempting to load a stream...\")\n    streaming_dataset = load_dataset('tiiuae/falcon-refinedweb', streaming=True)\n    print(\"Success!\")\nexcept Exception as e:\n    print(e)\n\n### Expected behavior\n\nThe load_dataset command should return a StreamingDataset object without raising an error, allowing iteration over the dataset.\n\nActual Behavior\nThe code fails and prints the following error traceback:\n\n[PASTE THE FULL ERROR TRACEBACK HERE]\n(Note: Copy the entire error message you received, from Traceback... to the final error line, and paste it in this section.)\n\n### Environment info\n\nPlatform: Google Colab\n\ndatasets version: [Run !pip show datasets in Colab and paste the version here]\n\nhuggingface_hub version: [Run !pip show huggingface_hub and paste the version here]\n\nPython version: [Run !python --version and paste the version here]<end_of_turn>\n<start_of_turn>model\nNotImplementedError when using streaming=True in Google Colab environment<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ndataset viewer requires no-script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHi,\n\nCurrently datasets does not support RaggedTensor output on batch-level.\nWhen building a Object Detection Dataset (with TensorFlow) I need to enable RaggedTensors as that's how BBoxes & classes are expected from the Keras Model POV.\n\nCurrently there's a error thrown saying that \"Nested Data is not supported\".\n\nIt'd be very helpful if this was fixed! :)\n\n### Motivation\n\nEnabling Object Detection pipelines for TensorFlow.\n\n### Your contribution\n\nWith guidance I'd happily help making the PR.\n\nThe current implementation with DataCollator and later enforcing `np.array` is the problematic part (at the end of `np_get_batch` in `tf_utils.py`). As `numpy` don't support \"Raggednes\"<end_of_turn>\n<start_of_turn>model\nTensorFlow RaggedTensor Support (batch-level)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe link provided for the dataset is broken,\r\ndata_files = \r\n[https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst](url)\r\n\r\nThe \n\n### Steps to reproduce the bug\n\nSteps to reproduce:\r\n\r\n1) Head over to [https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt#big-data-datasets-to-the-rescue](url)\r\n\r\n2) In the Section  \"What is the Pile?\", you can see a code snippet that contains the broken link.\n\n### Expected behavior\n\nThe link should Redirect to the  \"PubMed Abstracts dataset\" as expected .\n\n### Environment info\n\n.<end_of_turn>\n<start_of_turn>model\nBroken Link to PubMed Abstracts dataset .<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi team!\r\n\r\nI observe that there seems to be a side effect of `Dataset.set_format`: after setting a format and creating a FAISS index, the method `get_nearest_examples` from the `Dataset` class, fails to retrieve anything else but the embeddings themselves - not super useful. This is not the case if not using the `set_format` method: you can also retrieve any other feature value, such as an index/id/etc.\r\n\r\nAre you able to reproduce what I observe?\n\n### Steps to reproduce the bug\n\n```python\r\n\r\nfrom datasets import Dataset\r\nimport numpy as np\r\n\r\nfoo = {'vectors': np.random.random((100,1024)), 'ids': [str(u) for u in range(100)]}\r\nfoo = Dataset.from_dict(foo)\r\nfoo.set_format('numpy', ['vectors'])\r\nfoo.add_faiss_index('vectors')\r\nnew_vector = np.random.random(1024)\r\nscores, res = foo.get_nearest_examples('vectors', new_vector, k=3)\r\n```\r\nThis will return, for the resulting most similar vectors to `new_vector` - in particular it will not return the `ids` feature:\r\n```\r\n{'vectors': array([[random values ...]])}\r\n```\n\n### Expected behavior\n\nThe expected behavior happens when the `set_format` method is not called:\r\n```python\r\n\r\nfrom datasets import Dataset\r\nimport numpy as np\r\n\r\nfoo = {'vectors': np.random.random((100,1024)), 'ids': [str(u) for u in range(100)]}\r\nfoo = Dataset.from_dict(foo)\r\n# foo.set_format('numpy', ['vectors'])\r\nfoo.add_faiss_index('vectors')\r\nnew_vector = np.random.random(1024)\r\nscores, res = foo.get_nearest_examples('vectors', new_vector, k=3)\r\n```\r\nThis *will*  return the `ids` of the similar vectors - with unfortunately a list of lists in lieu of the array I think for caching reasons - read it elsewhere\r\n```\r\n{'vectors': [[random values on multiple lines...]], 'ids': ['x', 'y', 'z']}\r\n```\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.4.0-155-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n<end_of_turn>\n<start_of_turn>model\nDataset.get_nearest_examples does not return all feature values for the k most similar datapoints - side effect of Dataset.set_format<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport JSON lines with missing columns.\r\n\r\nFix #7169.\r\n\r\nThe implemented test raised:\r\n```\r\ndatasets.table.CastError: Couldn't cast\r\nage: int64\r\nto\r\n{'age': Value(dtype='int32', id=None), 'name': Value(dtype='string', id=None)}\r\nbecause column names don't match\r\n```\r\n\r\nRelated to:\r\n- #7160 \r\n- #7162<end_of_turn>\n<start_of_turn>model\nSupport JSON lines with missing columns<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCannot load samples with optional fields from different files. The schema cannot be correctly derived.\n\n### Steps to reproduce the bug\n\nWhen I place two samples with an optional field `some_extra_field` within a single parquet file, it can be loaded via `load_dataset`.\n\n```python\nimport pandas as pd\nfrom datasets import load_dataset\n\ndata = [\n    {'conversations': {'role': 'user', 'content': 'hello'}},\n    {'conversations': {'role': 'user', 'content': 'hi', 'some_extra_field': 'some_value'}}\n]\ndf = pd.DataFrame(data)\ndf.to_parquet('data.parquet')\n\ndataset = load_dataset('parquet', data_files='data.parquet', split='train')\nprint(dataset.features)\n```\n\nThe schema can be derived. `some_extra_field` is set to None for the first row where it is absent.\n```\n{'conversations': {'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None), 'some_extra_field': Value(dtype='string', id=None)}}\n```\n\nHowever, when I separate the samples into different files, it cannot be loaded.\n```python\nimport pandas as pd\nfrom datasets import load_dataset\n\ndata1 = [{'conversations': {'role': 'user', 'content': 'hello'}}]\npd.DataFrame(data1).to_parquet('data1.parquet')\ndata2 = [{'conversations': {'role': 'user', 'content': 'hi', 'some_extra_field': 'some_value'}}]\npd.DataFrame(data2).to_parquet('data2.parquet')\n\ndataset = load_dataset('parquet', data_files=['data1.parquet', 'data2.parquet'], split='train')\nprint(dataset.features)\n```\n\nTraceback:\n```\nTraceback (most recent call last):\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/builder.py\", line 1854, in _prepare_split_single\n    for _, table in generator:\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 106, in _generate_tables\n    yield f\"{file_idx}_{batch_idx}\", self._cast_table(pa_table)\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 73, in _cast_table\n    pa_table = table_cast(pa_table, self.info.features.arrow_schema)\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2292, in table_cast\n    return cast_table_to_schema(table, schema)\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2245, in cast_table_to_schema\n    arrays = [\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2246, in <listcomp>\n    cast_array_to_feature(\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 1795, in wrapper\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 1795, in <listcomp>\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\n  File \"/home/tiger/.local/lib/python3.9/site-packages/datasets/table.py\", line 2108, in cast_array_to_feature\n    raise TypeError(f\"Couldn't cast array of type\\n{_short_str(array.type)}\\nto\\n{_short_str(feature)}\")\nTypeError: Couldn't cast array of type\nstruct<content: string, role: string, some_extra_field: string>\nto\n{'content': Value(dtype='string', id=None), 'role': Value(dtype='string', id=None)}\n```\n\n\n### Expected behavior\n\nCorrectly load data with optional fields from different parquet files.\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: Linux-5.10.135.bsk.4-amd64-x86_64-with-glibc2.31\n- Python version: 3.9.2\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 17.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nCannot load data with different schemas from different parquet files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nRelated to https://github.com/huggingface/transformers/issues/24441. Not sure if this is a tokenizer issue or caching issue, so filing in both.\r\n\r\nPassing the tokenizer to the dataset map function causes the tokenizer to be fingerprinted weirdly. After calling the tokenizer with arguments like padding and truncation the tokenizer object changes interanally, even though the hash remains the same.\r\n\r\nBut dumps is able to detect that internal change which causes the tokenizer object's fingerprint to change.\r\n\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nfrom transformers import AutoTokenizer\r\nfrom datasets.utils.py_utils import dumps # Huggingface datasets\r\n\r\nt = AutoTokenizer.from_pretrained('bert-base-uncased')\r\nt.save_pretrained(\"tok1\")\r\nth1 = hash(dumps(t))\r\ntext = \"This is an example text\"\r\nttext = t(text, max_length=512, padding=\"max_length\", truncation=True)\r\nt.save_pretrained(\"tok2\")\r\nth2 = hash(dumps(t))\r\n\r\nassert th1 == th2 # Assertion Error\r\n```\r\n\r\nBut if you use just the hash of the object without dumps, the hashes don't change\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\nfrom datasets.utils.py_utils import dumps # Huggingface datasets\r\n\r\nt = AutoTokenizer.from_pretrained('bert-base-uncased')\r\nth1 = hash(t)  # Just hash no dumps\r\ntext = \"This is an example text\"\r\nttext = t(text, max_length=512, padding=\"max_length\", truncation=True)\r\nth2 = hash(t) # Just hash no dumps\r\n\r\nassert th1 == th2 # This is OK\r\n```\r\n\r\nThis causes situations such as the following\r\n\r\n1. Create a text file like this `yes \"This is an example text\" | head -n 10000 > lines.txt`\r\n\r\n```python\r\nfrom transformers import AutoTokenizer\r\nimport datasets\r\n\r\n\r\nclass TokenizeMapper(object):\r\n    \"\"\"Mapper for tokenizer.\r\n\r\n    This is needed because the caching mechanism of HuggingFace does not work on\r\n    lambdas. Each time a new lambda will be created by a new process which will\r\n    lead to a different hash.\r\n    This way we can have a universal mapper object in init and reuse it with the same\r\n    hash for each process.\r\n    \"\"\"\r\n\r\n    def __init__(self, tokenizer):\r\n        \"\"\"Initialize the tokenizer.\"\"\"\r\n        self.tokenizer = tokenizer\r\n\r\n    def __call__(self, examples, **kwargs):\r\n        \"\"\"Run the mapper.\"\"\"\r\n        texts = examples[\"text\"]\r\n        tt = self.tokenizer(texts, max_length=256, padding=\"max_length\", truncation=True)\r\n        batch_outputs = {\r\n            \"input_ids\": tt.input_ids,\r\n            \"attention_mask\": tt.attention_mask,\r\n        }\r\n        return batch_outputs\r\n\r\n\r\nt = AutoTokenizer.from_pretrained('bert-base-uncased')\r\nmapper = TokenizeMapper(t)\r\n\r\nds = datasets.load_dataset(\"text\", data_files=\"lines.txt\")\r\n\r\nmds1 = ds.map(\r\n    mapper,\r\n    batched=False,\r\n    remove_columns=[\"text\"],\r\n).with_format(\"torch\")\r\n\r\nmds2 = ds.map(\r\n    mapper,\r\n    batched=False,\r\n    remove_columns=[\"text\"],\r\n).with_format(\"torch\")\r\n```\r\n\r\nThe second call to map should reuse the cached processed dataset from mds1, but it instead it redoes the tokenization because of the behavior of dumps.\n\n### Expected behavior\n\nWe should be able to initialize a tokenizer. And reusing it should let us reuse the same map computation for the same dataset.\r\n\r\nThe second call to map should reuse the cached processed dataset from mds1, but it instead it redoes the tokenization because of the behavior of dumps.\n\n### Environment info\n\n- `datasets` version: 2.13.0\r\n- Platform: Linux-6.1.31_1-x86_64-with-glibc2.36\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2<end_of_turn>\n<start_of_turn>model\nCannot reuse tokenizer object for dataset map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRe-enable raising error from huggingface-hub FutureWarning in tests, once that the fix in transformers\r\n- https://github.com/huggingface/transformers/pull/31007\r\n\r\nwas just released yesterday in transformers-4.42.0: https://github.com/huggingface/transformers/releases/tag/v4.42.0\r\n\r\n\r\nFix #7010.<end_of_turn>\n<start_of_turn>model\nRe-enable raising error from huggingface-hub FutureWarning in CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWith this pr https://github.com/huggingface/datasets/pull/6309, it is formatting the entire base path into regex, which results in the undesired formatting error `doesn't match the pattern` because of the line in `glob_pattern_to_regex`: `.replace(\"//\", \"/\")`:\r\n- Input: `hf://datasets/...`\r\n- Output: `hf:/datasets/...`\r\n\r\nThis fix will only convert the `split_pattern` to regex and keep the `base_path` unchanged.\r\n\r\ncc @albertvillanova hopefully this still works with your implementation<end_of_turn>\n<start_of_turn>model\nFix regex `get_data_files` formatting for base paths<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen load a dataset from datasets and pass a wrong path to json with the data, error message does not contain something abount \"wrong path\" or \"file do not exist\"  - \r\n```SchemaInferenceError: Please pass `features` or at least one example when writing data``` \r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nload_dataset('json', data_files='/home/alexey/unreal_file.json')\r\n```\r\n\r\n### Expected behavior\r\n\r\nRaise os FileNotFound error or custom error with informative message\r\n\r\n### Environment info\r\n```\r\n# packages in environment at /home/alexey/.conda/envs/alex_LoRA:\r\n#\r\n# Name                    Version                   Build  Channel\r\n_libgcc_mutex             0.1                        main  \r\n_openmp_mutex             5.1                       1_gnu  \r\naccelerate                0.21.0                   pypi_0    pypi\r\naiohttp                   3.8.5                    pypi_0    pypi\r\naiosignal                 1.3.1                    pypi_0    pypi\r\nantlr4-python3-runtime    4.9.3                    pypi_0    pypi\r\nappdirs                   1.4.4                    pypi_0    pypi\r\nasttokens                 2.0.5              pyhd3eb1b0_0  \r\nasync-timeout             4.0.3                    pypi_0    pypi\r\nattrs                     23.1.0                   pypi_0    pypi\r\nbackcall                  0.2.0              pyhd3eb1b0_0  \r\nbitsandbytes              0.41.1                   pypi_0    pypi\r\nbzip2                     1.0.8                h7b6447c_0  \r\nca-certificates           2023.05.30           h06a4308_0  \r\ncertifi                   2023.7.22                pypi_0    pypi\r\ncharset-normalizer        3.2.0                    pypi_0    pypi\r\nclick                     8.1.6                    pypi_0    pypi\r\ncmake                     3.27.2                   pypi_0    pypi\r\ncomm                      0.1.2           py310h06a4308_0  \r\ncontourpy                 1.1.0                    pypi_0    pypi\r\ncycler                    0.11.0                   pypi_0    pypi\r\ndatasets                  2.14.4                   pypi_0    pypi\r\ndebugpy                   1.6.7           py310h6a678d5_0  \r\ndecorator                 5.1.1              pyhd3eb1b0_0  \r\ndill                      0.3.7                    pypi_0    pypi\r\ndocker-pycreds            0.4.0                    pypi_0    pypi\r\nexecuting                 0.8.3              pyhd3eb1b0_0  \r\nfilelock                  3.12.2                   pypi_0    pypi\r\nfire                      0.5.0                    pypi_0    pypi\r\nfonttools                 4.42.0                   pypi_0    pypi\r\nfrozenlist                1.4.0                    pypi_0    pypi\r\nfsspec                    2023.6.0                 pypi_0    pypi\r\ngitdb                     4.0.10                   pypi_0    pypi\r\ngitpython                 3.1.32                   pypi_0    pypi\r\nhuggingface-hub           0.16.4                   pypi_0    pypi\r\nidna                      3.4                      pypi_0    pypi\r\nipykernel                 6.25.0          py310h2f386ee_0  \r\nipython                   8.12.2          py310h06a4308_0  \r\nipython-genutils          0.2.0                    pypi_0    pypi\r\nipywidgets                8.0.4           py310h06a4308_0  \r\njedi                      0.18.1          py310h06a4308_1  \r\njinja2                    3.1.2                    pypi_0    pypi\r\njsonschema                4.19.0                   pypi_0    pypi\r\njsonschema-specifications 2023.7.1                 pypi_0    pypi\r\njupyter_client            8.1.0           py310h06a4308_0  \r\njupyter_core              5.3.0           py310h06a4308_0  \r\njupyterlab_widgets        3.0.5           py310h06a4308_0  \r\nkiwisolver                1.4.4                    pypi_0    pypi\r\nld_impl_linux-64          2.38                 h1181459_1  \r\nlibffi                    3.3                  he6710b0_2  \r\nlibgcc-ng                 11.2.0               h1234567_1  \r\nlibgomp                   11.2.0               h1234567_1  \r\nlibsodium                 1.0.18               h7b6447c_0  \r\nlibstdcxx-ng              11.2.0               h1234567_1  \r\nlibuuid                   1.41.5               h5eee18b_0  \r\nlightning-utilities       0.9.0                    pypi_0    pypi\r\nlit                       16.0.6                   pypi_0    pypi\r\nmarkupsafe                2.1.3                    pypi_0    pypi\r\nmatplotlib                3.7.2                    pypi_0    pypi\r\nmatplotlib-inline         0.1.6           py310h06a4308_0  \r\nmpmath                    1.3.0                    pypi_0    pypi\r\nmultidict                 6.0.4                    pypi_0    pypi\r\nmultiprocess              0.70.15                  pypi_0    pypi\r\nnbformat                  4.2.0                    pypi_0    pypi\r\nncurses                   6.4                  h6a678d5_0  \r\nnest-asyncio              1.5.6           py310h06a4308_0  \r\nnetworkx                  3.1                      pypi_0    pypi\r\nnumpy                     1.25.2                   pypi_0    pypi\r\nnvidia-cublas-cu11        11.10.3.66               pypi_0    pypi\r\nnvidia-cuda-cupti-cu11    11.7.101                 pypi_0    pypi\r\nnvidia-cuda-nvrtc-cu11    11.7.99                  pypi_0    pypi\r\nnvidia-cuda-runtime-cu11  11.7.99                  pypi_0    pypi\r\nnvidia-cudnn-cu11         8.5.0.96                 pypi_0    pypi\r\nnvidia-cufft-cu11         10.9.0.58                pypi_0    pypi\r\nnvidia-curand-cu11        10.2.10.91               pypi_0    pypi\r\nnvidia-cusolver-cu11      11.4.0.1                 pypi_0    pypi\r\nnvidia-cusparse-cu11      11.7.4.91                pypi_0    pypi\r\nnvidia-nccl-cu11          2.14.3                   pypi_0    pypi\r\nnvidia-nvtx-cu11          11.7.91                  pypi_0    pypi\r\nomegaconf                 2.3.0                    pypi_0    pypi\r\nopenssl                   1.1.1v               h7f8727e_0  \r\npackaging                 23.0            py310h06a4308_0  \r\npandas                    2.0.3                    pypi_0    pypi\r\nparso                     0.8.3              pyhd3eb1b0_0  \r\npathtools                 0.1.2                    pypi_0    pypi\r\npeft                      0.4.0                    pypi_0    pypi\r\npexpect                   4.8.0              pyhd3eb1b0_3  \r\npickleshare               0.7.5           pyhd3eb1b0_1003  \r\npillow                    10.0.0                   pypi_0    pypi\r\npip                       23.2.1          py310h06a4308_0  \r\nplatformdirs              2.5.2           py310h06a4308_0  \r\nplotly                    5.16.1                   pypi_0    pypi\r\nprompt-toolkit            3.0.36          py310h06a4308_0  \r\nprotobuf                  4.24.0                   pypi_0    pypi\r\npsutil                    5.9.0           py310h5eee18b_0  \r\nptyprocess                0.7.0              pyhd3eb1b0_2  \r\npure_eval                 0.2.2              pyhd3eb1b0_0  \r\npyarrow                   12.0.1                   pypi_0    pypi\r\npygments                  2.15.1          py310h06a4308_1  \r\npyparsing                 3.0.9                    pypi_0    pypi\r\npython                    3.10.0               h12debd9_5  \r\npython-dateutil           2.8.2              pyhd3eb1b0_0  \r\npytorch-lightning         2.0.6                    pypi_0    pypi\r\npytz                      2023.3                   pypi_0    pypi\r\npyyaml                    6.0.1                    pypi_0    pypi\r\npyzmq                     25.1.0          py310h6a678d5_0  \r\nreadline                  8.2                  h5eee18b_0  \r\nreferencing               0.30.2                   pypi_0    pypi\r\nregex                     2023.8.8                 pypi_0    pypi\r\nrequests                  2.31.0                   pypi_0    pypi\r\nrpds-py                   0.9.2                    pypi_0    pypi\r\nsafetensors               0.3.2                    pypi_0    pypi\r\nscipy                     1.11.1                   pypi_0    pypi\r\nsentencepiece             0.1.99                   pypi_0    pypi\r\nsentry-sdk                1.29.2                   pypi_0    pypi\r\nsetproctitle              1.3.2                    pypi_0    pypi\r\nsetuptools                68.0.0          py310h06a4308_0  \r\nsix                       1.16.0             pyhd3eb1b0_1  \r\nsmmap                     5.0.0                    pypi_0    pypi\r\nsqlite                    3.41.2               h5eee18b_0  \r\nstack_data                0.2.0              pyhd3eb1b0_0  \r\nsympy                     1.12                     pypi_0    pypi\r\ntenacity                  8.2.3                    pypi_0    pypi\r\ntermcolor                 2.3.0                    pypi_0    pypi\r\ntk                        8.6.12               h1ccaba5_0  \r\ntokenizers                0.13.3                   pypi_0    pypi\r\ntorch                     2.0.1                    pypi_0    pypi\r\ntorchmetrics              1.0.3                    pypi_0    pypi\r\ntornado                   6.3.2           py310h5eee18b_0  \r\ntqdm                      4.66.1                   pypi_0    pypi\r\ntraitlets                 5.7.1           py310h06a4308_0  \r\ntransformers              4.31.0                   pypi_0    pypi\r\ntriton                    2.0.0                    pypi_0    pypi\r\ntyping-extensions         4.7.1                    pypi_0    pypi\r\ntzdata                    2023.3                   pypi_0    pypi\r\nurllib3                   2.0.4                    pypi_0    pypi\r\nwandb                     0.15.8                   pypi_0    pypi\r\nwcwidth                   0.2.5              pyhd3eb1b0_0  \r\nwheel                     0.38.4          py310h06a4308_0  \r\nwidgetsnbextension        4.0.5           py310h06a4308_0  \r\nxxhash                    3.3.0                    pypi_0    pypi\r\nxz                        5.4.2                h5eee18b_0  \r\nyarl                      1.9.2                    pypi_0    pypi\r\nzeromq                    4.3.4                h2531618_0  \r\nzlib                      1.2.13               h5eee18b_0  \r\n    active environment : None\r\n       user config file : /home/alexey/.condarc\r\n populated config files : \r\n          conda version : 23.1.0\r\n    conda-build version : 3.22.0\r\n         python version : 3.9.13.final.0\r\n       virtual packages : __archspec=1=x86_64\r\n                          __cuda=12.0=0\r\n                          __glibc=2.35=0\r\n                          __linux=5.19.0=0\r\n                          __unix=0=0\r\n       base environment : /opt/anaconda/anaconda3  (read only)\r\n      conda av data dir : /opt/anaconda/anaconda3/etc/conda\r\n  conda av metadata url : None\r\n           channel URLs : https://repo.anaconda.com/pkgs/main/linux-64\r\n                          https://repo.anaconda.com/pkgs/main/noarch\r\n                          https://repo.anaconda.com/pkgs/r/linux-64\r\n                          https://repo.anaconda.com/pkgs/r/noarch\r\n          package cache : /opt/anaconda/anaconda3/pkgs\r\n                          /home/alexey/.conda/pkgs\r\n       envs directories : /home/alexey/.conda/envs\r\n                          /opt/anaconda/anaconda3/envs\r\n               platform : linux-64\r\n             user-agent : conda/23.1.0 requests/2.31.0 CPython/3.9.13 Linux/5.19.0-46-generic ubuntu/22.04.2 glibc/2.35\r\n                UID:GID : 1009:1009\r\n             netrc file : /home/alexey/.netrc\r\n           offline mode : False\r\n```<end_of_turn>\n<start_of_turn>model\nLoad dataset with  non-existent file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nWhenever I `pip install datasets[jax]` it downgrades jax to version 0.3.25. I seem to be able to install this library first then upgrade jax back to version 0.4.13.\r\n\r\n### Motivation\r\n\r\nIt would be nice to not overwrite currently installed version of jax if possible.\r\n\r\n### Your contribution\r\n\r\nI would be willing to beta test. Or maybe write some code if I could get pointed in the right direction, I'm not super familiar with this codebase.<end_of_turn>\n<start_of_turn>model\navoid downgrading jax version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\n\r\nb = load_dataset_builder(\"lambdalabs/pokemon-blip-captions\", \"default\")\r\nprint(b.info)\r\n```\r\n\r\nwhich should return\r\n\r\n```\r\nDatasetInfo(\r\n  features={'image': Image(decode=True, id=None), 'text': Value(dtype='string', id=None)}, \r\n  dataset_name='pokemon-blip-captions',\r\n  config_name='default', \r\n  version=0.0.0,\r\n  splits={'train': SplitInfo(name='train', num_bytes=119417410.0, num_examples=833, shard_lengths=None, dataset_name='pokemon-blip-captions')},\r\n  download_checksums=None,\r\n  download_size=99672355,\r\n  dataset_size=119417410.0,\r\n  size_in_bytes=219089765.0,\r\n  ...\r\n)\r\n```\r\n\r\ninstead of and empty dataset info.\r\n\r\nThe dataset has a dataset_infos.json file with a deprecated config name \"lambdalabs--pokemon-blip-captions\". We switched those config names to \"default\" in 2.14, so the builder.info should take this into account.<end_of_turn>\n<start_of_turn>model\nRename old push_to_hub configs to \"default\" in dataset_infos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI was preparing some datasets for AI training and noticed that `datasets` by HuggingFace uses the conventional `open` mechanism to read the file and split it into chunks. I thought it can be significantly accelerated, and [started with a benchmark](https://gist.github.com/ashvardanian/55c2052e9f78b05b8d614aa90cb12347):\r\n\r\n```sh\r\n$ pip install --upgrade --force-reinstall datasets\r\n$ python benchmark_huggingface_datasets.py xlsum.csv \r\nGenerating train split: 1004598 examples [00:47, 21116.16 examples/s]\r\nTime taken to load the dataset: 48.66838526725769 seconds\r\nTime taken to chunk the dataset into parts of size 10000: 0.11466407775878906 seconds\r\nTotal time taken: 48.78304934501648 seconds\r\n```\r\n\r\nFor benchmarks I've used a [large CSV file with mixed UTF-8 content](https://github.com/ashvardanian/StringZilla/blob/main/CONTRIBUTING.md#benchmarking-datasets), most common in modern large-scale pre-training pipelines. I've later patched the `datasets` library to use `stringzilla`, which resulted in significantly lower memory consumption and in 2.9x  throughput improvement on the AWS `r7iz` instances. That's using slow SSDs mounted over the network. Performance on local SSDs on something like a DGX-H100 should be even higher:\r\n\r\n```sh\r\n$ pip install -e .\r\n$ python benchmark_huggingface_datasets.py xlsum.csv \r\nGenerating train split: 1004598 examples [00:15, 64529.90 examples/s]\r\nTime taken to load the dataset: 16.45028805732727 seconds\r\nTime taken to chunk the dataset into parts of size 10000: 0.1291060447692871 seconds\r\nTotal time taken: 16.579394102096558 seconds\r\n```\r\n\r\nI've already [pushed the patches to my fork](https://github.com/ashvardanian/datasets/tree/faster-text-parsers), and would love to contribute them to the upstream repository.\r\n\r\n---\r\n\r\nAll the tests pass, but they leave a couple of important questions open. The default Python `open(..., newline=None)` uses universal newlines, where `\\n`, `\\r`, and `\\r\\n` are all converted to `\\n` on the fly. I am not sure if its a good idea for a general purpose dataset preparation pipeline?\r\n\r\nI can simulate the same behavior (which I don't yet do) for `\"line\"` splitter. Adjusting it for `\"paragraph\"`-splitter would be harder. Should we stick exactly to the old Pythonic behavior or stay closer to how C and other programming languages do that?<end_of_turn>\n<start_of_turn>model\n3x Faster Text Preprocessing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI'm using to_tf_dataset to convert a large dataset to tf.data.Dataset and use Keras fit to train model. \r\nCurrently, to_tf_dataset only supports full size shuffle, which can be very slow on large dataset. \r\ntf.data.Dataset support buffer shuffle by default. \r\nshuffle(\r\n    buffer_size, seed=None, reshuffle_each_iteration=None, name=None\r\n)\n\n### Motivation\n\nI'm very frustrated to find the loading with shuffling large dataset is very slow. It seems impossible to shuffle before training Keras with big dataset. \n\n### Your contribution\n\nNA<end_of_turn>\n<start_of_turn>model\nSupport buffer shuffle for to_tf_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCannot install `datasets` under `python 3.13t` due to dependency on `aiohttp` and aiohttp cannot be built for free-threading python. \n\nThe `free threading` support issue in `aiothttp` is active since August 2024! Ouch.\n\nhttps://github.com/aio-libs/aiohttp/issues/8796#issue-2475941784\n\n`pip install dataset`\n\n```bash\n(vm313t) root@gpu-base:~/GPTQModel# pip install datasets\nWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/datasets/                                                                                                                                             \nCollecting datasets                                                                                                                                                                          \n  Using cached datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\nRequirement already satisfied: filelock in /root/vm313t/lib/python3.13t/site-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (2.2.5)\nCollecting pyarrow>=15.0.0 (from datasets)\n  Using cached pyarrow-20.0.0-cp313-cp313t-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\nCollecting dill<0.3.9,>=0.3.0 (from datasets)\n  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\nCollecting pandas (from datasets)\n  Using cached pandas-2.2.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\nRequirement already satisfied: requests>=2.32.2 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (4.67.1)\nCollecting xxhash (from datasets)\n  Using cached xxhash-3.5.0-cp313-cp313t-linux_x86_64.whl\nCollecting multiprocess<0.70.17 (from datasets)\n  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nCollecting aiohttp (from datasets)\n  Using cached aiohttp-3.11.18.tar.gz (7.7 MB)\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n  Preparing metadata (pyproject.toml) ... done\nRequirement already satisfied: huggingface-hub>=0.24.0 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /root/vm313t/lib/python3.13t/site-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /root/vm313t/lib/python3.13t/site-packages (from datasets) (6.0.2)\nCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nCollecting aiosignal>=1.1.2 (from aiohttp->datasets)\n  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\nCollecting attrs>=17.3.0 (from aiohttp->datasets)\n  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting frozenlist>=1.1.1 (from aiohttp->datasets)\n  Using cached frozenlist-1.6.0-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n  Using cached multidict-6.4.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp->datasets)\n  Using cached propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n  Using cached yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (72 kB)\nRequirement already satisfied: idna>=2.0 in /root/vm313t/lib/python3.13t/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets) (3.10)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /root/vm313t/lib/python3.13t/site-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /root/vm313t/lib/python3.13t/site-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /root/vm313t/lib/python3.13t/site-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /root/vm313t/lib/python3.13t/site-packages (from requests>=2.32.2->datasets) (2025.4.26)\nCollecting python-dateutil>=2.8.2 (from pandas->datasets)\n  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\nCollecting pytz>=2020.1 (from pandas->datasets)\n  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\nCollecting tzdata>=2022.7 (from pandas->datasets)\n  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->datasets)\n  Using cached six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\nUsing cached datasets-3.5.1-py3-none-any.whl (491 kB)\nUsing cached dill-0.3.8-py3-none-any.whl (116 kB)\nUsing cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\nUsing cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\nUsing cached multidict-6.4.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\nUsing cached yarl-1.20.0-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (404 kB)\nUsing cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nUsing cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nUsing cached attrs-25.3.0-py3-none-any.whl (63 kB)\nUsing cached frozenlist-1.6.0-cp313-cp313t-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (385 kB)\nUsing cached propcache-0.3.1-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\nUsing cached pyarrow-20.0.0-cp313-cp313t-manylinux_2_28_x86_64.whl (42.2 MB)\nUsing cached pandas-2.2.3-cp313-cp313t-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.9 MB)\nUsing cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\nUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\nUsing cached six-1.17.0-py2.py3-none-any.whl (11 kB)\nUsing cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\nBuilding wheels for collected packages: aiohttp\n  Building wheel for aiohttp (pyproject.toml) ... error\n  error: subprocess-exited-with-error\n  \n  \u00d7 Building wheel for aiohttp (pyproject.toml) did not run successfully.\n  \u2502 exit code: 1\n  \u2570\u2500> [156 lines of output]\n      *********************\n      * Accelerated build *\n      *********************\n      /tmp/pip-build-env-wjqi8_7w/overlay/lib/python3.13t/site-packages/setuptools/dist.py:759: SetuptoolsDeprecationWarning: License classifiers are deprecated.\n      !!\n      \n              ********************************************************************************\n              Please consider removing the following classifiers in favor of a SPDX license expression:\n      \n              License :: OSI Approved :: Apache Software License\n      \n              See https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license for details.\n              ********************************************************************************\n      \n      !!\n        self._finalize_license_expression()\n      running bdist_wheel\n      running build\n      running build_py\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/typedefs.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_parser.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_reqrep.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_ws.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_app.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_websocket.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/resolver.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/tracing.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_writer.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http_exceptions.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/log.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/__init__.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_runner.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/worker.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/connector.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_exceptions.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_middlewares.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/tcp_helpers.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_response.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_server.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_request.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_urldispatcher.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_exceptions.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/formdata.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/streams.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/multipart.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_routedef.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_ws.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/payload.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client_proto.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_log.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/base_protocol.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/payload_streamer.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/http.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_fileresponse.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/test_utils.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/client.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/cookiejar.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/compression_utils.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/hdrs.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/helpers.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/pytest_plugin.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/web_protocol.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/abc.py -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/__init__.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/writer.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/models.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader_c.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/helpers.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader_py.py -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      running egg_info\n      writing aiohttp.egg-info/PKG-INFO\n      writing dependency_links to aiohttp.egg-info/dependency_links.txt\n      writing requirements to aiohttp.egg-info/requires.txt\n      writing top-level names to aiohttp.egg-info/top_level.txt\n      reading manifest file 'aiohttp.egg-info/SOURCES.txt'\n      reading manifest template 'MANIFEST.in'\n      warning: no files found matching 'aiohttp' anywhere in distribution\n      warning: no files found matching '*.pyi' anywhere in distribution\n      warning: no previously-included files matching '*.pyc' found anywhere in distribution\n      warning: no previously-included files matching '*.pyd' found anywhere in distribution\n      warning: no previously-included files matching '*.so' found anywhere in distribution\n      warning: no previously-included files matching '*.lib' found anywhere in distribution\n      warning: no previously-included files matching '*.dll' found anywhere in distribution\n      warning: no previously-included files matching '*.a' found anywhere in distribution\n      warning: no previously-included files matching '*.obj' found anywhere in distribution\n      warning: no previously-included files found matching 'aiohttp/*.html'\n      no previously-included directories found matching 'docs/_build'\n      adding license file 'LICENSE.txt'\n      writing manifest file 'aiohttp.egg-info/SOURCES.txt'\n      copying aiohttp/_cparser.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_find_header.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_headers.pxi -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_http_parser.pyx -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/_http_writer.pyx -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      copying aiohttp/py.typed -> build/lib.linux-x86_64-cpython-313t/aiohttp\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_cparser.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_find_header.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_http_parser.pyx.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/_http_writer.pyx.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/.hash/hdrs.py.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/.hash\n      copying aiohttp/_websocket/mask.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/mask.pyx -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      copying aiohttp/_websocket/reader_c.pxd -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket\n      creating build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      copying aiohttp/_websocket/.hash/mask.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      copying aiohttp/_websocket/.hash/mask.pyx.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      copying aiohttp/_websocket/.hash/reader_c.pxd.hash -> build/lib.linux-x86_64-cpython-313t/aiohttp/_websocket/.hash\n      running build_ext\n      building 'aiohttp._websocket.mask' extension\n      creating build/temp.linux-x86_64-cpython-313t/aiohttp/_websocket\n      x86_64-linux-gnu-gcc -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fno-omit-frame-pointer -mno-omit-leaf-frame-pointer -fstack-protector-strong -fstack-clash-protection -Wformat -Werror=format-security -fcf-protection -fPIC -I/root/vm313t/include -I/usr/include/python3.13t -c aiohttp/_websocket/mask.c -o build/temp.linux-x86_64-cpython-313t/aiohttp/_websocket/mask.o\n      aiohttp/_websocket/mask.c:1864:80: error: unknown type name \u2018__pyx_vectorcallfunc\u2019; did you mean \u2018vectorcallfunc\u2019?\n       1864 | static CYTHON_INLINE PyObject *__Pyx_PyVectorcall_FastCallDict(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw);\n            |                                                                                ^~~~~~~~~~~~~~~~~~~~\n            |                                                                                vectorcallfunc\n      aiohttp/_websocket/mask.c: In function \u2018__pyx_f_7aiohttp_10_websocket_4mask__websocket_mask_cython\u2019:\n      aiohttp/_websocket/mask.c:2905:3: warning: \u2018Py_OptimizeFlag\u2019 is deprecated [-Wdeprecated-declarations]\n       2905 |   if (unlikely(__pyx_assertions_enabled())) {\n            |   ^~\n      In file included from /usr/include/python3.13t/Python.h:76,\n                       from aiohttp/_websocket/mask.c:16:\n      /usr/include/python3.13t/cpython/pydebug.h:13:37: note: declared here\n         13 | Py_DEPRECATED(3.12) PyAPI_DATA(int) Py_OptimizeFlag;\n            |                                     ^~~~~~~~~~~~~~~\n      aiohttp/_websocket/mask.c: At top level:\n      aiohttp/_websocket/mask.c:4846:69: error: unknown type name \u2018__pyx_vectorcallfunc\u2019; did you mean \u2018vectorcallfunc\u2019?\n       4846 | static PyObject *__Pyx_PyVectorcall_FastCallDict_kw(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw)\n            |                                                                     ^~~~~~~~~~~~~~~~~~~~\n            |                                                                     vectorcallfunc\n      aiohttp/_websocket/mask.c:4891:80: error: unknown type name \u2018__pyx_vectorcallfunc\u2019; did you mean \u2018vectorcallfunc\u2019?\n       4891 | static CYTHON_INLINE PyObject *__Pyx_PyVectorcall_FastCallDict(PyObject *func, __pyx_vectorcallfunc vc, PyObject *const *args, size_t nargs, PyObject *kw)\n            |                                                                                ^~~~~~~~~~~~~~~~~~~~\n            |                                                                                vectorcallfunc\n      aiohttp/_websocket/mask.c: In function \u2018__Pyx_CyFunction_CallAsMethod\u2019:\n      aiohttp/_websocket/mask.c:5580:6: error: unknown type name \u2018__pyx_vectorcallfunc\u2019; did you mean \u2018vectorcallfunc\u2019?\n       5580 |      __pyx_vectorcallfunc vc = __Pyx_CyFunction_func_vectorcall(cyfunc);\n            |      ^~~~~~~~~~~~~~~~~~~~\n            |      vectorcallfunc\n      aiohttp/_websocket/mask.c:1954:45: warning: initialization of \u2018int\u2019 from \u2018vectorcallfunc\u2019 {aka \u2018struct _object * (*)(struct _object *, struct _object * const*, long unsigned int,  struct _object *)\u2019} makes integer from pointer without a cast [-Wint-conversion]\n       1954 | #define __Pyx_CyFunction_func_vectorcall(f) (((PyCFunctionObject*)f)->vectorcall)\n            |                                             ^\n      aiohttp/_websocket/mask.c:5580:32: note: in expansion of macro \u2018__Pyx_CyFunction_func_vectorcall\u2019\n       5580 |      __pyx_vectorcallfunc vc = __Pyx_CyFunction_func_vectorcall(cyfunc);\n            |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      aiohttp/_websocket/mask.c:5583:16: warning: implicit declaration of function \u2018__Pyx_PyVectorcall_FastCallDict\u2019 [-Wimplicit-function-declaration]\n       5583 |         return __Pyx_PyVectorcall_FastCallDict(func, vc, &PyTuple_GET_ITEM(args, 0), (size_t)PyTuple_GET_SIZE(args), kw);\n            |                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      aiohttp/_websocket/mask.c:5583:16: warning: returning \u2018int\u2019 from a function with return type \u2018PyObject *\u2019 {aka \u2018struct _object *\u2019} makes pointer from integer without a cast [-Wint-conversion]\n       5583 |         return __Pyx_PyVectorcall_FastCallDict(func, vc, &PyTuple_GET_ITEM(args, 0), (size_t)PyTuple_GET_SIZE(args), kw);\n            |                ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n      error: command '/usr/bin/x86_64-linux-gnu-gcc' failed with exit code 1\n      [end of output]\n  \n  note: This error originates from a subprocess, and is likely not a problem with pip.\n  ERROR: Failed building wheel for aiohttp\nFailed to build aiohttp                                                                                                                                                                      \nERROR: Failed to build installable wheels for some pyproject.toml based projects (aiohttp)\n```\n\n### Steps to reproduce the bug\n\nSee above\n\n### Expected behavior\n\nInstall\n\n### Environment info\n\nUbuntu 24.04<end_of_turn>\n<start_of_turn>model\nPython 3.13t (free threads) Compat<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTODO:\r\n- [x] fix tests<end_of_turn>\n<start_of_turn>model\nRemove default `trust_remote_code=True`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...for datasets without scripts\r\n\r\nIt takes into account the changes in cache from\r\n- https://github.com/huggingface/datasets/pull/6493: switch to `config/version/commit_sha` schema\r\n- https://github.com/huggingface/datasets/pull/6454: fix `DataFilesDict` keys ordering when hashing\r\n\r\nrequires https://github.com/huggingface/datasets/pull/6493 to be merged<end_of_turn>\n<start_of_turn>model\nCache backward compatibility with 2.15.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen parquet files are saved in \"train\" and \"val\" subdirectories under a root directory, and datasets are then loaded using `load_dataset(\"parquet\", data_dir=\"root_directory\")`, the resulting dataset has duplicated rows for both the training and validation sets.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Create a root directory, e.g., \"testing123\".\r\n2. Under \"testing123\", create two subdirectories: \"train\" and \"val\".\r\n3. Create and save a parquet file with 3 unique rows in the \"train\" subdirectory.\r\n4. Create and save a parquet file with 4 unique rows in the \"val\" subdirectory.\r\n5. Load the datasets from the root directory using `load_dataset(\"parquet\", data_dir=\"testing123\")`\r\n6. Iterate through the datasets and print the rows\r\n\r\nHere's a collab reproducing these steps:\r\n\r\nhttps://colab.research.google.com/drive/11NEdImnQ3OqJlwKSHRMhr7jCBesNdLY4?usp=sharing\r\n\r\n### Expected behavior\r\n\r\n- Training set should contain 3 unique rows.\r\n- Validation set should contain 4 unique rows.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.2\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nDuplicated Rows When Loading Parquet Files from Root Directory with Subdirectories<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSome datasets may have unexpectedly long features/types (e.g. if the files are not formatted correctly).\r\n\r\nIn that case we should still be able to log something readable<end_of_turn>\n<start_of_turn>model\nShorten long logs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdates the `wmt_t2t` test to pin the `revision` to the version with a loading script (cc @albertvillanova). \r\n\r\nAdditionally, it replaces the occurrences of the `lhoestq/test` repo id with `hf-internal-testing/dataset_with_script` and re-enables logging checks in the `Dataset.from_sql` tests.<end_of_turn>\n<start_of_turn>model\nFix CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHey there,\r\nI\u2019m using Dataset.from_generator() to convert a torch_dataset to the Huggingface Dataset.\r\nHowever, when I debug my code on vscode, I find that it runs really slow on Dataset.from_generator() which may even 20 times longer then run the script on terminal.\n\n### Steps to reproduce the bug\n\nI write a simple test code :\r\n\r\n```python\r\nimport os\r\nfrom functools import partial\r\nfrom typing import Callable\r\n\r\nimport torch\r\nimport time\r\nfrom torch.utils.data import Dataset as TorchDataset\r\n\r\nfrom datasets import load_from_disk, Dataset as HFDataset\r\n  \r\nimport torch  \r\nfrom torch.utils.data import Dataset  \r\n  \r\nclass SimpleDataset(Dataset):  \r\n    def __init__(self, data):  \r\n        self.data = data  \r\n        self.keys = list(data[0].keys())\r\n      \r\n    def __len__(self):  \r\n        return len(self.data)  \r\n      \r\n    def __getitem__(self, index):  \r\n        sample = self.data[index]  \r\n        return {key: sample[key] for key in self.keys}  \r\n  \r\n\r\ndef TorchDataset2HuggingfaceDataset(torch_dataset: TorchDataset, cache_dir: str = None\r\n) -> HFDataset:\r\n    \r\n    \"\"\"\r\n        convert torch dataset to huggingface dataset\r\n    \"\"\"\r\n    generator : Callable[[], TorchDataset] = lambda: (sample for sample in torch_dataset)   \r\n\r\n    return HFDataset.from_generator(generator, cache_dir=cache_dir)\r\n\r\nif __name__ == '__main__':\r\n    data = [  \r\n        {'id': 1, 'name': 'Alice'},  \r\n        {'id': 2, 'name': 'Bob'},  \r\n        {'id': 3, 'name': 'Charlie'}  \r\n    ]\r\n    \r\n    torch_dataset = SimpleDataset(data)\r\n    start_time = time.time() \r\n    huggingface_dataset = TorchDataset2HuggingfaceDataset(torch_dataset)\r\n    end_time = time.time()\r\n    print(\"time: \", end_time - start_time)\r\n    print(huggingface_dataset)\r\n```\n\n### Expected behavior\n\nthis test on my machine report that the running time on terminal is 0.086,\r\nhowever the running time in debugging mode on vscode is 0.25, which I think is much longer than expected.\r\n\r\nI\u2019d like to know is the anything wrong in the code or just because of debugging?\r\nI have traced the code and I find is this func which I get stuck.\r\n\r\n```python\r\ndef create_config_id(\r\n        self,\r\n        config_kwargs: dict,\r\n        custom_features: Optional[Features] = None,\r\n    ) -> str:\r\n...\r\n# stuck in this line\r\nsuffix = Hasher.hash(config_kwargs_to_add_to_suffix)\r\n```\r\n\r\n\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.3\r\n- Huggingface_hub version: 0.17.2\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.1<end_of_turn>\n<start_of_turn>model\nDataset.from_generator() cost much more time in vscode debugging mode then running mode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI am using Amazon Sagemaker notebook (Amazon Linux 2) with python 3.10 based Conda environment.\r\nI have a dataset in parquet format locally. When I try to iterate over it, the loader is stuck forever. Note that the same code is working for python 3.6 based conda environment seamlessly. What should be my next steps here?\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\ntrain_dataset = load_dataset(\r\n    \"parquet\", data_files = {'train': tr_data_path + '*.parquet'}, \r\n    split = 'train', \r\n    collate_fn = streaming_data_collate_fn,\r\n    streaming = True\r\n).with_format('torch')\r\n\r\ntrain_dataloader = DataLoader(train_dataset, batch_size = 2, num_workers = 0)\r\n\r\nt = time.time()\r\niter_ = 0\r\nfor batch in train_dataloader:\r\n    iter_ += 1\r\n    \r\n    if iter_ == 1000:\r\n        break\r\n        \r\nprint (time.time() - t) \r\n```\r\n\r\n### Expected behavior\r\n\r\nThe snippet should work normally and load the next batch of data.\r\n\r\n### Environment info\r\n\r\ndatasets: '2.14.0'\r\npyarrow: '12.0.0'\r\ntorch: '2.0.0'\r\nPython: 3.10.10 | packaged by conda-forge | (main, Mar 24 2023, 20:08:06) [GCC 11.3.0]\r\n\r\n!uname -r\r\n5.10.178-162.673.amzn2.x86_64<end_of_turn>\n<start_of_turn>model\nIterating over DataLoader based on HF datasets is stuck forever<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently if we push data as default config with `.push_to_hub` to a repo that has a legacy `dataset_infos.json` file containing a legacy default config name like `{username}--{dataset_name}`, new key `\"default\"` is added to `dataset_infos.json` along with the legacy one. I think the legacy one should be dropped in this case.\r\n\r\nAlso, in `load.py` I suggest to check if a legacy config name is indeed a legacy config name because after this fix it might not be the case (this check was first introduced in https://github.com/huggingface/datasets/pull/6218)<end_of_turn>\n<start_of_turn>model\nOverwrite legacy default config name in `dataset_infos.json` in packaged datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n#7356<end_of_turn>\n<start_of_turn>model\nAdd with_split to DatasetDict.map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`common_voice_test = load_dataset(\"audiofolder\", data_dir=\"./dataset/\",cache_dir=\"./cache\",split=datasets.Split.TEST)`\r\nwhen i run the code above, i got the error as below:\r\n--------------------------------------------\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.3.2/datasets/audiofolder/audiofolder.py (ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/2.3.2/datasets/audiofolder/audiofolder.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f299ed082e0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\")))\r\n\r\n\r\n--------------------------------------------------\r\nMy all data is on local machine, why does it need to connect the internet? how can i fix it, because my machine cannot connect the internet.\n\n### Steps to reproduce the bug\n\n1\n\n### Expected behavior\n\nno error when i use the load_dataset func\n\n### Environment info\n\npython=3.8.15<end_of_turn>\n<start_of_turn>model\nwhen i use datasets.load_dataset, i encounter the http connect error!<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nthis should fix the CI which currently uses transformers on 3.9 while it's now unsupported<end_of_turn>\n<start_of_turn>model\nDefine CI future<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7481<end_of_turn>\n<start_of_turn>model\nSupport underscore int read instruction<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nAccording to the [documentation](https://huggingface.co/docs/datasets/v2.14.5/loading#slice-splits) is should be possible to run the following command:\r\n\r\n`train_test_ds = datasets.load_dataset(\"bookcorpus\", split=\"train+test\")`\r\n\r\nto load the train and test sets from the dataset. \r\n\r\nHowever executing the equivalent code:\r\n\r\n`speech_commands_v1 = load_dataset(\"superb\", \"ks\", split=\"train+test\")`\r\n\r\nonly yields the following output:\r\n\r\n> Dataset({\r\n>     features: ['file', 'audio', 'label'],\r\n>     num_rows: 54175\r\n> })\r\n\r\nWhere loading the dataset without the split argument yields:\r\n\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 51094\r\n>     })\r\n>     validation: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 6798\r\n>     })\r\n>     test: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 3081\r\n>     })\r\n> })\r\n\r\nThus, the API seems to be broken in this regard. \r\n\r\nThis is a bit annoying since I want to be able to use the split argument with `split=\"train[:10%]+test[:10%]\"` to have smaller dataset to work with when validating my model is working correctly. \r\n\r\n### Steps to reproduce the bug\r\n\r\n`speech_commands_v1 = load_dataset(\"superb\", \"ks\", split=\"train+test\")`\r\n\r\n### Expected behavior\r\n\r\n> DatasetDict({\r\n>     train: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 51094\r\n>     })\r\n>     test: Dataset({\r\n>         features: ['file', 'audio', 'label'],\r\n>         num_rows: 3081\r\n>     })\r\n> })\r\n\r\n### Environment info\r\n\r\n```\r\nimport datasets\r\nprint(datasets.__version__)\r\n```\r\n> 2.14.5\r\n\r\n```\r\nimport sys\r\nprint(sys.version)\r\n```\r\n> 3.9.17 (main, Jul  5 2023, 20:41:20) \r\n> [GCC 11.2.0]<end_of_turn>\n<start_of_turn>model\nDataset slice splits can't load training and validation at the same time<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- [x] Fixes #7276<end_of_turn>\n<start_of_turn>model\nLet soundfile directly read local audio files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse `ruff` instead of `black` for formatting to be consistent with `transformers` ([PR](https://github.com/huggingface/transformers/pull/27144)) and `huggingface_hub` ([PR 1](https://github.com/huggingface/huggingface_hub/pull/1783) and [PR 2](https://github.com/huggingface/huggingface_hub/pull/1789)).<end_of_turn>\n<start_of_turn>model\nUse `ruff` for formatting<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.16.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ne.g. with `u23429/stock_1_minute_ticker`\r\n\r\n```ipython\r\nIn [1]: from datasets import *\r\n\r\nIn [2]: b = load_dataset_builder(\"u23429/stock_1_minute_ticker\")\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 627/627 [00:00<00:00, 246kB/s]\r\n\r\nIn [3]: b.config.data_files\r\nOut[3]: \r\n{NamedSplit('train'): ['hf://datasets/u23429/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166/train/train.parquet',\r\n  'hf://datasets/u23429/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166/train/train.parquet'],\r\n NamedSplit('validation'): ['hf://datasets/u23429/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166/validation/validation.parquet',\r\n  'hf://datasets/u23429/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166/validation/validation.parquet'],\r\n NamedSplit('test'): ['hf://datasets/u23429/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166/test/test.parquet',\r\n  'hf://datasets/u23429/stock_1_minute_ticker@65c973cf4ec061f01a363b40da4c1bb128ba4166/test/test.parquet']}\r\n\r\n```\r\n\r\nThis bug issue is present in the current `datasets` 2.14.5 and also on `main` even after https://github.com/huggingface/datasets/pull/6244 cc @mariosasko <end_of_turn>\n<start_of_turn>model\nDuplicate `data_files` when named `<split>/<split>.parquet`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n**Description**\r\nProviding a generator in an instantiation of IterableDataset.from_generator() fails with `TypeError: cannot pickle 'generator' object` when the generator argument is supplied with a generator. \r\n\r\n**Code example**\r\n```\r\ndef line_generator(files: List[Path]):\r\n\r\n    if isinstance(files, str):\r\n        files = [Path(files)]\r\n\r\n    for file in files:\r\n        if isinstance(file, str):\r\n            file = Path(file)\r\n        yield from open(file,'r').readlines()\r\n\r\n...\r\nmodel_training_files = ['file1.txt', 'file2.txt', 'file3.txt']\r\ntrain_dataset = IterableDataset.from_generator(generator=line_generator(model_training_files))\r\n```\r\n\r\n**Traceback**\r\nTraceback (most recent call last):\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\r\n    self.gen.throw(type, value, traceback)\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 691, in _no_cache_fields\r\n    yield\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 701, in dumps\r\n    dump(obj, file)\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 676, in dump\r\n    Pickler(file, recurse=True).dump(obj)\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/dill/_dill.py\", line 394, in dump\r\n    StockPickler.dump(self, obj)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py\", line 487, in dump\r\n    self.save(obj)\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 666, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/dill/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py\", line 560, in save\r\n    f(self, obj)  # Call unbound method with explicit self\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/dill/_dill.py\", line 1186, in save_module_dict\r\n    StockPickler.save_dict(pickler, obj)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py\", line 971, in save_dict\r\n    self._batch_setitems(obj.items())\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py\", line 997, in _batch_setitems\r\n    save(v)\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 666, in save\r\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\r\n  File \"/Users/d3p692/code/clem_bert/venv/lib/python3.9/site-packages/dill/_dill.py\", line 388, in save\r\n    StockPickler.save(self, obj, save_persistent_id)\r\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/pickle.py\", line 578, in save\r\n    rv = reduce(self.proto)\r\nTypeError: cannot pickle 'generator' object\r\n\n\n### Steps to reproduce the bug\n\n1. Create a set of text files to iterate over. \r\n2. Create a generator that returns the lines in each file until all files are exhausted.\r\n3. Instantiate the dataset over the generator by instantiating an IterableDataset.from_generator().\r\n4. Wait for the explosion.\n\n### Expected behavior\n\nI would expect that since the function claims to accept a generator that there would be no crash. Instead, I would expect the dataset to return all the lines in the files as queued up in the `line_generator()` function.\n\n### Environment info\n\ndatasets.__version__ == '2.13.1'\r\nPython 3.9.6\r\nPlatform: Darwin WE35261 22.5.0 Darwin Kernel Version 22.5.0: Thu Jun  8 22:22:22 PDT 2023; root:xnu-8796.121.3~7/RELEASE_X86_64 x86_64\r\n<end_of_turn>\n<start_of_turn>model\nIterableDataset.from_generator() fails with pickle error when provided a generator or iterator<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt seems there is a bug when downloading a dict of dicts of URLs introduced by:\r\n- #6794\r\n\r\n## Steps to reproduce the bug:\r\n```python\r\nfrom datasets import DownloadManager\r\n\r\ndl_manager = DownloadManager()\r\npaths = dl_manager.download({\"train\": {\"frr\": \"hf://datasets/wikimedia/wikipedia/20231101.frr/train-00000-of-00001.parquet\"}})\r\n```\r\n\r\nStack trace:\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\n<ipython-input-7-0e0d76d25b09> in <module>\r\n----> 1 paths = dl_manager.download({\"train\": {\"frr\": \"hf://datasets/wikimedia/wikipedia/20231101.frr/train-00000-of-00001.parquet\"}})\r\n\r\n.../huggingface/datasets/src/datasets/download/download_manager.py in download(self, url_or_urls)\r\n    255         start_time = datetime.now()\r\n    256         with stack_multiprocessing_download_progress_bars():\r\n--> 257             downloaded_path_or_paths = map_nested(\r\n    258                 download_func,\r\n    259                 url_or_urls,\r\n\r\n.../huggingface/datasets/src/datasets/utils/py_utils.py in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\r\n    506                 batch_size = max(len(iterable) // num_proc + int(len(iterable) % num_proc > 0), 1)\r\n    507             iterable = list(iter_batched(iterable, batch_size))\r\n--> 508         mapped = [\r\n    509             _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\r\n    510             for obj in hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n\r\n.../huggingface/datasets/src/datasets/utils/py_utils.py in <listcomp>(.0)\r\n    507             iterable = list(iter_batched(iterable, batch_size))\r\n    508         mapped = [\r\n--> 509             _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\r\n    510             for obj in hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    511         ]\r\n\r\n.../huggingface/datasets/src/datasets/utils/py_utils.py in _single_map_nested(args)\r\n    375         and all(not isinstance(v, types) for v in data_struct)\r\n    376     ):\r\n--> 377         return [mapped_item for batch in iter_batched(data_struct, batch_size) for mapped_item in function(batch)]\r\n    378 \r\n    379     # Reduce logging to keep things readable in multiprocessing with tqdm\r\n\r\n.../huggingface/datasets/src/datasets/utils/py_utils.py in <listcomp>(.0)\r\n    375         and all(not isinstance(v, types) for v in data_struct)\r\n    376     ):\r\n--> 377         return [mapped_item for batch in iter_batched(data_struct, batch_size) for mapped_item in function(batch)]\r\n    378 \r\n    379     # Reduce logging to keep things readable in multiprocessing with tqdm\r\n\r\n.../huggingface/datasets/src/datasets/download/download_manager.py in _download_batched(self, url_or_filenames, download_config)\r\n    311             )\r\n    312         else:\r\n--> 313             return [\r\n    314                 self._download_single(url_or_filename, download_config=download_config)\r\n    315                 for url_or_filename in url_or_filenames\r\n\r\n.../huggingface/datasets/src/datasets/download/download_manager.py in <listcomp>(.0)\r\n    312         else:\r\n    313             return [\r\n--> 314                 self._download_single(url_or_filename, download_config=download_config)\r\n    315                 for url_or_filename in url_or_filenames\r\n    316             ]\r\n\r\n.../huggingface/datasets/src/datasets/download/download_manager.py in _download_single(self, url_or_filename, download_config)\r\n    321             # append the relative path to the base_path\r\n    322             url_or_filename = url_or_path_join(self._base_path, url_or_filename)\r\n--> 323         out = cached_path(url_or_filename, download_config=download_config)\r\n    324         out = tracked_str(out)\r\n    325         out.set_origin(url_or_filename)\r\n\r\n.../huggingface/datasets/src/datasets/utils/file_utils.py in cached_path(url_or_filename, download_config, **download_kwargs)\r\n    220     elif is_local_path(url_or_filename):\r\n    221         # File, but it doesn't exist.\r\n--> 222         raise FileNotFoundError(f\"Local file {url_or_filename} doesn't exist\")\r\n    223     else:\r\n    224         # Something unknown\r\n\r\nFileNotFoundError: Local file .../huggingface/datasets/{'frr': 'hf:/datasets/wikimedia/wikipedia/20231101.frr/train-00000-of-00001.parquet'} doesn't exist\r\n```\r\n\r\nRelated to:\r\n- #6850\r\n<end_of_turn>\n<start_of_turn>model\nDownload is broken for dict of dicts: FileNotFoundError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI get the following error message: You are trying to load a dataset that was saved using `save_to_disk`. Please use `load_from_disk` instead.\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Save a dataset with `save_to_disk`\r\n2. Try to load it with `load_datasets`\r\n\r\n### Expected behavior\r\n\r\nI am able to load the dataset again with `load_datasets` which most packages uses over `load_from_disk`. I want to have a workaround that allows me to create the same indexing that `push_to_hub` creates for you before using `save_to_disk` - how can that be achieved?\r\n\r\n### Environment info\r\n\r\ndatasets 2.17.1, python 3.10<end_of_turn>\n<start_of_turn>model\nUnable to load dataset that was saved with `save_to_disk`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #5925 \r\n\r\nPlus, deprecate `list_datasets`/`inspect_dataset` in favor of `huggingface_hub.list_datasets`/\"git clone workflow\" (downloads data files)<end_of_turn>\n<start_of_turn>model\nAlways return list in `list_datasets`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMinor refactoring to remove redundant condition.<end_of_turn>\n<start_of_turn>model\nRemove redundant condition in builders<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nsignal.alarm and signal.signal are used in the load.py module, but these are not supported by Windows.\n\n### Steps to reproduce the bug\n\n lighteval accelerate --model_args \"pretrained=gpt2,trust_remote_code=True\" --tasks \"community|kinit_sts\" --custom_tasks \"community_tasks/kinit_evals.py\" --output_dir \"./evals\"\n\n### Expected behavior\n\nproceed with input(..) method\n\n### Environment info\n\nWindows 11<end_of_turn>\n<start_of_turn>model\nWindows do not supprot signal.alarm and singal.signal<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUseful for dataset streaming for multimodal datasets, and especially for lerobot.\r\nIt speeds up streaming up to 20 times.\r\n\r\nWhen decoding is enabled (default), media types are decoded:\r\n\r\n* audio -> dict of \"array\" and \"sampling_rate\" and \"path\"\r\n* image -> PIL.Image\r\n* video -> torchvision.io.VideoReader\r\n\r\nYou can enable multithreading using `num_threads`. This is especially useful to speed up remote\r\ndata streaming. However it can be slower than `num_threads=0` for local data on fast disks.\r\n\r\nPS: Disabling decoding is useful if you want to iterate on the paths or bytes of the media files\r\nwithout actually decoding their content.\r\n\r\nExample: Speed up streaming with multithreading:\r\n\r\n```py\r\n>>> import os\r\n>>> from datasets import load_dataset\r\n>>> from tqdm import tqdm\r\n>>> ds = load_dataset(\"sshh12/planet-textures\", split=\"train\", streaming=True)\r\n>>> num_threads = min(32, (os.cpu_count() or 1) + 4)\r\n>>> ds = ds.decode(num_threads=num_threads)\r\n>>> for _ in tqdm(ds):  # 20 times faster !\r\n...     ...\r\n```\r\n\r\nwhy not multiprocessing ? decoding is done with the GIL released in soundfile/PIL/torchvision so multiprocessing would just use more memory\r\n\r\nTODO\r\n- [x] test\r\n- [x] add to docs<end_of_turn>\n<start_of_turn>model\nAdd IterableDataset.decode with multithreading<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\nIn the latest version of datasets==4.0.0, i cannot print the audio data on the Colab notebook. But it works on the 3.6.0 version.\n\n!pip install -q -U datasets huggingface_hub fsspec\n\nfrom datasets import load_dataset\n\ndownloaded_dataset = load_dataset(\"ymoslem/MediaSpeech\", \"tr\", split=\"train\")\nprint(downloaded_dataset[\"audio\"][0])\n\n---------------------------------------------------------------------------\nImportError                               Traceback (most recent call last)\n[/tmp/ipython-input-4-90623240.py](https://localhost:8080/#) in <cell line: 0>()\n----> 1 downloaded_dataset[\"audio\"][0]\n\n10 frames\n[/usr/local/lib/python3.11/dist-packages/datasets/features/audio.py](https://localhost:8080/#) in decode_example(self, value, token_per_repo_id)\n    170             from ._torchcodec import AudioDecoder\n    171         else:\n--> 172             raise ImportError(\"To support decoding audio data, please install 'torchcodec'.\")\n    173 \n    174         if not self.decode:\n\nImportError: To support decoding audio data, please install 'torchcodec'. \n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n- Python version: 3.11.13\n- `huggingface_hub` version: 0.33.2\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nTo support decoding audio data, please install 'torchcodec'.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen parsing this image in the ImageNet1K dataset, the `datasets` crashs whole training process just because unable to parse an invalid EXIF tag.\n![Image](https://github.com/user-attachments/assets/3c840203-ac8c-41a0-9cf7-45f64488037d)\n\n### Steps to reproduce the bug\n\nUse the `datasets.Image.decode_example` method to decode the aforementioned image could reproduce the bug.\nThe decoding function will throw an unhandled exception at the `image.getexif()` method call due to invalid utf-8 stream in EXIF tags.\n```\nFile lib/python3.12/site-packages/datasets/features/image.py:188, in Image.decode_example(self, value, token_per_repo_id)\n    186     image = PIL.Image.open(BytesIO(bytes_))\n    187 image.load()  # to avoid \"Too many open files\" errors\n--> 188 if image.getexif().get(PIL.Image.ExifTags.Base.Orientation) is not None:\n    189     image = PIL.ImageOps.exif_transpose(image)\n    190 if self.mode and self.mode != image.mode:\n\nFile lib/python3.12/site-packages/PIL/Image.py:1542, in Image.getexif(self)\n   1540 xmp_tags = self.info.get(\"XML:com.adobe.xmp\")\n   1541 if not xmp_tags and (xmp_tags := self.info.get(\"xmp\")):\n-> 1542     xmp_tags = xmp_tags.decode(\"utf-8\")\n   1543 if xmp_tags:\n   1544     match = re.search(r'tiff:Orientation(=\"|>)([0-9])', xmp_tags)\n\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xa8 in position 4312: invalid start byte\n```\n\n### Expected behavior\n\nThe invalid EXIF tag should simply be ignored or issue a warning message, instead of crash the whole program at once.\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-6.5.0-18-generic-x86_64-with-glibc2.35\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.33.0\n- PyArrow version: 20.0.0\n- Pandas version: 2.3.0\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nBroken EXIF crash the whole program<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDatasets stored in the JSON format cannot be loaded using `json.load()`\n\n### Steps to reproduce the bug\n\n```\r\nimport json\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"squad\")\r\ntrain_dataset, test_dataset = dataset[\"train\"], dataset[\"validation\"]\r\ntest_dataset.to_json(\"full_dataset.json\")\r\n\r\n# This works\r\nloaded_test = load_dataset(\"json\", data_files=\"full_dataset.json\")\r\n\r\n# This fails\r\nloaded_test = json.load(open(\"full_dataset.json\", \"r\"))\r\n```\n\n### Expected behavior\n\nThe JSON should be correctly formatted when writing so that it can be loaded using `json.load()`.\n\n### Environment info\n\nColab: https://colab.research.google.com/drive/1st1iStFUVgu9ZPvnzSzL4vDeYWDwYpUm?usp=sharing<end_of_turn>\n<start_of_turn>model\nUnable to load JSON saved using `to_json`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nunicodedecodeerror: 'utf-8' codec can't decode byte 0xac in position 25: invalid start byte\r\n\r\n### Steps to reproduce the bug\r\n```\r\nimport sys\r\nsys.getdefaultencoding()\r\n'utf-8'\r\n\r\n\r\n\r\nfrom datasets import load_dataset\r\n\r\nprint(f\"Train dataset size: {len(dataset['train'])}\")\r\nprint(f\"Test dataset size: {len(dataset['test'])}\")\r\n\r\nResolving data files: 100%\r\n159/159 [00:00<00:00, 9909.28it/s]\r\nUsing custom data configuration samsum-0b1209637541c9e6\r\nDownloading and preparing dataset json/samsum to C:/Users/Administrator/.cache/huggingface/datasets/json/samsum-0b1209637541c9e6/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\r\nDownloading data files: 100%\r\n3/3 [00:00<00:00, 119.99it/s]\r\nExtracting data files: 100%\r\n3/3 [00:00<00:00, 9.54it/s]\r\nGenerating train split:\r\n88392/0 [00:15<00:00, 86848.17 examples/s]\r\nGenerating test split:\r\n0/0 [00:00<?, ? examples/s]\r\n---------------------------------------------------------------------------\r\nArrowInvalid                              Traceback (most recent call last)\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:132, in Json._generate_tables(self, files)\r\n    131 try:\r\n--> 132     pa_table = paj.read_json(\r\n    133         io.BytesIO(batch), read_options=paj.ReadOptions(block_size=block_size)\r\n    134     )\r\n    135     break\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\_json.pyx:290, in pyarrow._json.read_json()\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:144, in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyarrow\\error.pxi:100, in pyarrow.lib.check_status()\r\n\r\nArrowInvalid: JSON parse error: Invalid value. in row 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nUnicodeDecodeError                        Traceback (most recent call last)\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py:1819, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1818 _time = time.time()\r\n-> 1819 for _, table in generator:\r\n   1820     if max_shard_size is not None and writer._num_bytes > max_shard_size:\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\packaged_modules\\json\\json.py:153, in Json._generate_tables(self, files)\r\n    152     with open(file, encoding=\"utf-8\") as f:\r\n--> 153         dataset = json.load(f)\r\n    154 except json.JSONDecodeError:\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:293, in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\r\n    276 \"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\r\n    277 a JSON document) to a Python object.\r\n    278 \r\n   (...)\r\n    291 kwarg; otherwise ``JSONDecoder`` is used.\r\n    292 \"\"\"\r\n--> 293 return loads(fp.read(),\r\n    294     cls=cls, object_hook=object_hook,\r\n    295     parse_float=parse_float, parse_int=parse_int,\r\n    296     parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\codecs.py:322, in BufferedIncrementalDecoder.decode(self, input, final)\r\n    321 data = self.buffer + input\r\n--> 322 (result, consumed) = self._buffer_decode(data, self.errors, final)\r\n    323 # keep undecoded input until the next call\r\n\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xac in position 25: invalid start byte\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\nCell In[81], line 5\r\n      1 from datasets import load_dataset\r\n      3 # Load dataset from the hub\r\n      4 #dataset = load_dataset(\"json\",data_files=\"C:/Users/Administrator/Desktop/samsum/samsum/data/corpus/train.json\",field=\"data\")\r\n----> 5 dataset = load_dataset('json',\"samsum\")\r\n      6 #dataset = load_dataset(\"samsum\")\r\n      7 print(f\"Train dataset size: {len(dataset['train'])}\")\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:1758, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\r\n   1755 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   1757 # Download and prepare data\r\n-> 1758 builder_instance.download_and_prepare(\r\n   1759     download_config=download_config,\r\n   1760     download_mode=download_mode,\r\n   1761     ignore_verifications=ignore_verifications,\r\n   1762     try_from_hf_gcs=try_from_hf_gcs,\r\n   1763     num_proc=num_proc,\r\n   1764 )\r\n   1766 # Build dataset for splits\r\n   1767 keep_in_memory = (\r\n   1768     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   1769 )\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py:860, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    858     if num_proc is not None:\r\n    859         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 860     self._download_and_prepare(\r\n    861         dl_manager=dl_manager,\r\n    862         verify_infos=verify_infos,\r\n    863         **prepare_split_kwargs,\r\n    864         **download_and_prepare_kwargs,\r\n    865     )\r\n    866 # Sync info\r\n    867 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py:953, in DatasetBuilder._download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    949 split_dict.add(split_generator.split_info)\r\n    951 try:\r\n    952     # Prepare split will record examples associated to the split\r\n--> 953     self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    954 except OSError as e:\r\n    955     raise OSError(\r\n    956         \"Cannot find data file. \"\r\n    957         + (self.manual_download_instructions or \"\")\r\n    958         + \"\\nOriginal error:\\n\"\r\n    959         + str(e)\r\n    960     ) from None\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py:1708, in ArrowBasedBuilder._prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1706 gen_kwargs = split_generator.gen_kwargs\r\n   1707 job_id = 0\r\n-> 1708 for job_id, done, content in self._prepare_split_single(\r\n   1709     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1710 ):\r\n   1711     if done:\r\n   1712         result = content\r\n\r\nFile ~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py:1851, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1849     if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1850         e = e.__context__\r\n-> 1851     raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1853 yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n### Expected behavior\r\n\r\ncan't load dataset\r\n\r\n### Environment info\r\n\r\ndataset:samsum\r\nsystem :win10\r\ngpu:m40 24G<end_of_turn>\n<start_of_turn>model\nunicodedecodeerror: 'utf-8' codec can't decode byte 0xac in position 25: invalid start byte<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n```python\r\ndownload_config = DownloadConfig(proxies={'https': '<my proxy>'})\r\nbuilder = load_dataset_builder(..., download_config=download_config)\r\n```\r\nBut, when getting the dataset_info from HfApi, the http requests not using the proxies.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Setup proxies in DownloadConfig.\r\n2. Call `load_dataset_build` with download_config.\r\n3. Inspect the call stack in HfApi.dataset_info.\r\n\r\n![image](https://github.com/huggingface/datasets/assets/138426806/33e538a8-2e22-4e63-b634-343febe5324b)\r\n\r\n### Expected behavior\r\n\r\nDownloadConfig.proxies works for getting dataset_info.\r\n\r\n### Environment info\r\n\r\nhttps://github.com/huggingface/datasets/commit/406b2212263c0d33f267e35b917f410ff6b3bc00\r\nPython 3.11.4<end_of_turn>\n<start_of_turn>model\nDownloadConfig.proxies not work when load_dataset_builder calling HfApi.dataset_info  <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTests should use the same pythin path as they are launched with, which in the case of FreeBSD is /usr/local/bin/python3.11\r\n\r\nFailure:\r\n```\r\n                if err_filename is not None:\r\n>                   raise child_exception_type(errno_num, err_msg, err_filename)\r\nE                   FileNotFoundError: [Errno 2] No such file or directory: 'python'\r\n```\r\n\n\n### Steps to reproduce the bug\n\nregular test run using PyTest\n\n### Expected behavior\n\nn/a\n\n### Environment info\n\nFreeBSD 14.1<end_of_turn>\n<start_of_turn>model\nThe test test_move_script_doesnt_change_hash fails because it runs the 'python' command while the python executable has a different name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI currently have a dataset which has not been splited. When pushing the dataset to my hugging face dataset repository, it is split into a testing and training set. How can I prevent the split from happening?\n\n### Steps to reproduce the bug\n\n1. Have a unsplit dataset \r\n\r\n```python\r\nDataset({ features: ['input', 'output', 'Attack', '__index_level_0__'], num_rows: 944685 })\r\n```\r\n\r\n2. Push it to huggingface\r\n\r\n```python\r\ndataset.push_to_hub(dataset_name)\r\n```\r\n\r\n3. On the hugging face dataset repo, the dataset then appears to be splited:\r\n\r\n![image](https://github.com/huggingface/datasets/assets/29337128/b4fbc141-42b0-4f49-98df-dd479648fe09)\r\n\r\n4. Indeed, when loading the dataset from this repo, the dataset is split in two testing and training set.\r\n\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\n\r\ndataset = load_dataset(\"Jetlime/NF-CSE-CIC-IDS2018-v2\", streaming=True)\r\ndataset\r\n```\r\noutput: \r\n\r\n```\r\nIterableDatasetDict({\r\n    train: IterableDataset({\r\n        features: ['input', 'output', 'Attack', '__index_level_0__'],\r\n        n_shards: 2\r\n    })\r\n    test: IterableDataset({\r\n        features: ['input', 'output', 'Attack', '__index_level_0__'],\r\n        n_shards: 1\r\n    })\r\n```\n\n### Expected behavior\n\nThe dataset shall not be splited, as not requested.\n\n### Environment info\n\n- `datasets` version: 2.19.1\r\n- Platform: Linux-6.2.0-35-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.23.0\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\n```push_to_hub()``` - Prevent Automatic Generation of Splits <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse `huggingface_hub`'s RepoCard API instead of `DatasetMetadata` for modifying the card's YAML, and deprecate `datasets.utils.metadata` and `datasets.utils.readme`.\r\n\r\nAfter removing these modules, we can also delete `datasets.utils.resources` since the moon landing repo now stores its own version of these resources for the metadata UI.\r\n\r\nPS: this change requires bumping `huggingface_hub` to 0.13.0 (Transformers requires 0.14.0, so should be ok)<end_of_turn>\n<start_of_turn>model\nReplace metadata utils with `huggingface_hub`'s RepoCard API<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAlign filename prefix splitting with WebDataset library.\r\n\r\nThis PR uses the same `base_plus_ext` function as the one used by the `webdataset` library.\r\n\r\nFix #7150.\r\n\r\nRelated to #7144.<end_of_turn>\n<start_of_turn>model\nAlign filename prefix splitting with WebDataset library<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nKannada dataset(Conversations, Wikipedia etc)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nBased on https://github.com/huggingface/huggingface_hub/pull/1971 in `hfh`<end_of_turn>\n<start_of_turn>model\nMigrate from `setup.cfg` to `pyproject.toml`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIs there a way to disable execution of dataset loading script using `load_dataset`? This is a security vulnerability that could lead to arbitrary code execution. \r\n\r\nAny suggested workarounds are welcome as well. \n\n### Motivation\n\nThis is a security vulnerability that could lead to arbitrary code execution. \n\n### Your contribution\n\nn/a<end_of_turn>\n<start_of_turn>model\nSafely load datasets by disabling execution of dataset loading script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe JSON loader implicitly coerces floats to integers.\r\n\r\nThe column values `[0.0, 1.0, 2.0]` are coerced to `[0, 1, 2]`.\r\nSee CI error in dataset-viewer: https://github.com/huggingface/dataset-viewer/actions/runs/9290164936/job/25576926446\r\n```\r\n =================================== FAILURES ===================================\r\n___________________________ test_statistics_endpoint ___________________________\r\n\r\nnormal_user_public_json_dataset = 'DVUser/tmp-dataset-17170199043860'\r\n\r\n    def test_statistics_endpoint(normal_user_public_json_dataset: str) -> None:\r\n        dataset = normal_user_public_json_dataset\r\n        config, split = get_default_config_split()\r\n        statistics_response = poll_until_ready_and_assert(\r\n            relative_url=f\"/statistics?dataset={dataset}&config={config}&split={split}\",\r\n            check_x_revision=True,\r\n            dataset=dataset,\r\n        )\r\n    \r\n        content = statistics_response.json()\r\n        assert len(content) == 3\r\n        assert sorted(content) == [\"num_examples\", \"partial\", \"statistics\"], statistics_response\r\n        statistics = content[\"statistics\"]\r\n        num_examples = content[\"num_examples\"]\r\n        partial = content[\"partial\"]\r\n    \r\n        assert isinstance(statistics, list), statistics\r\n        assert len(statistics) == 6\r\n        assert num_examples == 4\r\n        assert partial is False\r\n    \r\n        string_label_column = statistics[0]\r\n        assert \"column_name\" in string_label_column\r\n        assert \"column_statistics\" in string_label_column\r\n        assert \"column_type\" in string_label_column\r\n        assert string_label_column[\"column_name\"] == \"col_1\"\r\n        assert string_label_column[\"column_type\"] == \"string_label\"  # 4 unique values -> label\r\n        assert isinstance(string_label_column[\"column_statistics\"], dict)\r\n        assert string_label_column[\"column_statistics\"] == {\r\n            \"nan_count\": 0,\r\n            \"nan_proportion\": 0.0,\r\n            \"no_label_count\": 0,\r\n            \"no_label_proportion\": 0.0,\r\n            \"n_unique\": 4,\r\n            \"frequencies\": {\r\n                \"There goes another one.\": 1,\r\n                \"Vader turns round and round in circles as his ship spins into space.\": 1,\r\n                \"We count thirty Rebel ships, Lord Vader.\": 1,\r\n                \"The wingman spots the pirateship coming at him and warns the Dark Lord\": 1,\r\n            },\r\n        }\r\n    \r\n        int_column = statistics[1]\r\n        assert \"column_name\" in int_column\r\n        assert \"column_statistics\" in int_column\r\n        assert \"column_type\" in int_column\r\n        assert int_column[\"column_name\"] == \"col_2\"\r\n        assert int_column[\"column_type\"] == \"int\"\r\n        assert isinstance(int_column[\"column_statistics\"], dict)\r\n        assert int_column[\"column_statistics\"] == {\r\n            \"histogram\": {\"bin_edges\": [0, 1, 2, 3, 3], \"hist\": [1, 1, 1, 1]},\r\n            \"max\": 3,\r\n            \"mean\": 1.5,\r\n            \"median\": 1.5,\r\n            \"min\": 0,\r\n            \"nan_count\": 0,\r\n            \"nan_proportion\": 0.0,\r\n            \"std\": 1.29099,\r\n        }\r\n    \r\n        float_column = statistics[2]\r\n        assert \"column_name\" in float_column\r\n        assert \"column_statistics\" in float_column\r\n        assert \"column_type\" in float_column\r\n        assert float_column[\"column_name\"] == \"col_3\"\r\n>       assert float_column[\"column_type\"] == \"float\"\r\nE       AssertionError: assert 'int' == 'float'\r\nE         - float\r\nE         + int\r\n\r\ntests/test_14_statistics.py:72: AssertionError\r\n\r\n=========================== short test summary info ============================\r\nFAILED tests/test_14_statistics.py::test_statistics_endpoint - AssertionError: assert 'int' == 'float'\r\n  - float\r\n  + int\r\n``` \r\n\r\nThis bug was introduced after:\r\n- #6914\r\n\r\nWe have reported the issue to pandas:\r\n- https://github.com/pandas-dev/pandas/issues/58866<end_of_turn>\n<start_of_turn>model\nJSON loader implicitly coerces floats to integers<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen there is only one config and only the dataset name is entered when using datasets.load_dataset(), it works fine. But if I create a second builder_config for my dataset and enter the config name when using datasets.load_dataset(), the following error will happen.\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/chenx/.cache/huggingface/datasets/my_dataset/0_shot_multiple_choice/1.0.0/97c3854a012cfd6b045e3be4c864739902af2d818bb9235b047baa94c302e9a2.incomplete/my_dataset-test-00000-00000-of-NNNNN.arrow'\r\n\r\nThe \"XXX.incomplete folder\" in the cache folder of my dataset will disappear before \"generating test split\", which does not happen when config name is not entered and the config name is \"default\"\r\n\r\nC:\\Users\\chenx\\.cache\\huggingface\\datasets\\my_dataset\\0_shot_multiple_choice\\1.0.0    \r\nThe folder that is supposed to remain under the above directory will disappear, and the data generator will not have a place to generate data into. \n\n### Steps to reproduce the bug\n\ntest = load_dataset('my_dataset', '0_shot_multiple_choice')\n\n### Expected behavior\n\nFileNotFoundError: [Errno 2] No such file or directory: 'C:/Users/chenx/.cache/huggingface/datasets/my_dataset/0_shot_multiple_choice/1.0.0/97c3854a012cfd6b045e3be4c864739902af2d818bb9235b047baa94c302e9a2.incomplete/my_dataset-test-00000-00000-of-NNNNN.arrow'\n\n### Environment info\n\ndatasets                  2.14.5\r\npython                    3.8.18<end_of_turn>\n<start_of_turn>model\nFileNotFoundError for dataset with multiple builder config<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7515 <end_of_turn>\n<start_of_turn>model\nPreserve formatting in concatenated IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAll files except lock file have proper permission obeying `ACL` property if it is set.\r\n\r\nIf the cache directory has `ACL` property, it should be respected instead of just using `umask` for permission.\r\nTo fix it, just create a lock file and pass the created `mode`.\r\n\r\nBy creating a lock file with `touch()` before `FileLock` create it with `mode`,\r\n- if `ACL` is not set, same as before\r\n- if `ACL` is set, `ACL` is respected\r\n\r\nIf it is acceptable, it should be also applied to [`huggingface_hub`](https://github.com/huggingface/huggingface_hub/blob/2702ec2a2bd0124cc1fddfd72ccb1297b2478148/src/huggingface_hub/utils/_fixes.py#L95) I guess.<end_of_turn>\n<start_of_turn>model\nFix lock permission<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently, `DatasetDict.column_names` returns a dict, with each split name as keys and the corresponding list of column names as values.\r\n\r\nHowever, by construction, all splits have the same column names.\r\n\r\nI think it makes more sense to return a single list with the column names, which is the same for all the split keys.<end_of_turn>\n<start_of_turn>model\nMake DatasetDict.column_names return a list instead of dict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nScenario:\r\n- Interleaving two iterable datasets of unequal lengths (`all_exhausted`), followed by a batch mapping with batch size 2 to effectively merge the two datasets and get a sample from each dataset in a single batch, with `drop_last_batch=True` to skip the last batch in case it doesn't have two samples.\r\n\r\nWhat works:\r\n- Using DataLoader with `num_workers=0`\r\n\r\nWhat does not work:\r\n- Using DataLoader with `num_workers=1`, errors in the last batch.\r\n\r\nBasically, `drop_last_batch=True` is ignored when using multiple dataloading workers.\r\n\r\nPlease take a look at the minimal repro script below.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import Dataset, interleave_datasets\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\ndef merge_samples(batch):\r\n    assert len(batch['a']) == 2, \"Batch size must be 2\"\r\n    batch['c'] = [batch['a'][0]]\r\n    batch['d'] = [batch['a'][1]]\r\n    return batch\r\n\r\n\r\ndef gen1():\r\n    for ii in range(1, 8385):\r\n        yield {\"a\": ii}\r\n\r\n\r\ndef gen2():\r\n    for ii in range(1, 5302):\r\n        yield {\"a\": ii}\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    dataset1 = Dataset.from_generator(gen1).to_iterable_dataset(num_shards=1024)\r\n    dataset2 = Dataset.from_generator(gen2).to_iterable_dataset(num_shards=1024)\r\n\r\n    interleaved = interleave_datasets([dataset1, dataset2], stopping_strategy=\"all_exhausted\")\r\n    mapped = interleaved.map(merge_samples, batched=True, batch_size=2, remove_columns=interleaved.column_names,\r\n                             drop_last_batch=True)\r\n\r\n    # Works\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=0)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n    print(\"DataLoader with num_workers=0 works\")\r\n\r\n    # Doesn't work\r\n    loader = DataLoader(mapped, batch_size=32, num_workers=1)\r\n    i = 0\r\n    for b in loader:\r\n        print(i, b['c'].shape, b['d'].shape)\r\n        i += 1\r\n\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n `drop_last_batch=True` should have same behaviour for `num_workers=0` and `num_workers>=1`\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.1\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.6.0\r\n\r\nI have also tested on Linux and got the same behavior.<end_of_turn>\n<start_of_turn>model\n `drop_last_batch=True` for IterableDataset map function is ignored with multiprocessing DataLoader <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI am trying to create a PEFT model from DISTILBERT model, and run a training loop. However, the trainer.train() is giving me this error: ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']\r\nHere is my code:\r\n\r\n### Steps to reproduce the bug\r\n\r\n#Creating a PEFT Config \r\nfrom peft import LoraConfig\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\r\nfrom peft import get_peft_model\r\n\r\nlora_config = LoraConfig(\r\n    task_type=\"SEQ_CLASS\",\r\n    r=8,\r\n    lora_alpha=32,\r\n    target_modules=[\"q_lin\", \"k_lin\", \"v_lin\"],\r\n    lora_dropout=0.01,\r\n)\r\n\r\n#Converting a Transformers Model into a PEFT Model\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\r\n    \"distilbert-base-uncased\",\r\n    num_labels=2, #Binary classification, 1 = positive, 0 = negative\r\n    \r\n\r\n)\r\nlora_model = get_peft_model(model, lora_config)\r\n\r\nprint(lora_model)\r\n\r\nTokenize data set\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\r\n\r\n# Load the train and test splits dataset\r\ndataset  = load_dataset(\"fancyzhx/amazon_polarity\")\r\n\r\n#create a smaller subset for train and test\r\nsubset_size = 5000\r\nsmall_train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(subset_size))\r\nsmall_test_dataset = dataset[\"test\"].shuffle(seed=42).select(range(subset_size))\r\n\r\n#Tokenize data\r\ndef tokenize_function(example):\r\n    return tokenizer(example[\"content\"], padding=\"max_length\", truncation=True)\r\n\r\ntokenized_train_dataset = small_train_dataset.map(tokenize_function, batched=True)\r\ntokenized_test_dataset = small_test_dataset.map(tokenize_function, batched=True)\r\n\r\ntrain_lora = tokenized_train_dataset.rename_column('label', 'labels')\r\ntest_lora = tokenized_test_dataset.rename_column('label', 'labels')\r\n\r\nprint(tokenized_train_dataset.column_names)\r\nprint(tokenized_test_dataset.column_names)\r\n\r\n#Train the PEFT model\r\n\r\nimport numpy as np\r\nfrom transformers import Trainer, TrainingArguments, default_data_collator, DataCollatorWithPadding\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\r\n\r\ndef compute_metrics(eval_pred):\r\n    predictions, labels = eval_pred\r\n    predictions = np.argmax(predictions, axis=1)\r\n    return {\"accuracy\": (predictions == labels).mean()}\r\n\r\ntrainer = Trainer(\r\n    model=lora_model,\r\n    args=TrainingArguments(\r\n        output_dir=\".\",\r\n        learning_rate=2e-3,\r\n        # Reduce the batch size if you don't have enough memory\r\n        per_device_train_batch_size=1,\r\n        per_device_eval_batch_size=1,\r\n        num_train_epochs=3,\r\n        weight_decay=0.01,\r\n        evaluation_strategy=\"epoch\",\r\n        save_strategy=\"epoch\",\r\n        load_best_model_at_end=True,\r\n    ),\r\n    train_dataset=tokenized_train_dataset,\r\n    eval_dataset=tokenized_test_dataset,\r\n    tokenizer=tokenizer,\r\n    data_collator=DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\"),\r\n    compute_metrics=compute_metrics,\r\n)\r\n\r\n\r\ntrainer.train()\r\n                                \r\n\r\n\r\n### Expected behavior\r\n\r\nExample of output:\r\n [558/558 01:04, Epoch XX]\r\n\r\nEpoch | Training Loss | Validation Loss | Accuracy\r\n-- | -- | -- | --\r\n1 | No log | 0.046478 | 0.988341\r\n2 | 0.052800 | 0.048840 | 0.988341\r\n\r\n\r\n### Environment info\r\n\r\nUsing python and jupyter notbook<end_of_turn>\n<start_of_turn>model\nValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently, the commit message keeps on adding:\r\n\r\n- `Upload dataset (part 00000-of-00002)`\r\n- `Upload dataset (part 00000-of-00002) (part 00001-of-00002)`\r\n\r\nIntroduced in https://github.com/huggingface/datasets/pull/6269\r\n\r\nThis PR fixes this issue to have\r\n\r\n- `Upload dataset (part 00000-of-00002)`\r\n- `Upload dataset (part 00001-of-00002)`<end_of_turn>\n<start_of_turn>model\nFix commit message formatting in multi-commit uploads<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn docs of about_arrow.md, in the below example code\r\n![image](https://github.com/huggingface/datasets/assets/74114936/fc70e152-e15f-422e-949a-1c4c4c9aa116)\r\nThe variable name 'time' was being used in a way that could potentially lead to a namespace conflict with Python's built-in 'time' module. It is not a good convention and can lead to unintended variable shadowing for any user re-using the example code.\r\nTo ensure code clarity, and prevent potential naming conflicts renamed the variable 'time' to 'elapsed_time' in the example  code.<end_of_turn>\n<start_of_turn>model\ndocs: resolving namespace conflict, refactored variable <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIn the JSON builder, use explicitly requested feature types before or while converting to Arrow.\n\n### Motivation\n\nWorking with JSON datasets is really hard because of Arrow.  At the very least, it seems like it should be possible to work-around these problems by explicitly setting problematic columns's types.  But it seems like this is not possible because the features are only used *after* converting to arrow.\n\nHere's a simple example where the Arrow error could potentially be avoided by converting the column to a string: https://colab.research.google.com/drive/16QHRdbUwKSrpwVfGwu8V8AHr8v2dv0dt?usp=sharing\n\n### Your contribution\n\nMaybe with some guidance.  I'm not very familiar with arrow or pandas.<end_of_turn>\n<start_of_turn>model\nJson builder: Allow features to override problematic Arrow types<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis would make the data files resolution faster: no need to list all the data files to infer the dataset builder to use.\r\n\r\nfix https://github.com/huggingface/datasets/issues/6140<end_of_turn>\n<start_of_turn>model\nUse yaml instead of get data patterns when possible<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix datasets export to JSON<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\nhttps://github.com/huggingface/datasets/blob/4.0.0/src/datasets/load.py#L1411-L1418\n\n```\n\ndef load_dataset(\n    ...\n    dl_manager: Optional[DownloadManager] = None,  # add this new argument\n    **config_kwargs,\n) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset]:\n\n    ...\n\n    # Create a dataset builder\n    builder_instance = load_dataset_builder(\n        path=path,\n        name=name,\n        data_dir=data_dir,\n        data_files=data_files,\n        cache_dir=cache_dir,\n        features=features,\n        download_config=download_config,\n        download_mode=download_mode,\n        revision=revision,\n        token=token,\n        storage_options=storage_options,\n        **config_kwargs,\n    )\n\n    # Return iterable dataset in case of streaming\n    if streaming:\n        return builder_instance.as_streaming_dataset(split=split)\n\n    # Note: This is the revised part\n    if dl_manager is None:\n        if download_config is None:\n            download_config = DownloadConfig(\n                cache_dir=builder_instance._cache_downloaded_dir,\n                force_download=download_mode == DownloadMode.FORCE_REDOWNLOAD,\n                force_extract=download_mode == DownloadMode.FORCE_REDOWNLOAD,\n                use_etag=False,\n                num_proc=num_proc,\n                token=builder_instance.token,\n                storage_options=builder_instance.storage_options,\n            )  # We don't use etag for data files to speed up the process\n\n        dl_manager = DownloadManager(\n            dataset_name=builder_instance.dataset_name,\n            download_config=download_config,\n            data_dir=builder_instance.config.data_dir,\n            record_checksums=(\n                builder_instance._record_infos or verification_mode == VerificationMode.ALL_CHECKS\n            ),\n        )\n\n    # Download and prepare data\n    builder_instance.download_and_prepare(\n        download_config=download_config,\n        download_mode=download_mode,\n        verification_mode=verification_mode,\n        dl_manager=dl_manager,  # pass the new argument\n        num_proc=num_proc,\n        storage_options=storage_options,\n    )\n\n    ...\n\n```\n\n### Motivation\n\nIn my case, I'm hoping to deal with the cache files downloading manually (not using hash filenames and save to another location, or using potential existing local files).\n\n### Your contribution\n\nIt's already implemented above. If maintainers think this should be considered, I'll open a PR.<end_of_turn>\n<start_of_turn>model\nCustom `dl_manager` in `load_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nEither update the documentation, or re-introduce the flag (and corresponding logic to convert the audio to mono)\n\n### Steps to reproduce the bug\n\nAudio(sampling_rate=16000, mono=True) raises the error \nTypeError: Audio.__init__() got an unexpected keyword argument 'mono'\n\nHowever, in the class documentation, is says:\nArgs:\n        sampling_rate (`int`, *optional*):\n            Target sampling rate. If `None`, the native sampling rate is used.\n        mono (`bool`, defaults to `True`):\n            Whether to convert the audio signal to mono by averaging samples across\n            channels.\n        [...]\n\n### Expected behavior\n\nThe above call should either work, or the documentation within the Audio class should be updated\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: Linux-5.15.0-124-generic-x86_64-with-glibc2.35\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.34.4\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.2\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nAudio mono argument no longer supported, despite class documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix conda release by adding pyarrow-hotfix dependency.\r\n\r\nNote that conda release failed in latest 2.14.7 release: https://github.com/huggingface/datasets/actions/runs/6874667214/job/18696761723\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/share/miniconda/envs/build-datasets/conda-bld/datasets_1700036460222/test_tmp/run_test.py\", line 2, in <module>\r\n    import datasets\r\n  File \"/usr/share/miniconda/envs/build-datasets/conda-bld/datasets_1700036460222/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.12/site-packages/datasets/__init__.py\", line 22, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"/usr/share/miniconda/envs/build-datasets/conda-bld/datasets_1700036460222/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 67, in <module>\r\n    from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n  File \"/usr/share/miniconda/envs/build-datasets/conda-bld/datasets_1700036460222/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.12/site-packages/datasets/arrow_writer.py\", line 27, in <module>\r\n    from .features import Features, Image, Value\r\n  File \"/usr/share/miniconda/envs/build-datasets/conda-bld/datasets_1700036460222/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.12/site-packages/datasets/features/__init__.py\", line 18, in <module>\r\n    from .features import Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\r\n  File \"/usr/share/miniconda/envs/build-datasets/conda-bld/datasets_1700036460222/_test_env_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold_placehold/lib/python3.12/site-packages/datasets/features/features.py\", line 34, in <module>\r\n    import pyarrow_hotfix  # noqa: F401  # to fix vulnerability on pyarrow<14.0.1\r\n    ^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'pyarrow_hotfix'\r\n```<end_of_turn>\n<start_of_turn>model\nFix conda release by adding pyarrow-hotfix dependency<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI am trying to tokenize a few million documents with multiple workers but the tokenization process is taking forever. \r\n\r\nCode snippet:\r\n\r\n```\r\nraw_datasets.map(\r\n            encode_function,\r\n            batched=False,\r\n            num_proc=args.preprocessing_num_workers,\r\n            load_from_cache_file=not args.overwrite_cache,\r\n            remove_columns=[name for name in raw_datasets[\"train\"].column_names if name not in [\"input_ids\", \"labels\", \"attention_mask\"]],\r\n            desc=\"Tokenizing data\",\r\n        )\r\n```\r\n\r\nDetails:\r\n\r\n```\r\ntransformers==4.28.0.dev0\r\ndatasets==4.28.0.dev0\r\npreprocessing_num_workers==48\r\n```\r\ntokenizer == decapoda-research/llama-7b-hf\r\n\r\n\r\n<end_of_turn>\n<start_of_turn>model\nTokenization with multiple workers is too slow<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7561<end_of_turn>\n<start_of_turn>model\nAdd missing property on `RepeatExamplesIterable`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe dl_manager base path is remote (e.g. a hf:// path), so local cached paths should be passed as absolute paths.\r\nThis could happen if users provide a relative path as `cache_dir`\r\n\r\nfix https://github.com/huggingface/datasets/issues/6536<end_of_turn>\n<start_of_turn>model\nFix dl_manager.extract returning FileNotFoundError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nCurrently, The server is off-line.  I am using the glue metric from the local file downloaded from the hub. \r\nI download / cached datasets using `load_dataset('glue','sst2', cache_dir='/xxx')` to cache them and then in the off-line mode, I use `load_dataset('xxx/glue.py','sst2', cache_dir='/xxx')`. I can successfully reuse cached datasets.\r\n\r\nMy problem is about the load_metric. \r\nWhen I run `load_dataset('xxx/glue_metric.py','sst2',cache_dir='/xxx')` , it returns \r\n\r\n`  File \"xx/lib64/python3.9/site-packages/datasets/utils/deprecation_utils.py\", line 46, in wrapper\r\n    return deprecated_function(*args, **kwargs)\r\n  File \"xx//lib64/python3.9/site-packages/datasets/load.py\", line 1392, in load_metric\r\n    metric = metric_cls(\r\nTypeError: 'NoneType' object is not callable`\r\n\r\nThanks in advance for help! \r\n### Steps to reproduce the bug\r\n\r\nN/A\r\n\r\n### Expected behavior\r\n\r\nN/A\r\n\r\n### Environment info\r\n\r\n`datasets == 2.12.0`<end_of_turn>\n<start_of_turn>model\nread metric glue.py from local file <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix for https://github.com/huggingface/datasets/issues/5963<end_of_turn>\n<start_of_turn>model\nDon't reference self in Spark._validate_cache_dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nwhen using audio folder builder (`load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")`), it cannot detect custom split name other than train/validation/test\n\n### Steps to reproduce the bug\n\ni have the following folder structure\n```\nmy_dataset/\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 lorem.wav\n\u2502   \u251c\u2500\u2500 \u2026\n\u2502   \u2514\u2500\u2500 metadata.csv\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 ipsum.wav\n\u2502   \u251c\u2500\u2500 \u2026\n\u2502   \u2514\u2500\u2500 metadata.csv\n\u251c\u2500\u2500 validation/\n\u2502   \u251c\u2500\u2500 dolor.wav\n\u2502   \u251c\u2500\u2500 \u2026\n\u2502   \u2514\u2500\u2500 metadata.csv\n\u2514\u2500\u2500 custom/\n    \u251c\u2500\u2500 sit.wav\n    \u251c\u2500\u2500 \u2026\n    \u2514\u2500\u2500 metadata.csv\n```\nusing `ds = load_dataset(\"audiofolder\", data_dir=\"/path/to/my_dataset\")`\n\n### Expected behavior\n\ni got `ds` with only 3 splits train/validation/test, whenever i rename train/validation/test folder it also disappear if i re-create `ds`\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Windows-11-10.0.26100-SP0\n- Python version: 3.12.8\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\naudio folder builder cannot detect custom split name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n**Task Done:** \r\n\r\n- Resolve issue #7788 : Add the missing argument mapping in Dataset.to_sql (`src/datasets/arrow_dataset.py`)\r\n\r\n\r\n<end_of_turn>\n<start_of_turn>model\nfix: add `num_proc` argument to `Dataset.to_sql`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn the client documentation, the markdown 'TIP' paragraph for paragraph in docs/stream#shuffle is not well executed \u2014 not as the other in the same page / while markdown is correctly considering it.\n\nDocumentation:\nhttps://huggingface.co/docs/datasets/v4.3.0/en/stream#shuffle:~:text=%5B!TIP%5D%5BIterableDataset.shuffle()%5D(/docs/datasets/v4.3.0/en/package_reference/main_classes%23datasets.IterableDataset.shuffle)%20will%20also%20shuffle%20the%20order%20of%20the%20shards%20if%20the%20dataset%20is%20sharded%20into%20multiple%20files.\n\nGithub source:\nhttps://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#:~:text=Casting%20only%20works%20if%20the%20original%20feature%20type%20and%20new%20feature%20type%20are%20compatible.%20For%20example%2C%20you%20can%20cast%20a%20column%20with%20the%20feature%20type%20Value(%27int32%27)%20to%20Value(%27bool%27)%20if%20the%20original%20column%20only%20contains%20ones%20and%20zeros.<end_of_turn>\n<start_of_turn>model\n[DOCS][minor] TIPS paragraph not compiled in docs/stream<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSupport PathLike input in save_to_disk / load_from_disk<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWe recently enabled the (experimental) parallel backend switch for data download and extraction but not for `map` yet.\r\n\r\nRight now we're using our `iflatmap_unordered` implementation for multiprocessing that uses a shared Queue to gather progress updates from the subprocesses and show a progress bar in the main process.\r\n\r\nIf a Queue implementation that would work on any joblib backend by leveraging the filesystem that is shared among workers, we can have `iflatmap_unordered` for joblib and therefore a `map` with any joblib backend with a progress bar !\r\n\r\nNote that the Queue doesn't need to be that optimized though since we can choose a small frequency for progress updates (like 1 update per second).<end_of_turn>\n<start_of_turn>model\n`map` with any joblib backend<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ne.g. [thangvip](https://huggingface.co/thangvip)/[cosmopedia_vi_math](https://huggingface.co/datasets/thangvip/cosmopedia_vi_math) has 300 splits and it takes a very long time to load only one split.\r\n\r\nThis is due to `load_dataset()` resolving the files of all the splits even if only one is needed.\r\n\r\nIn `dataset-viewer` the splits are loaded in different jobs so it results in 300 jobs that resolve 300 splits -> 90k calls to `/paths-info`<end_of_turn>\n<start_of_turn>model\n[Streaming] Only load requested splits without resolving files for the other splits<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.14.3<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm trying to load codeparrot/codeparrot-clean-train, but get the following error:\r\n\r\nConnectionError: Couldn't reach https://huggingface.co/datasets/codeparrot/codeparrot-clean-train/resolve/main/dataset_infos.json (ConnectionError(ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))))\r\n\r\n\n\n### Steps to reproduce the bug\n\ntrain_data = load_dataset('codeparrot/codeparrot-clean-train', split='train')\r\n\n\n### Expected behavior\n\ndownload the dataset\n\n### Environment info\n\ncentos7<end_of_turn>\n<start_of_turn>model\nConnectionError: Couldn't reach dataset_infos.json <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCLI convert_to_parquet cannot create \"script\" branch on 3rd party repos.\r\n\r\nIt can only create it on repos where the user executing the script has write access.\r\n\r\nOtherwise, a 403 Forbidden HTTPError is raised:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 304, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden for url: https://huggingface.co/api/datasets/ORG/DATASET/branch/script\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/datasets-cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/commands/datasets_cli.py\", line 41, in main\r\n    service.run()\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/commands/convert_to_parquet.py\", line 92, in run\r\n    create_branch(dataset_id, branch=\"script\", repo_type=\"dataset\", token=token, exist_ok=True)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py\", line 5503, in create_branch\r\n    hf_raise_for_status(response)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py\", line 367, in hf_raise_for_status\r\n    raise HfHubHTTPError(message, response=response) from e\r\nhuggingface_hub.utils._errors.HfHubHTTPError:  (Request ID: Root=1-6645ee0d-4db1ed8a1fbe04956be15897;139a6e23-df7d-4f62-b5ba-adb6d8e6e696)\r\n\r\n403 Forbidden: Forbidden: cannot write to script.\r\nCannot access content at: https://huggingface.co/api/datasets/ORG/DATASET/branch/script.\r\nIf you are trying to create or update content,make sure you have a token with the `write` role.\r\n```<end_of_turn>\n<start_of_turn>model\nHTTPError 403 raised by CLI convert_to_parquet when creating script branch on 3rd party repos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## perf: reduce copies in TorchFormatter\r\n\r\nThis PR make changes the torch formatter to avoid unnecessary copies and casts when converting decoded batches to tensors.\r\n\r\nBecause many arrays are already in a torch-friendly memory layout and dtype, we can do zero\u2011copy conversions (`torch.from_numpy`) and only fall back to `as_tensor` when a dtype/device change is required. We also consolidate lists of same\u2011shape tensors with a cheap `stack` only when safe.\r\n\r\nWhy it helps\r\n- Avoids extra materialization and dtype churn during batched map and indexing.\r\n- Preserves API and outputs; only changes internal conversion logic.\r\n\r\n\r\nSmall benchmark script (based on https://github.com/huggingface/datasets/issues/6104)\r\n\r\n```python\r\nimport time\r\nfrom datasets import load_dataset\r\n\r\n\r\ndef main():\r\n    dataset = load_dataset(\"NightMachinery/hf_datasets_bug1\")\r\n    dataset = dataset[\"train\"] if \"train\" in dataset else dataset\r\n    t0 = time.time()\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # identity map with small batches\r\n    dataset = dataset.map(lambda x: x, batched=True, batch_size=20)\r\n\r\n    # force materialization\r\n    data = dataset[:300]\r\n    print(len(data.keys()))\r\n\r\n    t1 = time.time()\r\n    print(f\"Duration: {t1 - t0:.2f} s\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\nWithout changes\r\n\r\n```bash\r\nuv run bench.py\r\n```\r\n\r\n```bash\r\n# 303\r\n# Duration: 7.26 s\r\n```\r\n\r\nWith changes\r\n\r\n```bash\r\nuv run bench.py\r\n```\r\n\r\n```bash\r\n# 303\r\n# Duration: 4.43 s\r\n```\r\n\r\n\r\n# Updated reproduction scripts\r\n\r\nBelow are some simple test cases using `main` and this `refactor-torch-formatter` branch. I've included the two scripts and output when running on a local machine.\r\n\r\n```python\r\n# /// script\r\n# requires-python = \">=3.10\"\r\n# dependencies = [\r\n#     \"torch\",\r\n#     \"datasets\",\r\n#     \"pillow\",\r\n# ]\r\n#\r\n# [tool.uv.sources]\r\n# datasets = { git = \"https://github.com/huggingface/datasets.git\" }\r\n# ///\r\nimport time\r\nimport random\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom datasets import Dataset, load_dataset\r\nimport torch\r\n\r\n\r\ndef create_mock_images_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with PIL images.\"\"\"\r\n    random.seed(42)\r\n    np.random.seed(42)\r\n\r\n    images = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic RGB image\r\n        width, height = 64, 64\r\n        rgb_array = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\r\n        image = Image.fromarray(rgb_array)\r\n        images.append(image)\r\n        labels.append(i % 10)  # 10 classes\r\n\r\n    return Dataset.from_dict({\"image\": images, \"label\": labels})\r\n\r\n\r\ndef create_mock_text_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with text.\"\"\"\r\n    random.seed(42)\r\n\r\n    words = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\", \"honeydew\"]\r\n    texts = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic text\r\n        text_length = 5 + (i % 20)  # 5-24 words\r\n        text = \" \".join(random.choices(words, k=text_length))\r\n        texts.append(text)\r\n        labels.append(i % 3)  # 3 classes\r\n\r\n    return Dataset.from_dict({\"text\": texts, \"label\": labels})\r\n\r\n\r\ndef create_mock_ints_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with integers.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic integer arrays\r\n        arr = [random.randint(0, 1000) for _ in range(50)]  # 50 integers each\r\n        data.append(arr)\r\n        labels.append(i % 5)  # 5 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef create_mock_floats_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with floats.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic float arrays\r\n        arr = [random.uniform(0.0, 100.0) for _ in range(30)]  # 30 floats each\r\n        data.append(arr)\r\n        labels.append(i % 4)  # 4 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef benchmark_dataset(name, dataset, num_samples=1000):\r\n    \"\"\"Benchmark dataset access speed.\"\"\"\r\n    print(f\"\\n=== {name} Dataset Benchmark ===\")\r\n\r\n    t0 = time.time()\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # identity map with small batches\r\n    dataset = dataset.map(lambda x: x, batched=True, batch_size=20)\r\n\r\n    # force materialization\r\n    data = dataset[:num_samples]\r\n    print(f\"Keys: {list(data.keys())}\")\r\n    print(f\"Sample count: {len(data[list(data.keys())[0]])}\")\r\n\r\n    t1 = time.time()\r\n    print(f\"Duration: {t1 - t0:.2f} s\")\r\n    print(f\"Speed: {num_samples / (t1 - t0):.1f} samples/s\")\r\n\r\n\r\ndef main():\r\n    # PIL Images benchmark\r\n    images_dataset = create_mock_images_dataset()\r\n    benchmark_dataset(\"PIL Images\", images_dataset)\r\n\r\n    # Text benchmark\r\n    text_dataset = create_mock_text_dataset()\r\n    benchmark_dataset(\"Text\", text_dataset)\r\n\r\n    # Integers benchmark\r\n    ints_dataset = create_mock_ints_dataset()\r\n    benchmark_dataset(\"Integers\", ints_dataset)\r\n\r\n    # Floats benchmark\r\n    floats_dataset = create_mock_floats_dataset()\r\n    benchmark_dataset(\"Floats\", floats_dataset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\noutput\r\n\r\n```bash\r\nuv run --refresh example1.py\r\n```\r\n\r\n```text\r\n=== PIL Images Dataset Benchmark ===\r\nMap:   0%|                                                          | 0/5000 [00:00<?, ? examples/s]/Users/drbh/.cache/uv/environments-v2/example1-2aca1a30e84bdead/lib/python3.10/site-packages/datasets/features/image.py:352: UserWarning: Downcasting array dtype int64 to uint8 to be compatible with 'Pillow'\r\n  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:01<00:00, 3669.15 examples/s]\r\nKeys: ['image', 'label']\r\nSample count: 1000\r\nDuration: 2.14 s\r\nSpeed: 466.5 samples/s\r\n\r\n=== Text Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 141327.04 examples/s]\r\nKeys: ['text', 'label']\r\nSample count: 1000\r\nDuration: 0.04 s\r\nSpeed: 27004.3 samples/s\r\n\r\n=== Integers Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 112904.90 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.05 s\r\nSpeed: 21680.6 samples/s\r\n\r\n=== Floats Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 104084.25 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.05 s\r\nSpeed: 20215.1 samples/s\r\n```\r\n\r\nand this branch specifically\r\n\r\n```python\r\n# /// script\r\n# requires-python = \">=3.10\"\r\n# dependencies = [\r\n#     \"torch\",\r\n#     \"datasets\",\r\n#     \"pillow\",\r\n# ]\r\n#\r\n# [tool.uv.sources]\r\n# datasets = { git = \"https://github.com/huggingface/datasets.git\", rev = \"refactor-torch-formatter\" }\r\n# ///\r\nimport time\r\nimport random\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom datasets import Dataset, load_dataset\r\nimport torch\r\n\r\n\r\ndef create_mock_images_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with PIL images.\"\"\"\r\n    random.seed(42)\r\n    np.random.seed(42)\r\n\r\n    images = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic RGB image\r\n        width, height = 64, 64\r\n        rgb_array = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\r\n        image = Image.fromarray(rgb_array)\r\n        images.append(image)\r\n        labels.append(i % 10)  # 10 classes\r\n\r\n    return Dataset.from_dict({\"image\": images, \"label\": labels})\r\n\r\n\r\ndef create_mock_text_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with text.\"\"\"\r\n    random.seed(42)\r\n\r\n    words = [\r\n        \"apple\",\r\n        \"banana\",\r\n        \"cherry\",\r\n        \"date\",\r\n        \"elderberry\",\r\n        \"fig\",\r\n        \"grape\",\r\n        \"honeydew\",\r\n    ]\r\n    texts = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic text\r\n        text_length = 5 + (i % 20)  # 5-24 words\r\n        text = \" \".join(random.choices(words, k=text_length))\r\n        texts.append(text)\r\n        labels.append(i % 3)  # 3 classes\r\n\r\n    return Dataset.from_dict({\"text\": texts, \"label\": labels})\r\n\r\n\r\ndef create_mock_ints_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with integers.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic integer arrays\r\n        arr = [random.randint(0, 1000) for _ in range(50)]  # 50 integers each\r\n        data.append(arr)\r\n        labels.append(i % 5)  # 5 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef create_mock_floats_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with floats.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic float arrays\r\n        arr = [random.uniform(0.0, 100.0) for _ in range(30)]  # 30 floats each\r\n        data.append(arr)\r\n        labels.append(i % 4)  # 4 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef benchmark_dataset(name, dataset, num_samples=1000):\r\n    \"\"\"Benchmark dataset access speed.\"\"\"\r\n    print(f\"\\n=== {name} Dataset Benchmark ===\")\r\n\r\n    t0 = time.time()\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # identity map with small batches\r\n    dataset = dataset.map(lambda x: x, batched=True, batch_size=20)\r\n\r\n    # force materialization\r\n    data = dataset[:num_samples]\r\n    print(f\"Keys: {list(data.keys())}\")\r\n    print(f\"Sample count: {len(data[list(data.keys())[0]])}\")\r\n\r\n    t1 = time.time()\r\n    print(f\"Duration: {t1 - t0:.2f} s\")\r\n    print(f\"Speed: {num_samples / (t1 - t0):.1f} samples/s\")\r\n\r\n\r\ndef main():\r\n    # PIL Images benchmark\r\n    images_dataset = create_mock_images_dataset()\r\n    benchmark_dataset(\"PIL Images\", images_dataset)\r\n\r\n    # Text benchmark\r\n    text_dataset = create_mock_text_dataset()\r\n    benchmark_dataset(\"Text\", text_dataset)\r\n\r\n    # Integers benchmark\r\n    ints_dataset = create_mock_ints_dataset()\r\n    benchmark_dataset(\"Integers\", ints_dataset)\r\n\r\n    # Floats benchmark\r\n    floats_dataset = create_mock_floats_dataset()\r\n    benchmark_dataset(\"Floats\", floats_dataset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n```bash\r\nuv run --refresh example2.py\r\n```\r\n\r\n```text\r\n    Updated https://github.com/huggingface/datasets.git (2cb64d1b6503afb49d822b20979760efe4519d03)\r\n      Built datasets @ git+https://github.com/huggingface/datasets.git@2cb64d1b6503afb49d822b20979760efe\r\nUninstalled 1 package in 20ms\r\nInstalled 1 package in 5ms\r\n\r\n=== PIL Images Dataset Benchmark ===\r\nMap:   0%|                                                          | 0/5000 [00:00<?, ? examples/s]/Users/drbh/.cache/uv/environments-v2/example2-d4af608668b706ec/lib/python3.10/site-packages/datasets/features/image.py:352: UserWarning: Downcasting array dtype int64 to uint8 to be compatible with 'Pillow'\r\n  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:01<00:00, 3645.14 examples/s]\r\nKeys: ['image', 'label']\r\nSample count: 1000\r\nDuration: 2.04 s\r\nSpeed: 491.2 samples/s\r\n\r\n=== Text Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 169877.28 examples/s]\r\nKeys: ['text', 'label']\r\nSample count: 1000\r\nDuration: 0.03 s\r\nSpeed: 32236.1 samples/s\r\n\r\n=== Integers Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 131940.33 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.04 s\r\nSpeed: 25493.3 samples/s\r\n\r\n=== Floats Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 120621.64 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.04 s\r\nSpeed: 23370.6 samples/s\r\n```<end_of_turn>\n<start_of_turn>model\nfeat: avoid some copies in torch formatter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a dataset on HF [here](https://huggingface.co/datasets/kambale/luganda-english-parallel-corpus) that i've previously used to train a translation model [here](https://huggingface.co/kambale/pearl-11m-translate).\n\nnow i changed a few hyperparameters to increase number of tokens for the model, increase Transformer layers, and all\n\nhowever, when i try to load the dataset, this error keeps coming up.. i have tried everything.. i have re-written the code a hundred times, and this keep coming up\n\n### Steps to reproduce the bug\n\nImports:\n\n```bash\n!pip install datasets huggingface_hub fsspec\n```\n\nPython code:\n\n```python\nfrom datasets import load_dataset\n\nHF_DATASET_NAME = \"kambale/luganda-english-parallel-corpus\"\n\n# Load the dataset\ntry:\n    if not HF_DATASET_NAME or HF_DATASET_NAME == \"YOUR_HF_DATASET_NAME\":\n        raise ValueError(\n            \"Please provide a valid Hugging Face dataset name.\"\n        )\n\n    dataset = load_dataset(HF_DATASET_NAME)\n\n# Omitted code as the error happens on the line above \n\nexcept ValueError as ve:\n    print(f\"Configuration Error: {ve}\")\n    raise\nexcept Exception as e:\n    print(f\"An error occurred while loading the dataset '{HF_DATASET_NAME}': {e}\")\n    raise e\n```\n\nnow, i have tried going through this [issue](https://github.com/huggingface/datasets/issues/6737) and nothing helps \n\n### Expected behavior\n\nloading the dataset successfully and perform splits (train, test, validation)\n\n### Environment info\n\nfrom the imports, i do not install specific versions of these libraries, so the latest or available version is installed\n\n* `datasets` version: latest\n* `Platform`: Google Colab\n* `Hardware`: NVIDIA A100 GPU\n* `Python` version: latest\n* `huggingface_hub` version: latest\n* `fsspec` version: latest<end_of_turn>\n<start_of_turn>model\nValueError: Invalid pattern: '**' can only be an entire path component [Colab]<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix the `infer_module_for_data_files` and `infer_module_for_data_files_in_archives` functions when passed a data file name with uppercase extension, e.g. `filename.TXT`.\r\n\r\nBefore, `None` module was returned.<end_of_turn>\n<start_of_turn>model\nFix infer module for uppercase extensions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently, `Dataset.__repr__` outputs a dataset's column names and the number of rows. We could improve it by printing its features and the first few rows.\r\n\r\nWe should also implement `_repr_html_` to have a rich HTML representation in notebooks/Streamlit.<end_of_turn>\n<start_of_turn>model\nImprove `Dataset`'s string representation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nPython 3.14<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I load large parquet based datasets from the hub like `MLCommons/peoples_speech` using `load_dataset`, all my memory (500GB) is used and isn't released after loading, meaning that the process is terminated by the kernel if I try to load an additional dataset. This makes it impossible to train models using multiple large datasets.\n\n### Steps to reproduce the bug\n\n`uv run --with datasets==3.5.1 python`\n\n```python\nfrom datasets import load_dataset\nload_dataset('MLCommons/peoples_speech', 'clean')\nload_dataset('mozilla-foundation/common_voice_17_0', 'en')\n```\n\n### Expected behavior\n\nI would expect that a lot less than 500GB of RAM would be required to load the dataset, or at least that the RAM usage would be cleared as soon as the dataset is loaded (and thus reside as a memory mapped file) such that other datasets can be loaded.\n\n### Environment info\n\nI am currently using the latest datasets==3.5.1 but I have had the same problem with multiple other versions.<end_of_turn>\n<start_of_turn>model\nLarge memory use when loading large datasets to a ZFS pool<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n`Dataset` initialized from in-memory data (dictionary in my case, haven't tested with other types) does not create cache when processed with the `map` method, unlike `Dataset` initialized by other methods such as `load_dataset`.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\n# below code was run the second time so the map function can be loaded from cache if exists\r\n\r\nfrom datasets import load_dataset, Dataset\r\n\r\ndataset = load_dataset(\"tatsu-lab/alpaca\")['train']\r\n\r\ndataset = dataset.map(lambda x: {'input': x['input'] + 'hi'}) # some random map\r\nprint(len(dataset.cache_files))\r\n# 1\r\n\r\n# copy the exact same data but initialize from a dictionary\r\nmemory_dataset = Dataset.from_dict({\r\n    'instruction': dataset['instruction'],\r\n    'input': dataset['input'],\r\n    'output': dataset['output'],\r\n    'text': dataset['text']})\r\n\r\nmemory_dataset = memory_dataset.map(lambda x: {'input': x['input'] + 'hi'}) # exact same map\r\nprint(len(memory_dataset.cache_files))\r\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 52002[/52002]\r\n# 0\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe `map` function should create cache regardless of the method the `Dataset` was created.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.2\r\n- Platform: Linux-5.15.0-41-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\n[BUG] Dataset initialized from in-memory data does not create cache.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix `ConnectionError` for gated datasets and unauthenticated users. See:\r\n- https://github.com/huggingface/dataset-viewer/issues/3025\r\n\r\nNote that a recent change in the Hub returns dataset info for gated datasets and unauthenticated users, instead of raising a `GatedRepoError` as before. See:\r\n- https://github.com/huggingface/huggingface_hub/issues/2457\r\n\r\nThis PR adds an additional check (/auth-check) for gated datasets and raises `DatasetNotFoundError` for unauthenticated users, as it was the case before the change in the Hub.\r\n- Fix suggested by @Pierrci (thanks @Wauplin for pointing it out).\r\n\r\nFix #7109.<end_of_turn>\n<start_of_turn>model\nFix ConnectionError for gated datasets and unauthenticated users<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nA dict with its keys are all str but get following error\n```python\ntest_data=[{'input_ids':[1,2,3],'labels':[[Counter({2:1})]]}]\ndataset = datasets.Dataset.from_list(test_data)\n```\n```bash\npyarrow.lib.ArrowTypeError: Expected dict key of type str or bytes, got 'int'\n```\n\n### Steps to reproduce the bug\n\n.\n\n### Expected behavior\n\n.\n\n### Environment info\n\ndatasets 3.3.2<end_of_turn>\n<start_of_turn>model\npyarrow.lib.ArrowTypeError: Expected dict key of type str or bytes, got 'int'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have loaded a dataset, and then slice first 300 samples using `:` ops, however, the resulting dataset is not expected, as the output below:\r\n\r\n```bash\r\nlen(dataset)=1050324\r\nlen(dataset[:300])=2\r\nlen(dataset[0:300])=2\r\nlen(dataset.select(range(300)))=300\r\n```\n\n### Steps to reproduce the bug\n\nload a dataset then:\r\n```bash\r\n    dataset = load_from_disk(args.train_data_dir)\r\n\r\n    print(f\"{len(dataset)=}\", flush=True)\r\n    print(f\"{len(dataset[:300])=}\", flush=True)\r\n    print(f\"{len(dataset[0:300])=}\", flush=True)\r\n    print(f\"{len(dataset.select(range(300)))=}\", flush=True)\r\n```\n\n### Expected behavior\n\n```bash\r\nlen(dataset)=1050324\r\nlen(dataset[:300])=300\r\nlen(dataset[0:300])=300\r\nlen(dataset.select(range(300)))=300\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-5.15.0-60-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nStrange slicing behavior<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using `Dataset.map` with `num_proc > 1`, the speed slows down much if I add `import torch` to the start of the script even though I don't use it.\r\n\r\nI'm not sure if it's `torch` only or if any other package that is \"large\" will also cause the same result.\r\nBTW, `import lightning` also slows it down.\r\n\r\nBelow are the progress bars of `Dataset.map`, the only difference between them is with or without `import torch`, but the speed varies by 6-7 times.\r\n\r\n- without `import torch` ![image](https://github.com/huggingface/datasets/assets/47121592/0233055a-ced4-424a-9f0f-32a2afd802c2)\r\n- with `import torch` ![image](https://github.com/huggingface/datasets/assets/47121592/463eafb7-b81e-4eb9-91ca-fd7fe20f3d59)\r\n\n\n### Steps to reproduce the bug\n\nBelow is the code I used, but I don't think the dataset and the mapping function have much to do with the phenomenon.\r\n\r\n```python3\r\nfrom datasets import load_from_disk, disable_caching\r\nfrom transformers import AutoTokenizer\r\n# import torch\r\n# import lightning\r\n\r\ndef rearrange_datapoints(\r\n    batch,\r\n    tokenizer,\r\n    sequence_length,\r\n):\r\n    datapoints = []\r\n    \r\n    input_ids = []\r\n    for x in batch['input_ids']:\r\n        input_ids += x\r\n        while len(input_ids) >= sequence_length:\r\n            datapoint = input_ids[:sequence_length]\r\n            datapoints.append(datapoint)\r\n            input_ids[:sequence_length] = []\r\n\r\n    if input_ids:\r\n        paddings = [-1] * (sequence_length - len(input_ids))\r\n        datapoint = paddings + input_ids if tokenizer.padding_side == 'left' else input_ids + paddings\r\n        datapoints.append(datapoint)\r\n    \r\n    batch['input_ids'] = datapoints\r\n    return batch\r\n\r\n\r\nif __name__ == '__main__':\r\n    disable_caching()\r\n\r\n    tokenizer = AutoTokenizer.from_pretrained('...', use_fast=False)\r\n    dataset = load_from_disk('...')\r\n    \r\n    dataset = dataset.map(\r\n        rearrange_datapoints,\r\n        fn_kwargs=dict(\r\n            tokenizer=tokenizer,\r\n            sequence_length=2048,\r\n        ),\r\n        batched=True,\r\n        num_proc=8,\r\n    )\r\n\r\n```\n\n### Expected behavior\n\nThe multi-processed `Dataset.map` function speed between with and without `import torch` should be the same.\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-3.10.0-1127.el7.x86_64-x86_64-with-glibc2.31\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1<end_of_turn>\n<start_of_turn>model\nMulti-processed `Dataset.map` slows down a lot when `import torch`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6805<end_of_turn>\n<start_of_turn>model\nadd allow_primitive_to_str and allow_decimal_to_str instead of allow_number_to_str<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHello datasets!\r\n\r\nWe often encounter situations where we need to preprocess data differently depending on split types such as train, valid, and test. \r\n\r\nHowever, while DatasetDict.map has features to pass rank or index, there's no functionality to pass split types. \r\nTherefore, I propose adding a 'with_splits' parameter to DatasetDict, which would allow passing the split type through fn_kwargs.<end_of_turn>\n<start_of_turn>model\nFeature Request: Add functionality to pass split types like train, test in DatasetDict.map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe doc of `datasets` v2.17.0/v2.17.1 shows that `remove_columns` is in-place. [link](https://huggingface.co/docs/datasets/v2.17.1/en/package_reference/main_classes#datasets.DatasetDict.remove_columns)\r\n\r\nIn the text classification example of transformers v4.38.1, the columns are not removed.\r\nhttps://github.com/huggingface/transformers/blob/a0857740c0e6127485c11476650314df3accc2b6/examples/pytorch/text-classification/run_classification.py#L421\r\n\r\n### Steps to reproduce the bug\r\n\r\nhttps://github.com/huggingface/transformers/blob/a0857740c0e6127485c11476650314df3accc2b6/examples/pytorch/text-classification/run_classification.py#L421\r\n\r\n### Expected behavior\r\n\r\nActually remove the columns.\r\n\r\n### Environment info\r\n\r\n1. datasets v2.17.0\r\n2. transformers v4.38.1<end_of_turn>\n<start_of_turn>model\nremove_columns is not in-place but the doc shows it is in-place<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHow should I write \"config\" in readme when all the data, such as train test, is in a zip file\r\n\r\ntrain floder and test floder in data.zip<end_of_turn>\n<start_of_turn>model\nHow to configure multiple folders in the same zip package<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRaise error when `lines = False` and `batch_size < Dataset.num_rows` in `Dataset.to_json()`.\r\n\r\nIssue: #7037 \r\n\r\nRelated PRs:\r\n#7039 #7181 <end_of_turn>\n<start_of_turn>model\nRaise error for incorrect JSON serialization<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate ruff<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`Dataset.from_generator` function passes all of its arguments to `BuilderConfig.create_config_id`, including generator function itself. `BuilderConfig.create_config_id` function tries to hash all the args, and hashing a `generator` can take a large amount of time or even cause MemoryError if the dataset processed in a generator function is large enough.\r\n\r\nMaybe we should pop generator from `config_kwargs_to_add_to_suffix` before hashing to avoid it.\r\n\r\nThere is a more detailed description of the problem this PR solves in #7513 <end_of_turn>\n<start_of_turn>model\nDo not hash `generator` in `BuilderConfig.create_config_id`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\n\r\nMy task is classification of a dataset containing a large label set that includes a hierarchy. Even ignoring the hierarchy I'm not able to find an example using `datasets` where the label names aren't hard-coded. This works find for classification of a handful of labels but ideally there would be a way of loading the name/id mappings required for `datasets.features.ClassLabel` from a file.\r\n\r\nIt is possible to pass a file to ClassLabel but I cannot see an easy way of using this with `GeneratorBasedBuilder` since `self._info` is called before the `dl_manager` is constructed so even if my dataset contains say `label_mappings.json` there's no way of loading it in order to construct the `datasets.DatasetInfo`\r\n\r\nI can see other uses to accessing the `download_manager` from `self._info` - i.e. if the files contain a schema (i.e. `arrow` or `parquet` files) the `datasets.DatasetInfo` could be inferred.\r\n\r\nThe workaround that was suggested in the forum is to generate a `.py` file from the `label_mappings.json` and import it.\r\n\r\n```\r\nclass TestDatasetBuilder(datasets.GeneratorBasedBuilder):\r\n    VERSION = datasets.Version(\"1.0.0\")\r\n\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            description=_DESCRIPTION,\r\n            features=datasets.Features(\r\n                {\r\n                    \"text\": datasets.Value(\"string\"),\r\n                    \"label\": datasets.features.ClassLabel(names=[\"label_1\", \"label_2\"]),\r\n                }\r\n            ),\r\n            task_templates=[TextClassification(text_column=\"text\", label_column=\"label\")],\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        train_path = dl_manager.download_and_extract(_TRAIN_DOWNLOAD_URL)\r\n        test_path = dl_manager.download_and_extract(_TEST_DOWNLOAD_URL)\r\n        return [\r\n            datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": train_path}),\r\n            datasets.SplitGenerator(name=datasets.Split.TEST, gen_kwargs={\"filepath\": test_path}),\r\n        ]\r\n\r\n    def _generate_examples(self, filepath):\r\n        \"\"\"Generate AG News examples.\"\"\"\r\n        with open(filepath, encoding=\"utf-8\") as csv_file:\r\n            csv_reader = csv.DictReader(csv_file)\r\n            for id_, row in enumerate(csv_reader):\r\n                yield id_, row\r\n```\n\n### Motivation\n\nAllow `datasets.DatasetInfo` to be generated based on the contents of the dataset. \n\n### Your contribution\n\nI'm willing to work on a PR with guidence.<end_of_turn>\n<start_of_turn>model\nProvide ability to load label mappings from file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn the client documentation, the markdown 'TIP' paragraph for paragraph in docs/stream#shuffle is not well executed \u2014 not as the other in the same page / while markdown is correctly considering it.\n\nDocumentation:\nhttps://huggingface.co/docs/datasets/v4.3.0/en/stream#shuffle:~:text=%5B!TIP%5D%5BIterableDataset.shuffle()%5D(/docs/datasets/v4.3.0/en/package_reference/main_classes%23datasets.IterableDataset.shuffle)%20will%20also%20shuffle%20the%20order%20of%20the%20shards%20if%20the%20dataset%20is%20sharded%20into%20multiple%20files.\n\nGithub source:\nhttps://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#:~:text=Casting%20only%20works%20if%20the%20original%20feature%20type%20and%20new%20feature%20type%20are%20compatible.%20For%20example%2C%20you%20can%20cast%20a%20column%20with%20the%20feature%20type%20Value(%27int32%27)%20to%20Value(%27bool%27)%20if%20the%20original%20column%20only%20contains%20ones%20and%20zeros.<end_of_turn>\n<start_of_turn>model\n[DOCS][minor] TIPS paragraph not compiled in docs/stream<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRaise an error when a nested object is expected to be a mapping that displays the object<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe huggingface dataset library specifically looks for \u2018.py\u2019 file while loading the dataset using loading script approach and it does not work with \u2018.pyc\u2019 file.\r\nWhile deploying in production, it becomes an issue when we are restricted to use only .pyc files. Is there any work around for this ?\n\n### Steps to reproduce the bug\n\n1. Create a dataset  loading script to read the custom data.\r\n2. compile the code to make sure that .pyc file is created \r\n3. Delete the loading script and re-run the code. Usually, python should make use of complied .pyc files. However, in this case, the dataset library errors out with the message that it's unable to find the data loader loading script.\n\n### Expected behavior\n\nThe code should make use of .pyc file and run without any error.\n\n### Environment info\n\nNA<end_of_turn>\n<start_of_turn>model\nDataset loading script method does not work with .pyc file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n**Is your feature request related to a problem? Please describe.**\r\nI recently chose to use huggingface hub as the home for a large multi modal dataset I've been building. https://huggingface.co/datasets/Antreas/TALI\r\n\r\nIt combines images, text, audio and video. Now, I could very easily upload a dataset made via datasets.Dataset.from_generator, as long as it did not include video files. I found that including .mp4 files in the entries would not auto-upload those files. \r\n\r\nHence I tried to upload them myself. I quickly found out that uploading many small files is a very bad way to use git lfs, and that it would take ages, so, I resorted to using 7z to pack them all up. But then I had a new problem.\r\n\r\nMy dataset had a size of 1.9TB. Trying to upload such a large file with the default huggingface_hub API always resulted in time outs etc. So I decided to split the large files into chunks of 5GB each and reupload. \r\n\r\nSo, eventually it all worked out. But now the dataset can't be properly and natively used by the datasets API because of all the needed preprocessing -- and furthermore the hub is unable to visualize things. \r\n\r\n**Describe the solution you'd like**\r\nA native way to upload large datasets that include .mp4 or other video types.\r\n\r\n**Describe alternatives you've considered**\r\nAlready explained earlier\r\n\r\n**Additional context**\r\nhttps://huggingface.co/datasets/Antreas/TALI\r\n<end_of_turn>\n<start_of_turn>model\nA way to upload and visualize .mp4 files (millions of them) as part of a dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading a `.jsonl` file using `load_dataset(\"json\", data_files=\"data.jsonl\", split=\"train\")`, the function misinterprets the content of a string field  as if it were part of the dataset schema. \n\nIn my case there is a field `body:` with a string value \n```\n\"### Describe the bug (...) ,action: string, datetime: timestamp[s], author: string, (...) Pandas version: 1.3.4\"\n```\nAs a result, I got an exception \n```\n\"TypeError: Couldn't cast array of type timestamp[s] to null\". \n```\nFull stack-trace in the attached file below.\n\nI also attach a minimized dataset (data.json, a single entry) that reproduces the error.\n\n**Observations**(on the minimal example): \n- if I remove _all fields before_ `body`, a different error appears,\n- if I remove _all fields after_ `body`, yet another error appears,\n- if `body` is _the only field_, the error disappears.\n\nSo this might be one complex bug or several edge cases interacting. I haven\u2019t dug deeper.\n\nAlso changing the file extension to `.json` or `.txt` avoids the problem. This suggests **a possible workaround** for the general case: convert `.jsonl` to `.json`. Though I haven\u2019t verified correctness of that workaround yet.\n\nAnyway my understanding is that `load_dataset` with first argument set to \"json\" should properly handle `.jsonl` files. Correct me if I'm wrong.\n\n[stack_trace.txt](https://github.com/user-attachments/files/21004153/stack_trace.txt)\n[data.json](https://github.com/user-attachments/files/21004164/data.json)\n\nP.S.\nI discovered this while going through the HuggingFace tutorial. Specifically [this part](https://huggingface.co/learn/llm-course/chapter5/5?fw=pt).I will try to inform the tutorial team about this issue, as it can be a showstopper for young \ud83e\udd17 adepts.\n\n### Steps to reproduce the bug\n\n1. Download attached [data.json](https://github.com/user-attachments/files/21004164/data.json) file.\n2. Run the following code which should work correctly:\n```\nfrom datasets import load_dataset\nload_dataset(\"json\", data_files=\"data.json\", split=\"train\")\n```\n3. Change extension of the `data` file to `.jsonl` and run:\n```\nfrom datasets import load_dataset\nload_dataset(\"json\", data_files=\"data.jsonl\", split=\"train\")\n```\n\nThis will trigger an error like the one in the attached [stack_trace.txt](https://github.com/user-attachments/files/21004153/stack_trace.txt).\n\nOne can also try removing fields before the `body` field and after it. These actions give different errors.\n\n\n### Expected behavior\n\nParsing data in `.jsonl` format should yield the same result as parsing the same data in `.json` format. In any case, the content of a string field should never be interpreted as part of the dataset schema.\n\n### Environment info\n\ndatasets version: _3.6.0_\npyarrow version: _20.0.0_\nPython version: _3.11.9_\nplatform version: _macOS-15.5-arm64-arm-64bit_<end_of_turn>\n<start_of_turn>model\nFunction load_dataset() misinterprets string field content as part of dataset schema when dealing with `.jsonl` files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUnable to load `eli5` dataset with streaming.\n\n### Steps to reproduce the bug\n\nThis fails with FileNotFoundError: https://files.pushshift.io/reddit/submissions \r\n```\r\nfrom datasets import load_dataset\r\nload_dataset(\"eli5\", streaming=True)\r\n```\r\n\r\nThis works correctly.\r\n```\r\nfrom datasets import load_dataset\r\nload_dataset(\"eli5\")\r\n```\n\n### Expected behavior\n\n- Loading `eli5` dataset should not raise an error under the streaming mode.\r\n- Or at the very least, show a warning that streaming mode is not supported with `eli5` dataset.\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-6.2.0-39-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.6.0\r\n<end_of_turn>\n<start_of_turn>model\nUnable to load `eli5` dataset with streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi all! This is a PR adding a troubleshooting guide for Datasets docs. \r\nI went through the library's GitHub Issues and Forum questions and identified a few issues that are common enough that I think it would be valuable to include them in the troubleshooting guide. These are: \r\n- creating a dataset from a folder and not following the required format\r\n- authentication issues when using `push_to_hub`\r\n- `Too Many Requests` with `push_to_hub`\r\n- Pickling issues when using Dataset.from_generator()\r\n\r\nThere's also a section on asking for help. Please let me know if there are other common issues or advice that we can include here. <end_of_turn>\n<start_of_turn>model\n[docs] troubleshooting guide<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes a case when concatenating or interleaving datasets with `with_format(...)` call was slower.\r\nDetails here: https://github.com/huggingface/datasets/issues/6637\r\n\r\n@lhoestq I tried to minimize the duplication between iter and iter_arrow methods, not sure if this is against the design, can separate those if needed.<end_of_turn>\n<start_of_turn>model\nAdd support for arrow iterable when concatenating or interleaving<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...after last pr errors<end_of_turn>\n<start_of_turn>model\nFix ci<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have been fighting against my machine since this morning only to find out this is some kind of a bug.\r\n\r\nWhen loading a dataset composed of `metadata.jsonl`, if you have nullable values (Optional[str]), they can be ignored by the parser, shifting things around.\r\n\r\nE.g., let's take this example\r\n\r\nHere are two version of a same dataset:\r\n\r\n[not-buggy.tar.gz](https://github.com/user-attachments/files/16333532/not-buggy.tar.gz)\r\n[buggy.tar.gz](https://github.com/user-attachments/files/16333553/buggy.tar.gz)\r\n\n\n### Steps to reproduce the bug\n\n1. Load the `buggy.tar.gz` dataset\r\n2. Print baseline of `dts = load_dataset(\"./data\")[\"train\"][0][\"baselines]`\r\n3. Load the `not-buggy.tar.gz` dataset\r\n4. Print baseline of `dts = load_dataset(\"./data\")[\"train\"][0][\"baselines]`\n\n### Expected behavior\n\nBoth should have 4 baseline entries:\r\n1. Buggy should have None followed by three lists\r\n2. Non-Buggy should have four lists, and the first one should be an empty list.\r\n\r\nOne does not work, 2 works. Despite accepting None in another position than the first one.\n\n### Environment info\n\n\r\n- `datasets` version: 2.19.1\r\n- Platform: Linux-6.5.0-44-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.23.0\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.3.1\r\n<end_of_turn>\n<start_of_turn>model\nNone values are skipped when reading jsonl in subobjects<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSet `load_from_disk` path type as `PathLike`. This way it is aligned with `save_to_disk`.<end_of_turn>\n<start_of_turn>model\nSet load_from_disk path type as PathLike<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nwhen I use load_dataset to load my local datasets\uff0cit always goes to Hugging Face to download the data instead of loading the local dataset.\n\n### Steps to reproduce the bug\n\n`raw_datasets = load_dataset(\r\n        \u2018json\u2019,\r\n        data_files=data_files)`\r\n\n\n### Expected behavior\n\n![image](https://github.com/huggingface/datasets/assets/50519434/add3747f-6481-4da7-b374-8f81c5a6472c)\r\n\n\n### Environment info\n\npython version 3.8.5\r\ndatasets version 2.12\r\nos version unbuntu 18.04<end_of_turn>\n<start_of_turn>model\nUse load_dataset for local json files, but it not works<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRight now this code is failing on `main`:\r\n\r\n```python\r\nload_dataset(\"openbookqa\")\r\n```\r\n\r\nThis is because it tries to load the dataset from the Parquet export but the dataset has multiple configurations and the Parquet export doesn't know which one is the default one.\r\n\r\nI fixed this by simply falling back on using the dataset script (which tells the user to pass `trust_remote_code=True`):\r\n\r\n```python\r\nload_dataset(\"openbookqa\", trust_remote_code=True)\r\n```\r\n\r\nNote that if the user happened to specify a config name I don't fall back on the script since we can use the Parquet export in this case (no need to know which config is the default)\r\n\r\n\r\n```python\r\nload_dataset(\"openbookqa\", \"main\")\r\n```<end_of_turn>\n<start_of_turn>model\nFallback on dataset script if user wants to load default config<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\r\nOverview:\r\nThe repository (huggingface/datasets-viewer) was recently archived and when I tried to run the code, there was the error message \"AttributeError: module 'datasets.load' has no attribute 'prepare_module'\". I could not resolve the issue myself due to lack of documentation of that attribute. \r\n\r\nRequest:\r\nI kindly request the sharing of the code responsible for the dataset preview functionality or help with resolving the error. The dataset viewer on the Hugging Face website is incredibly useful since it is compatible with different types of inputs. It allows users to find datasets that meet their needs more efficiently. If needed, I am willing to contribute to the project by testing, documenting, and providing feedback on the dataset viewer code. \r\n\r\nThank you for considering this request, and I look forward to your response.<end_of_turn>\n<start_of_turn>model\nRequest to Share/Update Dataset Viewer Code<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport HTTP authentication in non-streaming mode, by support passing HTTP storage_options in non-streaming mode.\r\n- Note that currently, HTTP authentication is supported only in streaming mode.\r\n\r\nFor example, this is necessary if a remote HTTP host requires authentication to download the data.<end_of_turn>\n<start_of_turn>model\nSupport HTTP authentication in non-streaming mode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm on Windows and i'm trying to load a datasets but i'm having title error because files in the repository are named with charactere like < >which can't be in a name file. Could it be possible to load this datasets but removing those charactere ?\n\n### Steps to reproduce the bug\n\nload_dataset(\"CATMuS/medieval\") on Windows\n\n### Expected behavior\n\nMaking the function to erase the forbidden character to allow loading the datasets who have those characters. \n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Windows-10-10.0.19045-SP0\n- Python version: 3.12.2\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nOSError: [Errno 22] Invalid argument forbidden character<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn `ds.push_to_hub()` and `ds.save_to_disk()`, `num_shards` must be smaller than or equal to the number of rows in the dataset, but currently this is not checked anywhere inside these functions. Attempting to invoke these functions with `num_shards > len(dataset)` should raise an informative `ValueError`.\n\nI frequently work with datasets with a small number of rows where each row is pretty large, so I often encounter this issue, where the function runs until the shard index in `ds.shard(num_shards, indx)` goes out of bounds. Ideally, a `ValueError` should be raised before reaching this point (i.e. as soon as `ds.push_to_hub()` or `ds.save_to_disk()` is invoked with `num_shards > len(dataset)`).\n\nIt seems that adding something like:\n```python\nif len(self) < num_shards:\n   raise ValueError(f\"num_shards ({num_shards}) must be smaller than or equal to the number of rows in the dataset ({len(self)}). Please either reduce num_shards or increase max_shard_size to make sure num_shards <= len(dataset).\")\n```\nto the beginning of the definition of the `ds.shard()` function [here](https://github.com/huggingface/datasets/blob/f693f4e93aabafa878470c80fd42ddb10ec550d6/src/datasets/arrow_dataset.py#L4728) would deal with this issue for both `ds.push_to_hub()` and `ds.save_to_disk()`, but I'm not exactly sure if this is the best place to raise the `ValueError` (it seems that a more correct way to do it would be to write separate checks for `ds.push_to_hub()` and `ds.save_to_disk()`). I'd be happy to submit a PR if you think something along these lines would be acceptable.<end_of_turn>\n<start_of_turn>model\nindex error when num_shards > len(dataset)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nrelated to https://github.com/huggingface/datasets/issues/6066<end_of_turn>\n<start_of_turn>model\nfix tqdm lock deletion<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAfter loading an audio dataset, and looking at a sample entry, the `path` element, which is supposed to be the path to the audio file, doesn't actually exist.\r\n\n\n### Steps to reproduce the bug\n\nRun bug.py:\r\n\r\n```py\r\nimport os.path\r\n\r\nfrom datasets import load_dataset\r\n\r\ndef run() -> None:\r\n    cv13 = load_dataset(\r\n        \"mozilla-foundation/common_voice_13_0\",\r\n        \"hi\",\r\n        split=\"train\",\r\n    )\r\n\r\n    print(cv13[0])\r\n    audio_file = cv13[0][\"path\"]\r\n    if not os.path.exists(audio_file):\r\n        raise ValueError(f'File {audio_file} does not exist.')\r\n\r\nif __name__ == \"__main__\":\r\n    run()\r\n```\r\n\r\nThe result (on my machine):\r\n\r\n```json\r\n{'client_id': '0f018a99663f33afbb7d38aee281fb1afcfd07f9e7acd00383f604e1e17c38d6ed8adf1bd2ccbf927a52c5adefb8ac4b158ce27a7c2ed9581e71202eb302dfb3', 'path': 'C:\\\\Users\\\\rober\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\\\common_voice_hi_26008353.mp3', 'audio': {'path': 'C:\\\\Users\\\\rober\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\\\common_voice_hi_26008353.mp3', 'array': array([ 6.46234854e-26, -1.35709319e-25, -8.07793567e-26, ...,\r\n        1.06425944e-07,  4.46417090e-08,  2.61451660e-09]), 'sampling_rate': 48000}, 'sentence': '\u0939\u092e\u0928\u0947 \u0909\u0938\u0915\u093e \u091c\u0928\u094d\u092e\u0926\u093f\u0928 \u092e\u0928\u093e\u092f\u093e\u0964', 'up_votes': 2, 'down_votes': 0, 'age': '', 'gender': '', 'accent': '', 'locale': 'hi', 'segment': '' ', 'variant': ''}\r\n```\r\n\r\n```txt\r\nTraceback (most recent call last):\r\n  File \"F:\\eo-reco\\bug.py\", line 18, in <module>\r\n    run()\r\n  File \"F:\\eo-reco\\bug.py\", line 15, in run\r\n    raise ValueError(f'File {audio_file} does not exist.')\r\nValueError: File C:\\Users\\rober\\.cache\\huggingface\\datasets\\downloads\\extracted\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\common_voice_hi_26008353.mp3 does not exist.\r\n```\r\n\n\n### Expected behavior\n\nThe `path` element points to the correct file, which happens to be:\r\n\r\n```\r\nC:\\Users\\rober\\.cache\\huggingface\\datasets\\downloads\\extracted\\8d1479bc09b4609bc2675bd02d6869a4d5e09f7e6616f540bd55eacef46c6e2b\\hi_train_0\\common_voice_hi_26008353.mp3\r\n```\r\n\r\nThat is, there's an extra directory `hi_train_0` that is not in the `path` element.\r\n\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.11.3\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1\r\n- <end_of_turn>\n<start_of_turn>model\nFile not found for audio dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nupdate docs and docstrings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes a regression from #7553 breaking shuffling of iterable datasets \r\n<img width=\"884\" alt=\"Screenshot 2025-05-07 at 9 16 52\u202fAM\" src=\"https://github.com/user-attachments/assets/d2f43c5f-4092-4efe-ac31-a32cbd025fe3\" />\r\n<end_of_turn>\n<start_of_turn>model\ncheck for empty _formatting<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nYou get a pretty unhelpful error message when specifying a `cache_file_name` in a directory that doesn't exist, e.g. `cache_file_name=\"./cache/data.map\"`\r\n\r\n```python\r\nimport datasets\r\n\r\ncache_file_name=\"./cache/train.map\"\r\ndataset = datasets.load_dataset(\"ylecun/mnist\")\r\ndataset[\"train\"].map(lambda x: x, cache_file_name=cache_file_name)\r\n```\r\n\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: '/.../cache/tmp48r61siw'\r\n```\r\n\r\nIt is simple enough to create and I was expecting that this would have been the case.\r\n\r\ncc: @albertvillanova @lhoestq <end_of_turn>\n<start_of_turn>model\nAutomatically create `cache_dir` from `cache_file_name`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAfter ruff-0.5.0 release (https://github.com/astral-sh/ruff/releases/tag/0.5.0), our CI is broken due to E721 rule. \r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/9707641618/job/26793170961?pr=6983\r\n> src/datasets/features/features.py:844:12: E721 Use `is` and `is not` for type comparisons, or `isinstance()` for isinstance checks<end_of_turn>\n<start_of_turn>model\nCI is broken after ruff-0.5.0: E721<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIn the latest version of datasets there is a major regression, after creating an `IterableDataset` from a generator and applying a few operations (`map`, `select`), you can no longer iterate through the dataset multiple times.\r\n\r\nThe issue seems to stem from the recent addition of \"resumable IterableDatasets\" (#6658) (@lhoestq). It seems like it's keeping state when it shouldn't.\r\n\r\n\n\n### Steps to reproduce the bug\n\nMinimal Reproducible Example (comparing `datasets==2.17.0` and `datasets==2.20.0`)\r\n\r\n```\r\n#!/bin/bash\r\n\r\n# List of dataset versions to test\r\nversions=(\"2.17.0\" \"2.20.0\")\r\n\r\n# Loop through each version\r\nfor version in \"${versions[@]}\"; do\r\n    # Install the specific version of the datasets library\r\n    pip3 install -q datasets==\"$version\" 2>/dev/null\r\n\r\n    # Run the Python script\r\n    python3 - <<EOF\r\nfrom datasets import IterableDataset\r\nfrom datasets.features.features import Features, Value\r\n\r\ndef test_gen():\r\n    yield from [{\"foo\": i} for i in range(10)]\r\n\r\nfeatures = Features([(\"foo\", Value(\"int64\"))])\r\nd = IterableDataset.from_generator(test_gen, features=features)\r\nmapped = d.map(lambda row: {\"foo\": row[\"foo\"] * 2})\r\ncolumn = mapped.select_columns([\"foo\"])\r\nprint(\"Version $version - Iterate Once:\", list(column))\r\nprint(\"Version $version - Iterate Twice:\", list(column))\r\nEOF\r\ndone\r\n```\r\n\r\nThe output looks like this:\r\n```\r\nVersion 2.17.0 - Iterate Once: [{'foo': 0}, {'foo': 2}, {'foo': 4}, {'foo': 6}, {'foo': 8}, {'foo': 10}, {'foo': 12}, {'foo': 14}, {'foo': 16}, {'foo': 18}]\r\nVersion 2.17.0 - Iterate Twice: [{'foo': 0}, {'foo': 2}, {'foo': 4}, {'foo': 6}, {'foo': 8}, {'foo': 10}, {'foo': 12}, {'foo': 14}, {'foo': 16}, {'foo': 18}]\r\nVersion 2.20.0 - Iterate Once: [{'foo': 0}, {'foo': 2}, {'foo': 4}, {'foo': 6}, {'foo': 8}, {'foo': 10}, {'foo': 12}, {'foo': 14}, {'foo': 16}, {'foo': 18}]\r\nVersion 2.20.0 - Iterate Twice: []\r\n```\n\n### Expected behavior\n\nThe expected behavior is it version 2.20.0 should behave the same as 2.17.0.\n\n### Environment info\n\n`datasets==2.20.0` on any platform.<end_of_turn>\n<start_of_turn>model\n[Regression] IterableDataset is broken on 2.20.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am performing two operations I see on a hugging face tutorial (Fine-tune a language model), and I am defining every aspect inside the mapped functions, also some imports of the library because it doesnt identify anything not defined outside that function where the dataset elements are being mapped:\r\n\r\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb#scrollTo=iaAJy5Hu3l_B\r\n\r\n`- lm_datasets = tokenized_datasets.map(\r\n    group_texts,\r\n    batched=True,\r\n    batch_size=batch_size,\r\n    num_proc=4,\r\n)\r\n\r\n- tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=[\"text\"])\r\ndef tokenize_function(examples):\r\n    model_checkpoint = 'gpt2'\r\n    from transformers import AutoTokenizer\r\n    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\r\n    return tokenizer(examples[\"text\"])`\r\n\n\n### Steps to reproduce the bug\n\nCurrently handle all the imports inside the function\n\n### Expected behavior\n\nThe code must work es expected in the notebook, but currently this is not happening.\r\n\r\nhttps://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb#scrollTo=iaAJy5Hu3l_B\n\n### Environment info\n\nprint(transformers.__version__)\r\n\r\n4.46.1<end_of_turn>\n<start_of_turn>model\nload_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7793 \r\n\r\n## Problem\r\nLoading datasets with deeply nested structures (like `metr-evals/malt-public`) fails with:\r\n\r\nArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\r\nThis occurs when parquet files contain nested data (lists, structs, maps) that exceed PyArrow's 16MB chunk limit.\r\n\r\n## Root Cause\r\nPyArrow's C++ implementation explicitly rejects nested data conversions when data is split across multiple chunks. The limitation exists in the `WrapIntoListArray` function where repetition levels cannot be reconstructed across chunk boundaries.\r\n\r\n## Solution\r\n- **Fallback mechanism**: Catches the specific PyArrow error and switches to non-chunked reading\r\n- **Selective optimization**: Only combines chunks for problematic nested columns to minimize memory impact  \r\n- **Manual batching**: Maintains batching behavior even in fallback mode\r\n- **Backward compatibility**: Zero impact on existing datasets\r\n\r\n## Implementation Details\r\n- Added `_is_nested_type()` helper to detect nested PyArrow types\r\n- Added `_handle_nested_chunked_conversion()` for selective chunk combining\r\n- Modified `_generate_tables()` to catch and handle the specific error\r\n- Preserves all existing error handling and logging\r\n\r\n## Testing\r\n- [x] No regressions: Normal parquet datasets continue working\r\n- [x] Code follows existing patterns in the datasets codebase\r\n- [x]  tested by original reporter (gated dataset access needed)\r\n\r\n**Note**: This fix is based on thorough research of PyArrow limitations and similar issues in the ecosystem. While we cannot test with the original dataset due to access restrictions, the implementation follows established patterns for handling this PyArrow limitation.\r\n\r\n## Request for Testing\r\n@neevparikh Could you please test this fix with your original failing dataset? The implementation should resolve the nested data conversion error you encountered.<end_of_turn>\n<start_of_turn>model\nFix nested data conversions error in parquet loading (fixes #7793)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI didn't notice the path while reviewing the PR yesterday :(<end_of_turn>\n<start_of_turn>model\nMove `exceptions.py` to `utils/exceptions.py`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nuse `os.path.splitext` to parse field_name.\r\n\r\nfix filename which has dot. like:\r\n\r\n```\r\na.b.jpeg\r\na.b.txt\r\n```<end_of_turn>\n<start_of_turn>model\nfix webdataset filename split<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nIf one installs `apache-beam` alongside `datasets` (which is required for the [wikipedia](https://huggingface.co/datasets/wikipedia#dataset-summary) dataset) in certain environments (such as a Google Colab notebook), they appear to install successfully, however, actually trying to do something such as importing the `load_dataset` method from `datasets` results in a crashing error.\r\n\r\n\r\nI think the problem is that `apache-beam` version 2.49.0 requires `dill>=0.3.1.1,<0.3.2`, but the latest version of `multiprocess` (0.70.15) (on which `datasets` depends) requires  `dill>=0.3.7,`, so this is causing the dependency resolver to use an older version of `multiprocess` which leads to the `datasets` crashing since it doesn't actually appear to be compatible with older versions.\r\n\r\n### Steps to reproduce the bug\r\n\r\nSee this [Google Colab notebook](https://colab.research.google.com/drive/1PTeGlshamFcJZix_GiS3vMXX_YzAhGv0?usp=sharing) to easily reproduce the bug.\r\n\r\nIn some environments, I have been able to reproduce the bug by running the following in Bash:\r\n\r\n```bash\r\n$ pip install datasets apache-beam\r\n```\r\n\r\nthen the following in a Python shell:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n```\r\n\r\nHere is my stacktrace from running on Google Colab:\r\n\r\n<details>\r\n<summary>stacktrace</summary>\r\n\r\n```\r\n[/usr/local/lib/python3.10/dist-packages/datasets/__init__.py](https://localhost:8080/#) in <module>\r\n     20 __version__ = \"2.14.4\"\r\n     21 \r\n---> 22 from .arrow_dataset import Dataset\r\n     23 from .arrow_reader import ReadInstruction\r\n     24 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in <module>\r\n     64 \r\n     65 from . import config\r\n---> 66 from .arrow_reader import ArrowReader\r\n     67 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n     68 from .data_files import sanitize_patterns\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py](https://localhost:8080/#) in <module>\r\n     28 import pyarrow.parquet as pq\r\n     29 \r\n---> 30 from .download.download_config import DownloadConfig\r\n     31 from .naming import _split_re, filenames_for_dataset_split\r\n     32 from .table import InMemoryTable, MemoryMappedTable, Table, concat_tables\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/download/__init__.py](https://localhost:8080/#) in <module>\r\n      7 \r\n      8 from .download_config import DownloadConfig\r\n----> 9 from .download_manager import DownloadManager, DownloadMode\r\n     10 from .streaming_download_manager import StreamingDownloadManager\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/download/download_manager.py](https://localhost:8080/#) in <module>\r\n     33 from ..utils.info_utils import get_size_checksum_dict\r\n     34 from ..utils.logging import get_logger, is_progress_bar_enabled, tqdm\r\n---> 35 from ..utils.py_utils import NestedDataStructure, map_nested, size_str\r\n     36 from .download_config import DownloadConfig\r\n     37 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/utils/py_utils.py](https://localhost:8080/#) in <module>\r\n     38 import dill\r\n     39 import multiprocess\r\n---> 40 import multiprocess.pool\r\n     41 import numpy as np\r\n     42 from packaging import version\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py](https://localhost:8080/#) in <module>\r\n    607 #\r\n    608 \r\n--> 609 class ThreadPool(Pool):\r\n    610 \r\n    611     from .dummy import Process\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/pool.py](https://localhost:8080/#) in ThreadPool()\r\n    609 class ThreadPool(Pool):\r\n    610 \r\n--> 611     from .dummy import Process\r\n    612 \r\n    613     def __init__(self, processes=None, initializer=None, initargs=()):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/multiprocess/dummy/__init__.py](https://localhost:8080/#) in <module>\r\n     85 #\r\n     86 \r\n---> 87 class Condition(threading._Condition):\r\n     88     # XXX\r\n     89     if sys.version_info < (3, 0):\r\n\r\nAttributeError: module 'threading' has no attribute '_Condition'\r\n```\r\n\r\n</details>\r\n\r\nI've also found that attempting to install these `datasets` and `apache-beam` in certain environments (e.g. via pip inside a conda env) simply causes pip to hang indefinitely.\r\n\r\n### Expected behavior\r\n\r\nI would expect to be able to import methods from `datasets` without crashing. I have tested that this is possible as long as I do not attempt to install `apache-beam`.\r\n\r\n### Environment info\r\n\r\nGoogle Colab<end_of_turn>\n<start_of_turn>model\n`datasets` cannot be installed alongside `apache-beam`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI have been trying to load 'amazon_us_dataset\" but unable to do so. \r\n\r\n`amazon_us_reviews = load_dataset('amazon_us_reviews')`\r\n`print(amazon_us_reviews)`\r\n\r\n\r\n> [ValueError: Config name is missing.\r\n\r\nPlease pick one among the available configs: ['Wireless_v1_00', 'Watches_v1_00', 'Video_Games_v1_00', 'Video_DVD_v1_00', 'Video_v1_00', 'Toys_v1_00', 'Tools_v1_00', 'Sports_v1_00', 'Software_v1_00', 'Shoes_v1_00', 'Pet_Products_v1_00', 'Personal_Care_Appliances_v1_00', 'PC_v1_00', 'Outdoors_v1_00', 'Office_Products_v1_00', 'Musical_Instruments_v1_00', 'Music_v1_00', 'Mobile_Electronics_v1_00', 'Mobile_Apps_v1_00', 'Major_Appliances_v1_00', 'Luggage_v1_00', 'Lawn_and_Garden_v1_00', 'Kitchen_v1_00', 'Jewelry_v1_00', 'Home_Improvement_v1_00', 'Home_Entertainment_v1_00', 'Home_v1_00', 'Health_Personal_Care_v1_00', 'Grocery_v1_00', 'Gift_Card_v1_00', 'Furniture_v1_00', 'Electronics_v1_00', 'Digital_Video_Games_v1_00', 'Digital_Video_Download_v1_00', 'Digital_Software_v1_00', 'Digital_Music_Purchase_v1_00', 'Digital_Ebook_Purchase_v1_00', 'Camera_v1_00', 'Books_v1_00', 'Beauty_v1_00', 'Baby_v1_00', 'Automotive_v1_00', 'Apparel_v1_00', 'Digital_Ebook_Purchase_v1_01', 'Books_v1_01', 'Books_v1_02']\r\nExample of usage:\r\n\t`load_dataset('amazon_us_reviews', 'Wireless_v1_00')`]\r\n\r\n__________________________________________________________________________\r\n`amazon_us_reviews = load_dataset('amazon_us_reviews', 'Watches_v1_00')\r\nprint(amazon_us_reviews)`\r\n\r\n**ERROR**\r\n`Generating` train split: 0%\r\n0/960872 [00:00<?, ? examples/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n/usr/local/lib/python3.10/dist-packages/datasets/builder.py in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1692                         )\r\n-> 1693                     example = self.info.features.encode_example(record) if self.info.features is not None else record\r\n   1694                     writer.write(example, key)\r\n\r\n11 frames\r\nKeyError: 'marketplace'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n/usr/local/lib/python3.10/dist-packages/datasets/builder.py in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1710             if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1711                 e = e.__context__\r\n-> 1712             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1713 \r\n   1714         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\n\n### Motivation\n\nThe dataset I'm using\r\nhttps://huggingface.co/datasets/amazon_us_reviews\n\n### Your contribution\n\nWhat is the best way to load this data<end_of_turn>\n<start_of_turn>model\nHow do i get \"amazon_us_reviews<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe Search API only supports Faiss and ElasticSearch as vector stores, is somewhat difficult to maintain (e.g., it still doesn't support ElasticSeach 8.0, difficult testing, ...), does not have the best design (adds a bunch of methods to the `Dataset` class that are only useful after creating an index), the usage doesn't seem to be significant and is not integrated with the Hub. Since we have no plans/bandwidth to improve it and better alternatives such as `langchain` and `docarray` exist, I think it should be deprecated (and eventually removed).\r\n\r\nIf we decide to deprecate/remove it, the following usage instances need to be addressed:\r\n* [Course](https://github.com/huggingface/course/blob/0018bb434204d9750a03592cb0d4e846093218d8/chapters/en/chapter5/6.mdx#L342 ) and [Blog](https://github.com/huggingface/blog/blob/4897c6f73d4492a0955ade503281711d01840e09/image-search-datasets.md?plain=1#L252) - calling the FAISS API directly should be OK in these instances as it's pretty simple to use for basic scenarios. Alternatively, we can use `langchain`, but this adds an extra dependency\r\n*  [Transformers](https://github.com/huggingface/transformers/blob/50726f9ea7afc6113da617f8f4ca1ab264a5e28a/src/transformers/models/rag/retrieval_rag.py#L183) - we can use the FAISS API directly and store the index as a separate attribute (and instead of building the `wiki_dpr` index each time the dataset is generated, we can generate it once and push it to the Hub repo, and then read it from there\r\n\r\ncc @huggingface/datasets @LysandreJik for the opinion<end_of_turn>\n<start_of_turn>model\nDeprecate search API<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd support for `subset_name` as an alias for `config_name` in the datasets library and related tools (such as loading scripts, documentation, and metadata).\n\n### Motivation\n\nThe Hugging Face Hub dataset viewer displays a column named **\"Subset\"**, which refers to what is currently technically called config_name in the datasets library. This inconsistency has caused confusion for many users, especially those unfamiliar with the internal terminology.\n\nI have repeatedly received questions from users trying to understand what \"config\" means, and why it doesn\u2019t match what they see as \"subset\" on the Hub. Renaming everything to `subset_name` might be too disruptive, but introducing subset_name as a clear alias for config_name could significantly improve user experience without breaking backward compatibility.\n\nThis change would:\n- Align terminology across the Hub UI and datasets codebase\n- Reduce user confusion, especially for newcomers\n- Make documentation and examples more intuitive\n<end_of_turn>\n<start_of_turn>model\nIntroduce subset_name as an alias of config_name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixed the issue #6755 on the typo mistake<end_of_turn>\n<start_of_turn>model\nfixing the issue 6755(small typo)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport Arrow files (`.arrow`) that are in non streamable binary file formats.<end_of_turn>\n<start_of_turn>model\nfeat: support non streamable arrow file binary format<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfsspec 2024.12.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nloading the dataset using load_dataset(\"sentiment140\") returns the following error\r\nConnectionError: Couldn't reach http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip (error 403)\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun the following code (version should not matter).\r\n\r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(\"sentiment140\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset should be loaded just like any other.\r\nThe main issue is that it is no longer hosted by stanford. It is still available from a [Google Drive Link](https://docs.google.com/file/d/0B04GJPshIjmPRnZManQwWEdTZjg/edit).\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.5\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.10.8\r\n- Huggingface_hub version: 0.17.3    \r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.1<end_of_turn>\n<start_of_turn>model\nsentiment140 dataset unavailable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI've use FAISS with Datasets and save to FAISS.\r\n\r\nThen load to save FAISS then no error, then ds to None\r\n\r\n```python\r\nds.load_faiss_index('embeddings', 'my_index.faiss')\r\n```\n\n### Steps to reproduce the bug\n\n# 1.\r\n```python\r\nds_with_embeddings = ds.map(lambda example: {'embeddings': model(transforms(example['image']).unsqueeze(0)).squeeze()}, batch_size=64)\r\n\r\nds_with_embeddings.add_faiss_index(column='embeddings')\r\n\r\nds_with_embeddings.save_faiss_index('embeddings', 'index.faiss')\r\n```\r\n\r\n# 2.\r\n```python\r\nds.load_faiss_index('embeddings', 'my_index.faiss')\r\n```\n\n### Expected behavior\n\nAdd column in Datasets.\n\n### Environment info\n\nGoogle Colab, SageMaker Notebook<end_of_turn>\n<start_of_turn>model\nFAISS load to None<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nSince the squad structure is widely used, I think it could be beneficial to support it using a YAML parameter.\r\ncould you implement automatic data loading of squad-like data using squad JSON format, to read it from JSON files and view it in the correct squad structure.\r\nThe dataset structure should be like this:\r\nhttps://huggingface.co/datasets/squad\r\nColumns:id,title,context,question,answers\n\n### Motivation\n\nDataset repo requires arbitrary Python code execution\n\n### Your contribution\n\nThe dataset structure should be like this:\r\nhttps://huggingface.co/datasets/squad\r\nColumns:id,title,context,question,answers\r\ntrain and dev sets in squad structure JSON files<end_of_turn>\n<start_of_turn>model\nsupport squad structure datasets using a YAML parameter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCannot load the dataset https://huggingface.co/datasets/billion-word-benchmark/lm1b\n\n### Steps to reproduce the bug\n\n`dataset = datasets.load_dataset('lm1b', split=split)`\n\n### Expected behavior\n\n`Traceback (most recent call last):\r\n  File \"/home/hml/projects/DeepLearning/Generative_model/Diffusion-BERT/word_freq.py\", line 13, in <module>\r\n    train_data = DiffusionLoader(tokenizer=tokenizer).my_load(task_name='lm1b', splits=['train'])[0]\r\n  File \"/home/hml/projects/DeepLearning/Generative_model/Diffusion-BERT/dataloader.py\", line 20, in my_load\r\n    return [self._load(task_name, name) for name in splits]\r\n  File \"/home/hml/projects/DeepLearning/Generative_model/Diffusion-BERT/dataloader.py\", line 20, in <listcomp>\r\n    return [self._load(task_name, name) for name in splits]\r\n  File \"/home/hml/projects/DeepLearning/Generative_model/Diffusion-BERT/dataloader.py\", line 13, in _load\r\n    dataset = datasets.load_dataset('lm1b', split=split)\r\n  File \"/home/hml/.conda/envs/DB/lib/python3.10/site-packages/datasets/load.py\", line 2594, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/hml/.conda/envs/DB/lib/python3.10/site-packages/datasets/load.py\", line 2266, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/hml/.conda/envs/DB/lib/python3.10/site-packages/datasets/load.py\", line 1827, in dataset_module_factory\r\n    ).get_module()\r\n  File \"/home/hml/.conda/envs/DB/lib/python3.10/site-packages/datasets/load.py\", line 1040, in get_module\r\n    module_name, default_builder_kwargs = infer_module_for_data_files(\r\n  File \"/home/hml/.conda/envs/DB/lib/python3.10/site-packages/datasets/load.py\", line 598, in infer_module_for_data_files\r\n    raise DataFilesNotFoundError(\"No (supported) data files found\" + (f\" in {path}\" if path else \"\"))\r\ndatasets.exceptions.DataFilesNotFoundError: No (supported) data files found in lm1b`\n\n### Environment info\n\ndatasets: 2.20.0<end_of_turn>\n<start_of_turn>model\nDataFilesNotFoundError for datasets LM1B<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n* try not to merge DatasetInfos if they're equal\r\n\r\n* fixes losing DatasetInfo during parallel Dataset.map<end_of_turn>\n<start_of_turn>model\nkeep more info in DatasetInfo.from_merge #6585<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR builds on https://github.com/huggingface/datasets/pull/6639 to support downloading only the specified splits in `load_dataset`. For this to work, a builder's `_split_generators` need to be able to accept the requested splits (as a list) via a `splits` argument to avoid processing the non-requested ones. Also, the builder has to define a `_available_splits` method that lists all the possible `splits` values.\r\n\r\nClose https://github.com/huggingface/datasets/issues/4101, close https://github.com/huggingface/datasets/issues/2538 (I'm probably missing some)\r\n\r\nShould also make it possible to address https://github.com/huggingface/datasets/issues/6793<end_of_turn>\n<start_of_turn>model\nSupport downloading specific splits in `load_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAfter https://huggingface.co/datasets/cis-lmu/Glot500/commit/39060e01272ff228cc0ce1d31ae53789cacae8c3, the dataset viewer gives the following error:\r\n\r\n```\r\n{\r\n  \"error\": \"Cannot get the config names for the dataset.\",\r\n  \"cause_exception\": \"DataFilesNotFoundError\",\r\n  \"cause_message\": \"No (supported) data files found in cis-lmu/Glot500\",\r\n  \"cause_traceback\": [\r\n    \"Traceback (most recent call last):\\n\",\r\n    \" File \\\"/src/services/worker/src/worker/job_runners/dataset/config_names.py\\\", line 73, in compute_config_names_response\\n config_names = get_dataset_config_names(\\n\",\r\n    \" File \\\"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\\\", line 347, in get_dataset_config_names\\n dataset_module = dataset_module_factory(\\n\",\r\n    \" File \\\"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\\\", line 1873, in dataset_module_factory\\n raise e1 from None\\n\",\r\n    \" File \\\"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\\\", line 1854, in dataset_module_factory\\n return HubDatasetModuleFactoryWithoutScript(\\n\",\r\n    \" File \\\"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\\\", line 1245, in get_module\\n module_name, default_builder_kwargs = infer_module_for_data_files(\\n\",\r\n    \" File \\\"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\\\", line 595, in infer_module_for_data_files\\n raise DataFilesNotFoundError(\\\"No (supported) data files found\\\" + (f\\\" in {path}\\\" if path else \\\"\\\"))\\n\",\r\n    \"datasets.exceptions.DataFilesNotFoundError: No (supported) data files found in cis-lmu/Glot500\\n\"\r\n  ]\r\n}\r\n```\r\n\r\nbecause the deleted files were still listed in the README, see https://huggingface.co/datasets/cis-lmu/Glot500/discussions/4\r\n\r\nIdeally, the error message would include the name of the first configuration with missing files, to help the user understand how to fix it. Here, it would tell that configuration `aze_Ethi` has no supported data files, instead of telling that the `cis-lmu/Glot500` *dataset* has no supported data files (which is not true).\n\n### Motivation\n\nGiving more detail in the error would help the Datasets Hub users to debug why the dataset viewer does not work.\n\n### Your contribution\n\nNot sure how to best fix this, as there are a lot of loops on the dataset configs in the traceback methods. \"maybe\" it would be easier to handle if the code was completely isolating each config.<end_of_turn>\n<start_of_turn>model\nGive more details in `DataFilesNotFoundError` when getting the config names<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMoved the code from using collect() to using toLocalIterator, which allows for prefetching partitions that will be selected next, thus allowing for better performance when iterating. <end_of_turn>\n<start_of_turn>model\nMake IterableDataset.from_spark more efficient<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6476.<end_of_turn>\n<start_of_turn>model\nFix PermissionError on Windows CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nI am newer to huggingface, after i look up `datasets` docs, I can't find how to create the dataset contains 3d medical image (ends with '.mhd', '.dcm', '.nii')\r\n\r\n### Motivation\r\n\r\nhelp us to upload 3d medical dataset to huggingface!\r\n\r\n### Your contribution\r\n\r\nI'll submit a PR if I find a way to add this feature<end_of_turn>\n<start_of_turn>model\nHow to create 3d medical imgae dataset?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfollowing discussions at https://discuss.huggingface.co/t/how-to-load-this-simple-audio-data-set-and-use-dataset-map-without-memory-issues/17722/14<end_of_turn>\n<start_of_turn>model\nMore docs to from_dict to mention that the result lives in RAM<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI'm not sure if this is a bug, but `print(ds.builder_name)` in the following code sometimes prints out `rotten_tomatoes` instead of `parquet`:\r\n\r\n```python\r\nimport datasets\r\n\r\nfor _ in range(100):\r\n    ds = datasets.load_dataset(\"rotten_tomatoes\", split=\"train\")\r\n    print(ds.builder_name)  # prints out \"rotten_tomatoes\" sometimes instead of \"parquet\"\r\n```\r\n\r\nOutput:\r\n\r\n```\r\n...\r\nparquet\r\nparquet\r\nparquet\r\nrotten_tomatoes\r\nparquet\r\nparquet\r\nparquet\r\n...\r\n```\r\n\r\nHere's a reproduction using GitHub Actions:\r\nhttps://github.com/mlflow/mlflow/actions/runs/8153247984/job/22284263613?pr=11329#step:12:241\r\n\r\nOne of our tests is flaky because `builder_name` is not deterministic.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Run the code above.\r\n\r\n### Expected behavior\r\n\r\nAlways prints out `parquet`?\r\n\r\n### Environment info\r\n\r\n```\r\nCopy-and-paste the text below in your GitHub issue.\r\n- `datasets` version: 2.18.0\r\n- Platform: Linux-6.5.0-1015-azure-x86_64-with-glibc2.34\r\n- Python version: 3.8.18\r\n- `huggingface_hub` version: 0.21.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2024.2.0\r\n```<end_of_turn>\n<start_of_turn>model\nNon-deterministic `Dataset.builder_name` value<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to download the `librispeech_asr` `clean` dataset, which results in a `FSTimeoutError` exception after downloading around 61% of the data.\n\n### Steps to reproduce the bug\n\n```\r\nimport datasets\r\ndatasets.load_dataset(\"librispeech_asr\", \"clean\")\r\n```\r\n\r\nThe output is as follows:\r\n\r\n> Downloading data:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b         | 3.92G/6.39G [05:00<03:06, 13.2MB/s]Traceback (most recent call last):\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/asyn.py\", line 56, in _runner\r\n>     result[0] = await coro\r\n>                 ^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/implementations/http.py\", line 262, in _get_file\r\n>     chunk = await r.content.read(chunk_size)\r\n>             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/aiohttp/streams.py\", line 393, in read\r\n>     await self._wait(\"read\")\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/aiohttp/streams.py\", line 311, in _wait\r\n>     with self._timer:\r\n>          ^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/aiohttp/helpers.py\", line 713, in __exit__\r\n>     raise asyncio.TimeoutError from None\r\n> TimeoutError\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/load_dataset.py\", line 3, in <module>\r\n>     datasets.load_dataset(\"librispeech_asr\", \"clean\")\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/load.py\", line 2096, in load_dataset\r\n>     builder_instance.download_and_prepare(\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/builder.py\", line 924, in download_and_prepare\r\n>     self._download_and_prepare(\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/builder.py\", line 1647, in _download_and_prepare\r\n>     super()._download_and_prepare(\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/builder.py\", line 977, in _download_and_prepare\r\n>     split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n>                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/.cache/huggingface/modules/datasets_modules/datasets/librispeech_asr/2712a8f82f0d20807a56faadcd08734f9bdd24c850bb118ba21ff33ebff0432f/librispeech_asr.py\", line 115, in _split_generators\r\n>     archive_path = dl_manager.download(_DL_URLS[self.config.name])\r\n>                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py\", line 159, in download\r\n>     downloaded_path_or_paths = map_nested(\r\n>                                ^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 512, in map_nested\r\n>     _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 380, in _single_map_nested\r\n>     return [mapped_item for batch in iter_batched(data_struct, batch_size) for mapped_item in function(batch)]\r\n>                                                                                               ^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py\", line 216, in _download_batched\r\n>     self._download_single(url_or_filename, download_config=download_config)\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/download/download_manager.py\", line 225, in _download_single\r\n>     out = cached_path(url_or_filename, download_config=download_config)\r\n>           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py\", line 205, in cached_path\r\n>     output_path = get_from_cache(\r\n>                   ^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py\", line 415, in get_from_cache\r\n>     fsspec_get(url, temp_file, storage_options=storage_options, desc=download_desc, disable_tqdm=disable_tqdm)\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py\", line 334, in fsspec_get\r\n>     fs.get_file(path, temp_file.name, callback=callback)\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/asyn.py\", line 118, in wrapper\r\n>     return sync(self.loop, func, *args, **kwargs)\r\n>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/Users/Timon/Documents/iEEG_deeplearning/wav2vec_pretrain/.venv/lib/python3.12/site-packages/fsspec/asyn.py\", line 101, in sync\r\n>     raise FSTimeoutError from return_result\r\n> fsspec.exceptions.FSTimeoutError\r\n> Downloading data:  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b         | 3.92G/6.39G [05:00<03:09, 13.0MB/s]\n\n### Expected behavior\n\nComplete the download\n\n### Environment info\n\nPython version 3.12.6\r\n\r\nDependencies:\r\n> dependencies = [\r\n>     \"accelerate>=0.34.2\",\r\n>     \"datasets[audio]>=3.0.0\",\r\n>     \"ipython>=8.18.1\",\r\n>     \"librosa>=0.10.2.post1\",\r\n>     \"torch>=2.4.1\",\r\n>     \"torchaudio>=2.4.1\",\r\n>     \"transformers>=4.44.2\",\r\n> ]\r\n\r\nMacOS 14.6.1 (23G93)<end_of_turn>\n<start_of_turn>model\nfsspec.exceptions.FSTimeoutError when downloading dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nError when trying to push a dataset in a special format to hub\n\n### Steps to reproduce the bug\n\n```\r\nimport datasets\r\nfrom datasets import Dataset\r\ndataset_dict = {\r\n    \"filename\": [\"apple\", \"banana\"],\r\n    \"token\": [[[1,2],[3,4]],[[1,2],[3,4]]],\r\n    \"label\": [0, 1],\r\n}\r\ndataset = Dataset.from_dict(dataset_dict)\r\ndataset = dataset.cast_column(\"token\", datasets.features.features.Array2D(shape=(2, 2),dtype=\"int16\"))\r\n\r\ndataset.push_to_hub(\"SequenceModel/imagenet_val_256\")\r\n```\r\nError:\r\n```\r\n...\r\n\r\nConstructorError: could not determine a constructor for the tag 'tag:yaml.org,2002:python/tuple'\r\n  in \"<unicode string>\", line 8, column 16:\r\n            shape: !!python/tuple\r\n                   ^\r\n```\n\n### Expected behavior\n\nDataset being pushed to hub\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.19.0-1022-gcp-x86_64-with-glibc2.35\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nError Pushing to Hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove deprecated code, as part of the 3.0 release.\r\n\r\nFirst merge:\r\n- [x] #6983\r\n- [x] #6987\r\n- [x] #6999<end_of_turn>\n<start_of_turn>model\nRemove deprecated code<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nClarifies in the return statement of the docstring that the retrieval score is `IndexFlatL2` by default (see [PR](https://github.com/huggingface/transformers/issues/24739) and internal Slack [convo](https://huggingface.slack.com/archives/C01229B19EX/p1689105179711689)), and fixes the formatting because multiple return values are not supported.<end_of_turn>\n<start_of_turn>model\n[docs] Update return statement of index search<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nIn light of the recent server outages, I decided to look into whether I could somehow wrap my IterableDataset streams to retry rather than error out immediately. To my surprise, `datasets` [already supports retries](https://github.com/huggingface/datasets/issues/6172#issuecomment-1794876229). Since a commit by @lhoestq [last week](https://github.com/huggingface/datasets/commit/a188022dc43a76a119d90c03832d51d6e4a94d91), that code lives here:\r\n\r\nhttps://github.com/huggingface/datasets/blob/fe2bea6a4b09b180bd23b88fe96dfd1a11191a4f/src/datasets/utils/file_utils.py#L1097C1-L1111C19\r\n\r\nIf GitHub code snippets still aren't working, here's a copy:\r\n```python\r\n    def read_with_retries(*args, **kwargs):\r\n        disconnect_err = None\r\n        for retry in range(1, max_retries + 1):\r\n            try:\r\n                out = read(*args, **kwargs)\r\n                break\r\n            except (ClientError, TimeoutError) as err:\r\n                disconnect_err = err\r\n                logger.warning(\r\n                    f\"Got disconnected from remote data host. Retrying in {config.STREAMING_READ_RETRY_INTERVAL}sec [{retry}/{max_retries}]\"\r\n                )\r\n                time.sleep(config.STREAMING_READ_RETRY_INTERVAL)\r\n        else:\r\n            raise ConnectionError(\"Server Disconnected\") from disconnect_err\r\n        return out\r\n```\r\n\r\nWith the latest outage, the end of my stack trace looked like this:\r\n```\r\n...\r\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/datasets/download/streaming_download_manager.py\", line 342, in read_with_retries\r\n    out = read(*args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/draft/lib/python3.11/gzip.py\", line 301, in read\r\n    return self._buffer.read(size)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/draft/lib/python3.11/_compression.py\", line 68, in readinto\r\n    data = self.read(len(byte_view))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/draft/lib/python3.11/gzip.py\", line 505, in read\r\n    buf = self._fp.read(io.DEFAULT_BUFFER_SIZE)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/draft/lib/python3.11/gzip.py\", line 88, in read\r\n    return self.file.read(size)\r\n           ^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/fsspec/spec.py\", line 1856, in read\r\n    out = self.cache._fetch(self.loc, self.loc + length)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/fsspec/caching.py\", line 189, in _fetch\r\n    self.cache = self.fetcher(start, end)  # new block replaces old\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/huggingface_hub/hf_file_system.py\", line 626, in _fetch_range\r\n    hf_raise_for_status(r)\r\n  File \"/miniconda3/envs/draft/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py\", line 333, in hf_raise_for_status\r\n    raise HfHubHTTPError(str(e), response=response) from e\r\nhuggingface_hub.utils._errors.HfHubHTTPError: 504 Server Error: Gateway Time-out for url: https://huggingface.co/datasets/allenai/c4/resolve/1588ec454efa1a09f29cd18ddd04fe05fc8653a2/en/c4-train.00346-of-01024.json.gz\r\n```\r\n\r\nIndeed, the code for retries only catches `ClientError`s and `TimeoutError`s, and all other exceptions, *including HuggingFace's own custom HTTP error class*, **are not caught. Nothing is retried,** and instead the exception is propagated upwards immediately.\r\n\r\n### Steps to reproduce the bug\r\n\r\nNot sure how you reproduce this. Maybe unplug your Ethernet cable while streaming a dataset; the issue is pretty clear from the stack trace.\r\n\r\n### Expected behavior\r\n\r\nAll HTTP errors while iterating a streamable dataset should cause retries.\r\n\r\n### Environment info\r\n\r\nOutput from `datasets-cli env`:\r\n- `datasets` version: 2.18.0\r\n- Platform: Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.11.7\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nIterableDataset raises exception instead of retrying<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nupdate docs on N-dim arrays<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nAdd an option in load_from_disk to disable the progress bar even if the number of files is larger than 16.\r\n\r\n### Motivation\r\n\r\nI am reading a lot of datasets that it creates lots of logs.\r\n\r\n<img width=\"1432\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/57996478/8d4bbf03-6b89-44b6-937c-932f01b4eb2a\">\r\n\r\n\r\n### Your contribution\r\n\r\nSeems like an easy fix to make.  I can create a PR if necessary.<end_of_turn>\n<start_of_turn>model\nAdd option to disable progress bar when reading a dataset (\"Loading dataset from disk\")<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd a new 'Streaming best practices' page with practical patterns and pitfalls for large-scale/production use of IterableDataset. Includes examples for batched map with remove_columns, deterministic shuffling with set_epoch, multi-worker sharding, checkpoint/resume, and persistence to Parquet/Hub. Linked from How-to > General usage, next to Stream.<end_of_turn>\n<start_of_turn>model\ndocs: Streaming best practices<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nds = load_dataset(\"ai2_arc\", \"ARC-Easy\"), i have tried to force redownload, delete cache and changing the cache dir.\n\n### Steps to reproduce the bug\n\ndataset = []\r\ndataset_name = \"ai2_arc\"\r\npossible_configs = [\r\n    'ARC-Challenge',\r\n    'ARC-Easy'\r\n    ]\r\nfor config in possible_configs:\r\n    dataset_slice = load_dataset(dataset_name, config,ignore_verifications=True,cache_dir='ai2_arc_files')\r\n    dataset.append(dataset_slice)\n\n### Expected behavior\n\nall configs should get saved in cache with their respective names.\n\n### Environment info\n\nai2_arc<end_of_turn>\n<start_of_turn>model\ndataset cache only stores one config of the dataset in parquet dir, and uses that for all other configs resulting in showing same data in all configs.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen downloading a dataset, we frequently hit the below Permission Denied error. This looks to happen (at least) across datasets in HF, S3, and GCS.\n\nIt looks like the `temp_file` being passed [here](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L412) can sometimes be created with `000` permissions leading to the permission denied error (the user running the code is still the owner of the file). Deleting that particular file and re-running the code with 0 changes will usually succeed.\n\nIs there some race condition happening with the [umask](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L416), which is process global, and the [file creation](https://github.com/huggingface/datasets/blob/main/src/datasets/utils/file_utils.py#L404)?\n\n```\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n.venv/lib/python3.12/site-packages/datasets/load.py:2084: in load_dataset\n    builder_instance.download_and_prepare(\n.venv/lib/python3.12/site-packages/datasets/builder.py:925: in download_and_prepare\n    self._download_and_prepare(\n.venv/lib/python3.12/site-packages/datasets/builder.py:1649: in _download_and_prepare\n    super()._download_and_prepare(\n.venv/lib/python3.12/site-packages/datasets/builder.py:979: in _download_and_prepare\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\n.venv/lib/python3.12/site-packages/datasets/packaged_modules/folder_based_builder/folder_based_builder.py:120: in _split_generators\n    downloaded_files = dl_manager.download(files)\n.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:159: in download\n    downloaded_path_or_paths = map_nested(\n.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:514: in map_nested\n    _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\n.venv/lib/python3.12/site-packages/datasets/utils/py_utils.py:382: in _single_map_nested\n    return [mapped_item for batch in iter_batched(data_struct, batch_size) for mapped_item in function(batch)]\n.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:206: in _download_batched\n    return thread_map(\n.venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:69: in thread_map\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\n.venv/lib/python3.12/site-packages/tqdm/contrib/concurrent.py:51: in _executor_map\n    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\n.venv/lib/python3.12/site-packages/tqdm/std.py:1181: in __iter__\n    for obj in iterable:\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:619: in result_iterator\n    yield _result_or_cancel(fs.pop())\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:317: in _result_or_cancel\n    return fut.result(timeout)\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:449: in result\n    return self.__get_result()\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/_base.py:401: in __get_result\n    raise self._exception\n../../../_tool/Python/3.12.10/x64/lib/python3.12/concurrent/futures/thread.py:59: in run\n    result = self.fn(*self.args, **self.kwargs)\n.venv/lib/python3.12/site-packages/datasets/download/download_manager.py:229: in _download_single\n    out = cached_path(url_or_filename, download_config=download_config)\n.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:206: in cached_path\n    output_path = get_from_cache(\n.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:412: in get_from_cache\n    fsspec_get(url, temp_file, storage_options=storage_options, desc=download_desc, disable_tqdm=disable_tqdm)\n.venv/lib/python3.12/site-packages/datasets/utils/file_utils.py:331: in fsspec_get\n    fs.get_file(path, temp_file.name, callback=callback)\n.venv/lib/python3.12/site-packages/fsspec/asyn.py:118: in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n.venv/lib/python3.12/site-packages/fsspec/asyn.py:103: in sync\n    raise return_result\n.venv/lib/python3.12/site-packages/fsspec/asyn.py:56: in _runner\n    result[0] = await coro\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\nself = <s3fs.core.S3FileSystem object at 0x7f27c18b2e70>\nrpath = '<my-bucket>/<my-prefix>/img_1.jpg'\nlpath = '/home/runner/_work/_temp/hf_cache/downloads/6c97983efa4e24e534557724655df8247a0bd04326cdfc4a95b638c11e78222d.incomplete'\ncallback = <datasets.utils.file_utils.TqdmCallback object at 0x7f27c00cdbe0>\nversion_id = None, kwargs = {}\n_open_file = <function S3FileSystem._get_file.<locals>._open_file at 0x7f27628d1120>\nbody = <StreamingBody at 0x7f276344fa80 for ClientResponse at 0x7f27c015fce0>\ncontent_length = 521923, failed_reads = 0, bytes_read = 0\n\n    async def _get_file(\n        self, rpath, lpath, callback=_DEFAULT_CALLBACK, version_id=None, **kwargs\n    ):\n        if os.path.isdir(lpath):\n            return\n        bucket, key, vers = self.split_path(rpath)\n    \n        async def _open_file(range: int):\n            kw = self.req_kw.copy()\n            if range:\n                kw[\"Range\"] = f\"bytes={range}-\"\n            resp = await self._call_s3(\n                \"get_object\",\n                Bucket=bucket,\n                Key=key,\n                **version_id_kw(version_id or vers),\n                **kw,\n            )\n            return resp[\"Body\"], resp.get(\"ContentLength\", None)\n    \n        body, content_length = await _open_file(range=0)\n        callback.set_size(content_length)\n    \n        failed_reads = 0\n        bytes_read = 0\n    \n        try:\n>           with open(lpath, \"wb\") as f0:\nE           PermissionError: [Errno 13] Permission denied: '/home/runner/_work/_temp/hf_cache/downloads/6c97983efa4e24e534557724655df8247a0bd04326cdfc4a95b638c11e78222d.incomplete'\n\n.venv/lib/python3.12/site-packages/s3fs/core.py:1355: PermissionError\n```\n\n### Steps to reproduce the bug\n\nI believe this is a race condition and cannot reliably re-produce it, but it happens fairly frequently in our GitHub Actions tests and can also be re-produced (with lesser frequency) on cloud VMs.\n\n### Expected behavior\n\nThe dataset loads properly with no permission denied error.\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-5.10.0-34-cloud-amd64-x86_64-with-glibc2.31\n- Python version: 3.12.10\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\n[Errno 13] Permission denied: on `.incomplete` file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe current link leads to a 404 page.<end_of_turn>\n<start_of_turn>model\nfix link for rotten_tomatoes dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix for https://discuss.huggingface.co/t/caching-a-dataset-processed-with-randomness/65616<end_of_turn>\n<start_of_turn>model\nPickle support for `torch.Generator` objects<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI would like a programatic way of requesting access to gated datasets. The current solution to gain access forces me to visit a website and physically click an \"agreement\" button (as per the [documentation](https://huggingface.co/docs/hub/en/datasets-gated#access-gated-datasets-as-a-user)).\r\n\r\nAn ideal approach would be HF API download methods that negotiate access on my behalf based on information from my CLI login and/or token. I realise that may be naive given the various types of access semantics available to dataset authors (automatic versus manual approval, for example) and complexities it might add to existing methods, but something along those lines would be nice.\r\n\r\nPerhaps using the `*_access_request` methods available to dataset authors can be a precedent; see [`reject_access_request`](https://huggingface.co/docs/huggingface_hub/main/en/package_reference/hf_api#huggingface_hub.HfApi.reject_access_request) for example.\n\n### Motivation\n\nWhen trying to download files from a gated dataset, I'm met with a `GatedRepoError` and instructed to visit the repository's website to gain access:\r\n\r\n```\r\nCannot access gated repo for url https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3.1-70B-Instruct-details/resolve/main/meta-llama__Meta-Llama-3.1-70B-Instruct/samples_leaderboard_math_precalculus_hard_2024-07-19T18-47-29.522341.jsonl.\r\nAccess to dataset open-llm-leaderboard/meta-llama__Meta-Llama-3.1-70B-Instruct-details is restricted and you are not in the authorized list. Visit https://huggingface.co/datasets/open-llm-leaderboard/meta-llama__Meta-Llama-3.1-70B-Instruct-details to ask for access.\r\n```\r\n\r\nThis makes task automation extremely difficult. For example, I'm interested in studying sample-level responses of models on the LLM leaderboard -- how they answered particular questions on a given evaluation framework. As I come across more and more participants that gate their data, it's becoming unwieldy to continue my work (there over 2,000 participants, so in the worst case that's the number of website visits I'd need to manually undertake).\r\n\r\nOne approach is use Selenium to react to the `GatedRepoError`, but that seems like overkill; and a potential violation HF terms of service (?).\r\n\r\nAs mentioned in the previous section, there seems to be an [API for gated dataset owners](https://huggingface.co/docs/hub/en/datasets-gated#via-the-api) to managed access requests, and thus some appetite for allowing automated management of gating. This feature request is to extend that to dataset users.\n\n### Your contribution\n\nWhether I can help depends on a few things; one being the complexity of the underlying gated access design. If this feature request is accepted I am open to being involved in discussions and testing, and even development under the right time-outcome tradeoff.<end_of_turn>\n<start_of_turn>model\nAPI endpoints for gated dataset access requests<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUse pyupgrade --py39-plus<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nA probabilistic issue encountered when processing datasets containing PIL.Image columns using the huggingface/datasets library on Python 3.10. The process occasionally experiences a sudden and significant memory spike, reaching 100% utilization, leading to a complete freeze. During this freeze, the process becomes unresponsive, cannot be forcefully terminated, and does not throw any exceptions.\n\nI have attempted to mitigate this issue by setting `datasets.config.IN_MEMORY_MAX_SIZE`, but it had no effect. In fact, based on the document of `load_dataset`, I suspect that setting `IN_MEMORY_MAX_SIZE` might even have a counterproductive effect.\n\nThis bug is not consistently reproducible, but its occurrence rate significantly decreases or disappears entirely when upgrading Python to version 3.11 or higher. Therefore, this issue also serves to share a potential solution for others who might encounter similar problems.\n\n### Steps to reproduce the bug\n\nDue to the probabilistic nature of this bug, consistent reproduction cannot be guaranteed for every run. However, in my environment, processing large datasets like timm/imagenet-1k-wds(whether reading, casting, or mapping operations) almost certainly triggers the issue at some point.\nThe probability of the issue occurring drastically increases when num_proc is set to a value greater than 1 during operations.\n\nWhen the issue occurs, my system logs repeatedly show the following warnings:\n```\nWARN: very high memory utilization: 57.74GiB / 57.74GiB (100 %)\nWARN: container is unhealthy: triggered memory limits (OOM)\nWARN: container is unhealthy: triggered memory limits (OOM)\nWARN: container is unhealthy: triggered memory limits (OOM)\n```\n\n### Expected behavior\n\nThe dataset should be read and processed normally without memory exhaustion or freezing. If an unrecoverable error occurs, an appropriate exception should be raised.\n\nI have found that upgrading Python to version 3.11 or above completely resolves this issue. On Python 3.11, when memory usage approaches 100%, it suddenly drops before slowly increasing again. I suspect this behavior is due to an expected memory management action, possibly involving writing to disk cache, which prevents the complete freeze observed in Python 3.10.\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: Linux-5.15.0-71-generic-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- `huggingface_hub` version: 0.33.4\n- PyArrow version: 20.0.0\n- Pandas version: 2.3.1\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nProbabilistic High Memory Usage and Freeze on Python 3.10<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nImplement `get_dataset_default_config_name`.\r\n\r\nNow that we support setting a configuration as default in `push_to_hub` (see #6500), we need a programmatically way to know in advance which is the default configuration. This will be used in the Space to convert script-datasets to Parquet: https://huggingface.co/spaces/albertvillanova/convert-dataset-to-parquet\r\n\r\nFollow-up of:\r\n- #6500\r\n\r\nCC: @severo <end_of_turn>\n<start_of_turn>model\nImplement get dataset default config name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`from_polars` dataset cannot cast column to Image/Audio/Video, while it works on `from_pandas` and `from_dict`\n\n### Steps to reproduce the bug\n\n```python3\nimport datasets\nimport pandas as pd\nimport polars as pl\n\nimage_path = \"./sample.png\"\n\n# polars\ndf = pl.DataFrame({\"image_path\": [image_path]})\ndataset = datasets.Dataset.from_polars(df)\ndataset = dataset.cast_column(\"image_path\", datasets.Image())\n\n# # raises Error\npyarrow.lib.ArrowNotImplementedError: Unsupported cast from large_string to struct using function cast_struct\n\n\n# pandas \ndf = pd.DataFrame({\"image_path\": [image_path]})\ndataset = datasets.Dataset.from_pandas(df)\ndataset = dataset.cast_column(\"image_path\", datasets.Image())\n\n# # pass\n{'image_path': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=338x277 at 0x7FBA719D4050>}\n\n\n# dict\ndataset = datasets.Dataset.from_dict({\"image_path\": [image_path]})\ndataset = dataset.cast_column(\"image_path\", datasets.Image())\n\n# # pass\n{'image_path': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=338x277 at 0x7FBA719D4050>}\n```\n\n\n### Expected behavior\n\n`from_polars` case shouldn't raise error and have the same outputs as `from_pandas` and `from_dict`\n\n### Environment info\n\n```\n# Name                    Version                   Build  Channel\ndatasets                  4.0.0                    pypi_0    pypi\npandas                    2.3.1                    pypi_0    pypi\npolars                    1.32.3                   pypi_0    pypi\n```<end_of_turn>\n<start_of_turn>model\npolars dataset cannot cast column to Image/Audio/Video<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix streaming from arrow files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading any dataset that contains a column with strings that are not ASCII-compatible, looping over those records raises the following exception e.g. for `\u00e9` character `UnicodeEncodeError: 'ascii' codec can't encode character '\\xe9' in position 0: ordinal not in range(128)`.\n\n### Steps to reproduce the bug\n\nRunning the following script will eventually fail, when reaching to the batch that contains non-ASCII compatible strings.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"imdb\", split=\"train\")\r\ntfds = ds.to_tf_dataset(batch_size=16)\r\n\r\nfor batch in tfds:\r\n  print(batch)\r\n>>> UnicodeEncodeError: 'ascii' codec can't encode character '\\xe9' in position 0: ordinal not in range(128)\r\n```\n\n### Expected behavior\n\nThe following script to run properly, making sure that the strings are either `numpy.unicode_` or `numpy.string` instead of `numpy.bytes_` since some characters are not ASCII compatible and that would lead to an issue when applying the `map`.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"imdb\", split=\"train\")\r\ntfds = ds.to_tf_dataset(batch_size=16)\r\n\r\nfor batch in tfds:\r\n  print(batch)\r\n```\n\n### Environment info\n\n- `datasets` version: 2.12.1.dev0\r\n- Platform: macOS-13.3.1-arm64-arm-64bit\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\n`Dataset.to_tf_dataset` fails when strings cannot be encoded as `np.bytes_`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n1. Fixed broken link to the list of transforms in torchvison.\r\n2. Extended section about video image augmentations with an example from Albumentations.<end_of_turn>\n<start_of_turn>model\nAdd albumentations to use dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate to refer to `hf auth login`<end_of_turn>\n<start_of_turn>model\nUpdate cli.mdx to refer to the new \"hf\" CLI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nPlease see https://github.com/huggingface/huggingface_hub/issues/1872\n\n### Steps to reproduce the bug\n\nPlease see https://github.com/huggingface/huggingface_hub/issues/1872\n\n### Expected behavior\n\nPlease see https://github.com/huggingface/huggingface_hub/issues/1872\n\n### Environment info\n\nPlease see https://github.com/huggingface/huggingface_hub/issues/1872<end_of_turn>\n<start_of_turn>model\n`TypeError`: huggingface_hub.hf_file_system.HfFileSystem.find() got multiple values for keyword argument 'maxdepth'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI have a dataset of sequences, where each example in the sequence is a separate row in the dataset (similar to LeRobotDataset). When running `Dataset.save_to_disk` how can I provide indices where it's possible to shard the dataset such that no episode spans more than 1 shard. Consequently, when I run `Dataset.load_from_disk`, how can I load just a subset of the shards to save memory and time on different ranks?\n\nI guess an alternative to this would be, given a loaded `Dataset`, how can I run `Dataset.shard` such that sharding doesn't split any episode across shards?<end_of_turn>\n<start_of_turn>model\nShard Dataset at specific indices<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I  was training model on Multiple GPUs by DDP, the dataset is tokenized multiple times after main process. \r\n![1698820541284](https://github.com/huggingface/datasets/assets/14285786/0b2fe054-54d8-4e00-96e6-6ca5b69e662b)\r\n\r\n![1698820501568](https://github.com/huggingface/datasets/assets/14285786/dd62bf6f-a58f-41bf-9848-ea4fb3b62b9b)\r\n\r\nCode is modified from  [run_clm.py](https://github.com/huggingface/transformers/blob/7d8ff3629b2725ec43ace99c1a6e87ac1978d433/examples/pytorch/language-modeling/run_clm.py#L484)\r\n\n\n### Steps to reproduce the bug\n\n```\r\nblock_size = data_args.block_size\r\nIGNORE_INDEX = -100\r\nIgnore_Input = False\r\n\r\ndef tokenize_function(examples):\r\n    sources = []\r\n    targets = []\r\n    for instruction, inputs, output in zip(examples['instruction'], examples['input'], examples['output']):\r\n\r\n        source = instruction + inputs \r\n\r\n        target = f\"{output}{tokenizer.eos_token}\"\r\n\r\n        sources.append(source)\r\n        targets.append(target)\r\n\r\n    tokenized_sources = tokenizer(sources, return_attention_mask=False)\r\n\r\n    tokenized_targets = tokenizer(targets, return_attention_mask=False,\r\n                                  add_special_tokens=False\r\n                                  )\r\n\r\n    all_input_ids = []\r\n    all_labels = []\r\n    for s, t in zip(tokenized_sources['input_ids'], tokenized_targets['input_ids']):\r\n        if len(s) > block_size and Ignore_Input == False:\r\n            # print(s)\r\n            continue\r\n        input_ids = torch.LongTensor(s + t)[:block_size]\r\n        if Ignore_Input:\r\n            labels = torch.LongTensor([IGNORE_INDEX] * len(s) + t)[:block_size]\r\n        else:\r\n            labels = input_ids\r\n        assert len(input_ids) == len(labels)\r\n        all_input_ids.append(input_ids)\r\n        all_labels.append(labels)\r\n\r\n    results = {\r\n        'input_ids': all_input_ids,\r\n        'labels': all_labels,\r\n\r\n    }\r\n    return results\r\n\r\n\r\n\r\nwith training_args.main_process_first(desc=\"dataset map tokenization \", local=False):\r\n    # print('local_rank',training_args.local_rank)\r\n    if not data_args.streaming:\r\n        tokenized_datasets = raw_datasets.map(\r\n            tokenize_function,\r\n            batched=True,\r\n            num_proc=data_args.preprocessing_num_workers,\r\n            remove_columns=column_names,\r\n            load_from_cache_file=not data_args.overwrite_cache,\r\n            desc=\"Running tokenizer on dataset \",\r\n        )\r\n    else:\r\n        tokenized_datasets = raw_datasets.map(\r\n            tokenize_function,\r\n            batched=True,\r\n            remove_columns=column_names,\r\n            desc=\"Running tokenizer on dataset \"\r\n        )\r\n```\n\n### Expected behavior\n\nThis code should only tokenize the dataset in the main process, and the other processes load the dataset after waiting\n\n### Environment info\n\ntransformers == 4.34.1\r\ndatasets == 2.14.5<end_of_turn>\n<start_of_turn>model\nMulti process map did not load cache file correctly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n... to avoid an `ImportError` raised in `BeamBasedBuilder._save_info` when `apache_beam` is not installed (e.g., when downloading the processed version of a dataset from the HF GCS)\r\n\r\nFix https://github.com/huggingface/datasets/issues/6260<end_of_turn>\n<start_of_turn>model\nRemove `apache_beam` import in `BeamBasedBuilder._save_info`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nmodified , as AudioFolder and ImageFolder not in Dataset Library. \r\n\r\n``` from datasets import AudioFolder ```  and ```from datasets import ImageFolder```  to ```from datasets import load_dataset```\r\n\r\n``` \r\ncannot import name 'AudioFolder' from 'datasets' (/home/eswardivi/miniconda3/envs/Hugformers/lib/python3.10/site-packages/datasets/__init__.py)\r\n\r\n```<end_of_turn>\n<start_of_turn>model\nUpdate create_dataset.mdx<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nInternal Slack discussion: https://huggingface.slack.com/archives/C02V51Q3800/p1708424971135029<end_of_turn>\n<start_of_turn>model\nImprove error message for gated datasets on load<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nPlease see https://huggingface.co/datasets/ejschwartz/idioms/discussions/1\n\nError code: FileFormatMismatchBetweenSplitsError\n\nAll three splits, train, test, and validation, use webdataset. But only the train split has more than one file. How can I force the other two splits to also be interpreted as being the webdataset format?  (I don't think there is currently a way, but happy to be told that I am wrong.)\n\n### Steps to reproduce the bug\n\n```\nimport datasets\ndatasets.load_dataset(\"ejschwartz/idioms\")\n\n### Expected behavior\n\nThe dataset loads.  Alternatively, there is a YAML syntax for manually specifying the format.\n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n- Python version: 3.10.12\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 19.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nWebdataset data format problem<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTemporarily pin `pyarrow` < 14.0.0 until permanent solution is found.\r\n\r\nHot fix #6374.<end_of_turn>\n<start_of_turn>model\nTemporarily pin pyarrow < 14.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove canonical datasets from docs, now that we no longer have canonical datasets.<end_of_turn>\n<start_of_turn>model\nRemove canonical datasets from docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe error message in the fingerprint module was missing the f-string 'f' symbol, so the error message returned by fingerprint.py, line 469 was literally \"function {func} is missing parameters {fingerprint_names} in signature.\"\r\n\r\nThis has been fixed.<end_of_turn>\n<start_of_turn>model\nImprove error message for missing function parameters<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nInstalling both in latest versions are not possible\r\n\r\n`pip install \"datasets==3.0.1\" \"fsspec==2024.9.0\"`\r\n\r\nBut using older version of datasets is ok\r\n\r\n`pip install \"datasets==1.24.4\" \"fsspec==2024.9.0\"`\r\n\r\n\n\n### Steps to reproduce the bug\n\n`pip install \"datasets==3.0.1\" \"fsspec==2024.9.0\"`\r\n\n\n### Expected behavior\n\ninstall both versions.\n\n### Environment info\n\ndebian 11.\r\npython 3.10.15<end_of_turn>\n<start_of_turn>model\nDatasets conflicts with fsspec 2024.9<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nadd large_list type support in string_to_arrow() and _arrow_to_datasets_dtype() in features.py\r\n\r\nFix #6984\r\n<end_of_turn>\n<start_of_turn>model\nAdd large_list type support in string_to_arrow<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate release instructions.<end_of_turn>\n<start_of_turn>model\nUpdate release instructions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix bug #6877 due to maybe f becomes invaild after yield process\r\nthe results are below:\r\n\r\n\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:01<00:00, 420.41it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 26148.48it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 409731.44it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 289720.84it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 26663.42it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 434056.21it/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 13222.33files/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:04<00:00, 180.67files/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [01:35<00:00,  8.70files/s]\r\nGenerating train split: 1571592 examples [00:08, 176736.09 examples/s]\r\nGenerating test split: 85533 examples [00:01, 48224.56 examples/s]\r\nGenerating validation split: 86246 examples [00:01, 50164.16 examples/s]\r\n\r\nFix https://github.com/huggingface/datasets/issues/6877.\r\n\r\nCC: @natolambert\r\n<end_of_turn>\n<start_of_turn>model\nfix bug #6877<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm getting an error generating the-stack-dedup with datasets 2.13.1, and with 2.14.4 nothing happens.\n\n### Steps to reproduce the bug\n\nMy code: \r\n\r\n```\r\nimport os\r\nimport datasets as ds\r\n\r\nMY_CACHE_DIR = \"/home/ubuntu/the-stack-dedup-local\"\r\nMY_TOKEN=\"my-token\"\r\n\r\nthe_stack_ds = ds.load_dataset(\"bigcode/the-stack-dedup\", split=\"train\", download_mode=\"reuse_cache_if_exists\", cache_dir=MY_CACHE_DIR, use_auth_token=MY_TOKEN, num_proc=64)\r\n```\r\n\r\nThe exception:\r\n\r\n```\r\nGenerating train split: 233248251 examples [54:31, 57280.00 examples/s]\r\nmultiprocess.pool.RemoteTraceback:                                     \r\n\"\"\"                                                                    \r\nTraceback (most recent call last):                                     \r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/build\r\ner.py\", line 1879, in _prepare_split_single                            \r\n    for _, table in generator:                                         \r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/packa\r\nged_modules/parquet/parquet.py\", line 82, in _generate_tables          \r\n    yield f\"{file_idx}_{batch_idx}\", self._cast_table(pa_table)        \r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/packa\r\nged_modules/parquet/parquet.py\", line 61, in _cast_table               \r\n    pa_table = table_cast(pa_table, self.info.features.arrow_schema)   \r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/table\r\n.py\", line 2324, in table_cast                                         \r\n    return cast_table_to_schema(table, schema)                         \r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/table\r\n.py\", line 2282, in cast_table_to_schema                               \r\n    raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nb\r\necause column names don't match\")                                      \r\nValueError: Couldn't cast                                              \r\nhexsha: string                                                         \r\nsize: int64                                                            \r\next: string                                                            \r\nlang: string                                                           \r\nmax_stars_repo_path: string                                            \r\nmax_stars_repo_name: string                                            \r\nmax_stars_repo_head_hexsha: string                                     \r\nmax_stars_repo_licenses: list<item: string>                            \r\n  child 0, item: string                                                \r\nmax_stars_count: int64                                                 \r\nmax_stars_repo_stars_event_min_datetime: string                        \r\nmax_stars_repo_stars_event_max_datetime: string                        \r\nmax_issues_repo_path: string                                           \r\nmax_issues_repo_name: string                                           \r\nmax_issues_repo_head_hexsha: string                                    \r\nmax_issues_repo_licenses: list<item: string>                           \r\n  child 0, item: string                                                \r\nmax_issues_count: int64                                                \r\nmax_issues_repo_issues_event_min_datetime: string                      \r\nmax_issues_repo_issues_event_max_datetime: string                      \r\nmax_forks_repo_path: string                                            \r\nmax_forks_repo_name: string                                            \r\nmax_forks_repo_head_hexsha: string                                     \r\nmax_forks_repo_licenses: list<item: string>                            \r\n  child 0, item: string                                                \r\nmax_forks_count: int64\r\nmax_forks_repo_forks_event_min_datetime: string\r\nmax_forks_repo_forks_event_max_datetime: string\r\ncontent: string\r\navg_line_length: double\r\nmax_line_length: int64\r\nalphanum_fraction: double\r\n__id__: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"hexsha\": {\"dtype\": \"string\", \"_type' + 1979\r\nto\r\n{'hexsha': Value(dtype='string', id=None), 'size': Value(dtype='int64', id=None), 'ext': Value(dtype='string', id=None), 'lang': Value(dtype='string', id=None), 'max_stars_repo_path': Value(dtype='string', id=None), 'max_stars_repo_name': Value(dtype='string', id=None), 'max_stars_repo_head_hexsha': Value(dtype='string', id=None), 'max_stars_repo_licenses': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'max_stars_count': Value(dtype='int64', id=None), 'max_stars_repo_stars_event_min_datetime': Value(dtype='string', id=None), 'max_stars_repo_stars_event_max_datetime': Value(dtype='string', id=None), 'max_issues_repo_path': Value(dtype='string', id=None), 'max_issues_repo_name': Value(dtype='string', id=None), 'max_issues_repo_head_hexsha': Value(dtype='string', id=None), 'max_issues_repo_licenses': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'max_issues_count': Value(dtype='int64', id=None), 'max_issues_repo_issues_event_min_datetime': Value(dtype='string', id=None), 'max_issues_repo_issues_event_max_datetime': Value(dtype='string', id=None), 'max_forks_repo_path': Value(dtype='string', id=None), 'max_forks_repo_name': Value(dtype='string', id=None), 'max_forks_repo_head_hexsha': Value(dtype='string', id=None), 'max_forks_repo_licenses': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'max_forks_count': Value(dtype='int64', id=None), 'max_forks_repo_forks_event_min_datetime': Value(dtype='string', id=None), 'max_forks_repo_forks_event_max_datetime': Value(dtype='string', id=None), 'content': Value(dtype='string', id=None), 'avg_line_length': Value(dtype='float64', id=None), 'max_line_length': Value(dtype='int64', id=None), 'alphanum_fraction': Value(dtype='float64', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/multiprocess/p\r\nool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/utils\r\n/py_utils.py\", line 1328, in _write_generator_to_queue\r\n    for i, result in enumerate(func(**kwargs)):\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/build\r\ner.py\", line 1912, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating th\r\ne dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while genera\r\nting the dataset\r\n\"\"\"\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/download_the_stack.py\", line 7, in <module>\r\n    the_stack_ds = ds.load_dataset(\"bigcode/the-stack-dedup\", split=\"tr\r\nain\", download_mode=\"reuse_cache_if_exists\", cache_dir=MY_CACHE_DIR, us\r\ne_auth_token=MY_TOKEN, num_proc=64) \r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/load.\r\npy\", line 1809, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/build\r\ner.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/build\r\ner.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/build\r\ner.py\", line 1796, in _prepare_split\r\n    for job_id, done, content in iflatmap_unordered(\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/utils\r\n/py_utils.py\", line 1354, in iflatmap_unordered\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/datasets/utils\r\n/py_utils.py\", line 1354, in <listcomp>\r\n    [async_result.get(timeout=0.05) for async_result in async_results]\r\n  File \"/home/ubuntu/.local/lib/python3.10/site-packages/multiprocess/p\r\nool.py\", line 774, in get\r\n    raise self._value\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\n\n### Expected behavior\n\nThe dataset downloads properly. @lhoestq @loub\n\n### Environment info\n\nDatasets 2.13.1, large VM with 2TB RAM, Ubuntu 20.04<end_of_turn>\n<start_of_turn>model\nthe-stack-dedup fails to generate<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6090<end_of_turn>\n<start_of_turn>model\nMinor fix in `iter_files` for hidden files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nwith python 3.11 I execute:\r\n```py\r\nfrom transformers import Wav2Vec2Processor, Data2VecAudioModel\r\nimport torch\r\nfrom torch import nn\r\nfrom datasets import load_dataset, concatenate_datasets\r\n\r\n# load demo audio and set processor\r\ndataset_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", data_dir=\"data\", cache_dir=\"cache\")\r\n```\r\nThis fails in the last line with \r\n```log\r\nFound cached dataset librispeech_asr (file:///Users/as/Documents/Project/git/audio2vec/cache/librispeech_asr/clean-data_dir=data/2.1.0/cff5df6e7955c80a67f80e27e7e655de71c689e2d2364bece785b972acb37fe7)\r\nTraceback (most recent call last):\r\n  File \"/Users/as/Documents/Project/git/audio2vec/src/music2vec-v1.py\", line 7, in <module>\r\n    dataset_clean = load_dataset(\"librispeech_asr\", \"clean\", split=\"validation\", data_dir=\"data\", cache_dir=\"cache\")\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/as/anaconda3/lib/python3.11/site-packages/datasets/load.py\", line 1810, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/as/anaconda3/lib/python3.11/site-packages/datasets/builder.py\", line 1113, in as_dataset\r\n    raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\r\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nI setup an venv with requirements.txt\r\n```txt\r\ntransformers==4.40.2\r\ntorch==2.2.2\r\ndatasets==2.16.0\r\nfsspec==2023.9.2\r\n```\r\n\r\npip freeze is:\r\n```\r\naiohttp==3.9.5\r\naiosignal==1.3.1\r\nattrs==23.2.0\r\ncertifi==2024.2.2\r\ncharset-normalizer==3.3.2\r\ndatasets==2.16.0\r\ndill==0.3.7\r\nfilelock==3.14.0\r\nfrozenlist==1.4.1\r\nfsspec==2023.9.2\r\nhuggingface-hub==0.23.0\r\nidna==3.7\r\nJinja2==3.1.4\r\nMarkupSafe==2.1.5\r\nmpmath==1.3.0\r\nmultidict==6.0.5\r\nmultiprocess==0.70.15\r\nnetworkx==3.3\r\nnumpy==1.26.4\r\npackaging==24.0\r\npandas==2.2.2\r\npyarrow==16.0.0\r\npyarrow-hotfix==0.6\r\npython-dateutil==2.9.0.post0\r\npytz==2024.1\r\nPyYAML==6.0.1\r\nregex==2024.4.28\r\nrequests==2.31.0\r\nsafetensors==0.4.3\r\nsix==1.16.0\r\nsympy==1.12\r\ntokenizers==0.19.1\r\ntorch==2.2.2\r\ntqdm==4.66.4\r\ntransformers==4.40.2\r\ntyping_extensions==4.11.0\r\ntzdata==2024.1\r\nurllib3==2.2.1\r\nxxhash==3.4.1\r\nyarl==1.9.4\r\n\r\n```\r\n\r\nI execute this on a M1 Mac.\r\n\r\n\r\n### Expected behavior\r\n\r\nI don't understand the error message. Why is \"local\" caching not supported. Would it possible to give some additional hint with the error message how to solve this issue?\r\n\r\n\r\n### Environment info\r\n\r\nsource ....\r\npython -u example.py<end_of_turn>\n<start_of_turn>model\nload_dataset with data_dir and cache_dir set fail with not supported<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nJSON lines with empty struct raise ArrowTypeError: struct fields don't match or are in the wrong order\r\n\r\nSee example: https://huggingface.co/datasets/wikimedia/structured-wikipedia/discussions/5\r\n\r\n> ArrowTypeError: struct fields don't match or are in the wrong order: Input fields: struct<> output fields: struct<pov_count: int64, update_count: int64, citation_needed_count: int64>\r\n\r\nRelated to:\r\n- #7159<end_of_turn>\n<start_of_turn>model\nJSON lines with empty struct raise ArrowTypeError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix wrong SHA in CI tests of HubDatasetModuleFactoryWithParquetExport.<end_of_turn>\n<start_of_turn>model\nFix wrong SHA in CI tests of HubDatasetModuleFactoryWithParquetExport<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently passing columns= to load_dataset in streaming mode fails\r\n\r\n```\r\nTried to load parquet data with columns '['link']' with mismatching features '{'caption': Value(dtype='string', id=None), 'image': {'bytes': Value(dtype='binary', id=None), 'path': Value(dtype='null', id=None)}, 'link': Value(dtype='string', id=None), 'message_id': Value(dtype='string', id=None), 'timestamp': Value(dtype='string', id=None)}'\r\n```\r\n\r\nsimilar to https://github.com/huggingface/datasets/issues/6039\r\n\r\nreported at https://huggingface.co/datasets/laion/dalle-3-dataset/discussions/3#65259a09617407d4520f4ad9<end_of_turn>\n<start_of_turn>model\nChoose columns to stream parquet data in streaming mode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm currently using Clash for Windows as my proxy tunnel, after exporting HTTP_PROXY and HTTPS_PROXY to the port that clash provides\ud83e\udd14, it runs into a connection error saying \"Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py (ConnectionError(MaxRetryError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f969d391870>: Failed to establish a new connection: [Errno 111] Connection refused'))\")))\"\r\nI have already read the documentation provided on the hugginface, but I think I didn't see the detailed instruction on how to set up proxies for this library.\n\n### Steps to reproduce the bug\n\n1. Turn on any proxy software like Clash / ShadosocksR etc.\r\n2. export system varibles to the port provided by your proxy software in wsl (It's ok for other applications to use proxy expect dataset-library)\r\n3. load any dataset from hugginface online\n\n### Expected behavior\n\n---------------------------------------------------------------------------\r\nConnectionError                           Traceback (most recent call last)\r\nCell In[33], [line 3](vscode-notebook-cell:?execution_count=33&line=3)\r\n      [1](vscode-notebook-cell:?execution_count=33&line=1) from datasets import load_metric\r\n----> [3](vscode-notebook-cell:?execution_count=33&line=3) metric = load_metric(\"seqeval\")\r\n\r\nFile ~/.local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py:46, in deprecated.<locals>.decorator.<locals>.wrapper(*args, **kwargs)\r\n     [44](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py:44)     warnings.warn(warning_msg, category=FutureWarning, stacklevel=2)\r\n     [45](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py:45)     _emitted_deprecation_warnings.add(func_hash)\r\n---> [46](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/utils/deprecation_utils.py:46) return deprecated_function(*args, **kwargs)\r\n\r\nFile ~/.local/lib/python3.10/site-packages/datasets/load.py:2104, in load_metric(path, config_name, process_id, num_process, cache_dir, experiment_id, keep_in_memory, download_config, download_mode, revision, trust_remote_code, **metric_init_kwargs)\r\n   [2101](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2101) warnings.filterwarnings(\"ignore\", message=\".*https://huggingface.co/docs/evaluate$\", category=FutureWarning)\r\n   [2103](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2103) download_mode = DownloadMode(download_mode or DownloadMode.REUSE_DATASET_IF_EXISTS)\r\n-> [2104](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2104) metric_module = metric_module_factory(\r\n   [2105](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2105)     path,\r\n   [2106](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2106)     revision=revision,\r\n   [2107](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2107)     download_config=download_config,\r\n   [2108](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2108)     download_mode=download_mode,\r\n   [2109](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2109)     trust_remote_code=trust_remote_code,\r\n   [2110](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2110) ).module_path\r\n   [2111](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2111) metric_cls = import_main_class(metric_module, dataset=False)\r\n   [2112](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2112) metric = metric_cls(\r\n   [2113](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2113)     config_name=config_name,\r\n   [2114](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/load.py:2114)     process_id=process_id,\r\n...\r\n--> [633](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py:633)     raise ConnectionError(f\"Couldn't reach {url} ({repr(head_error)})\")\r\n    [634](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py:634) elif response is not None:\r\n    [635](https://vscode-remote+wsl-002bubuntu-002d22-002e04.vscode-resource.vscode-cdn.net/home/noodle/Transformers-Tutorials/LayoutLMv3/~/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py:635)     raise ConnectionError(f\"Couldn't reach {url} (error {response.status_code})\")\r\n\r\nConnectionError: Couldn't reach https://raw.githubusercontent.com/huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py (SSLError(MaxRetryError(\"HTTPSConnectionPool(host='raw.githubusercontent.com', port=443): Max retries exceeded with url: /huggingface/datasets/2.19.1/metrics/seqeval/seqeval.py (Caused by SSLError(SSLEOFError(8, '[SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)')))\")))\n\n### Environment info\n\n- `datasets` version: 2.19.1\r\n- Platform: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.23.0\r\n- PyArrow version: 16.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nConnection Error When Using By-pass Proxies<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRevert changes in #6396 and #6404. CVE-2023-47248 has been fixed since PyArrow v14.0.1. Meanwhile Python requirements requires `pyarrow>=15.0.0`.<end_of_turn>\n<start_of_turn>model\npackaging: Remove useless dependencies<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRight now, `interleave_datasets` function with probabilities will sample with replacement. The PR adds the ability to sample without replacement.\r\n\r\n```\r\nimport datasets\r\n\r\n# Create datasets of different sizes to test exhaustion\r\ndata_a = [{\"value\": i, \"source\": \"A\"} for i in range(5)]\r\ndata_b = [{\"value\": i, \"source\": \"B\"} for i in range(10, 15)]\r\n\r\nds_a = datasets.Dataset.from_list(data_a).to_iterable_dataset()\r\nds_b = datasets.Dataset.from_list(data_b).to_iterable_dataset()\r\n\r\n# Interleave with probabilities\r\nds_interleaved = datasets.interleave_datasets(\r\n    [ds_a, ds_b],\r\n    probabilities=[0.6, 0.4],\r\n    seed=42,\r\n    stopping_strategy=\"all_exhausted\",\r\n    sample_with_replacement=True,\r\n)\r\nfor i, example in enumerate(ds_interleaved):\r\n    print(f\"Sample:{i}: value:{example['value']:02d} source:{example['source']}\")\r\n```\r\nIn this example, `sample_with_replacement=True` and it prints:\r\n```\r\nSample:0: value:10 source:B\r\nSample:1: value:00 source:A\r\nSample:2: value:11 source:B\r\nSample:3: value:12 source:B\r\nSample:4: value:01 source:A\r\nSample:5: value:13 source:B\r\nSample:6: value:14 source:B\r\nSample:7: value:10 source:B\r\nSample:8: value:02 source:A\r\nSample:9: value:03 source:A\r\nSample:10: value:04 source:A\r\n```\r\nNote that sample with value:10 source: B is sampled twice (Sample:0 and Sample:7)\r\n\r\nRe-running with `sample_with_replacement=False` in prints:\r\n```\r\nSample:0: value:10 source:B\r\nSample:1: value:00 source:A\r\nSample:2: value:11 source:B\r\nSample:3: value:12 source:B\r\nSample:4: value:01 source:A\r\nSample:5: value:13 source:B\r\nSample:6: value:14 source:B\r\nSample:7: value:02 source:A\r\nSample:8: value:03 source:A\r\nSample:9: value:04 source:A\r\n```\r\nNote that we don't see any repeated items.<end_of_turn>\n<start_of_turn>model\nSample without replacement option when interleaving datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #6834 <end_of_turn>\n<start_of_turn>model\nSupport pyarrow LargeListType<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAI in biology is becoming a big thing. One thing that would be a huge benefit to the field that Huggingface Datasets doesn't currently have is native support for **sparse arrays**. \n\n\nArrow has support for sparse tensors. \nhttps://arrow.apache.org/docs/format/Other.html#sparse-tensor\n\n\nIt would be a big deal if Hugging Face Datasets supported sparse tensors as a feature type, natively. \n\n\n### Motivation\n\nThis is important for example in the field of transcriptomics (modeling and understanding gene expression), because a large fraction of the genes are not expressed (zero). More generally, in science, sparse arrays are very common, so adding support for them would be very benefitial, it would make just using Hugging Face Dataset objects a lot more straightforward and clean.\n\n\n### Your contribution\n\nWe can discuss this further once the team comments of what they think about the feature, and if there were previous attempts at making it work, and understanding their evaluation of how hard it would be. My intuition is that it should be fairly straightforward, as the Arrow backend already supports it.<end_of_turn>\n<start_of_turn>model\nSupport for sparse arrays with the Arrow Sparse Tensor format?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUnable to upload a large dataset zip from command line or UI. UI simply says error. I am trying to a upload a tar.gz file of 17GB.\r\n\r\n<img width=\"550\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f9d29024-06c8-49c4-a109-0492cff79d34\">\r\n<img width=\"755\" alt=\"image\" src=\"https://github.com/user-attachments/assets/a8d4acda-7f02-4279-9c2d-b2e0282b4faa\">\r\n\n\n### Steps to reproduce the bug\n\nUpload a large file \n\n### Expected behavior\n\nThe file should upload without any issue.\n\n### Environment info\n\nNone<end_of_turn>\n<start_of_turn>model\nUnable to upload a large dataset zip either from command line or UI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRight now, `interleave_datasets` function with probabilities will sample with replacement. The PR adds the ability to sample without replacement.\r\n\r\n```\r\nimport datasets\r\n\r\n# Create datasets of different sizes to test exhaustion\r\ndata_a = [{\"value\": i, \"source\": \"A\"} for i in range(5)]\r\ndata_b = [{\"value\": i, \"source\": \"B\"} for i in range(10, 15)]\r\n\r\nds_a = datasets.Dataset.from_list(data_a).to_iterable_dataset()\r\nds_b = datasets.Dataset.from_list(data_b).to_iterable_dataset()\r\n\r\n# Interleave with probabilities\r\nds_interleaved = datasets.interleave_datasets(\r\n    [ds_a, ds_b],\r\n    probabilities=[0.6, 0.4],\r\n    seed=42,\r\n    stopping_strategy=\"all_exhausted\",\r\n    sample_with_replacement=True,\r\n)\r\nfor i, example in enumerate(ds_interleaved):\r\n    print(f\"Sample:{i}: value:{example['value']:02d} source:{example['source']}\")\r\n```\r\nIn this example, `sample_with_replacement=True` and it prints:\r\n```\r\nSample:0: value:10 source:B\r\nSample:1: value:00 source:A\r\nSample:2: value:11 source:B\r\nSample:3: value:12 source:B\r\nSample:4: value:01 source:A\r\nSample:5: value:13 source:B\r\nSample:6: value:14 source:B\r\nSample:7: value:10 source:B\r\nSample:8: value:02 source:A\r\nSample:9: value:03 source:A\r\nSample:10: value:04 source:A\r\n```\r\nNote that sample with value:10 source: B is sampled twice (Sample:0 and Sample:7)\r\n\r\nRe-running with `sample_with_replacement=False` in prints:\r\n```\r\nSample:0: value:10 source:B\r\nSample:1: value:00 source:A\r\nSample:2: value:11 source:B\r\nSample:3: value:12 source:B\r\nSample:4: value:01 source:A\r\nSample:5: value:13 source:B\r\nSample:6: value:14 source:B\r\nSample:7: value:02 source:A\r\nSample:8: value:03 source:A\r\nSample:9: value:04 source:A\r\n```\r\nNote that we don't see any repeated items.<end_of_turn>\n<start_of_turn>model\nSample without replacement option when interleaving datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- adds support for the `fs.glob` changes introduced in `fsspec==2023.12.0` and unpins the current upper bound\r\n\r\nShould close #6644\r\nShould close #6645\r\n\r\nThe `test_data_files` glob/pattern tests pass for me in:\r\n- `fsspec==2023.10.0` (the pinned max version in datasets `main`)\r\n- `fsspec==2023.12.0` (#6644)\r\n- `fsspec==2024.2.0` (#6645)<end_of_turn>\n<start_of_turn>model\nfsspec: support fsspec>=2023.12.0 glob changes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #5651. The same problem occurs when loading from disk so I fixed it there too.\r\n\r\nI am not sure why the case distinction between local and remote filesystems is even necessary for `DatasetDict` when saving to disk. Imo this could be removed (leaving only `fs.makedirs(dataset_dict_path, exist_ok=True)`).<end_of_turn>\n<start_of_turn>model\nExpanduser in save_to_disk()<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported at https://huggingface.co/datasets/ProGamerGov/synthetic-dataset-1m-dalle3-high-quality-captions/discussions/1\r\n\r\n```\r\nFile \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/packaged_modules/webdataset/webdataset.py\", line 109, in _generate_examples\r\n    example[field_name] = {\"path\": example[\"__key__\"] + \".\" + field_name, \"bytes\": example[field_name]}\r\n```<end_of_turn>\n<start_of_turn>model\n[WebDataset] KeyError with user-defined `Features` when a field is missing in an example<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nhttps://github.com/huggingface/datasets/pull/7740#discussion_r2285605374\n\n### Motivation\n\n`datasets` has the `Features` class for representing nested features. HDF5 files have groups of datasets which are nested, though in #7690 the keys are flattened. We should preserve that structure for the user.\n\n### Your contribution\n\nI'll open a PR (#7743)<end_of_turn>\n<start_of_turn>model\nPreserve tree structure when loading HDF5<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfir this error when accessing a local pdf\r\n\r\n```\r\nFile ~/.pyenv/versions/3.12.2/envs/hf-datasets/lib/python3.12/site-packages/pdfminer/psparser.py:220, in PSBaseParser.seek(self, pos)\r\n    218 \"\"\"Seeks the parser to the given position.\"\"\"\r\n    219 log.debug(\"seek: %r\", pos)\r\n--> 220 self.fp.seek(pos)\r\n    221 # reset the status for nextline()\r\n    222 self.bufpos = pos\r\n\r\nValueError: seek of closed file\r\n```<end_of_turn>\n<start_of_turn>model\nFix local pdf loading<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- use `hf_hub_download()` from `huggingface_hub` for HF files\r\n- `datasets` cache_dir is still used for:\r\n  - caching datasets as Arrow files (that back `Dataset` objects)\r\n  - extracted archives, uncompressed files\r\n  - files downloaded via http (datasets with scripts)\r\n- I removed code that were made for http files (and also the dummy_data / mock_download_manager stuff that happened to rely on them and have been legacy for a while now)<end_of_turn>\n<start_of_turn>model\nUse `huggingface_hub` cache<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate: see comments below\r\n\r\n### Describe the bug\r\n\r\nOperations that save an image from a path are very slow.\r\nI believe the reason for this is that the image data (`numpy`) is converted into `pyarrow` format but then back to python using `.pylist()` before being converted to a numpy array again.\r\n\r\n`pylist` is already slow but used on a multi-dimensional numpy array such as an image it takes a very long time.\r\n\r\nFrom the trace below we can see that `__arrow_array__` takes a long time.\r\nIt is currently also called in `get_inferred_type`, this should be removable #6781 but doesn't change the underyling issue.\r\n\r\nThe conversion to `pyarrow` and back also leads to the `numpy` array having type `int64` which causes a warning message because the image type excepts `uint8`.\r\nHowever, originally the `numpy` image array was in `uint8`.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom PIL import Image\r\nimport numpy as np\r\nimport datasets\r\nimport cProfile\r\n\r\nimage = Image.fromarray(np.random.randint(0, 255, (2048, 2048, 3), dtype=np.uint8))\r\nimage.save(\"test_image.jpg\")\r\n\r\nds = datasets.Dataset.from_dict(\r\n    {\"image\": [\"test_image.jpg\"]},\r\n    features=datasets.Features({\"image\": datasets.Image(decode=True)}),\r\n)\r\n\r\n# load as numpy array, e.g. for further processing with map\r\n# same result as map returning numpy arrays\r\nds.set_format(\"numpy\")\r\n\r\ncProfile.run(\"ds.map(writer_batch_size=1, load_from_cache_file=False)\", \"restats\")\r\n```\r\n```bash\r\nFri Apr  5 14:56:17 2024    restats\r\n\r\n         66817 function calls (64992 primitive calls) in 33.382 seconds\r\n\r\n   Ordered by: cumulative time\r\n   List reduced from 1073 to 20 due to restriction <20>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n     46/1    0.000    0.000   33.382   33.382 {built-in method builtins.exec}\r\n        1    0.000    0.000   33.382   33.382 <string>:1(<module>)\r\n        1    0.000    0.000   33.382   33.382 arrow_dataset.py:594(wrapper)\r\n        1    0.000    0.000   33.382   33.382 arrow_dataset.py:551(wrapper)\r\n        1    0.000    0.000   33.379   33.379 arrow_dataset.py:2916(map)\r\n        4    0.000    0.000   33.327    8.332 arrow_dataset.py:3277(_map_single)\r\n        1    0.000    0.000   33.311   33.311 arrow_writer.py:465(write)\r\n        2    0.000    0.000   33.311   16.656 arrow_writer.py:423(write_examples_on_file)\r\n        1    0.000    0.000   33.311   33.311 arrow_writer.py:527(write_batch)\r\n        2   14.484    7.242   33.260   16.630 arrow_writer.py:161(__arrow_array__)\r\n        1    0.001    0.001   16.438   16.438 arrow_writer.py:121(get_inferred_type)\r\n        1    0.000    0.000   14.398   14.398 threading.py:637(wait)\r\n        1    0.000    0.000   14.398   14.398 threading.py:323(wait)\r\n        8   14.398    1.800   14.398    1.800 {method 'acquire' of '_thread.lock' objects}\r\n      4/2    0.000    0.000    4.337    2.169 table.py:1800(wrapper)\r\n        2    0.000    0.000    4.337    2.169 table.py:1950(cast_array_to_feature)\r\n        2    0.475    0.238    4.337    2.169 image.py:209(cast_storage)\r\n        9    2.583    0.287    2.583    0.287 {built-in method numpy.array}\r\n        2    0.000    0.000    1.284    0.642 image.py:319(encode_np_array)\r\n        2    0.000    0.000    1.246    0.623 image.py:301(image_to_bytes)\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe `numpy` image data should be passed through as it will be directly consumed by `pillow` to convert it to bytes.\r\n\r\nAs an example one can replace `list_of_np_array_to_pyarrow_listarray(data)` in `__arrow_array__` with just `out = data` as a test.\r\nWe have to change `cast_storage` of the `Image` feature so it handles the passed through data (& if to handle type before)\r\n```python\r\nbytes_array = pa.array(\r\n    [encode_np_array(arr)[\"bytes\"] if arr is not None else None for arr in storage],\r\n    type=pa.binary(),\r\n)\r\n```\r\nLeading to the following:\r\n```bash\r\nFri Apr  5 15:44:27 2024    restats\r\n\r\n         66419 function calls (64595 primitive calls) in 0.937 seconds\r\n\r\n   Ordered by: cumulative time\r\n   List reduced from 1023 to 20 due to restriction <20>\r\n\r\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\r\n     47/1    0.000    0.000    0.935    0.935 {built-in method builtins.exec}\r\n      2/1    0.000    0.000    0.935    0.935 <string>:1(<module>)\r\n      2/1    0.000    0.000    0.934    0.934 arrow_dataset.py:594(wrapper)\r\n      2/1    0.000    0.000    0.934    0.934 arrow_dataset.py:551(wrapper)\r\n      2/1    0.000    0.000    0.934    0.934 arrow_dataset.py:2916(map)\r\n        4    0.000    0.000    0.933    0.233 arrow_dataset.py:3277(_map_single)\r\n        1    0.000    0.000    0.883    0.883 arrow_writer.py:466(write)\r\n        2    0.000    0.000    0.883    0.441 arrow_writer.py:424(write_examples_on_file)\r\n        1    0.000    0.000    0.882    0.882 arrow_writer.py:528(write_batch)\r\n        2    0.000    0.000    0.877    0.439 arrow_writer.py:161(__arrow_array__)\r\n      4/2    0.000    0.000    0.877    0.439 table.py:1800(wrapper)\r\n        2    0.000    0.000    0.877    0.439 table.py:1950(cast_array_to_feature)\r\n        2    0.009    0.005    0.877    0.439 image.py:209(cast_storage)\r\n        2    0.000    0.000    0.868    0.434 image.py:335(encode_np_array)\r\n        2    0.000    0.000    0.856    0.428 image.py:317(image_to_bytes)\r\n        2    0.000    0.000    0.822    0.411 Image.py:2376(save)\r\n        2    0.000    0.000    0.822    0.411 PngImagePlugin.py:1233(_save)\r\n        2    0.000    0.000    0.822    0.411 ImageFile.py:517(_save)\r\n        2    0.000    0.000    0.821    0.411 ImageFile.py:545(_encode_tile)\r\n      589    0.803    0.001    0.803    0.001 {method 'encode' of 'ImagingEncoder' objects}\r\n```\r\n\r\nThis is of course only a test as it passes through all `numpy` arrays irrespective of if they should be an image.\r\nAlso I guess `cast_storage` is meant for casting `pyarrow` storage exclusively. \r\nConverting to `pyarrow` array seems like a good solution as it also handles `pytorch` tensors etc., maybe there is a more efficient way to create a PIL image from a `pyarrow` array?\r\n\r\nNot sure how this should be handled but I would be happy to help if there is a good solution.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.18.1.dev0\r\n- Platform: Linux-6.7.11-200.fc39.x86_64-x86_64-with-glibc2.38\r\n- Python version: 3.12.2\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nImage cast_storage very slow for arrays (e.g. numpy, tensors)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease 2.14.4<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, I'm trying to download a dataset. To not load the entire dataset in memory, I split it as described [here](https://huggingface.co/docs/datasets/v4.0.0/loading#slice-splits) in 10% steps.\n\nWhen doing so, the library returns this error:\n    raise ValueError(f\"Bad split: {split}. Available splits: {list(splits_generators)}\")\nValueError: Bad split: train[0%:10%]. Available splits: ['train']\n\nEdit: Same happens with a split like _train[:90000]_\n### Steps to reproduce the bug\n\n```\n  for split in range(10):\n    split_str = f\"train[{split*10}%:{(split+1)*10}%]\"\n    print(f\"Processing split {split_str}...\")\n    ds = load_dataset(\"user/dataset\", split=split_str, streaming=True)\n```\n\n\n### Expected behavior\n\nI'd expect the library to split my dataset in 10% steps.\n\n### Environment info\n\npython 3.12.11\nubuntu 24\ndataset 4.0.0<end_of_turn>\n<start_of_turn>model\nBad split error message when using percentages<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI\u2019m a beginner from NIT Srinagar proposing a dataset of 50 Tamil text pairs for discourse coherence (coherent/incoherent labels) to support NLP research in low-resource languages.\n- Size: 50 samples\n- Format: CSV with columns (text1, text2, label)\n- Use case: Training NLP models for coherence\nI\u2019ll use GitHub\u2019s web editor and Google Colab. Please confirm if this fits.<end_of_turn>\n<start_of_turn>model\nProposal: Small Tamil Discourse Coherence Dataset.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nupdate tips in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn order to keep PR pages less spammy / more readable.\r\nHaving the benchmarks on commits on `main` is enough imo<end_of_turn>\n<start_of_turn>model\nDisable benchmarks in PRs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRevert ci user<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTitle: Add wikipedia-2023-redirects dataset (redirect resolution + pageviews)\n\nSummary\n- New dataset loader: wikipedia_2023_redirects\n- Canonical Wikipedia pages enriched with:\n  - redirects (aliases pointing to the page)\n  - 2023 pageviews (aggregated)\n- Streaming support; robust parsing; license notes included\n- Tests with tiny dummy data (XML + TSVs); covers streaming\n\nMotivation\nRAG/retrieval often benefits from:\n- Query expansion via redirect aliases\n- Popularity prior via pageviews\nThis loader offers a practical, maintenance-light way to access canonical pages alongside their redirect aliases and 2023 pageview totals.\n\nFeatures\n- id: string\n- title: string\n- url: string\n- text: string\n- redirects: list[string]\n- pageviews_2023: int32\n- timestamp: string\n\nLicensing\n- Wikipedia text: CC BY-SA 3.0 (attribution and share-alike apply)\n- Pageviews: public domain\nThe PR docs mention both, and the module docstring cites sources.\n\nNotes\n- The URLs in _get_urls_for_config are wired to dummy files for tests. In production, these would point to Wikimedia dumps:\n  - XML page dumps: https://dumps.wikimedia.org/\n  - Pageviews: https://dumps.wikimedia.org/other/pageviews/\n- The schema is intentionally simple and stable. Pageview aggregation is per-title sum across 2023.\n\nTesting\n- make style && make quality\n- pytest -q tests/test_dataset_wikipedia_2023_redirects.py\n\nExample\n```python\nfrom datasets import load_dataset\nds = load_dataset(\"wikipedia_2023_redirects\", split=\"train\")\nprint(ds[0][\"title\"], ds[0][\"redirects\"][:5], ds[0][\"pageviews_2023\"])\n```\n\nAcknowledgements\n- Wikipedia/Wikimedia Foundation for the source data\n- Hugging Face Datasets for the dataset infrastructure\n<end_of_turn>\n<start_of_turn>model\nAdd wikipedia-2023-redirects dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI was running into\r\n\r\n```\r\nexample[field_name] = {\"path\": example[\"__key__\"] + \".\" + field_name, \"bytes\": example[field_name]}\r\nKeyError: 'png'\r\n```\r\n\r\nThe issue is that a filename may have multiple \".\" e.g. `22.05.png`. Changing `split` to `rsplit` fixes it.\r\n\r\nRelated https://github.com/huggingface/datasets/issues/6880<end_of_turn>\n<start_of_turn>model\nFix key error in webdataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`get_deletions_and_dataset_card` stuck at error a commit has happened error since push to hub for http error 412 for tag 4.1.0. The error does not exists in 4.0.0. \n\n### Steps to reproduce the bug\n\nCreate code to use push_to_hub, ran twice each time with different content for datasets.Dataset.\nThe code will stuck in time.sleep loop for `get_deletions_and_dataset_card`. If error is explicitly printed, the error is HTTP 412.\n\n### Expected behavior\n\nNew datasets overwrite existing one on repo.\n\n### Environment info\n\ndatasets 4.1.0<end_of_turn>\n<start_of_turn>model\npush_to_hub not overwriting but stuck in a loop when there are existing commits<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading a dataset from with a data_dir argument generates a NonMatchingSplitsSizesError if there are multiple directories in the dataset. \r\n\r\nThis appears to happen because the expected split is calculated based on the data in all the directories whereas the recorded split is calculated based on the data in the directory specified using the data_dir argument. \r\n\r\nThis is recent behavior. Until the past few weeks loading using the data_dir argument worked without any issue.\n\n### Steps to reproduce the bug\n\nSimple test dataset available here: https://huggingface.co/datasets/srehaag/hf-bug-temp\r\n\r\nThe dataset contains two directories \"data1\" and \"data2\", each with a file called \"train.parquet\" with a 2 x 5 table.\r\n\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"srehaag/hf-bug-temp\", data_dir = \"data1\")\r\n\r\nGenerates:\r\n\r\n---------------------------------------------------------------------------\r\nNonMatchingSplitsSizesError               Traceback (most recent call last)\r\nCell In[3], <a href='vscode-notebook-cell:?execution_count=3&line=2'>line 2</a>\r\n      <a href='vscode-notebook-cell:?execution_count=3&line=1'>1</a> from datasets import load_dataset\r\n----> <a href='vscode-notebook-cell:?execution_count=3&line=2'>2</a> dataset = load_dataset(\"srehaag/hf-bug-temp\", data_dir = \"data1\")\r\n\r\nFile ~/.python/current/lib/python3.10/site-packages/datasets/load.py:2609, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2606'>2606</a>     return builder_instance.as_streaming_dataset(split=split)\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2608'>2608</a> # Download and prepare data\r\n-> <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2609'>2609</a> builder_instance.download_and_prepare(\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2610'>2610</a>     download_config=download_config,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2611'>2611</a>     download_mode=download_mode,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2612'>2612</a>     verification_mode=verification_mode,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2613'>2613</a>     num_proc=num_proc,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2614'>2614</a>     storage_options=storage_options,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2615'>2615</a> )\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2617'>2617</a> # Build dataset for splits\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2618'>2618</a> keep_in_memory = (\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2619'>2619</a>     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/load.py:2620'>2620</a> )\r\n\r\nFile ~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1027, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1025'>1025</a>     if num_proc is not None:\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1026'>1026</a>         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n-> <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1027'>1027</a>     self._download_and_prepare(\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1028'>1028</a>         dl_manager=dl_manager,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1029'>1029</a>         verification_mode=verification_mode,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1030'>1030</a>         **prepare_split_kwargs,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1031'>1031</a>         **download_and_prepare_kwargs,\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1032'>1032</a>     )\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1033'>1033</a> # Sync info\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1034'>1034</a> self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1140, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1137'>1137</a>     dl_manager.manage_extracted_files()\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1139'>1139</a> if verification_mode == VerificationMode.BASIC_CHECKS or verification_mode == VerificationMode.ALL_CHECKS:\r\n-> <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1140'>1140</a>     verify_splits(self.info.splits, split_dict)\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1142'>1142</a> # Update the info object with the splits.\r\n   <a href='~/.python/current/lib/python3.10/site-packages/datasets/builder.py:1143'>1143</a> self.info.splits = split_dict\r\n\r\nFile ~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:101, in verify_splits(expected_splits, recorded_splits)\r\n     <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:95'>95</a> bad_splits = [\r\n     <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:96'>96</a>     {\"expected\": expected_splits[name], \"recorded\": recorded_splits[name]}\r\n     <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:97'>97</a>     for name in expected_splits\r\n     <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:98'>98</a>     if expected_splits[name].num_examples != recorded_splits[name].num_examples\r\n     <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:99'>99</a> ]\r\n    <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:100'>100</a> if len(bad_splits) > 0:\r\n--> <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:101'>101</a>     raise NonMatchingSplitsSizesError(str(bad_splits))\r\n    <a href='~/.python/current/lib/python3.10/site-packages/datasets/utils/info_utils.py:102'>102</a> logger.info(\"All the splits matched successfully.\")\r\n\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=212, num_examples=10, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=106, num_examples=5, shard_lengths=None, dataset_name='hf-bug-temp')}]\r\n\r\n__________\r\n\r\nBy contrast, this loads the data from both data1/train.parquet and data2/train.parquet without any error message:\r\n\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"srehaag/hf-bug-temp\")\r\n\r\n\n\n### Expected behavior\n\nShould load the 5 x 2 table from data1/train.parquet without error message.\n\n### Environment info\n\nUsed Codespaces to simplify environment (see details below), but bug is present across various configurations.\r\n\r\n- `datasets` version: 2.19.1\r\n- Platform: Linux-6.5.0-1021-azure-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.23.1\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nNonMatchingSplitsSizesError when using data_dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs a hotfix for CI, temporarily mark test_convert_to_parquet as expected to fail.\r\n\r\nFix #7073.\r\n\r\nRevert once root cause is fixed.<end_of_turn>\n<start_of_turn>model\nFix CI by temporarily marking test_convert_to_parquet as expected to fail<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...and use `huggingface_hub.get_token()` instead<end_of_turn>\n<start_of_turn>model\nRemove deprecated HfFolder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse the `filelock` package instead of `datasets.utils.filelock` for file locking to be consistent with `huggingface_hub` and not to be responsible for improving the `filelock` capabilities \ud83d\ude42. \r\n\r\n(Reverts https://github.com/huggingface/datasets/pull/859, but these `INFO` logs are not printed by default (anymore?), so this should be okay)\r\n\r\n <end_of_turn>\n<start_of_turn>model\nUse `filelock` package for file locking<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd support for NIfTI.\r\n\r\nsupports #7804\r\n\r\nThis PR follows https://github.com/huggingface/datasets/pull/7325 very closely\r\n\r\nI am a bit unsure what we need to add to the `document_dataset.mdx` and `document_load.mdx`. I should probably create a dataset on the hub first to create this guide instead of copy+pasting from PDF. \r\n\r\nOpen todos:\r\n - [x] create nifti dataset on the hub\r\n - ~[ ] update `document_dataset.mdx` and `document_load.mdx`~\r\n \r\n EDIT:\r\n I tested with two datasets I created on the hub: \r\n  - https://huggingface.co/datasets/TobiasPitters/test-nifti-unzipped\r\n  - https://huggingface.co/datasets/TobiasPitters/test-nifti\r\n  \r\n for zipped (file extension `.nii.gz` and unzipped `.nii`) files and both seem to work fine. Also tested loading locally and that seems to work as well.\r\nHere is the scriptsthat I ran against the hub:\r\n\r\n```python\r\nfrom pathlib import Path\r\n\r\nfrom datasets import load_dataset\r\nimport nibabel as nib\r\n\r\n\r\ndataset = load_dataset(\r\n        \"TobiasPitters/test-nifti-unzipped\",\r\n        split=\"test\"  # Load as single Dataset, not DatasetDict\r\n)\r\n\r\nprint(\"length dataset unzipped:\", len(dataset))\r\nfor item in dataset:\r\n    isinstance(item[\"nifti\"], nib.nifti1.Nifti1Image)\r\n\r\ndataset = load_dataset(\r\n        \"TobiasPitters/test-nifti\",\r\n        split=\"train\"  # Load as single Dataset, not DatasetDict\r\n)\r\nprint(\"length dataset zipped:\", len(dataset))\r\nfor item in dataset:\r\n    isinstance(item[\"nifti\"], nib.nifti1.Nifti1Image)\r\n```<end_of_turn>\n<start_of_turn>model\nAdd nifti support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, \n\nUpon using `iwslt2017.py` in `IWSLT/iwslt2017` on the Hub for loading the datasets, I am unable to obtain the datasets for the language pairs `de-it`, `de-ro`, `de-nl`, `it-de`, `nl-de`, and `ro-de` using it. These 6 pairs do not show up when using `get_dataset_config_names()` to obtain the list of all the configs present in `IWSLT/iwslt2017`. This should not be the case since as mentioned in their original paper (please see https://aclanthology.org/2017.iwslt-1.1.pdf), the authors specify that \"_this year we proposed the multilingual translation between any pair of languages from {Dutch, English, German, Italian, Romanian}..._\" and because these datasets are indeed present in `data/2017-01-trnmted/texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.zip`.\n\nBest Regards,\nAnand\n\n### Steps to reproduce the bug\n\nCheck the output of `get_dataset_config_names(\"IWSLT/iwslt2017\", trust_remote_code=True)`: only 24 language pairs are present and the following 6 config names are absent: `iwslt2017-de-it`, `iwslt2017-de-ro`, `iwslt2017-de-nl`, `iwslt2017-it-de`, `iwslt2017-nl-de`, and `iwslt2017-ro-de`.\n\n### Expected behavior\n\nThe aforementioned 6 language pairs should also be present and hence, all these 6 language pairs' IWSLT2017 datasets must also be available for further use. \n\nI would suggest removing `de` from the `BI_LANGUAGES` list and moving it over to the `MULTI_LANGUAGES` list instead in `iwslt2017.py` to account for all the 6 missing language pairs (the same `de-en` dataset is present in both `data/2017-01-trnmted/texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.zip` and `data/2017-01-trnted/texts/de/en/de-en.zip` but the `de-ro`, `de-nl`, `it-de`, `nl-de`, and `ro-de` datasets are only present in  `data/2017-01-trnmted/texts/DeEnItNlRo/DeEnItNlRo/DeEnItNlRo-DeEnItNlRo.zip`: so, its unclear why the following comment: _`# XXX: Artificially removed DE from here, as it also exists within bilingual data`_ has been added as `L71` in `iwslt2017.py`). The `README.md` file in `IWSLT/iwslt2017`must then be re-created using `datasets-cli test path/to/iwslt2017.py --save_info --all_configs` to pass all split size verification checks for the 6 new language pairs which were previously non-existent. \n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-6.8.0-56-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nMissing multilingual directions in IWSLT2017 dataset's processing script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHello, I'm working with an audio dataset. I want to transcribe the audio that the dataset contain, and for that I use whisper. My issue is that the dataset load everything in the RAM when I map the dataset, obviously, when RAM usage is too high, the program crashes.\r\n\r\nTo fix this issue, I'm using writer_batch_size that I set to 10, but in this case, the mapping of the dataset is extremely slow.\r\nTo illustrate this, on 50 examples, with `writer_batch_size` set to 10, it takes 123.24 seconds to process the dataset, but without `writer_batch_size` set to 10, it takes about ten seconds to process the dataset, but then the process remains blocked (I assume that it is writing the dataset and therefore suffers from the same problem as `writer_batch_size`)\r\n\r\n### Steps to reproduce the bug\r\n\r\nHug ram usage but fast (but actually slow when saving the dataset):\r\n```py\r\nfrom datasets import load_dataset\r\nimport time\r\n\r\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\r\n\r\n\r\n# map the dataset\r\ndef transcribe_audio(row):\r\n    audio = row[\"audio\"]  # get the audio but do nothing with it\r\n    row[\"transcribed\"] = True\r\n    return row\r\n\r\n\r\ntime1 = time.time()\r\nds = ds.map(\r\n    transcribe_audio\r\n) \r\n\r\nfor row in ds:\r\n    pass  # do nothing, just iterate to trigger the map function\r\n\r\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\r\n```\r\n\r\nLow ram usage but very very slow:\r\n\r\n```py\r\nfrom datasets import load_dataset\r\nimport time\r\n\r\nds = load_dataset(\"WaveGenAI/audios2\", split=\"train[:50]\")\r\n\r\n\r\n# map the dataset\r\ndef transcribe_audio(row):\r\n    audio = row[\"audio\"]  # get the audio but do nothing with it\r\n    row[\"transcribed\"] = True\r\n    return row\r\n\r\n\r\ntime1 = time.time()\r\nds = ds.map(\r\n    transcribe_audio, writer_batch_size=10\r\n)  # set low writer_batch_size to avoid memory issues\r\n\r\nfor row in ds:\r\n    pass  # do nothing, just iterate to trigger the map function\r\n\r\nprint(f\"Time taken: {time.time() - time1:.2f} seconds\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nI think the processing should be much faster, on only 50 audio examples, the mapping takes several minutes while nothing is done (just loading the audio).\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.21.0\r\n- Platform: Linux-6.10.5-arch1-1-x86_64-with-glibc2.40\r\n- Python version: 3.10.4\r\n- `huggingface_hub` version: 0.24.5\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2024.6.1\r\n\r\n# Extra\r\n\r\nThe dataset has been generated by using audio folder, so I don't think anything specific in my code is causing this problem.\r\n```py\r\nimport argparse\r\n\r\nfrom datasets import load_dataset\r\n\r\nparser = argparse.ArgumentParser()\r\nparser.add_argument(\"--folder\", help=\"folder path\", default=\"/media/works/test/\")\r\nargs = parser.parse_args()\r\n\r\ndataset = load_dataset(\"audiofolder\", data_dir=args.folder)\r\n\r\n# push the dataset to hub\r\ndataset.push_to_hub(\"WaveGenAI/audios\")\r\n```\r\n\r\nAlso, it's the combination of `audio = row[\"audio\"]` and `row[\"transcribed\"] = True` which causes problems, `row[\"transcribed\"] = True `alone does nothing and `audio = row[\"audio\"]` alone sometimes causes problems, sometimes not.<end_of_turn>\n<start_of_turn>model\nAudio dataset load everything in RAM and is very slow<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi, as I followed the instructions, with lasted \"datasets\" version:\r\n\"\r\nfrom datasets import load_dataset\r\nexamples = load_dataset('facebook/winoground', use_auth_token=<YOUR USER ACCESS TOKEN>)\r\n\"\r\nI got slightly different datasets in colab and in my hpc environment. Specifically, the pixel values of images are slightly different. \r\nI thought it was due to the package version difference, but today's morning I found out that my winoground dataset in colab became the same with the one in my hpc environment. The dataset in colab can produce the correct result but now it is gone as well. \r\n\r\nCan you help me with this? What causes the datasets to have the wrong pixel values?<end_of_turn>\n<start_of_turn>model\nChanging pixel values of images in the Winoground dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 4.1.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nThe [how to process dataset guide](https://huggingface.co/docs/datasets/main/en/process) currently does not mention the [`select_columns`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.select_columns) function. It would be nice to include it in the guide.\n\n### Motivation\n\nThis function is a commonly requested feature (see this [forum thread](https://discuss.huggingface.co/t/how-to-create-a-new-dataset-from-another-dataset-and-select-specific-columns-and-the-data-along-with-the-column/15120) and #5468 #5474). However, it has not been included in the guide since its implementation by PR #5480.\r\n\r\nMentioning it in the guide would help future users discover this added feature.\n\n### Your contribution\n\nI could submit a PR to add a brief description of the function to said guide.<end_of_turn>\n<start_of_turn>model\n[Docs] The \"Process\" how-to guide lacks description of `select_columns` function<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## What's in this PR?\r\n\r\nThis PR solves #5887 since there was a mismatch between the tokenizer and the model used, since the tokenizer was `bert-base-cased` while the model was `distilbert-base-case` both for the PyTorch and TensorFlow alternatives. Since DistilBERT doesn't use/need the `token_type_ids`, the `**batch` was failing, as the batch contained `input_ids`, `attention_mask`, `token_type_ids`, `start_positions` and `end_positions`, and `token_type_ids` was not required.\r\n\r\nBesides that, at the end `seqeval` was being used to evaluate the model predictions, and just `evaluate` was being installed, so I've also included the `seqeval` installation.\r\n\r\nFinally, I've re-run everything in Google Colab, and every cell was successfully executed!\r\n\r\n## What was done on top of the original PR?\r\n\r\nBased on the comments from @mariosasko and @stevhliu, I've updated the contents of this PR to also review the `quickstart.mdx` and update what was needed, besides that, we may eventually move the `Overview.ipynb` dataset to `huggingface/notebooks` following @stevhliu suggestions.<end_of_turn>\n<start_of_turn>model\nFix `Overview.ipynb` & detach Jupyter Notebooks from `datasets` repository<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate documentation to align with `Dataset.__repr__` change after #423<end_of_turn>\n<start_of_turn>model\nUpdate the print message for chunked_dataset in process.mdx<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR removes unused `allowed_extensions` parameter from `create_builder_configs_from_metadata_configs`.<end_of_turn>\n<start_of_turn>model\nRemove unused allowed_extensions param<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen I was in load_dataset, it said \"DatasetInfo.__init__() got an unexpected keyword argument '_column_requires_decoding'\". The second time I ran it, there was no error and the dataset object worked\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 dataset = load_dataset(\r\n      2     \"/home/aihao/workspace/DeepLearningContent/datasets/manga\",\r\n      3     data_dir=\"/home/aihao/workspace/DeepLearningContent/datasets/manga\",\r\n      4     split=\"train\",\r\n      5 )\r\n\r\nFile [~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/load.py:2146](https://vscode-remote+ssh-002dremote-002bhome.vscode-resource.vscode-cdn.net/home/aihao/workspace/DeepLearningContent/datasets/~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/load.py:2146), in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2142 # Build dataset for splits\r\n   2143 keep_in_memory = (\r\n   2144     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2145 )\r\n-> 2146 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n   2147 # Rename and cast features to match task schema\r\n   2148 if task is not None:\r\n   2149     # To avoid issuing the same warning twice\r\n\r\nFile [~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py:1190](https://vscode-remote+ssh-002dremote-002bhome.vscode-resource.vscode-cdn.net/home/aihao/workspace/DeepLearningContent/datasets/~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/builder.py:1190), in DatasetBuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\r\n   1187 verification_mode = VerificationMode(verification_mode or VerificationMode.BASIC_CHECKS)\r\n   1189 # Create a dataset for each of the given splits\r\n-> 1190 datasets = map_nested(\r\n   1191     partial(\r\n   1192         self._build_single_dataset,\r\n...\r\nFile [~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/info.py:379](https://vscode-remote+ssh-002dremote-002bhome.vscode-resource.vscode-cdn.net/home/aihao/workspace/DeepLearningContent/datasets/~/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/info.py:379), in DatasetInfo.copy(self)\r\n    378 def copy(self) -> \"DatasetInfo\":\r\n--> 379     return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n\r\nTypeError: DatasetInfo.__init__() got an unexpected keyword argument '_column_requires_decoding'\r\n```\r\n### Steps to reproduce the bug\r\n/home/aihao/workspace/DeepLearningContent/datasets/images/images.py\r\n```python\r\nfrom logging import config\r\nimport datasets\r\nimport os\r\nfrom PIL import Image\r\nimport csv\r\nimport json\r\n\r\n\r\nclass ImagesConfig(datasets.BuilderConfig):\r\n    def __init__(self, **kwargs):\r\n        super(ImagesConfig, self).__init__(**kwargs)\r\n\r\n\r\nclass Images(datasets.GeneratorBasedBuilder):\r\n    def _split_generators(self, dl_manager: datasets.DownloadManager):\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN,\r\n                gen_kwargs={\"split\": datasets.Split.TRAIN},\r\n            )\r\n        ]\r\n\r\n    BUILDER_CONFIGS = [\r\n        ImagesConfig(\r\n            name=\"similar_pairs\",\r\n            description=\"simliar pair dataset,item is a pair of similar images\",\r\n        ),\r\n        ImagesConfig(\r\n            name=\"image_prompt_pairs\",\r\n            description=\"image prompt pairs\",\r\n        ),\r\n    ]\r\n\r\n    def _info(self):\r\n        if self.config.name == \"similar_pairs\":\r\n            return datasets.Features(\r\n                {\r\n                    \"image1\": datasets.features.Image(),\r\n                    \"image2\": datasets.features.Image(),\r\n                    \"similarity\": datasets.Value(\"float32\"),\r\n                }\r\n            )\r\n        elif self.config.name == \"image_prompt_pairs\":\r\n            return datasets.Features(\r\n                {\"image\": datasets.features.Image(), \"prompt\": datasets.Value(\"string\")}\r\n            )\r\n\r\n    def _generate_examples(self, split):\r\n        data_path = os.path.join(self.config.data_dir, \"data\")\r\n        if self.config.name == \"similar_pairs\":\r\n            prompts = {}\r\n            with open(os.path.join(data_path ,\"prompts.json\"), \"r\") as f:\r\n                prompts = json.load(f)\r\n            with open(os.path.join(data_path, \"similar_pairs.csv\"), \"r\") as f:\r\n                reader = csv.reader(f)\r\n                for row in reader:\r\n                    image1_path, image2_path, similarity = row\r\n                    yield image1_path + \":\" + image2_path + \":\", {\r\n                        \"image1\": Image.open(image1_path),\r\n                        \"prompt1\": prompts[image1_path],\r\n                        \"image2\": Image.open(image2_path),\r\n                        \"prompt2\": prompts[image2_path],\r\n                        \"similarity\": float(similarity),\r\n                    }\r\n```\r\n\r\nCode that indicates an error:\r\n```python\r\nfrom datasets import load_dataset\r\nimport json\r\nimport csv\r\nimport ast\r\nimport torch\r\ndata_dir = \"/home/aihao/workspace/DeepLearningContent/datasets/images\"\r\ndataset = load_dataset(data_dir, data_dir=data_dir, name=\"similar_pairs\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe first execution gives an error, but it works fine\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.3\r\n- Platform: Linux-6.2.0-26-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nDatasetInfo.__init__() got an unexpected keyword argument '_column_requires_decoding'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI tried to load local json file as dataset but failed to parsing json file because some columns are 'float' type.\n\n### Steps to reproduce the bug\n\n1. load json file with certain columns are 'float' type. For example `data = load_data(\"json\", data_files=JSON_PATH)`\r\n2. Then, the error will be triggered like `ArrowInvalid: Could not convert '-0.2253' with type str: tried to convert to double\n\n### Expected behavior\n\nShould allow some columns are 'float' type, at least it should convert those columns to str type.\r\nI tried to avoid the error by naively convert the float item to str:\r\n```python\r\n# if col type is not str, we need to convert it to str\r\nmapping = {}\r\nfor col in keys:\r\n    if isinstance(dataset[0][col], str):\r\n        mapping[col] = [row.get(col) for row in dataset]\r\n    else:\r\n        mapping[col] = [str(row.get(col)) for row in dataset]\r\n```\n\n### Environment info\n\n- `datasets` version: 2.14.2\r\n- Platform: Linux-5.4.0-52-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1<end_of_turn>\n<start_of_turn>model\nload local json_file as dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI added scan options, useful to control buffering and caching when streaming and docs, including how to select a subset of columns and apply filters<end_of_turn>\n<start_of_turn>model\nAdd parquet scan options and docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nImprove documentation to clarify sharding behavior (#6270)<end_of_turn>\n<start_of_turn>model\nImprove documentation of dataset.from_generator<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSimplifies the existing filesystem logic (e.g., to avoid unnecessary if-else as mentioned in https://github.com/huggingface/datasets/pull/6098#issue-1827655071)<end_of_turn>\n<start_of_turn>model\nSimplify filesystem logic<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ndisable aiohttp depend for python 3.13t free-threading compat<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe audio column remain as non-decoded objects even when accessing them.\n\n```python\ndataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")\ndataset[0] # see that it doesn't show 'array' etc...\n```\n\nWorks fine with `datasets==3.6.0`\n\nFollowed the docs in \n\n- https://huggingface.co/docs/datasets/en/audio_load\n\n### Steps to reproduce the bug\n\n```python\ndataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")\ndataset[0] # see that it doesn't show 'array' etc...\n```\n\n\n### Expected behavior\n\nIt should decode when accessing the elemenet\n\n### Environment info\n\n4.1.1\nubuntu 22.04\n\n\nRelated\n\n- https://github.com/huggingface/datasets/issues/7707<end_of_turn>\n<start_of_turn>model\nAudio dataset is not decoding on 4.1.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have granted permissions to several users to access a gated huggingface dataset. The users accepted the invite and when trying to load the dataset using their access token they get \r\n`FileNotFoundError: Couldn't find a dataset script at .....` . Also when they try to click the url link for the dataset they get a 404 error.\n\n### Steps to reproduce the bug\n\n1. Grant access to gated dataset for specific users \r\n2. Users accept invitation\r\n3. Users login to hugging face hub using cli login\r\n4. Users run load_dataset\n\n### Expected behavior\n\nDataset is loaded normally for users who were granted access to the gated dataset.\n\n### Environment info\n\ndatasets==2.15.0\r\n<end_of_turn>\n<start_of_turn>model\nTrouble Loading a Gated Dataset For User with Granted Permission<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nMore Parquet streaming docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval_gpt4_baseline\", trust_remote_code=True)` \r\nused to work till 2.20.0 but doesn't work in 2.21.0\r\n\r\nIn 2.20.0:\r\n![Screenshot 2024-08-16 at 3 57 10\u202fPM](https://github.com/user-attachments/assets/0516489b-8187-486d-bee8-88af3381dee9)\r\n\r\n\r\nin 2.21.0:\r\n![Screenshot 2024-08-16 at 3 57 24\u202fPM](https://github.com/user-attachments/assets/bc257570-f461-41e4-8717-90a69ed7c24f)\r\n\n\n### Steps to reproduce the bug\n\n1. Spin up a new google collab\r\n2. `pip install datasets==2.21.0`\r\n3. `import datasets`\r\n4. `eval_set = datasets.load_dataset(\"tatsu-lab/alpaca_eval\", \"alpaca_eval_gpt4_baseline\", trust_remote_code=True)` \r\n5. Will throw an error.\r\n\r\n\n\n### Expected behavior\n\nTry steps 1-5 again but replace datasets version with 2.20.0, it will work\n\n### Environment info\n\n\r\n\r\n- `datasets` version: 2.21.0\r\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.23.5\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2024.5.0\r\n<end_of_turn>\n<start_of_turn>model\nload_dataset broken in 2.21.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nspeedup<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhy does the program get stuck when I use load_dataset method, and it still gets stuck after loading for several hours? In fact, my json file is only 21m, and I can load it in one go using open('', 'r').\n\n### Steps to reproduce the bug\n\n1. pip install datasets==2.19.2\r\n2. from datasets import Dataset, DatasetDict, NamedSplit, Split, load_dataset\r\n3. data = load_dataset('json', data_files='train.json')\n\n### Expected behavior\n\nIt is able to load my json correctly\n\n### Environment info\n\ndatasets==2.19.2<end_of_turn>\n<start_of_turn>model\nload_dataset error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI tried creating an `imagefolder` dataset with a 714MB `metadata.jsonl` but got the error below.  This pull request fixes the problem by increasing the block size like the message suggests.\r\n```\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"data-for-upload\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/path/to/datasets/load.py\", line 2609, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n      ...\r\n  File \"/path/to/datasets/packaged_modules/folder_based_builder/folder_based_builder.py\", line 245, in _read_metadata\r\n    return paj.read_json(f)\r\n  File \"pyarrow/_json.pyx\", line 308, in pyarrow._json.read_json\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: straddling object straddles two block boundaries (try to increase block size?)\r\n```<end_of_turn>\n<start_of_turn>model\nSupport folder-based datasets with large metadata.jsonl<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\ncan't load c4 datasets\r\n\r\nWhen I replace the datasets package to 2.12.2 I get raise datasets.utils.info_utils.ExpectedMoreSplits: {'train'}\r\n\r\nHow can I fix this\uff1f\r\n\r\n### Steps to reproduce the bug\r\n\r\n1.from datasets import load_dataset\r\n2.dataset = load_dataset('allenai/c4', data_files={'validation': 'en/c4-validation.00003-of-00008.json.gz'}, split='validation')\r\n3.   raise FileNotFoundError(\r\nFileNotFoundError: Couldn't find a dataset script at local_path/c4_val/allenai/c4/c4.py or any data file in the same directory. Couldn't find 'allenai/c4' on the Hugging Face Hub either: FileNotFoundError: Unable to find 'hf://datasets/allenai/c4@1588ec454efa1a09f29cd18ddd04fe05fc8653a2/en/c4-validation.00003-of-00008.json.gz' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.geoparquet', '.gpq', '.arrow', '.txt', '.tar', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']\r\n### Expected behavior\r\n\r\n\r\nThe data was successfully imported\r\n\r\n### Environment info\r\n\r\npython version 3.9\r\ndatasets version 2.19.2<end_of_turn>\n<start_of_turn>model\nFileNotFoundError\uff1aerror  when loading C4 dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nso that it corresponds to the one that is being run in CI<end_of_turn>\n<start_of_turn>model\nUpdate `ruff` version in pre-commit config <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis is from https://github.com/huggingface/diffusers/issues/5573\r\n<end_of_turn>\n<start_of_turn>model\nLoad the local dataset folder from other place<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to changes made in https://github.com/iterative/dvc/pull/9475<end_of_turn>\n<start_of_turn>model\nFix JSON generation in benchmarks CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nThe idea (discussed in the Discord server with @lhoestq ) is to have a Pdf type like Image/Audio/Video. For example [Video](https://github.com/huggingface/datasets/blob/main/src/datasets/features/video.py) was recently added and contains how to decode a video file encoded in a dictionary like {\"path\": ..., \"bytes\": ...} as a VideoReader using decord. We want to do the same with pdf and get a [pypdfium2.PdfDocument](https://pypdfium2.readthedocs.io/en/stable/_modules/pypdfium2/_helpers/document.html#PdfDocument).\n\n### Motivation\n\nIn many cases PDFs contain very valuable information beyond text (e.g. images, figures). Support for PDFs would help create datasets where all the information is preserved.\n\n### Your contribution\n\nI can start the implementation of the Pdf type :)<end_of_turn>\n<start_of_turn>model\nIntroduce support for PDFs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR addresses an issue encountered when utilizing uint16 or uint32 datatypes with datasets, followed by attempting to convert these datasets into PyTorch-compatible formats. Currently, doing so results in a TypeError due to incompatible datatype conversion, as illustrated by the following example:\r\n```python\r\nfrom datasets import Dataset, Sequence, Value, Features\r\n\r\ndef gen():\r\n    for i in range(100):\r\n        yield {'seq': list(range(i, i + 20))}\r\n\r\nds = Dataset.from_generator(gen, features=Features({'seq': Sequence(feature=Value(dtype='uint16'), length=-1)}))\r\n\r\nds.set_format('torch')\r\n\r\nprint(ds[0])\r\n```\r\nThis code snippet triggers the following error due to the inability to convert numpy.uint16 arrays to a PyTorch-supported format:\r\n```\r\nTypeError: can't convert np.ndarray of type numpy.uint16. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool.\r\n```\r\n\r\nThis PR introduces an automatic mechanism to convert np.uint16 and np.uint32 datatypes to np.int64 for seamless compatibility with PyTorch formats, simplifying workflows and improving developer experience by eliminating the need for manual conversion handling.<end_of_turn>\n<start_of_turn>model\nAutomatic Conversion for uint16/uint32 to Compatible PyTorch Dtypes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nExample for glue sst2:\r\n\r\nDataset\r\n\r\n```\r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 1821\r\n    })\r\n    train: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 67349\r\n    })\r\n    validation: Dataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        num_rows: 872\r\n    })\r\n})\r\n```\r\n\r\nIterableDataset (new)\r\n\r\n```\r\nIterableDatasetDict({\r\n    test: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n    train: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n    validation: IterableDataset({\r\n        features: ['sentence', 'label', 'idx'],\r\n        n_shards: 1\r\n    })\r\n})\r\n```\r\n\r\nIterableDataset (before)\r\n\r\n```\r\n{'test': <datasets.iterable_dataset.IterableDataset object at 0x130d421f0>, 'train': <datasets.iterable_dataset.IterableDataset object at 0x136f3aaf0>, 'validation': <datasets.iterable_dataset.IterableDataset object at 0x136f4b100>}\r\n{'sentence': 'hide new secretions from the parental units ', 'label': 0, 'idx': 0}\r\n```<end_of_turn>\n<start_of_turn>model\nAdd IterableDataset `__repr__`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nSupport loading a dataset from a SQLite file\r\n\r\nhttps://huggingface.co/datasets/severo/test_iris_sqlite/tree/main\n\n### Motivation\n\nSQLite is a popular file format.\n\n### Your contribution\n\nSee discussion on slack: https://huggingface.slack.com/archives/C04L6P8KNQ5/p1702481859117909 (internal)\r\n\r\nIn particular: a SQLite file can contain multiple tables, which might be matched to multiple configs. Maybe the detail of splits and configs should be defined in the README YAML, or use the same format as for ZIP files: `Iris.sqlite::Iris`. \r\n\r\nSee dataset here: https://huggingface.co/datasets/severo/test_iris_sqlite\r\n\r\nNote: should we also support DuckDB files?<end_of_turn>\n<start_of_turn>model\nSupport SQLite files?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRequired for `load_dataset(<format>, data_files=[\"path/to/.hidden_file\"])` to work as expected<end_of_turn>\n<start_of_turn>model\nDon't skip hidden files in `dl_manager.iter_files` when they are given as input<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nNotImplementedError                       Traceback (most recent call last)\r\nCell In[14], line 11\r\n      9 for idx, example in enumerate(dataset['train']):\r\n     10     image_path = example['image']\r\n---> 11     mask_image = process_image(image_path)\r\n     12     mask_image.save(f\"mask_{idx}.png\")\r\n\r\nCell In[14], line 4, in process_image(image_path)\r\n      2 def process_image(image_path):\r\n      3     print(\"Processing image:\", image_path)\r\n----> 4     result = inferencer(image_path)['predictions']\r\n      5     mask = np.where(result == 12, 255, 0).astype('uint8')\r\n      6     return Image.fromarray(mask)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmseg/apis/mmseg_inferencer.py:183, in MMSegInferencer.__call__(self, inputs, return_datasamples, batch_size, show, wait_time, out_dir, img_out_dir, pred_out_dir, **kwargs)\r\n    180     pred_out_dir = ''\r\n    181     img_out_dir = ''\r\n--> 183 return super().__call__(\r\n    184     inputs=inputs,\r\n    185     return_datasamples=return_datasamples,\r\n    186     batch_size=batch_size,\r\n    187     show=show,\r\n    188     wait_time=wait_time,\r\n    189     img_out_dir=img_out_dir,\r\n    190     pred_out_dir=pred_out_dir,\r\n    191     **kwargs)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmengine/infer/infer.py:221, in BaseInferencer.__call__(self, inputs, return_datasamples, batch_size, **kwargs)\r\n    218 inputs = self.preprocess(\r\n    219     ori_inputs, batch_size=batch_size, **preprocess_kwargs)\r\n    220 preds = []\r\n--> 221 for data in (track(inputs, description='Inference')\r\n    222              if self.show_progress else inputs):\r\n    223     preds.extend(self.forward(data, **forward_kwargs))\r\n    224 visualization = self.visualize(\r\n    225     ori_inputs, preds,\r\n    226     **visualize_kwargs)  # type: ignore  # noqa: E501\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/rich/progress.py:168, in track(sequence, description, total, auto_refresh, console, transient, get_time, refresh_per_second, style, complete_style, finished_style, pulse_style, update_period, disable, show_speed)\r\n    157 progress = Progress(\r\n    158     *columns,\r\n    159     auto_refresh=auto_refresh,\r\n   (...)\r\n    164     disable=disable,\r\n    165 )\r\n    167 with progress:\r\n--> 168     yield from progress.track(\r\n    169         sequence, total=total, description=description, update_period=update_period\r\n    170     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/rich/progress.py:1210, in Progress.track(self, sequence, total, task_id, description, update_period)\r\n   1208 if self.live.auto_refresh:\r\n   1209     with _TrackThread(self, task_id, update_period) as track_thread:\r\n-> 1210         for value in sequence:\r\n   1211             yield value\r\n   1212             track_thread.completed += 1\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmengine/infer/infer.py:291, in BaseInferencer.preprocess(self, inputs, batch_size, **kwargs)\r\n    266 \"\"\"Process the inputs into a model-feedable format.\r\n    267 \r\n    268 Customize your preprocess by overriding this method. Preprocess should\r\n   (...)\r\n    287     Any: Data processed by the ``pipeline`` and ``collate_fn``.\r\n    288 \"\"\"\r\n    289 chunked_data = self._get_chunk_data(\r\n    290     map(self.pipeline, inputs), batch_size)\r\n--> 291 yield from map(self.collate_fn, chunked_data)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmengine/infer/infer.py:588, in BaseInferencer._get_chunk_data(self, inputs, chunk_size)\r\n    586 chunk_data = []\r\n    587 for _ in range(chunk_size):\r\n--> 588     processed_data = next(inputs_iter)\r\n    589     chunk_data.append(processed_data)\r\n    590 yield chunk_data\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmcv/transforms/base.py:12, in BaseTransform.__call__(self, results)\r\n      9 def __call__(self,\r\n     10              results: Dict) -> Optional[Union[Dict, Tuple[List, List]]]:\r\n---> 12     return self.transform(results)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmcv/transforms/wrappers.py:88, in Compose.transform(self, results)\r\n     79 \"\"\"Call function to apply transforms sequentially.\r\n     80 \r\n     81 Args:\r\n   (...)\r\n     85     dict or None: Transformed results.\r\n     86 \"\"\"\r\n     87 for t in self.transforms:\r\n---> 88     results = t(results)  # type: ignore\r\n     89     if results is None:\r\n     90         return None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmcv/transforms/base.py:12, in BaseTransform.__call__(self, results)\r\n      9 def __call__(self,\r\n     10              results: Dict) -> Optional[Union[Dict, Tuple[List, List]]]:\r\n---> 12     return self.transform(results)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/mmseg/datasets/transforms/loading.py:496, in InferencerLoader.transform(self, single_input)\r\n    494     inputs = single_input\r\n    495 else:\r\n--> 496     raise NotImplementedError\r\n    498 if 'img' in inputs:\r\n    499     return self.from_ndarray(inputs)\r\n\r\nNotImplementedError: \r\n````\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('Andyrasika/cat_kingdom')\r\ndataset\r\n\r\nfrom mmseg.apis import MMSegInferencer\r\n\r\ncheckpoint_name = 'segformer_mit-b5_8xb2-160k_ade20k-640x640'\r\n\r\ninferencer = MMSegInferencer(model=checkpoint_name)\r\n\r\n# Define a function to apply the code to each image in the dataset\r\ndef process_image(image_path):\r\n    print(\"Processing image:\", image_path)\r\n    result = inferencer(image_path)['predictions']\r\n    mask = np.where(result == 12, 255, 0).astype('uint8')\r\n    return Image.fromarray(mask)\r\n\r\n# Process and save masks for each image in the dataset\r\nfor idx, example in enumerate(dataset['train']):\r\n    image_path = example['image']\r\n    mask_image = process_image(image_path)\r\n    mask_image.save(f\"mask_{idx}.png\")\r\n```\n\n### Expected behavior\n\ncreate a separate column with masks in the dataset and further shows as a separate column in hub\n\n### Environment info\n\njupyter notebook RTX 3090<end_of_turn>\n<start_of_turn>model\nApply inference on all images in the dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nWould be great to add a code example of how to do multi-GPU processing with \ud83e\udd17 Datasets in the documentation. cc @stevhliu\r\n\r\nCurrently the docs has a small [section](https://huggingface.co/docs/datasets/v2.3.2/en/process#map) on this saying \"your big GPU call goes here\", however it didn't work for me out-of-the-box.\r\n\r\nLet's say you have a PyTorch model that can do translation, and you have multiple GPUs. In that case, you'd like to duplicate the model on each GPU, each processing (translating) a chunk of the data in parallel.\r\n\r\nHere's how I tried to do that:\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\r\nfrom multiprocess import set_start_method\r\nimport torch\r\nimport os\r\n\r\ndataset = load_dataset(\"mlfoundations/datacomp_small\")\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/nllb-200-distilled-600M\")\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\")\r\n\r\n# put model on each available GPU\r\n# also, should I do it like this or use nn.DataParallel?\r\nmodel.to(\"cuda:0\")\r\nmodel.to(\"cuda:1\")\r\n\r\nset_start_method(\"spawn\")\r\n\r\ndef translate_captions(batch, rank):\r\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank % torch.cuda.device_count())\r\n    \r\n    texts = batch[\"text\"]\r\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\").to(model.device)\r\n\r\n    translated_tokens = model.generate(\r\n        **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"], max_length=30\r\n    )\r\n    translated_texts = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)\r\n\r\n    batch[\"translated_text\"] = translated_texts\r\n    \r\n    return batch\r\n\r\nupdated_dataset = dataset.map(translate_captions, with_rank=True, num_proc=2, batched=True, batch_size=256)\r\n```\r\n\r\nI've personally tried running this script on a machine with 2 A100 GPUs.\r\n\r\n## Error 1\r\n\r\nRunning the code snippet above from the terminal (python script.py) resulted in the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/spawn.py\", line 125, in _main\r\n    prepare(preparation_data)\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/spawn.py\", line 236, in prepare\r\n    _fixup_main_from_path(data['init_main_from_path'])\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/spawn.py\", line 287, in _fixup_main_from_path\r\n    main_content = runpy.run_path(main_path,\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/runpy.py\", line 289, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/runpy.py\", line 96, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/niels/python_projects/datacomp/datasets_multi_gpu.py\", line 16, in <module>\r\n    set_start_method(\"spawn\")\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/context.py\", line 247, in set_start_method\r\n    raise RuntimeError('context has already been set')\r\nRuntimeError: context has already been set\r\n```\r\n\r\n## Error 2\r\nThen, based on [this Stackoverflow answer](https://stackoverflow.com/a/71616344/7762882), I put the `set_start_method(\"spawn\")` section in a try: catch block. This resulted in the following error:\r\n```\r\nFile \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/datasets/dataset_dict.py\", line 817, in <dictcomp>\r\n    k: dataset.map(\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2926, in map\r\n    with Pool(nb_of_missing_shards, initargs=initargs, initializer=initializer) as pool:\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/context.py\", line 119, in Pool\r\n    return Pool(processes, initializer, initargs, maxtasksperchild,\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/pool.py\", line 215, in __init__\r\n    self._repopulate_pool()\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/pool.py\", line 306, in _repopulate_pool\r\n    return self._repopulate_pool_static(self._ctx, self.Process,\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/pool.py\", line 329, in _repopulate_pool_static\r\n    w.start()\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/process.py\", line 121, in start\r\n    self._popen = self._Popen(self)\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/context.py\", line 288, in _Popen\r\n    return Popen(process_obj)\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/popen_spawn_posix.py\", line 32, in __init__\r\n    super().__init__(process_obj)\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/popen_fork.py\", line 19, in __init__\r\n    self._launch(process_obj)\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/popen_spawn_posix.py\", line 42, in _launch\r\n    prep_data = spawn.get_preparation_data(process_obj._name)\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/spawn.py\", line 154, in get_preparation_data\r\n    _check_not_importing_main()\r\n  File \"/home/niels/anaconda3/envs/datacomp/lib/python3.10/site-packages/multiprocess/spawn.py\", line 134, in _check_not_importing_main\r\n    raise RuntimeError('''\r\nRuntimeError: \r\n        An attempt has been made to start a new process before the\r\n        current process has finished its bootstrapping phase.\r\n\r\n        This probably means that you are not using fork to start your\r\n        child processes and you have forgotten to use the proper idiom\r\n        in the main module:\r\n\r\n            if __name__ == '__main__':\r\n                freeze_support()\r\n                ...\r\n\r\n        The \"freeze_support()\" line can be omitted if the program\r\n        is not going to be frozen to produce an executable.\r\n```\r\n\r\nSo then I put the last line under a `if __name__ == '__main__':` block. Then the code snippet seemed to work, but it seemed that it's only leveraging a single GPU (based on monitoring `nvidia-smi`):\r\n\r\n```\r\nMon Aug 28 12:19:24 2023       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 515.65.01    Driver Version: 515.65.01    CUDA Version: 11.7     |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|                               |                      |               MIG M. |\r\n|===============================+======================+======================|\r\n|   0  NVIDIA A100-SXM...  On   | 00000000:01:00.0 Off |                    0 |\r\n| N/A   55C    P0    76W / 275W |   8747MiB / 81920MiB |      0%      Default |\r\n|                               |                      |             Disabled |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\r\n| N/A   67C    P0   274W / 275W |  59835MiB / 81920MiB |    100%      Default |\r\n|                               |                      |             Disabled |\r\n```\r\n\r\nBoth GPUs should have equal GPU usage, but I've always noticed that the last GPU has way more usage than the other ones. This made me think that `os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(rank % torch.cuda.device_count())` might not work inside a Python script, especially if done after importing PyTorch?\r\n\r\n### Motivation\r\n\r\nWould be great to clarify how to do multi-GPU data processing.\r\n\r\n### Your contribution\r\n\r\nIf my code snippet can be fixed, I can contribute it to the docs :) <end_of_turn>\n<start_of_turn>model\nFeature request: add code example of multi-GPU processing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.19.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nGetting a 404 from the Hugging Face Datasets docs page:\r\nhttps://huggingface.co/docs/datasets/index\r\n\n\n### Steps to reproduce the bug\n\n1. Go to URL https://huggingface.co/docs/datasets/index\r\n2. Notice 404 not found\n\n### Expected behavior\n\nURL should either show docs or redirect to new location\n\n### Environment info\n\nhugginface.co<end_of_turn>\n<start_of_turn>model\n404 on Datasets Documentation Page<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHey,\n\nI noticed that the creation of datasets with `Dataset.from_generator` is broken if dicts and lists are nested in a certain way and a schema is being passed. See below for details.\n\nBest,\nTim\n\n### Steps to reproduce the bug\n\nRuning this code:\n\n```python\nfrom datasets import Dataset, Features, Sequence, Value\n\n\ndef generator():\n    yield {\n        \"a\": [{\"b\": {\"c\": 0}}],\n    }\n\n\nfeatures = Features(\n    {\n        \"a\": Sequence(\n            feature={\n                \"b\": {\n                    \"c\": Value(\"int32\"),\n                },\n            },\n            length=1,\n        )\n    }\n)\n\ndataset = Dataset.from_generator(generator, features=features)\n```\n\nleads to \n```\nGenerating train split: 1 examples [00:00, 540.85 examples/s]\nTraceback (most recent call last):\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1635, in _prepare_split_single\n    num_examples, num_bytes = writer.finalize()\n                              ^^^^^^^^^^^^^^^^^\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 657, in finalize\n    self.write_examples_on_file()\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 510, in write_examples_on_file\n    self.write_batch(batch_examples=batch_examples)\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 629, in write_batch\n    pa_table = pa.Table.from_arrays(arrays, schema=schema)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/table.pxi\", line 4851, in pyarrow.lib.Table.from_arrays\n  File \"pyarrow/table.pxi\", line 1608, in pyarrow.lib._sanitize_arrays\n  File \"pyarrow/array.pxi\", line 399, in pyarrow.lib.asarray\n  File \"pyarrow/array.pxi\", line 1004, in pyarrow.lib.Array.cast\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/pyarrow/compute.py\", line 405, in cast\n    return call_function(\"cast\", [arr], options, memory_pool)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/_compute.pyx\", line 598, in pyarrow._compute.call_function\n  File \"pyarrow/_compute.pyx\", line 393, in pyarrow._compute.Function.call\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowNotImplementedError: Unsupported cast from fixed_size_list<item: struct<c: int32>>[1] to struct using function cast_struct\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/user/test/tools/hf_test2.py\", line 23, in <module>\n    dataset = Dataset.from_generator(generator, features=features)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 1114, in from_generator\n    ).read()\n      ^^^^^^\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/io/generator.py\", line 49, in read\n    self.builder.download_and_prepare(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 925, in download_and_prepare\n    self._download_and_prepare(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1649, in _download_and_prepare\n    super()._download_and_prepare(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1001, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1487, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n  File \"/home/user/miniconda3/envs/test/lib/python3.11/site-packages/datasets/builder.py\", line 1644, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n\nProcess finished with exit code 1\n```\n\n### Expected behavior\n\nI expected this code not to lead to an error. \n\nI have done some digging and figured out that the problem seems to be the `get_nested_type` function in `features.py`, which, for whatever reason, flips Sequences and dicts whenever it encounters a dict inside of a sequence. This seems to be necessary, as disabling that flip leads to another error. However, by keeping that flip enabled for the highest level and disabling it for all subsequent levels, I was able to work around this problem. Specifically, by patching `get_nested_type` as follows, it works on the given example (emphasis on the `level` parameter I added):\n\n```python\ndef get_nested_type(schema: FeatureType, level=0) -> pa.DataType:\n    \"\"\"\n    get_nested_type() converts a datasets.FeatureType into a pyarrow.DataType, and acts as the inverse of\n        generate_from_arrow_type().\n\n    It performs double-duty as the implementation of Features.type and handles the conversion of\n        datasets.Feature->pa.struct\n    \"\"\"\n    # Nested structures: we allow dict, list/tuples, sequences\n    if isinstance(schema, Features):\n        return pa.struct(\n            {key: get_nested_type(schema[key], level = level + 1) for key in schema}\n        )  # Features is subclass of dict, and dict order is deterministic since Python 3.6\n    elif isinstance(schema, dict):\n        return pa.struct(\n            {key: get_nested_type(schema[key], level = level + 1) for key in schema}\n        )  # however don't sort on struct types since the order matters\n    elif isinstance(schema, (list, tuple)):\n        if len(schema) != 1:\n            raise ValueError(\"When defining list feature, you should just provide one example of the inner type\")\n        value_type = get_nested_type(schema[0], level = level + 1)\n        return pa.list_(value_type)\n    elif isinstance(schema, LargeList):\n        value_type = get_nested_type(schema.feature, level = level + 1)\n        return pa.large_list(value_type)\n    elif isinstance(schema, Sequence):\n        value_type = get_nested_type(schema.feature, level = level + 1)\n        # We allow to reverse list of dict => dict of list for compatibility with tfds\n        if isinstance(schema.feature, dict) and level == 1:\n            data_type = pa.struct({f.name: pa.list_(f.type, schema.length) for f in value_type})\n        else:\n            data_type = pa.list_(value_type, schema.length)\n        return data_type\n\n    # Other objects are callable which returns their data type (ClassLabel, Array2D, Translation, Arrow datatype creation methods)\n    return schema()\n```\n\nI have honestly no idea what I am doing here, so this might produce other issues for different inputs.\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-6.8.0-59-generic-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n\nAlso tested it with 3.5.0, same result.<end_of_turn>\n<start_of_turn>model\nDataset creation is broken if nesting a dict inside a dict inside a list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPlease update error messages to include relevant information for debugging when loading datasets with `load_dataset()` that may have a few corrupted files. \n\nWhenever downloading a full dataset, some files might be corrupted (either at the source or from downloading corruption). \n\nHowever the errors often only let me know it was a tar file if `tarfile.ReadError` appears on the traceback, and I imagine similarly for other file types. \n\nThis makes it really hard to debug which file is corrupted, and when dealing with very large datasets, it shouldn't be necessary to force download everything again.<end_of_turn>\n<start_of_turn>model\nAdd filename in error message when ReadError or similar occur<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe actual behavior of the interface `Dataset.with_format` is inconsistent with the documentation.\r\nhttps://huggingface.co/docs/datasets/use_with_pytorch#n-dimensional-arrays\r\nhttps://huggingface.co/docs/datasets/v2.19.0/en/use_with_tensorflow#n-dimensional-arrays\r\n\r\n> If your dataset consists of N-dimensional arrays, you will see that by default they are considered as nested lists.\r\n> In particular, a PyTorch formatted dataset outputs nested lists instead of a single tensor.\r\n> A TensorFlow formatted dataset outputs a RaggedTensor instead of a single tensor.\r\n\r\nBut I get a single tensor by default, which is  inconsistent with the description.\r\n\r\nActually the current behavior seems more reasonable to me. Therefore, the document needs to be modified.\n\n### Steps to reproduce the bug\n\n```python\r\n>>> from datasets import Dataset\r\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\r\n>>> ds = Dataset.from_dict({\"data\": data})\r\n>>> ds = ds.with_format(\"torch\")\r\n>>> ds[0]\r\n{'data': tensor([[1, 2],\r\n        [3, 4]])}\r\n>>> ds = ds.with_format(\"tf\")\r\n>>> ds[0]\r\n{'data': <tf.Tensor: shape=(2, 2), dtype=int64, numpy=\r\narray([[1, 2],\r\n       [3, 4]])>}\r\n```\n\n### Expected behavior\n\n```python\r\n>>> from datasets import Dataset\r\n>>> data = [[[1, 2],[3, 4]],[[5, 6],[7, 8]]]\r\n>>> ds = Dataset.from_dict({\"data\": data})\r\n>>> ds = ds.with_format(\"torch\")\r\n>>> ds[0]\r\n{'data': [tensor([1, 2]), tensor([3, 4])]}\r\n>>> ds = ds.with_format(\"tf\")\r\n>>> ds[0]\r\n{'data': <tf.RaggedTensor [[1, 2], [3, 4]]>}\r\n```\n\n### Environment info\n\ndatasets==2.19.1\r\ntorch==2.1.0\r\ntensorflow==2.13.1<end_of_turn>\n<start_of_turn>model\n`Dataset.with_format` behaves inconsistently with documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n**Describe the bug**\r\n\r\nGetting a `412 Client Error: Precondition Failed` when trying to write a dataset to the HF hub.\r\n\r\n```\r\nhuggingface_hub.utils._errors.HfHubHTTPError: 412 Client Error: Precondition Failed for url: https://huggingface.co/api/datasets/GLorr/test-dask/commit/main (Request ID: Root=1-657ae26f-3bd92bf861bb254b2cc0826c;50a09ab7-9347-406a-ba49-69f98abee9cc)\r\n\r\nA commit has happened since. Please refresh and try again.\r\n```\r\n\r\n**Steps to reproduce the bug**\r\nThis is a minimal reproducer:\r\n```\r\nimport dask.dataframe as dd\r\nimport pandas as pd\r\nimport random\r\nimport os\r\n\r\nimport huggingface_hub\r\n\r\nimport datasets\r\n\r\nhuggingface_hub.login(token=os.getenv(\"HF_TOKEN\"))\r\n\r\ndata = {\"number\": [random.randint(0,10) for _ in range(1000)]}\r\ndf = pd.DataFrame.from_dict(data)\r\ndataframe = dd.from_pandas(df, npartitions=1)\r\ndataframe = dataframe.repartition(npartitions=3)\r\n\r\nschema = datasets.Features({\"number\": datasets.Value(\"int64\")}).arrow_schema\r\n\r\nrepo_id = \"GLorr/test-dask\"\r\nrepo_path = f\"hf://datasets/{repo_id}\"\r\nhuggingface_hub.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\r\ndd.to_parquet(dataframe, path=f\"{repo_path}/data\", schema=schema)\r\n```\r\n\r\n**Expected behavior**\r\nWould expect to write to the hub without any problem.\r\n\r\n**Environment info**\r\n```\r\ndatasets==2.15.0\r\nhuggingface-hub==0.19.4\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nError when writing a dataset to HF Hub: A commit has happened since. Please refresh and try again.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI used:\r\n```\r\ndataset = load_dataset(\r\n    \"oscar-corpus/OSCAR-2201\",\r\n    token=True,\r\n    language=\"fr\",\r\n    streaming=True,\r\n    split=\"train\"\r\n)\r\n```\r\nUnfortunately, the server had a problem during the training process. I saved the step my training stopped at.\r\nBut how can I resume download from step 1_000_\u00b4000 without re-streaming all the first 1 million docs of the dataset?\r\n\r\n`download_config=DownloadConfig(resume_download=True)` seems to not work with streaming=True.\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset, DownloadConfig\r\ndataset = load_dataset(\r\n    \"oscar-corpus/OSCAR-2201\",\r\n    token=True,\r\n    language=\"fr\",\r\n    streaming=True,  # optional\r\n    split=\"train\",\r\n    download_config=DownloadConfig(resume_download=True)\r\n)\r\n# interupt the run and try to relaunch it => this restart from scratch\r\n```\n\n### Expected behavior\n\nI would expect a parameter to start streaming from a given index in the dataset.\n\n### Environment info\n\n- `datasets` version: 2.14.0\r\n- Platform: Linux-5.19.0-45-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.0<end_of_turn>\n<start_of_turn>model\nresume_download with streaming=True<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf you `map` a dataset and save it to a specific `cache_file_name` with a specific `num_proc`, and then call map again with that same existing `cache_file_name` but a different `num_proc`, the dataset will be re-mapped.\n\n### Steps to reproduce the bug\n\n1. Download a dataset\n```python\nimport datasets\n\ndataset = datasets.load_dataset(\"ylecun/mnist\")\n```\n\n```\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60000/60000 [00:00<00:00, 116429.85 examples/s]\nGenerating test split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10000/10000 [00:00<00:00, 103310.27 examples/s]\n```\n\n2. `map` and cache it with a specific `num_proc`\n\n```python\ncache_file_name=\"./cache/train.map\"\ndataset[\"train\"].map(lambda x: x, cache_file_name=cache_file_name, num_proc=2)\n```\n\n```\nMap (num_proc=2): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60000/60000 [00:01<00:00, 53764.03 examples/s]\n```\n\n3. `map` it with a different `num_proc` and the same `cache_file_name` as before\n\n```python\ndataset[\"train\"].map(lambda x: x, cache_file_name=cache_file_name, num_proc=3)\n```\n\n```\nMap (num_proc=3): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 60000/60000 [00:00<00:00, 65377.12 examples/s]\n```\n\n### Expected behavior\n\nIf I specify an existing `cache_file_name`, I don't expect using a different `num_proc` than the one that was used to generate it to cause the dataset to have be be re-mapped.\n\n### Environment info\n\n```console\n$ datasets-cli env\n\n- `datasets` version: 3.3.2\n- Platform: Linux-5.15.0-131-generic-x86_64-with-glibc2.35\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n```<end_of_turn>\n<start_of_turn>model\n`Dataset.map` ignores existing caches and remaps when ran with different `num_proc`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsee https://discuss.huggingface.co/t/update-datasets-getting-started-to-new-git-security/65893<end_of_turn>\n<start_of_turn>model\ndocs: add reference Git over SSH<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMake `push_to_hub` return `CommitInfo`.\r\n\r\nThis is useful, for example, if we pass `create_pr=True` and we want to know the created PR ID.\r\n\r\nCC: @severo for the use case in https://huggingface.co/datasets/jmhessel/newyorker_caption_contest/discussions/4<end_of_turn>\n<start_of_turn>model\nMake push_to_hub return CommitInfo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAllow for anonymized viewing of datasets. For instance, something similar to [Anonymous GitHub](https://anonymous.4open.science/).\n\n### Motivation\n\nWe generally publish our data through Hugging Face. This has worked out very well as it's both our repository and archive (thanks to the DOI feature!). However, we have an increasing challenge when it comes to sharing our datasets for paper (both conference and journal) submissions. Due to the need to share data anonymously, we can't use the Hugging Face URLs, but datasets tend to be too large for inclusion as a zip. Being able to have an anonymous link would be great since we can't be double-publishing the data.\n\n### Your contribution\n\nSorry, I don't have a contribution to make to the implementation of this. Perhaps it would be possible to work off the [Anonymous GitHub](https://github.com/tdurieux/anonymous_github) code to generate something analogous with pointers to the data still on Hugging Face's servers (instead of the duplication of data required for the GitHub version)?<end_of_turn>\n<start_of_turn>model\nOption for Anonymous Dataset link<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nhttps://github.com/bigcode-project/opt-out-v2 - opt out is not consent. kindly quit this ridiculous nonsense.\r\n\r\n### Motivation\r\n\r\n[EDITED: insults not tolerated]\r\n\r\n### Your contribution\r\n\r\n[EDITED: insults not tolerated]<end_of_turn>\n<start_of_turn>model\nScraping the whole of github including private repos is bad; kindly stop<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI used map to store raw audio waveforms of variable lengths in a column of a dataset the `map` call fails with ArrowInvalid: Value X too large to fit in C integer type.\n\n```\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"...lib/python3.12/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3526, in _map_single\n    writer.write_batch(batch)\n  File \"...lib/python3.12/site-packages/datasets/arrow_writer.py\", line 605, in write_batch\n    arrays.append(pa.array(typed_sequence))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/array.pxi\", line 252, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 114, in pyarrow.lib._handle_arrow_array_protocol\n  File \"...lib/python3.12/site-packages/datasets/arrow_writer.py\", line 225, in __arrow_array__\n    out = list_of_np_array_to_pyarrow_listarray(data)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/features/features.py\", line 1538, in list_of_np_array_to_pyarrow_listarray\n    return list_of_pa_arrays_to_pyarrow_listarray(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"...lib/python3.12/site-packages/datasets/features/features.py\", line 1530, in list_of_pa_arrays_to_pyarrow_listarray\n    offsets = pa.array(offsets, type=pa.int32())\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/array.pxi\", line 362, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 87, in pyarrow.lib._ndarray_to_array\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowInvalid: Value 2148479376 too large to fit in C integer type\n```\n\n\n\n### Steps to reproduce the bug\n\nCalling map on a dataset that returns a column with long 1d numpy arrays of variable length.\n\nExample:\n```python\n# %%\nimport logging\nimport datasets\nimport pandas as pd\nimport numpy as np\n# %%\n\n\ndef process_batch(batch, rank):\n    res = []\n    for _ in batch[\"id\"]:\n        res.append(np.zeros((2**30)).astype(np.uint16))\n    \n    return {\"audio\": res}\n\n\nif __name__ == \"__main__\":\n    df = pd.DataFrame(\n        {\n            \"id\": list(range(400)),\n        }\n    )\n\n    ds = datasets.Dataset.from_pandas(df)\n    try:\n        from multiprocess import set_start_method\n\n        set_start_method(\"spawn\")\n    except RuntimeError:\n        print(\"Spawn method already set, continuing...\")\n    mapped_ds = ds.map(\n        process_batch,\n        batched=True,\n        batch_size=2,\n        with_rank=True,\n        num_proc=2,\n        cache_file_name=\"path_to_cache/tmp.arrow\",\n        writer_batch_size=200,\n        remove_columns=ds.column_names,\n        # disable_nullable=True,\n    )\n\n```\n\n### Expected behavior\n\nI think the offsets should be pa.int64() if needed  and not forced to be `pa.int32()` \nin https://github.com/huggingface/datasets/blob/3e13d30823f8ec498d56adbc18c6880a5463b313/src/datasets/features/features.py#L1535\n\n### Environment info\n\n- `datasets` version: 3.3.1\n- Platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.29.0\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nBuilding a dataset with large variable size arrays results in error ArrowInvalid: Value X too large to fit in C integer type<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #5906<end_of_turn>\n<start_of_turn>model\nUnpin responses<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs reported by @meg-huggingface, the order of the JSON object keys is not preserved while loading a dataset from a JSON file with a list of objects.\r\n\r\nFor example, when loading a JSON files with a list of objects, each with the following ordered keys:\r\n- [ID, Language, Topic], \r\n\r\nthe resulting dataset may have columns:\r\n- [ID, Topic, Language], or\r\n- [Topic, Language, ID], or\r\n- [Topic, ID, Language],...\r\n\r\nThis issue is caused by the use of a Python set (which does not preserve the order):\r\nhttps://github.com/huggingface/datasets/blob/60d21efbc01e15d0b596ac1072750cbecd91548a/src/datasets/packaged_modules/json/json.py#L168\r\nintroduced in\r\n- #5772<end_of_turn>\n<start_of_turn>model\nColumn order is nondeterministic when loading from JSON<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen a dataset is updated on the hub, using `load_dataset` will load the locally cached dataset instead of re-downloading the updated dataset\n\n### Steps to reproduce the bug\n\nHere is a minimal example script to\r\n1. create an initial dataset and upload\r\n2. download it so it is stored in cache\r\n3. change the dataset and re-upload\r\n4. redownload \r\n\r\n```python\r\nimport time                                                                                                                                                    \r\n                                                                                                                                                               \r\nfrom datasets import Dataset, DatasetDict, DownloadMode, load_dataset                                                                                 \r\n       \r\nusername = \"YOUR_USERNAME_HERE\"                                                                                                                                          \r\n                                                                                                                                                               \r\ninitial = Dataset.from_dict({\"foo\": [1, 2, 3]})                                                                                                                \r\nprint(f\"Intial {initial['foo']}\")                                                                                                                              \r\ninitial_ds = DatasetDict({\"train\": initial})                                                                                                                   \r\ninitial_ds.push_to_hub(\"test\")                                                                                                                                 \r\n                                                                                                                                                               \r\ntime.sleep(1)                                                                                                                                                  \r\n                                                                                                                                                               \r\ndownload = load_dataset(f\"{username}/test\", split=\"train\")                                                                                                     \r\nchanged = download.map(lambda x: {\"foo\": x[\"foo\"] + 1})                                                                                                        \r\nprint(f\"Changed {changed['foo']}\")                                                                                                                             \r\nchanged.push_to_hub(\"test\")                                                                                                                                    \r\n                                                                                                                                                               \r\ntime.sleep(1)                                                                                                                                                  \r\n                                                                                                                                                               \r\ndownload_again = load_dataset(f\"{username}/test\", split=\"train\")                                                                                               \r\nprint(f\"Download Changed {download_again['foo']}\")                                                                                                             \r\n# >>> gives the out-dated [1,2,3] when it should be changed [2,3,4]                                                                                                                                                                               \r\n```\r\n\r\nThe redownloaded dataset should be the changed dataset but it is actually the cached, initial dataset. Force-redownloading gives the correct dataset\r\n\r\n```python\r\ndownload_again_force = load_dataset(f\"{username}/test\", split=\"train\", download_mode=DownloadMode.FORCE_REDOWNLOAD)                                            \r\nprint(f\"Force Download Changed {download_again_force['foo']}\")                                                                                                 \r\n# >>> [2,3,4]                                                                                                                                                                              \r\n```\r\n\r\n \n\n### Expected behavior\n\nI assumed there should be some sort of hashing that should check for changes in the dataset and re-download if the hashes don't match\n\n### Environment info\n\n- `datasets` version: 2.15.0                                                   \u2502\r\n- Platform: Linux-5.15.0-1028-nvidia-x86_64-with-glibc2.17                     \u2502\r\n- Python version: 3.8.17                                                       \u2502\r\n- `huggingface_hub` version: 0.19.4                                            \u2502\r\n- PyArrow version: 13.0.0                                                      \u2502\r\n- Pandas version: 2.0.3                                                        \u2502\r\n- `fsspec` version: 2023.6.0    <end_of_turn>\n<start_of_turn>model\n`load_dataset` uses out-of-date cache instead of re-downloading a changed dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCode:\r\n`!pipuninstall -y pyarrow\r\n!pip install --no-cache-dir pyarrow\r\n\r\n!pip uninstall -y pyarrow\r\n!pip install pyarrow --no-cache-dir\r\n!pip install --upgrade datasets transformers pyarrow\r\n\r\n!pip install pyarrow.parquet\r\n! pip install pyarrow-core libparquet\r\n\r\n!pip install pyarrow --no-cache-dir\r\n!pip install pyarrow\r\n!pip install transformers\r\n!pip install --upgrade datasets\r\n!pip install datasets\r\n! pip install pyarrow\r\n!  pip install pyarrow.lib\r\n! pip install pyarrow.parquet\r\n!pip install transformers\r\n\r\nimport pyarrow as pa\r\nprint(pa.__version__)\r\n\r\n\r\nfrom datasets import load_dataset\r\nimport pyarrow.parquet as pq\r\nimport pyarrow.lib as lib\r\nimport pandas as pd\r\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\n\r\n! pip install pyarrow-core libparquet\r\n# Load the dataset for content moderation\r\ndataset = load_dataset(\"PolyAI/banking77\")  # Example dataset for customer support\r\n\r\n# Initialize the tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\r\n\r\n# Tokenize the dataset\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True)\r\n\r\n# Apply tokenization to the entire dataset\r\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\r\n\r\n# Check the first few tokenized samples\r\nprint(tokenized_datasets['train'][0])\r\nfrom transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\r\n\r\n# Load the model\r\nmodel = AutoModelForSequenceClassification.from_pretrained(\"facebook/opt-350m\", num_labels=77)\r\n\r\n# Define training arguments\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./results\",\r\n    per_device_train_batch_size=16,\r\n    per_device_eval_batch_size=16,\r\n    num_train_epochs=3,\r\n    eval_strategy=\"epoch\",  #\r\n    save_strategy=\"epoch\",\r\n    logging_dir=\"./logs\",\r\n    learning_rate=2e-5,\r\n)\r\n\r\n# Initialize the Trainer\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=tokenized_datasets[\"train\"],\r\n    eval_dataset=tokenized_datasets[\"test\"],\r\n)\r\n\r\n# Train the model\r\ntrainer.train()\r\n\r\n# Evaluate the model\r\ntrainer.evaluate()\r\n\r\n` \r\n\r\n\r\nAttributeError                            Traceback (most recent call last)\r\n[<ipython-input-23-60bed3143a93>](https://localhost:8080/#) in <cell line: 22>()\r\n     20 \r\n     21 \r\n---> 22 from datasets import load_dataset\r\n     23 import pyarrow.parquet as pq\r\n     24 import pyarrow.lib as lib\r\n\r\n5 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/__init__.py](https://localhost:8080/#) in <module>\r\n     15 __version__ = \"2.21.0\"\r\n     16 \r\n---> 17 from .arrow_dataset import Dataset\r\n     18 from .arrow_reader import ReadInstruction\r\n     19 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in <module>\r\n     74 \r\n     75 from . import config\r\n---> 76 from .arrow_reader import ArrowReader\r\n     77 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n     78 from .data_files import sanitize_patterns\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py](https://localhost:8080/#) in <module>\r\n     27 \r\n     28 import pyarrow as pa\r\n---> 29 import pyarrow.parquet as pq\r\n     30 from tqdm.contrib.concurrent import thread_map\r\n     31 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/__init__.py](https://localhost:8080/#) in <module>\r\n     18 # flake8: noqa\r\n     19 \r\n---> 20 from .core import *\r\n\r\n[/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py](https://localhost:8080/#) in <module>\r\n     31 \r\n     32 try:\r\n---> 33     import pyarrow._parquet as _parquet\r\n     34 except ImportError as exc:\r\n     35     raise ImportError(\r\n\r\n/usr/local/lib/python3.10/dist-packages/pyarrow/_parquet.pyx in init pyarrow._parquet()\r\n\r\nAttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'\r\n\n\n### Steps to reproduce the bug\n\nhttps://colab.research.google.com/drive/1HNbsg3tHxUJOHVtYIaRnNGY4T2PnLn4a?usp=sharing\n\n### Expected behavior\n\nLooks like there is an issue with datasets  and pyarrow\n\n### Environment info\n\ngoogle colab\r\npython\r\nhuggingface\r\nFound existing installation: pyarrow 17.0.0\r\nUninstalling pyarrow-17.0.0:\r\n  Successfully uninstalled pyarrow-17.0.0\r\nCollecting pyarrow\r\n  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\r\nRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.26.4)\r\nDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\r\n   \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 39.9/39.9 MB 188.9 MB/s eta 0:00:00\r\nInstalling collected packages: pyarrow\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\r\nibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\r\nSuccessfully installed pyarrow-17.0.0\r\nWARNING: The following packages were previously imported in this runtime:\r\n  [pyarrow]\r\nYou must restart the runtime in order to use newly installed versions.<end_of_turn>\n<start_of_turn>model\nmodule 'pyarrow.lib' has no attribute 'ListViewType'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 3.4.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading image data in a huggingface dataset results in very slow read speeds, approximately 1000 times longer than reading the same data from a pytorch dataset. This applies even when the dataset is loaded into RAM using a `keep_in_memory=True` flag.\n\nThe following script reproduces the result with random data, but it applies equally to datasets that are loaded from the hub.\n\n### Steps to reproduce the bug\n\nThe following script should reproduce the behavior\n```\nimport torch\nimport time\nfrom datasets import Dataset\n\nimages = torch.randint(0, 255, (1000, 3, 224, 224), dtype=torch.uint8)\nlabels = torch.randint(0, 200, (1000,), dtype=torch.uint8)\n\npt_dataset = torch.utils.data.TensorDataset(images, labels)\n\nhf_dataset = Dataset.from_dict({'image': images, 'label':labels})\nhf_dataset.set_format('torch', dtype=torch.uint8)\nhf_in_memory = hf_dataset.map(lambda x: x, keep_in_memory=True)\n\n# measure access speeds\ndef time_access(dataset, img_col):\n    start_time = time.time()\n    for i in range(1000):\n        _ = dataset[i][img_col].shape\n    end_time = time.time()\n    return end_time - start_time\n\n\nprint(f\"In-memory Tensor access: {time_access(pt_dataset, 0):.4f} seconds\")\nprint(f\"HF Dataset access: {time_access(hf_dataset, 'image'):.4f} seconds\")\nprint(f\"In-memory HF Dataset access: {time_access(hf_in_memory, 'image'):.4f} seconds\")\n```\n\n\n\n### Expected behavior\n\nFor me, the above script produces\n```\nIn-memory Tensor access: 0.0025 seconds\nHF Dataset access: 2.9317 seconds\nIn-memory HF Dataset access: 2.8082 seconds\n```\n\nI think that this difference is larger than expected.\n\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: macOS-14.7.7-arm64-arm-64bit\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.34.3\n- PyArrow version: 18.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\ndatasets massively slows data reads, even in memory<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nwe should not override if the features exist already\r\n\r\nhttps://github.com/huggingface/datasets/blob/d26abadce0b884db32382b92422d8a6aa997d40a/src/datasets/packaged_modules/webdataset/webdataset.py#L78-L85<end_of_turn>\n<start_of_turn>model\nWebDataset ignores features defined in YAML or passed to load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nE.g., to have info about a dataset's number of examples for more informative TQDM bars.<end_of_turn>\n<start_of_turn>model\nExpand no-code dataset info with datasets-server info<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI load a Huggingface Dataset and do `train_test_split`. I'm expecting the underlying table for the dataset to also be split, but it's not.\n\n### Steps to reproduce the bug\n\n![image](https://github.com/huggingface/datasets/assets/8068268/83e5768f-8b4c-422a-945c-832a7585afff)\r\n\n\n### Expected behavior\n\nThe expected behavior is when `len(hf_dataset[\"train\"].data)` should match the length of the train split, and not be the entire unsplit dataset.\n\n### Environment info\n\ndatasets 2.10.1\r\npython 3.10.11<end_of_turn>\n<start_of_turn>model\nLength of table does not accurately reflect the split<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`hfh` and `transformers` have dropped Python 3.7 support, so we should do the same :).\r\n\r\n(Based on the stats, it seems less than 10% of the users use `datasets` with Python 3.7)<end_of_turn>\n<start_of_turn>model\nDrop Python 3.7 support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix this code provided by @clefourrier \r\n\r\n```python\r\nimport datasets\r\nimport os\r\n\r\ntoken = os.getenv(\"TOKEN\")\r\n\r\nresults = datasets.load_dataset(\"gaia-benchmark/results_public\", \"2023\", token=token, download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)\r\nresults[\"test\"] = datasets.Dataset.from_list([row for row in results[\"test\"] if row[\"model\"] != \"StateFlow\"])\r\nresults[\"test\"].push_to_hub(\"gaia-benchmark/results_public\", \"2023\", token=token, split=\"test\")\r\n```\r\n\r\n```\r\nValueError                                Traceback (most recent call last)\r\nCell In[43], line 1\r\n----> 1 results[\"test\"].push_to_hub(\"gaia-benchmark/results_public\", \"2023\", token=token, split=\"test\")\r\n\r\nFile ~/miniconda3/envs/default310/lib/python3.10/site-packages/datasets/arrow_dataset.py:5498, in Dataset.push_to_hub(self, repo_id, config_name, split, private, token, branch, max_shard_size, num_shards, embed_external_files)\r\n   5496         repo_info.dataset_size = (repo_info.dataset_size or 0) + dataset_nbytes\r\n   5497         repo_info.size_in_bytes = repo_info.download_size + repo_info.dataset_size\r\n-> 5498         repo_info.splits[split] = SplitInfo(\r\n   5499             split, num_bytes=dataset_nbytes, num_examples=len(self), dataset_name=dataset_name\r\n   5500         )\r\n   5501         info_to_dump = repo_info\r\n   5502 # create the metadata configs if it was uploaded with push_to_hub before metadata configs existed\r\n\r\nFile ~/miniconda3/envs/default310/lib/python3.10/site-packages/datasets/splits.py:541, in SplitDict.__setitem__(self, key, value)\r\n    539     raise ValueError(f\"Cannot add elem. (key mismatch: '{key}' != '{value.name}')\")\r\n    540 if key in self:\r\n--> 541     raise ValueError(f\"Split {key} already present\")\r\n    542 super().__setitem__(key, value)\r\n\r\nValueError: Split test already present\r\n```<end_of_turn>\n<start_of_turn>model\nAllow SplitDict setitem to replace existing SplitInfo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis code was failing\r\n\r\n```python\r\n\r\nIn [1]: from datasets import load_dataset\r\n\r\nIn [2]: \r\n   ...:         ds = load_dataset(\"xnli\", \"all_languages\", split=\"test\", streaming=True)\r\n   ...: \r\n   ...:         sample_data = next(iter(ds))[\"premise\"]  # pick up one data\r\n   ...:         input_text = list(sample_data.values())\r\n```\r\n\r\n```\r\nFile ~/hf/datasets/src/datasets/features/translation.py:104, in TranslationVariableLanguages.encode_example(self, translation_dict)\r\n    102     return translation_dict\r\n    103 elif self.languages and set(translation_dict) - lang_set:\r\n--> 104     raise ValueError(\r\n    105         f'Some languages in example ({\", \".join(sorted(set(translation_dict) - lang_set))}) are not in valid set ({\", \".join(lang_set)}).'\r\n    106     )\r\n    108 # Convert dictionary into tuples, splitting out cases where there are\r\n    109 # multiple translations for a single language.\r\n    110 translation_tuples = []\r\n\r\nValueError: Some languages in example (language, translation) are not in valid set (ur, fr, hi, sw, vi, el, de, th, en, tr, zh, ar, bg, ru, es).\r\n```\r\n\r\nbecause in streaming mode we expect features encode methods to be no-ops if the example is already encoded.\r\n\r\nI fixed `TranslationVariableLanguages` to account for that<end_of_turn>\n<start_of_turn>model\nFix streaming xnli<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen using `Dataset.from_parquet(path_or_paths, columns=[...])` and a subset of columns, loading fails with a variant of the following\r\n\r\n```\r\nValueError: Couldn't cast\r\na: int64\r\n-- schema metadata --\r\npandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 273\r\nto\r\n{'a': Value(dtype='int64', id=None), 'b': Value(dtype='int64', id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n```\r\n\r\nLooks to be triggered by https://github.com/huggingface/datasets/blob/c02a44715c036b5261686669727394b1308a3a4b/src/datasets/table.py#L2285-L2286\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nimport pandas as pd\r\nfrom datasets import Dataset\r\n\r\n\r\npd.DataFrame([{\"a\": 1, \"b\": 2}]).to_parquet(\"test.pq\")\r\nDataset.from_parquet(\"test.pq\", columns=[\"a\"])\r\n```\r\n\r\n### Expected behavior\r\n\r\nA subset of columns should be loaded without error\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.10.0-23-cloud-amd64-x86_64-with-glibc2.2.5\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nDataset.from_parquet cannot load subset of columns<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTest get_dataset_config_info with non-existing/gated/private dataset.\r\n\r\nRelated to:\r\n- #7109\r\n\r\nSee also:\r\n- https://github.com/huggingface/dataset-viewer/pull/3037: https://github.com/huggingface/dataset-viewer/pull/3037/commits/bb1a7e00c53c242088597cab6572e4fd57797ecb<end_of_turn>\n<start_of_turn>model\nTest get_dataset_config_info with non-existing/gated/private dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWe do not have the online docs for the latest minor release 2.16 (2.16.0 nor 2.16.1).\r\n\r\nIn the online docs, the latest version appearing is 2.15.0: https://huggingface.co/docs/datasets/index\r\n\r\n![Screenshot from 2024-01-09 08-43-08](https://github.com/huggingface/datasets/assets/8515462/83613222-867f-41f4-8833-7a4a76582f44)\r\n<end_of_turn>\n<start_of_turn>model\nNo online docs for 2.16 release<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`datasets==4.0.0`\n```\ncolumns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions']\ntrain_ds.set_format(type='tf', columns=columns_to_return)\n```\n`train_ds`:\n```\ntrain_ds type: <class 'datasets.arrow_dataset.Dataset'>, shape: (1000, 9)\ncolumns: ['question', 'sentences', 'answer', 'str_idx', 'end_idx', 'input_ids', 'attention_mask', 'start_positions', 'end_positions']\nfeatures:{'question': Value('string'), 'sentences': Value('string'), 'answer': Value('string'), 'str_idx': Value('int64'), 'end_idx': Value('int64'), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'start_positions': Value('int64'), 'end_positions': Value('int64')}\n```\n`train_ds_tensor = train_ds['start_positions'].to_tensor(shape=(-1,1))` hits the following error:\n```\nAttributeError: 'Column' object has no attribute 'to_tensor'\n```\n`tf.reshape(train_ds['start_positions'], shape=[-1,1])` hits the following error:\n```\nTypeError: Scalar tensor has no `len()`\n```<end_of_turn>\n<start_of_turn>model\nError processing scalar columns using tensorflow.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds a test for the new `as_iterable_dataset()` method introduced in PR #7628.\r\n\r\nThe test:\r\n- Loads a builder using `load_dataset_builder(\"c4\", \"en\")`\r\n- Runs `download_and_prepare()`\r\n- Streams examples using `builder.as_iterable_dataset(split=\"train[:100]\")`\r\n- Verifies streamed examples contain the \"text\" field\r\n\r\nThis ensures that the builder correctly streams data from cached Arrow files.\r\n<end_of_turn>\n<start_of_turn>model\nAdd test for `as_iterable_dataset()` method in DatasetBuilder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPyYAML, the YAML framework used in this library, allows the use of LibYAML to accelerate the methods `load` and `dump`. To use it, a user would need to first install a PyYAML version that uses LibYAML (not available in PyPI; needs to be manually installed). Then, to actually use them, PyYAML suggests importing the LibYAML version of the `Loader` and `Dumper` and falling back to the default ones. This PR implements this change. See [PyYAML docs](https://pyyaml.org/wiki/PyYAMLDocumentation) for more info.\r\n\r\nThis change was motivated after trying to use any of [the SugarCREPE datasets in the Hub](https://huggingface.co/datasets?search=sugarcrepe) provided by [the org HuggingFaceM4](https://huggingface.co/datasets/HuggingFaceM4). Such datasets save a lot of information (~1MB) in the YAML metadata from the `README.md` file and I noticed this slowed down the data loading process. BTW, I also noticed cache files for it is also slow because it tries to hash an instance of `DatasetInfo`, which in turn has all this metadata.\r\n\r\nAlso, I changed two list comprehensions into generator expressions to avoid allocating extra memory unnecessarily.\r\n\r\nAnd BTW, there's [an issue in PyYAML suggesting to make this automatic](https://github.com/yaml/pyyaml/issues/437).<end_of_turn>\n<start_of_turn>model\nUse LibYAML with PyYAML if available<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen importing certain libraries, users will encounter the following error which can be traced back to the datasets library.\n\nmodule 'pyarrow' has no attribute 'PyExtensionType'.\n\nExample issue: https://github.com/explodinggradients/ragas/issues/2170\n\nThe issue occurs due to the following. I will proceed to submit a PR with the below fix:\n\n**Issue Reason**\nThe issue is that PyArrow version 21.0.0 doesn\u2019t have PyExtensionType. This was changed in newer versions of PyArrow. The\n\nPyExtensionType class was renamed to ExtensionType in PyArrow 13.0.0 and later versions.\n\n\n** Issue Solution**\nMaking the following changes to the following lib files should temporarily resolve the issue.\n\nI will submit a PR to the dataets library in the meantime.\n\nenv_name/lib/python3.10/site-packages/datasets/features/features.py:\n\n```\n> 521            self.shape = tuple(shape)\n       522            self.value_type = dtype\n       523            self.storage_dtype = self._generate_dtype(self.value_type)\n       524 -          pa.PyExtensionType.__init__(self, self.storage_dtype)\n       524 +          pa.ExtensionType.__init__(self, self.storage_dtype)\n       525\n       526        def __reduce__(self):\n       527            return self.__class__, (\n```\n\nUpdated venv_name/lib/python3.10/site-packages/datasets/features/features.py:\n```      \n 510        _type: str = field(default=\u201cArray5D\u201d, init=False, repr=False)\n       511\n       512\n       513 -  class _ArrayXDExtensionType(pa.PyExtensionType):\n       513 +  class _ArrayXDExtensionType(pa.ExtensionType):\n       514        ndims: Optional[int] = None\n       515\n       516        def __init__(self, shape: tuple, dtype: str):\n```\n\n### Steps to reproduce the bug\n\nRagas version: 0.3.1\nPython version: 3.11\n\n**Code to Reproduce**\n\n_**In notebook:**_\n\n!pip install ragas\nfrom ragas import evaluate\n\n\n### Expected behavior\n\nThe required package installs without issue.\n\n### Environment info\n\nIn Jupyter Notebook.\nvenv<end_of_turn>\n<start_of_turn>model\nmodule 'pyarrow' has no attribute 'PyExtensionType'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport ruff 0.5.0 in CI.\r\n\r\nAlso revert:\r\n- #7007<end_of_turn>\n<start_of_turn>model\nSupport ruff 0.5.0 in CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Metrics for KLUE-MRC (Korean Language Understanding Evaluation \u2014 Machine Reading Comprehension)\r\n\r\nAdding metrics for [KLUE-MRC](https://huggingface.co/datasets/klue).\r\nKLUE-MRC is very similar to SQuAD 2.0 but has a slightly different format which is why I added metrics for KLUE-MRC.\r\n\r\nSpecifically, in the case of [LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness), it leverages the scoring script of SQuAD to evaluate SQuAD 2.0 and KorQuAD. But the script isn't suitable for KLUE-MRC because KLUE-MRC is a bit different from SQuAD 2.0. And this is why I added the scoring script for KLUE-MRC.\r\n\r\n- [x] All tests passed\r\n- [x] Added a metric card (referred the metric card of SQuAD 2.0)\r\n- [x] Compatibility test with [LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness) passed\r\n\r\n### References\r\n- [KLUE: Korean Language Understanding Evaluation](https://datasets-benchmarks-proceedings.neurips.cc/paper_files/paper/2021/file/98dce83da57b0395e163467c9dae521b-Paper-round2.pdf)\r\n- [KLUE on Hugging Face Datasets](https://huggingface.co/datasets/klue)\r\n- #2416<end_of_turn>\n<start_of_turn>model\nAdd KLUE-MRC metrics<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds `dataset_id_suffix` parameter to 'Dataset.from_generator' function.\r\n\r\n`Dataset.from_generator` function passes all of its arguments to `BuilderConfig.create_config_id`, including generator function itself. `BuilderConfig.create_config_id` function tries to hash all the args, which can take a large amount of time or even cause MemoryError if the dataset processed in a generator function is large enough.\r\n\r\nThis PR allows user to pass a custom fingerprint (`dataset_id_suffix`) to be used as a suffix in a dataset name instead of the one generated by hashing the args.\r\n\r\nThis PR is a possible solution of #7513 <end_of_turn>\n<start_of_turn>model\nAdd custom fingerprint support to `from_generator`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTemporarily pin `pandas` < 2.1.0 until permanent solution is found.\r\n\r\nHot fix #6197.<end_of_turn>\n<start_of_turn>model\nTemporarily pin pandas < 2.1.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 4.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nLoading image folder with a caption column using `load_dataset(<image_folder_path>)` doesn't load the captions. \r\n\r\nWhen loading a local image folder with captions using `datasets==2.13.0`\r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(<image_folder_path>)\r\ndata.column_names\r\n```\r\nyields \r\n`{'train': ['image', 'prompt']}`\r\n\r\nbut when using `datasets==2.15.0`\r\nyeilds \r\n`{'train': ['image']}`\r\n\r\nPutting the images and `metadata.jsonl` file into a nested `train` folder **or** loading with `load_dataset(\"imagefolder\", data_dir=<image_folder_path>)` solves the issue and\r\nyields \r\n`{'train': ['image', 'prompt']}`\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. create a folder `<image_folder_path>` that contains images and a metadata file with additional features- e.g. \"prompt\"\r\n2. run:\r\n```\r\nfrom datasets import load_dataset\r\ndata = load_dataset(\"<image_folder_path>\")\r\ndata.column_names\r\n```\r\n\r\n### Expected behavior\r\n\r\n`{'train': ['image', 'prompt']}`\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0<end_of_turn>\n<start_of_turn>model\nTrouble loading image folder with additional features -  metadata file ignored<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently, when datasets are created, they can be versioned by passing the `version` argument to `load_dataset(...)`. For example creating `outcomes.csv` on the command line\n\n```\necho \"id,value\\n1,0\\n2,0\\n3,1\\n4,1\\n\" > outcomes.csv\n```\nand creating it\n```\nimport datasets\ndataset = datasets.load_dataset(\n    \"csv\",\n    data_files =\"outcomes.csv\",\n    keep_in_memory = True,\n    version = '1.0.0')\n```\n\nThe version info is stored in the `info` and can be accessed e.g. by `next(iter(dataset.values())).info.version`\n\nThis dataset can be uploaded to the hub with `dataset.push_to_hub(repo_id = \"maomlab/example_dataset\")`. This will create a dataset  on the hub with the following in the `README.md`, but it doesn't upload the version information:\n\n```\n---\ndataset_info:\n  features:\n  - name: id\n    dtype: int64\n  - name: value\n    dtype: int64\n  splits:\n  - name: train\n    num_bytes: 64\n    num_examples: 4\n  download_size: 1332\n  dataset_size: 64\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n---\n```\n\nHowever, when I download from the hub, the version information is missing:\n\n```\ndataset_from_hub_no_version = datasets.load_dataset(\"maomlab/example_dataset\")\nnext(iter(dataset.values())).info.version\n```\n\nI can add the version information manually to the hub, by appending it to the end of config section:\n\n```\n...\nconfigs:\n- config_name: default\n  data_files:\n  - split: train\n    path: data/train-*\n  version: 1.0.0\n---\n```\n\nAnd then when I download it, the version information is correct.\n\n\n\n\n\n### Motivation\n\n### Why adding version information for each config makes sense\n  1. The version information is already recorded in the dataset config info data structure and is able to parse it correctly, so it makes sense to sync it with `push_to_hub`. \n  2. Keeping the version info in at the config level is different from version info at the branch level. As the former relates to the version of the specific dataset the config refers to rather than the version of the dataset curation itself.\n\n## A explanation for the current behavior:\nIn [datasets/src/datasets/info.py:159](https://github.com/huggingface/datasets/blob/fb91fd3c9ea91a818681a777faf8d0c46f14c680/src/datasets/info.py#L159C1-L160C1\n), the `_INCLUDED_INFO_IN_YAML` variable doesn't include `\"version\"`.\n\nIf my reading of the code is right, adding `\"version\"` to `_INCLUDED_INFO_IN_YAML`, would allow the version information to be uploaded to the hub.\n\n### Your contribution\n\nRequest: add `\"version\"` to `_INCLUDE_INFO_IN_YAML` in [datasets/src/datasets/info.py:159](https://github.com/huggingface/datasets/blob/fb91fd3c9ea91a818681a777faf8d0c46f14c680/src/datasets/info.py#L159C1-L160C1\n)\n<end_of_turn>\n<start_of_turn>model\nAllow pushing config version to hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello,\r\n\r\nI'm trying to load a dataset from Google Colab but the process hangs at `Resolving data files`:\r\n\r\n![image](https://github.com/huggingface/datasets/assets/99779/7175ad85-e571-46ed-9f87-92653985777d)\r\n\r\nIt is happening when the `_get_origin_metadata` definition is invoked:\r\n\r\n```python\r\ndef _get_origin_metadata(\r\n    data_files: List[str],\r\n    max_workers=64,\r\n    download_config: Optional[DownloadConfig] = None,\r\n) -> Tuple[str]:\r\n    return thread_map(\r\n        partial(_get_single_origin_metadata, download_config=download_config),\r\n        data_files,\r\n        max_workers=max_workers,\r\n        tqdm_class=hf_tqdm,\r\n        desc=\"Resolving data files\",\r\n        disable=len(data_files) <= 16,\r\n```\r\n\r\nThe thread is then stuck at `waiter.acquire()` in the builtin `threading.py` file.\r\n\r\nI can load the dataset just fine on my machine.\r\n\r\nCheers,\r\n\r\nThomas\r\n\n\n### Steps to reproduce the bug\n\nIn Google Colab:\r\n\r\n```python\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"colour-science/color-checker-detection-dataset\")\r\n```\n\n### Expected behavior\n\nThe dataset should be loaded.\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.1\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0<end_of_turn>\n<start_of_turn>model\nLoading a dataset from Google Colab hangs at \"Resolving data files\".<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Dataset configurations cannot be created in YAML/README\r\n\r\nHello! I'm trying to follow the docs here in order to create structure in my dataset as added from here (#5331): https://github.com/huggingface/datasets/blob/8b8e6ee067eb74e7965ca2a6768f15f9398cb7c8/docs/source/repository_structure.mdx#L110-L118\r\n\r\nI have the exact example in my config file for [my data repo](https://huggingface.co/datasets/tsor13/test):\r\n```\r\nconfigs:\r\n- config_name: main_data\r\n  data_files: \"main_data.csv\"\r\n- config_name: additional_data\r\n  data_files: \"additional_data.csv\"\r\n```\r\n\r\nYet, I'm unable to load different configurations:\r\n```\r\nfrom datasets import get_dataset_config_names\r\nget_dataset_config_names('tsor13/test', use_auth_token=True)\r\n```\r\nreturns a single split, `['tsor13--test']`\r\n\r\nDoes anyone have any insights?\r\n\r\n@polinaeterna thank you for adding this feature, it is super useful. Do you happen to have any ideas?\r\n\r\n### Steps to reproduce the bug\r\n\r\nfrom datasets import get_dataset_config_names\r\nget_dataset_config_names('tsor13/test')\r\n\r\n### Expected behavior\r\n\r\nI would expect there to be two splits, `main_data` and `additional_data`. However, only `['tsor13--test']` test is returned.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.4-arm64-arm-64bit\r\n- Python version: 3.11.4\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.1<end_of_turn>\n<start_of_turn>model\nConfigurations in yaml not working<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nRequest for a comparison of huggingface datasets compared with other data format especially webdataset\n\n### Motivation\n\nI see huggingface datasets uses Apache Arrow as its backend, it seems to be great, but I'm curious about how it is good compared with other dataset format, like webdataset, what's the pros/cons of them.\n\n### Your contribution\n\nMore information<end_of_turn>\n<start_of_turn>model\nRequest for a comparison of huggingface datasets compared with other data format especially webdataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThere ~10 unnecessary `/api/datasets/user/dataset/revision`calls due to multithreading in data files resolution.\r\n\r\nI disabled multithreading, which was actually not useful anymore since `HfFileSystem` has been using caching for a while now.<end_of_turn>\n<start_of_turn>model\nLess api calls when resolving data_files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhat does this PR do?\r\n1. remove `list_files_info` in favor of `list_repo_tree`. As of `0.23`, `list_files_info` will be removed for good. `datasets` had a utility to support both pre-0.20 and post-0.20 versions. Since `hfh` version is already pinned to `>=0.21.2`, I removed the legacy part.\r\n2. `preupload_lfs_files` had also a different behavior between `<0.20` and `>=0.20`. I remove it since huggingface_hub is now pinned to `>=0.21.2`\r\n3. `hf_hub_url` is overwritten to default to the dataset repo_type. I do think it is misleading to keep the same method naming for it. I renamed it to `get_dataset_url` for clarity. Let me know if you prefer to see this change reverted.<end_of_turn>\n<start_of_turn>model\nRemove deprecated code<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nadd support for pyarrow string view in features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a bug in the JSON loader where columns containing float values like `[0.0, 1.0, 2.0]` were being implicitly coerced to `int`, due to pandas or Arrow type inference.\r\n\r\nThis caused issues downstream in statistics computation (e.g., dataset-viewer) where such columns were incorrectly labeled as `\"int\"` instead of `\"float\"`.\r\n\r\n### \ud83d\udd0d What was happening:\r\nWhen the JSON loader falls back to `pandas_read_json()` (after `pa.read_json()` fails), pandas/Arrow can coerce float values to integers if all values are integer-like (e.g., `0.0 == 0`).\r\n\r\n### \u2705 What this PR does:\r\n- Adds a check in the fallback path of `_generate_tables()`\r\n- Ensures that columns made entirely of floats are preserved as `\"float64\"` even if they are integer-like (e.g. `0.0`, `1.0`)\r\n- This prevents loss of float semantics when creating the Arrow table\r\n\r\n### \ud83e\uddea Reproducible Example:\r\n```json\r\n[{\"col\": 0.0}, {\"col\": 1.0}, {\"col\": 2.0}]\r\n````\r\n\r\nPreviously loaded as:\r\n\r\n* `int`\r\n\r\nNow correctly loaded as:\r\n\r\n* `float`\r\n\r\nFixes #6937\r\n<end_of_turn>\n<start_of_turn>model\nFix: Preserve float columns in JSON loader when values are integer-like (e.g. 0.0, 1.0)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHow about adding if the user already know about the pattern?\n\nhttps://github.com/huggingface/datasets/blob/a256b85cbc67aa3f0e75d32d6586afc507cf535b/src/datasets/data_files.py#L252\n\n### Motivation\n\nWhile using this load_dataset people might use 10M of images for the local files.\nHowever, due to searching all the appropriate file pattern in fsspec, purely searching this pattern takes more than 10 hours (real use-case).\n\n### Your contribution\n\nYeah I can make this happen if this seems valid. @lhoestq WDYT?\nsuch like\n```\ndef _get_data_files_patterns(pattern_resolver: Callable[[str], list[str]], patterns: PATTERNS) -> dict[str, list[str]]:\n```<end_of_turn>\n<start_of_turn>model\nAdding argument to `_get_data_files_patterns`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ntodo:\r\n- [x] fix TestPushToHub.test_push_dataset_dict_to_hub_iterable_num_proc<end_of_turn>\n<start_of_turn>model\nTests typing and fixes for push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nIt would be useful to be able to straightforwardly repeat iterable datasets indefinitely, to provide complete control over starting and ending of iteration to the user.\r\n\r\nAn IterableDataset.repeat(n) function could do this automatically\r\n\r\n### Motivation\r\n\r\nThis feature was discussed in this issue https://github.com/huggingface/datasets/issues/7147, and would resolve the need to use the hack of interleave datasets with probability 0 as a simple way to achieve this functionality.\r\n\r\nAn additional benefit might be the simplification of the use of iterable datasets in a distributed setting:\r\nIf the user can assume that datasets will repeat indefinitely, then issues around different numbers of samples appearing on different devices (e.g. https://github.com/huggingface/datasets/issues/6437, https://github.com/huggingface/datasets/issues/6594, https://github.com/huggingface/datasets/issues/6623, https://github.com/huggingface/datasets/issues/6719) can potentially be straightforwardly resolved by simply doing:\r\n\r\nids.repeat(None).take(n_samples_per_epoch)\r\n\r\n\r\n### Your contribution\r\n\r\nI'm not familiar enough with the codebase to assess how straightforward this would be to implement.\r\n\r\nIf it might be very straightforward, I could possibly have a go.<end_of_turn>\n<start_of_turn>model\nAdd repeat() for iterable datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLooks like `IterableDataset.from_spark` does not support multiple workers in pytorch `Dataloader` if I'm not missing anything.\r\n\r\nAlso, returns not consistent error messages, which probably depend on the nondeterministic order of worker executions\r\n\r\nSome exampes I've encountered:\r\n\r\n```\r\nFile \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1/lib/python3.10/site-packages/datasets/packaged_modules/spark/spark.py\", line 79, in __iter__\r\n    yield from self.generate_examples_fn()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1/lib/python3.10/site-packages/datasets/packaged_modules/spark/spark.py\", line 49, in generate_fn\r\n    df_with_partition_id = df.select(\"*\", pyspark.sql.functions.spark_partition_id().alias(\"part_id\"))\r\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 54, in wrapper\r\n    logger.log_failure(\r\n  File \"/databricks/spark/python/pyspark/databricks/usage_logger.py\", line 70, in log_failure\r\n    self.logger.recordFunctionCallFailureEvent(\r\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\r\n    return_value = get_return_value(\r\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 188, in deco\r\n    return f(*a, **kw)\r\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 342, in get_return_value\r\n    return OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\nKeyError: 'c'\r\n```\r\n\r\n```\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1/lib/python3.10/site-packages/datasets/packaged_modules/spark/spark.py\", line 79, in __iter__\r\n    yield from self.generate_examples_fn()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1/lib/python3.10/site-packages/datasets/packaged_modules/spark/spark.py\", line 49, in generate_fn\r\n    df_with_partition_id = df.select(\"*\", pyspark.sql.functions.spark_partition_id().alias(\"part_id\"))\r\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 162, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"/databricks/spark/python/pyspark/sql/functions.py\", line 4893, in spark_partition_id\r\n    return _invoke_function(\"spark_partition_id\")\r\n  File \"/databricks/spark/python/pyspark/sql/functions.py\", line 98, in _invoke_function\r\n    return Column(jf(*args))\r\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\r\n    return_value = get_return_value(\r\n  File \"/databricks/spark/python/pyspark/errors/exceptions/captured.py\", line 188, in deco\r\n    return f(*a, **kw)\r\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 342, in get_return_value\r\n    return OUTPUT_CONVERTER[type](answer[2:], gateway_client)\r\nKeyError: 'm'\r\n```\r\n\r\n```\r\nFile \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1/lib/python3.10/site-packages/datasets/packaged_modules/spark/spark.py\", line 79, in __iter__\r\n    yield from self.generate_examples_fn()\r\n  File \"/local_disk0/.ephemeral_nfs/envs/pythonEnv-68c05436-3512-41c4-88ca-5630012b70d1/lib/python3.10/site-packages/datasets/packaged_modules/spark/spark.py\", line 49, in generate_fn\r\n    df_with_partition_id = df.select(\"*\", pyspark.sql.functions.spark_partition_id().alias(\"part_id\"))\r\n  File \"/databricks/spark/python/pyspark/sql/utils.py\", line 162, in wrapped\r\n    return f(*args, **kwargs)\r\n  File \"/databricks/spark/python/pyspark/sql/functions.py\", line 4893, in spark_partition_id\r\n    return _invoke_function(\"spark_partition_id\")\r\n  File \"/databricks/spark/python/pyspark/sql/functions.py\", line 97, in _invoke_function\r\n    jf = _get_jvm_function(name, SparkContext._active_spark_context)\r\n  File \"/databricks/spark/python/pyspark/sql/functions.py\", line 88, in _get_jvm_function\r\n    return getattr(sc._jvm.functions, name)\r\n  File \"/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1725, in __getattr__\r\n    raise Py4JError(message)\r\npy4j.protocol.Py4JError: functions does not exist in the JVM\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nimport pandas as pd\r\nimport numpy as np\r\n\r\nbatch_size = 16\r\n\r\npdf = pd.DataFrame({\r\n    key: np.random.rand(16*100) for key in ['feature', 'target']\r\n})\r\ntest_df = spark.createDataFrame(pdf)\r\n\r\nfrom datasets import IterableDataset\r\nfrom torch.utils.data import DataLoader\r\n\r\nids = IterableDataset.from_spark(test_df)\r\nfor batch in DataLoader(ids, batch_size=16, num_workers=4):\r\n    for k, b in batch.items():\r\n        print(k, b.shape, sep='\\t')\r\n    print('\\n')\r\n```\n\n### Expected behavior\n\nFor `num_workers` equal to 0 or 1 works fine as expected:\r\n\r\n```\r\nfeature\ttorch.Size([16])\r\ntarget\ttorch.Size([16])\r\n\r\n\r\nfeature\ttorch.Size([16])\r\ntarget\ttorch.Size([16])\r\n....\r\n```\r\n\r\nExpected to support workers >1.\n\n### Environment info\n\nDatabricks 13.3 LTS ML runtime - Spark 3.4.1\r\npyspark==3.4.1\r\npy4j==0.10.9.7\r\ndatasets==2.13.1 and also tested with datasets==2.14.6<end_of_turn>\n<start_of_turn>model\n`IterableDataset.from_spark` does not support multiple workers in pytorch `Dataloader`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs proposed by @severo, maybe we should add this functionality as well to the CLI command to convert a script-dataset to Parquet. See: https://github.com/huggingface/datasets/pull/6795#discussion_r1562819168\r\n> When providing support, we sometimes suggest that users store their script in a script branch. What do you think of this alternative to deleting the files?<end_of_turn>\n<start_of_turn>model\nMake convert_to_parquet CLI command create script branch<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIn the documentation `datasets.Dataset.map` arg `cache_file_name` is said to be a string, but it doesn't work.\n\n### Steps to reproduce the bug\n\n1. pick a dataset\r\n2. write a map function\r\n3. do `ds.map(..., cache_file_name='some_filename')`\r\n4. it crashes\n\n### Expected behavior\n\nIt will tell you the filename you specified does not exist or it will generate a new file and tell you the filename does not exist.\n\n### Environment info\n\n- `datasets` version: 2.16.0\r\n- Platform: Linux-5.10.201-168.748.amzn2int.x86_64-x86_64-with-glibc2.26\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\ndatasets map `cache_file_name` does not work<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7297<end_of_turn>\n<start_of_turn>model\nUpdate iterable_dataset.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nRequest for adding rasterio support to load geotiff as a part of ImageFolder, instead of using PIL\n\n### Motivation\n\nAs of now, there are many datasets in HuggingFace Hub which are predominantly focussed towards RemoteSensing or are from RemoteSensing. The current ImageFolder (if I have understood correctly) uses PIL. This is not really optimized because mostly these datasets have images with many channels and additional metadata. Using PIL makes one loose it unless we provide a custom script. Hence, maybe an API could be added to have this in common?\n\n### Your contribution\n\nIf the issue is accepted - i can contribute the code, because I would like to have it automated and generalised.<end_of_turn>\n<start_of_turn>model\nSupport for loading geotiff files as a part of the ImageFolder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`load_dataset` allows loading a subset of columns from a parquet file with the `columns` argument. Since version 2.13, this produces the following error:\r\n\r\n```\r\nTraceback (most recent call last):                      \r\n  File \"/usr/lib/python3.10/site-packages/datasets/builder.py\", line 1879, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"/usr/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 68, in _generate_tables\r\n    raise ValueError(\r\nValueError: Tried to load parquet data with columns '['sepal_length']' with mismatching features '{'sepal_length': Value(dtype='float64', id=None), 'sepal_width': Value(dtype='float64', id=None), 'petal_length': Value(dtype='float64', id=None), 'petal_width': Value(dtype='float64', id=None), 'species': Value(dtype='string', id=None)}'\r\n```\r\n\r\nThis seems to occur because `datasets` is checking whether the columns in the schema exactly match the provided list of columns, instead of whether they are a subset.\n\n### Steps to reproduce the bug\n\n```python\r\n# Prepare some sample data\r\n\r\nimport pandas as pd\r\n\r\niris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')\r\niris.to_parquet('iris.parquet')\r\n\r\n# ['sepal_length', 'sepal_width', 'petal_length', 'petal_width',  'species']\r\nprint(iris.columns)\r\n\r\n# Load data with datasets\r\n\r\nfrom datasets import load_dataset\r\n\r\n# Load full parquet file\r\ndataset = load_dataset('parquet', data_files='iris.parquet')\r\n\r\n# Load column subset; throws error for datasets>=2.13\r\ndataset = load_dataset('parquet', data_files='iris.parquet', columns=['sepal_length'])\r\n```\n\n### Expected behavior\n\nNo error should be thrown and the given column subset should be loaded.\n\n### Environment info\n\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.9\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nLoading column subset from parquet file produces error since version 2.13<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n![image](https://github.com/huggingface/datasets/assets/1328316/1f4f0086-3db9-4c79-906b-05a375357cce)\r\n\r\n\r\n![image](https://github.com/huggingface/datasets/assets/1328316/733ebd3d-89b9-4ece-b80a-00ab5b0a4122)\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nUse link as reference document written https://colab.research.google.com/github/huggingface/datasets/blob/main/notebooks/Overview.ipynb#scrollTo=biqDH9vpvSVz\r\n\r\n```python\r\n# Now let's train our model\r\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n\r\nmodel.train().to(device)\r\nfor i, batch in enumerate(dataloader):\r\n    batch.to(device)\r\n    outputs = model(**batch)\r\n    loss = outputs.loss\r\n    loss.backward()\r\n    optimizer.step()\r\n    model.zero_grad()\r\n    print(f'Step {i} - loss: {loss:.3}')\r\n    if i > 5:\r\n        break\r\n```\r\n\r\nError\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n[<ipython-input-44-7040b885f382>](https://localhost:8080/#) in <cell line: 5>()\r\n      5 for i, batch in enumerate(dataloader):\r\n      6     batch.to(device)\r\n----> 7     outputs = model(**batch)\r\n      8     loss = outputs.loss\r\n      9     loss.backward()\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py](https://localhost:8080/#) in _call_impl(self, *args, **kwargs)\r\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\r\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1501             return forward_call(*args, **kwargs)\r\n   1502         # Do not call functions when jit is used\r\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\r\n\r\nTypeError: DistilBertForQuestionAnswering.forward() got an unexpected keyword argument 'token_type_ids'\r\n```\r\n\r\nhttps://github.com/huggingface/datasets/assets/1328316/5d8b1d61-9337-4d59-8423-4f37f834c156\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nRun success on Google Colab (free)\r\n\r\n### Environment info\r\n\r\nWindows 11 x64, Google Colab free (my Google Drive just empty about 200 MB, but I don't think it cause problem)<end_of_turn>\n<start_of_turn>model\nHuggingsFace dataset example give error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIt seems the list of valid splits recorded by the package becomes incorrectly overwritten when using the `data_files` argument.\n\nIf I run\n```python\nfrom datasets import load_dataset\nload_dataset(\"allenai/super\", split=\"all_examples\", data_files=\"tasks/expert.jsonl\")\n```\nthen I get the error\n```\nValueError: Unknown split \"all_examples\". Should be one of ['train'].\n```\n\nHowever, if I run\n```python\nfrom datasets import load_dataset\nload_dataset(\"allenai/super\", split=\"train\", name=\"Expert\")\n```\nthen I get\n```\nValueError: Unknown split \"train\". Should be one of ['all_examples'].\n```\n\n### Steps to reproduce the bug\n\nRun\n```python\nfrom datasets import load_dataset\nload_dataset(\"allenai/super\", split=\"all_examples\", data_files=\"tasks/expert.jsonl\")\n```\n\n### Expected behavior\n\nNo error.\n\n### Environment info\n\nPython = 3.12\ndatasets = 3.2.0<end_of_turn>\n<start_of_turn>model\nUsing load_dataset with data_files and split arguments yields an error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport librosa and numpy 2.0 for Python 3.10 by installing soxr 0.4.0b1 pre-release:\r\n- https://github.com/dofuuz/python-soxr/releases/tag/v0.4.0b1\r\n- https://github.com/dofuuz/python-soxr/issues/28<end_of_turn>\n<start_of_turn>model\nSupport librosa and numpy 2.0 for Python 3.10<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe second line starting with the bracket doesn't properly render on huggingface/docs. Added \"> \" to address it.\r\n\r\nIn the client documentation, the markdown 'TIP' paragraph in docs/stream#shuffle is not well executed, not as the other in the same page / while markdown is correctly considering it.\r\n\r\nDocumentation:\r\nhttps://huggingface.co/docs/datasets/v4.3.0/en/stream#shuffle:~:text=%5B!TIP%5D%5BIterableDataset.shuffle()%5D(/docs/datasets/v4.3.0/en/package_reference/main_classes%23datasets.IterableDataset.shuffle)%20will%20also%20shuffle%20the%20order%20of%20the%20shards%20if%20the%20dataset%20is%20sharded%20into%20multiple%20files.\r\n\r\nGithub source:\r\nhttps://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#:~:text=Casting%20only%20works%20if%20the%20original%20feature%20type%20and%20new%20feature%20type%20are%20compatible.%20For%20example%2C%20you%20can%20cast%20a%20column%20with%20the%20feature%20type%20Value(%27int32%27)%20to%20Value(%27bool%27)%20if%20the%20original%20column%20only%20contains%20ones%20and%20zeros.<end_of_turn>\n<start_of_turn>model\nFix: Properly render [!TIP] block in stream.shuffle documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfixes https://github.com/huggingface/datasets/issues/7346\r\n\r\n(also updated `ruff` and appleid style changes)<end_of_turn>\n<start_of_turn>model\nCatch OSError for arrow<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHi, I'm trying to load the dataset `togethercomputer/RedPajama-Data-1T-Sample` with `load_dataset` in streaming mode, i.e., `streaming=True`, but `FileNotFoundError` occurs.\r\n\r\n### Steps to reproduce the bug\r\n\r\nI've downloaded the dataset and save it to the cache dir in advance. My hope is loading the files in offline environment and without taking too much hours to prepross the entire data before running into the training process.\r\nSo I try the following code to load the files streamingly\r\n```py\r\ndataset = load_dataset('togethercomputer/RedPajama-Data-1T-Sample', streaming=True)\r\nprint(next(iter(dataset['train'])))\r\n```\r\nSadly, it raises the following:\r\n```\r\nFileNotFoundError: [Errno 2] No such file or directory: 'CURRENT_CODE_PATH/arxiv_sample.jsonl'\r\n```\r\nI've noticed that the dataset can be properly found in the begining\r\n```\r\nUsing the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/datasets/togethercomputer--RedPajama-Data-1T-Sample/6ea3bc8ec2e84ec6d2df1930942e9028ace8c5b9d9143823cf911c50bbd92039 (last modified on Sat Oct 21 20:12:57 2023) since it couldn't be found locally at togethercomputer/RedPajama-Data-1T-Sample., or remotely on the Hugging Face Hub.\r\n```\r\nBut it seems that the paths couldn't be properly parsed when loading iteratively.\r\n \r\nHow should I fix this error. I've tried specifying `data_files` or `data_dir` as `.../arxiv_sample.jsonl` but none of them works.\r\n\r\nThanks.\r\n\r\n### Expected behavior\r\n\r\nProperly load the dataset.\r\n\r\n### Environment info\r\n\r\n`datasets==2.14.5`<end_of_turn>\n<start_of_turn>model\nFileNotFoundError when trying to load the downloaded dataset with `load_dataset(..., streaming=True)`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfixed a typo<end_of_turn>\n<start_of_turn>model\nUpdate README.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThere is a YAML error at the top of the page, and I don't think it's supposed to be there \n\n### Steps to reproduce the bug\n\n1. Browse to [this tutorial document](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md)\r\n2. Observe a big red error at the top\r\n3. The rest of the document remains functional\n\n### Expected behavior\n\nI think the YAML block should be displayed or ignored.\n\n### Environment info\n\nN/A<end_of_turn>\n<start_of_turn>model\ndatasets template guide :: issue in documentation YAML<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHello,\r\n\r\nI've been struggling with a problem using Huggingface datasets caused by PyArrow memory allocation. I finally managed to solve it, and thought to document it since similar issues have been raised here before (https://github.com/huggingface/datasets/issues/5710, https://github.com/huggingface/datasets/issues/6176).\r\n\r\nIn my case, I was trying to load ~70k dataset files from disk using `datasets.load_from_disk(data_path)` (meaning 70k repeated calls to load_from_disk). This triggered an (uninformative) exception around 64k loaded files:\r\n```\r\n  File \"pyarrow/io.pxi\", line 1053, in pyarrow.lib.memory_map\r\n  File \"pyarrow/io.pxi\", line 1000, in pyarrow.lib.MemoryMappedFile._open\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\nOSError: Memory mapping file failed: Cannot allocate memory\r\n```\r\nDespite system RAM usage being very low. After a lot of digging around, I discovered that my Ubuntu machine had a limit on the maximum number of memory mapped files in `/proc/sys/vm/max_map_count` set to 65530, which was causing my data loader to crash. Increasing the limit in the file (`echo <new_mmap_size> | sudo tee /proc/sys/vm/max_map_count`) made the issue go away.\r\n\r\nWhile this isn't a bug as such in either Datasets or PyArrow, this behavior can be very confusing to users. Maybe this should be mentioned in documentation? I suspect the other issues raised here about memory mapping OOM errors could actually be consequence of system configuration.\r\n\r\nBr,\r\nLauri\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nimport numpy as np\r\nimport pyarrow as pa\r\nimport tqdm\r\n\r\n# Write some data to disk\r\narr = pa.array(np.arange(100))\r\nschema = pa.schema([\r\n    pa.field('nums', arr.type)\r\n])\r\nwith pa.OSFile('arraydata.arrow', 'wb') as sink:\r\n    with pa.ipc.new_file(sink, schema=schema) as writer:\r\n        batch = pa.record_batch([arr], schema=schema)\r\n        writer.write(batch)\r\n\r\n# Number of times to open the memory map\r\nnums = 70000\r\n\r\n# Read the data back\r\narrays = [pa.memory_map('arraydata.arrow', 'r') for _ in tqdm.tqdm(range(nums))]\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo errors.\r\n\r\n### Environment info\r\n\r\ndatasets: 2.18.0\r\npyarrow: 15.0.0<end_of_turn>\n<start_of_turn>model\nPyArrow 'Memory mapping file failed: Cannot allocate memory' bug<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi!\r\n\r\nI'm currenty using the map function to tokenize a somewhat large dataset, so I need to use the cache to save ~25 mins.\r\n\r\nChanging num_proc incduces the recomputation of the map, I'm not sure why and if this is excepted behavior? \r\n\r\nhere it says, that cached files are loaded sequentially:\r\n\r\nhttps://github.com/huggingface/datasets/blob/bb2664cf540d5ce4b066365e7c8b26e7f1ca4743/src/datasets/arrow_dataset.py#L3005-L3006\r\n\r\nit seems like I can pass in a fingerprint, and load it directly:\r\n\r\nhttps://github.com/huggingface/datasets/blob/bb2664cf540d5ce4b066365e7c8b26e7f1ca4743/src/datasets/arrow_dataset.py#L3108-L3125\r\n\r\n**Environment Setup:**\r\n\r\n- Python 3.11.9\r\n- datasets  2.19.1 conda-forge\r\n- Linux 6.1.83-1.el9.elrepo.x86_64\r\n\r\n**MRE**\r\n```python\r\nfixed raw_datasets\r\nfixed tokenize_function\r\n\r\ntokenized_datasets = raw_datasets.map(\r\n                        tokenize_function,\r\n                        batched=True,\r\n                        num_proc=9,\r\n                        remove_columns=['text'],\r\n                        load_from_cache_file= True,\r\n                        desc=\"Running tokenizer on dataset line_by_line\",\r\n                    )\r\n\r\n\r\ntokenized_datasets = raw_datasets.map(\r\n                        tokenize_function,\r\n                        batched=True,\r\n                        num_proc=5,\r\n                        remove_columns=['text'],\r\n                        load_from_cache_file= True,\r\n                        desc=\"Running tokenizer on dataset line_by_line\",\r\n                    )\r\n```<end_of_turn>\n<start_of_turn>model\nCaching map result of DatasetDict.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen a large commit is split up, it seems the commit index in the message is zero-based while the total number is one-based. I came across this running `convert-to-parquet` and was wondering why there was no `6-of-6` commit. This PR fixes that by adding one to the commit index, so both are one-based.\r\n\r\nCurrent behavior:\r\n<img width=\"463\" alt=\"Screenshot 2025-04-17 at 1 00 17\u202fPM\" src=\"https://github.com/user-attachments/assets/7f3d389e-cb92-405d-a3c2-f2b1cdf0cb79\" /><end_of_turn>\n<start_of_turn>model\nFix indexing in split commit messages<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWe're filtering dataset of ~1M (small-ish) records. At some point in the code we do `dataset.filter`, before (including 3.2.0) it was taking couple of seconds, and now it takes 4 hours.\n\nWe use 16 threads/workers, and stack trace at them look as follows:\n```\nTraceback (most recent call last):\n  File \"/python/lib/python3.12/site-packages/multiprocess/process.py\", line 314, in _bootstrap\n    self.run()\n  File \"/python/lib/python3.12/site-packages/multiprocess/process.py\", line 108, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/python/lib/python3.12/site-packages/multiprocess/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/python/lib/python3.12/site-packages/datasets/utils/py_utils.py\", line 678, in _write_generator_to_queue\n    for i, result in enumerate(func(**kwargs)):\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3511, in _map_single\n    for i, batch in iter_outputs(shard_iterable):\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3461, in iter_outputs\n    yield i, apply_function(example, i, offset=offset)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 3390, in apply_function\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/python/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 6416, in get_indices_from_mask_function\n    indices_array = indices_mapping.column(0).take(indices_array)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/table.pxi\", line 1079, in pyarrow.lib.ChunkedArray.take\n  File \"/python/lib/python3.12/site-packages/pyarrow/compute.py\", line 458, in take\n    def take(data, indices, *, boundscheck=True, memory_pool=None):\n```\n\n### Steps to reproduce the bug\n\n1. Save dataset of 1M records in arrow\n2. Filter it with 16 threads\n3. Watch it take too long\n\n### Expected behavior\n\nFiltering done fast\n\n### Environment info\n\ndatasets 3.3.0, python 3.12<end_of_turn>\n<start_of_turn>model\nPerformance regression in `dataset.filter`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nbefore release 4.0\r\n\r\n(I also did minor improvements to `features` to not show their `id=None` in their `__repr__()`)<end_of_turn>\n<start_of_turn>model\nFixes in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPreviously, the output of `score, indices = Dataset.search(...)` would be numpy arrays.\r\n\r\nThe definition in `SearchResult` is a `List[int]` so this PR now matched the expected type.\r\n\r\nThe previous behavior is a bit annoying as `Dataset.__getitem__` doesn't support `numpy.int64` which forced me to convert `indices` to int eg:\r\n\r\n```python\r\nscore, indices = ds.search(...)\r\nitem = ds[int(indices[0])]\r\n```<end_of_turn>\n<start_of_turn>model\nImprove typing of Dataset.search, matching definition<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCreate `DefunctDatasetError` as a specific error to be raised when a dataset is defunct and no longer accessible.\r\n\r\nSee Hub discussion: https://huggingface.co/datasets/the_pile_books3/discussions/7#6523c13a94f3a1a2092d251b<end_of_turn>\n<start_of_turn>model\nCreate DefunctDatasetError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHF datasets are awesome in supporting text and images. Will be great to see such a support in videos :) \n\n### Motivation\n\nVideo generation :) \n\n### Your contribution\n\nWill probably be limited to raising this feature request ;)<end_of_turn>\n<start_of_turn>model\nSupport Video <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHi, \r\n\r\nI've downloaded an Arrow-formatted dataset offline using the hugggingface's datasets library by:\r\n\r\n```\r\nimport json\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"pixparse/cc3m-wds\")\r\ndataset.save_to_disk(\"./cc3m_1\") \r\n```\r\n\r\nnow I need to convert it to WebDataset's TAR format for offline data ingestion. \r\nIs there a straightforward method to achieve this conversion without an internet connection? Can I simply convert it by \r\n```\r\ntar -cvf \r\n```\r\n\r\nbtw, when I tried:\r\n```\r\nimport webdataset as wds\r\nfrom huggingface_hub import get_token\r\nfrom torch.utils.data import DataLoader\r\n\r\nhf_token = get_token()\r\nurl = \"https://huggingface.co/datasets/timm/imagenet-12k-wds/resolve/main/imagenet12k-train-{{0000..1023}}.tar\"\r\nurl = f\"pipe:curl -s -L {url} -H 'Authorization:Bearer {hf_token}'\"\r\ndataset = wds.WebDataset(url).decode()\r\ndataset.save_to_disk(\"./cc3m_webdataset\") \r\n```\r\nerror occured:\r\n```\r\nAttributeError: 'WebDataset' object has no attribute 'save_to_disk'\r\n```\r\n\r\nThanks a lot!\n\n### Motivation\n\nConverting Arrow to WebDataset TAR Format\n\n### Your contribution\n\nNo clue yet<end_of_turn>\n<start_of_turn>model\nConverting Arrow to WebDataset TAR Format for Offline Use<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`write_examples_on_file` and `write_batch` are broken in `ArrowWriter` since #6636. The order between the columns and the schema is not preserved anymore. So these functions don't work anymore unless the order happens to align well.\n\n### Steps to reproduce the bug\n\nTry to do `write_batch` with anything that has many columns, and it's likely to break.\n\n### Expected behavior\n\nI expect these functions to work, instead of it trying to cast a column to its incorrect type.\n\n### Environment info\n\n- `datasets` version: 2.17.0\r\n- Platform: Linux-5.15.0-1040-aws-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\n`write_examples_on_file` and `write_batch` are broken in `ArrowWriter`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nA number of processes parameter to the dataset.push_to_hub method\n\n### Motivation\n\nShards are currently uploaded serially which makes it slow for many shards, uploading can be done in parallel and much faster\n<end_of_turn>\n<start_of_turn>model\nAdd num_proc parameter to push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI think these are at least the main places where we should put content. Ideally it is not just repeated in the final version\r\n\r\n\r\nref #7690 \r\n\r\n\r\n- [x] Wait for #7743 to land<end_of_turn>\n<start_of_turn>model\nDocument HDF5 support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nexample: https://huggingface.co/datasets/lhoestq/overlapping-subsets-imagefolder/tree/main\r\n\r\nTo make multiple subsets for an imagefolder (one metadata file per subset), e.g.\r\n\r\n```yaml\r\nconfigs:\r\n  - config_name: default\r\n    metadata_filenames:\r\n      - metadata.csv\r\n  - config_name: other\r\n    metadata_filenames:\r\n      - metadata2.csv\r\n```<end_of_turn>\n<start_of_turn>model\nCustom metadata filenames<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Related Issues/PRs\r\n\r\nFixes #6152\r\n\r\n---\r\n\r\n### What changes are proposed in this pull request?\r\n\r\nThis PR adds a dedicated validation check in the `_info()` method of the `FolderBasedBuilder` class to ensure that users provide either `data_dir` or `data_files` when loading folder-based datasets (such as `audiofolder`, `imagefolder`, etc.).\r\n\r\n---\r\n\r\n### Why this change?\r\n\r\nPreviously, when calling:\r\n\r\n```python\r\nload_dataset(\"audiofolder\")\r\n````\r\n\r\nwithout specifying `data_dir` or `data_files`, the loader would silently fallback to the **current working directory**, leading to:\r\n\r\n* Long loading times\r\n* Unexpected behavior (e.g., scanning unrelated files)\r\n\r\nThis behavior was discussed in issue #6152. As suggested by maintainers, the fix has now been implemented directly inside the `FolderBasedBuilder._info()` method \u2014 keeping the logic localized to the specific builder instead of a generic loader function.\r\n\r\n---\r\n\r\n### How is this PR tested?\r\n\r\n* \u2705 Manually tested by calling `load_dataset(\"audiofolder\")` with no `data_dir` or `data_files` \u2192 a `ValueError` is now raised early.\r\n* \u2705 Existing functionality (with valid input) remains unaffected.\r\n\r\n---\r\n\r\n### Does this PR require documentation update?\r\n\r\n* [x] No\r\n\r\n---\r\n\r\n### Release Notes\r\n\r\n#### Is this a user-facing change?\r\n\r\n* [x] Yes\r\n\r\n> Folder-based datasets now raise an explicit error if neither `data_dir` nor `data_files` are specified, preventing unintended fallback to the current working directory.\r\n\r\n---\r\n\r\n#### What component(s) does this PR affect?\r\n\r\n* [x] `area/datasets`\r\n* [x] `area/load`\r\n\r\n---\r\n\r\n<a name=\"release-note-category\"></a>\r\n\r\n#### How should the PR be classified?\r\n\r\n* [x] `rn/bug-fix` - A user-facing bug fix\r\n\r\n---\r\n\r\n#### Should this be included in the next patch release?\r\n\r\n* [x] Yes<end_of_turn>\n<start_of_turn>model\nfix: raise error in FolderBasedBuilder when data_dir and data_files are missing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix aiohttp import<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm deciding to post here because I'm still not sure what the issue is, or if I am using IterableDatasets wrongly.\r\nI'm following the guide on here https://huggingface.co/learn/cookbook/en/fine_tuning_code_llm_on_single_gpu pretty much to a tee and have verified that it works when I'm fine-tuning on the provided dataset.\r\n\r\nHowever, I'm doing some data preprocessing steps (filtering out entries), when I try to swap out the dataset for mine, it fails to train. However, I eventually fixed this by simply setting `stream=False` in `load_dataset`.\r\n\r\nCoud this be some sort of network / firewall issue I'm facing?\n\n### Steps to reproduce the bug\n\nI made a post with greater description about how I reproduced this problem before I found my workaround: https://discuss.huggingface.co/t/problem-with-custom-iterator-of-streaming-dataset-not-returning-anything/94551\r\n\r\nHere is the problematic dataset snippet, which works when streaming=False (and with buffer keyword removed from shuffle)\r\n\r\n```\r\ncommitpackft = load_dataset(\r\n    \"chargoddard/commitpack-ft-instruct\", split=\"train\", streaming=True\r\n).filter(lambda example: example[\"language\"] == \"Python\")\r\n \r\n \r\ndef form_template(example):\r\n    \"\"\"Forms a template for each example following the alpaca format for CommitPack\"\"\"\r\n    example[\"content\"] = (\r\n        \"### Human: \" + example[\"instruction\"] + \" \" + example[\"input\"] + \" ### Assistant: \" + example[\"output\"]\r\n    )\r\n    return example\r\n \r\n \r\ndataset = commitpackft.map(\r\n    form_template,\r\n    remove_columns=[\"id\", \"language\", \"license\", \"instruction\", \"input\", \"output\"],\r\n).shuffle(\r\n    seed=42, buffer_size=10000\r\n)  # remove everything since its all inside \"content\" now\r\nvalidation_data = dataset.take(4000)\r\ntrain_data = dataset.skip(4000)\r\n```\r\n\r\nThe annoying part about this is that it only fails during training and I don't know when it will fail, except that it always fails during evaluation.\n\n### Expected behavior\n\nThe expected behavior is that I should be able to get something from the iterator when called instead of getting nothing / stuck in a loop somewhere.\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-5.4.0-121-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.7\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0\r\n\r\n<end_of_turn>\n<start_of_turn>model\nStreaming dataset not returning data<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n- We use shared `cache_dir` using `HF_HOME=\"{shared_directory}\"` \r\n- After dataset version 2.16.0, datasets uses `filelock` package for file locking #6445 \r\n- But, `filelock` package make `.lock` file with `644` permission\r\n- Dataset is not available to other users except the user who created the lock file via `load_dataset`.\n\n### Steps to reproduce the bug\n\n1. `pip install datasets==2.16.0`\r\n2. `export HF_HOME=\"{shared_directory}\"`\r\n3. download dataset with `load_dataset`\r\n4. logout and login another user\r\n5. `pip install datasets==2.16.0`\r\n6. `export HF_HOME=\"{shared_directory}\"`\r\n7. download dataset with `load_dataset`\r\n8. `PermissionError` occurs\n\n### Expected behavior\n\n- Users can share `cache_dir` using environment variable `HF_HOME`\n\n### Environment info\n\n- python == 3.9.10\r\n- datasets == 2.16.0\r\n- ubuntu 22.04\r\n- shared_directory has ACL\r\n![image (1)](https://github.com/huggingface/datasets/assets/106717516/5ca759db-ad0c-4883-9a97-9c8fccd00d8a)\r\n- users are same group (developers)\r\n<end_of_turn>\n<start_of_turn>model\nAfter `2.16.0` version, there are `PermissionError` when users use shared cache_dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAllow Polars round trip by supporting pyarrow large list.\r\n\r\nFix #6834, fix #6984.\r\n\r\nSupersede and close #4800, close #6835, close #6986.<end_of_turn>\n<start_of_turn>model\nSupport pyarrow large_list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses #5927 \r\nI've realized that the error was overlooked during testing due to the presence of only one None value in the sequence.\r\nUnfortunately, it was the only case where the function works as expected. When the sequence contained more than one None value, the function failed. Consequently, I've updated the tests to include sequences with multiple None values.<end_of_turn>\n<start_of_turn>model\nFix `to_numpy` when None values in the sequence<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nPublic spaces may stuck in Building after restarting, error log as follows:\n\nbuild error\nUnexpected job error\n\nERROR: failed to push spaces-registry.huggingface.tech/spaces/*:cpu-*-*: unexpected status from HEAD request to https://spaces-registry.huggingface.tech/v2/spaces/*/manifests/cpu-*-*: 401 Unauthorized\n\n### Steps to reproduce the bug\n\nRestart space / Factory rebuild cannot avoid it\n\n### Expected behavior\n\nFix this problem\n\n### Environment info\n\nno requirements.txt can still happen\npython gradio spaces<end_of_turn>\n<start_of_turn>model\nHow to solve \"Spaces stuck in Building\" problems<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHere is the full error stack when installing:\r\n\r\n```\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ndatasets 2.18.0 requires fsspec[http]<=2024.2.0,>=2023.1.0, but you have fsspec 2024.3.1 which is incompatible.\r\nSuccessfully installed aiobotocore-2.12.1 aioitertools-0.11.0 botocore-1.34.51 fsspec-2024.3.1 jmespath-1.0.1 s3fs-2024.3.1 urllib3-2.0.7 wrapt-1.16.0\r\n```\r\nWhen I install with pip, pip allows this error to exist while still installing s3fs, but this error breaks poetry, since poetry will refuse to install s3fs because of the dependency conflict.\r\n\r\nMaybe I'm missing something so maybe it's not a bug but some mistake on my end? Any input would be helpful. Thanks!\n\n### Steps to reproduce the bug\n\n1. conda create -n tmp python=3.10 -y\r\n2. conda activate tmp\r\n3. pip install datasets\r\n4. pip install s3fs\n\n### Expected behavior\n\nI would expect there to be no error.\n\n### Environment info\n\nMacOS (ARM), Python3.10, conda 23.11.0. <end_of_turn>\n<start_of_turn>model\nCompatibility issue between s3fs, fsspec, and datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI've taken a try at implementing a batched `IterableDataset` as requested in issue #6279. This PR adds a new `BatchedExamplesIterable` class and a `.batch()` method to the `IterableDataset` class.\r\n\r\nThe main changes are:\r\n1. A new `BatchedExamplesIterable` that groups examples into batches.\r\n2. A `.batch()` method for `IterableDataset` to easily create batched versions.\r\n3. Support for shuffling and sharding to work with PyTorch DataLoader and multiple workers.\r\n\r\nI'm not sure if this is exactly what you had in mind and also have not fully tested it atm, so I'd really appreciate your feedback. Does this seem like it's heading in the right direction? I'm happy to make any changes or explore different approaches if needed.\r\n\r\nPinging @lhoestq <end_of_turn>\n<start_of_turn>model\nAdd batching to `IterableDataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 4.1.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI wrote a notebook to load an existing dataset, process it, and upload as a private dataset using `dataset.push_to_hub(...)` at the end. The push to hub is failing with:\r\n\r\n```\r\nValueError: Invalid metadata in README.md.\r\n- Invalid YAML in README.md: unknown tag !<tag:yaml.org,2002:python[/tuple](http://192.168.1.128:8888/tuple)> (50:11)\r\n\r\n 47 |             - 4\r\n 48 |             - 4\r\n 49 |             - 8\r\n 50 |           - !!binary |\r\n----------------^\r\n 51 |             TwAAAA==\r\n 52 |           '1': !!python[/object/apply](http://192.168.1.128:8888/object/apply):nump ...\r\n```\r\n\r\nMy dataset has a `train` and `validation` dataset.  These are the features:\r\n\r\n```\r\n{'c1': Value(dtype='string', id=None),\r\n 'c2': Value(dtype='string', id=None),\r\n 'c3': [{'value': Value(dtype='string', id=None),\r\n   'start': Value(dtype='int64', id=None),\r\n   'end': Value(dtype='int64', id=None),\r\n   'label': Value(dtype='string', id=None)}],\r\n 'c4': Value(dtype='string', id=None),\r\n 'c5': Value(dtype='string', id=None),\r\n 'c6': Value(dtype='string', id=None),\r\n 'c7': Value(dtype='string', id=None),\r\n 'c8': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\r\n 'c9': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\r\n 'c10': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\r\n 'labels': Sequence(feature=ClassLabel(names=['O', 'B-ABC', 'I-ABC', ...], id=None), length=-1, id=None),\r\n 'c12': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\n```\r\n\r\nThis used to work until I decided to cast the `labels` feature to a `Sequence(ClassLabel(...))` type with:\r\n\r\n```\r\nds['train'] = ds['train'].cast_column(\"labels\", Sequence(ClassLabel(names=list(labels))))\r\nds['validation'] = ds['validation'].cast_column(\"labels\", Sequence(ClassLabel(names=list(labels))))\r\n```\n\n### Steps to reproduce the bug\n\n1. Start with any token classification dataset.\r\n2. Add a `labels` column with data such as `[0,0,0,12,13,13,13,0,0]`.\r\n3. Cast the label column from `Sequence` to `Sequence(ClassLabel))` with:\r\n\r\n```\r\nlabels = ['O', 'B-TEST', 'I-TEST']\r\nds = ds.cast_column(\"labels\", Sequence(ClassLabel(names=labels)))\r\n```\r\n\r\n4. Push to hub with `ds.push_to_hub(\"me/awesome-stuff-dataset\")`\n\n### Expected behavior\n\nI expected `push_to_hub` to successfully push my dataset to the hub without error.\n\n### Environment info\n\nPython 3.11.9\r\n\r\ndatasets==2.19.1\r\ntransformers==4.41.1\r\nPyYAML==6.0.1<end_of_turn>\n<start_of_turn>model\nInvalid YAML in README.md: unknown tag !<tag:yaml.org,2002:python/tuple><end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAvoid a redundant warning in `encode_np_array` by removing the identity check as NumPy `dtype`s can be equal without having identical `id`s.\r\n\r\nAdditionally, fix \"unreachable\" checks in `encode_np_array`.<end_of_turn>\n<start_of_turn>model\nAvoid redundant warning when encoding NumPy array as `Image`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelease on Jan 27, 2024: https://pypi.org/project/dill/0.3.8/#history\r\n<end_of_turn>\n<start_of_turn>model\nBump max range of dill to 0.3.8<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIf cache directory is set, information is not passed through.\r\nPass download config in as an arg too.<end_of_turn>\n<start_of_turn>model\nPass through information about location of cache directory.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 187, in _run_module_as_main\r\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n  File \"/usr/local/lib/python3.10/runpy.py\", line 110, in _get_module_details\r\n    __import__(pkg_name)\r\n  File \"/home/Medusa/axolotl/src/axolotl/cli/__init__.py\", line 23, in <module>\r\n    from axolotl.train import TrainDatasetMeta\r\n  File \"/home/Medusa/axolotl/src/axolotl/train.py\", line 23, in <module>\r\n    from axolotl.utils.trainer import setup_trainer\r\n  File \"/home/Medusa/axolotl/src/axolotl/utils/trainer.py\", line 13, in <module>\r\n    from datasets import set_caching_enabled\r\nImportError: cannot import name 'set_caching_enabled' from 'datasets' (/usr/local/lib/python3.10/site-packages/datasets/__init__.py)\n\n### Steps to reproduce the bug\n\n1\u3001axolotl\r\n2\u3001accelerate launch -m axolotl.cli.train examples/medusa/qwen_lora_stage1.yml\n\n### Expected behavior\n\nenable datasets\n\n### Environment info\n\npython3.10<end_of_turn>\n<start_of_turn>model\nImportError: cannot import name 'set_caching_enabled' from 'datasets' <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReduces the number of commits in `push_to_hub` by using the `preupload` API from https://github.com/huggingface/huggingface_hub/pull/1699. Each commit contains a maximum of 50 uploaded files.\r\n\r\nA shard's fingerprint no longer needs to be added as a suffix to support resuming an upload, meaning the shards' naming scheme is the same as the initial one.\r\n\r\nAlso, it adds support for the following params: `create_pr`, `commit_message` and `revision` (`branch` deprecated; unlike the previous implementation, this one creates a branch if the branch does not exist to be consistent with `transformers`). \r\n\r\n(Nit) This implementation keeps the markdown section of the generated README.md empty to enable importing the card template (when the card is accessed on the Hub).\r\n\r\nFixes https://github.com/huggingface/datasets/issues/5492, fixes https://github.com/huggingface/datasets/issues/6257, fixes https://github.com/huggingface/datasets/issues/5045, fixes https://github.com/huggingface/datasets/issues/6271\r\n\r\nTODO:\r\n- [x] set the minimal version to the next `hfh` release (once it's published)<end_of_turn>\n<start_of_turn>model\nReduce the number of commits in `push_to_hub`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\ni load a dataset from local csv file which has 187383612 examples, then use `map` to generate new columns for test.\r\nhere is my code : \r\n\r\n```\r\nimport os\r\n\r\nfrom datasets import load_dataset\r\nfrom datasets.features import Sequence, Value\r\n\r\ndef add_new_path(example):\r\n\r\n    example[\"ais_bbox\"] = [100,100,200,200]\r\n    example[\"ais_image_path\"] = os.path.join(\"images\", example[\"image_path\"]) if example[\"image_path\"] else \"\"\r\n    return example\r\n\r\nais_dataset = load_dataset(\"/data/ryan.gao/ais_dataset_cache/raw/1749/\")\r\nhf_ds = ais_dataset.map(add_new_path, batched=False, num_proc=32)\r\nds = hf_ds.cast_column(\"ais_bbox\", Sequence(Value(\"int32\"), length=4))\r\n```\r\n\r\nand the `cast_column` raise an exception\r\n```\r\nCasting the dataset:   3%|\u2588\u2588\u2588\u2589\r\n...\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2110, in cast_column\r\n    return self.cast(features)\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2055, in cast\r\n    dataset = dataset.map(\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3097, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3474, in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 3353, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/table.py\", line 2329, in table_cast\r\n    return cast_table_to_schema(table, schema)\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/table.py\", line 2288, in cast_table_to_schema\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/table.py\", line 2288, in <listcomp>\r\n    arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/table.py\", line 1831, in wrapper\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/table.py\", line 1831, in <listcomp>\r\n    return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/table.py\", line 2145, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\nTypeError: Couldn't cast array of type\r\nlist<item: int64>\r\nto\r\nSequence(feature=Value(dtype='int32', id=None), length=4, id=None)\r\n```\r\ni check the source code and make debug info:\r\nin datasets/table.py:2092\r\n```\r\n2091             if feature.length > -1:\r\n2092                 if feature.length * len(array) == len(array.values):\r\n2093                     return pa.FixedSizeListArray.from_arrays(_c(array.values, feature.feature), feature.length)\r\n2094                 print(len(array))\r\n2095                 print(len(array.values))\r\n```\r\nmy feature.length is 4.  but feature.length * len(array) == len(array.values) is false.\r\nprint(len(array)) is 262\r\nprint(len(array.values)) is 4000\r\n\r\nthen I use \"for item in array\" to print each item then get 262 * [100,100,200,200]\r\nand use \"for item in array.values\" to print each item and get 4000 int32 which are 1000 * [100,100,200,200]\r\n\r\ni'm wondering the `chunk` in each `array.chunks`, the \"chunk.values\" may get all the chunks's value rather than single chunk? but i check the pyarrow's doc seems chunk.values is chunk's value not all.\r\n\r\n### Steps to reproduce the bug\r\n\r\ncode provided above.\r\n\r\n### Expected behavior\r\n\r\nfeature.length * len(array) == len(array.values) should be true. and there should not has Exception.\r\n\r\n### Environment info\r\n\r\npython3.9\r\nx86_64\r\ndatasets: 2.14.4\r\npyarrow: 13.0.0 or 10.0.0<end_of_turn>\n<start_of_turn>model\ncast_column to Sequence with length=4 occur exception raise in datasets/table.py:2146<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen running `map` on parquet dataset loaded from local machine, the RAM usage increases linearly eventually leading to OOM. I was wondering if I should I save the `cache_file` after every n steps in order to prevent this?\n\n### Steps to reproduce the bug\n\n\r\n\r\n```\r\nds = load_dataset(\"parquet\", data_files=dataset_path, split=\"train\")\r\nds = ds.shard(num_shards=4, index=0)\r\n\r\nds = ds.cast_column(\"audio\", datasets.features.Audio(sampling_rate=16_000))\r\nds = ds.map(prepare_dataset,\r\n            num_proc=32,\r\n            writer_batch_size=1000,\r\n            keep_in_memory=False,\r\n            desc=\"preprocess dataset\")\r\n```\r\n\r\n```\r\ndef prepare_dataset(batch):\r\n    # load audio\r\n    sample = batch[\"audio\"]\r\n\r\n    inputs = feature_extractor(sample[\"array\"], sampling_rate=16000)\r\n    batch[\"input_values\"] = inputs.input_values[0]\r\n    batch[\"input_length\"] = len(sample[\"array\"].squeeze())\r\n\r\n    return batch\r\n```\n\n### Expected behavior\n\nIt shouldn't run into OOM problem.\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.4.0-91-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.19\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\n`map` with `num_proc` > 1 leads to OOM<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nsupport fsspec 2025.10.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis behavior is aligned with:\r\n- the behavior of `datasets` before merging #6519\r\n  - the behavior described in the corresponding docstring\r\n- the behavior of `huggingface_hub.create_repo`\r\n\r\nRevert \"Support push_to_hub canonical datasets (#6519)\"\r\n- This reverts commit a887ee78835573f5d80f9e414e8443b4caff3541.\r\n\r\nFix #6597.<end_of_turn>\n<start_of_turn>model\nSupport push_to_hub without org/user to default to logged-in user<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses #7607\r\n## New signatures\r\n### Audio\r\n```python\r\nAudio(sampling_rate: Optional[int] = None, mono: bool = True, decode: bool = True, stream_index: Optional[int] = None)\r\n\r\nAudio.encode_example(self, value: Union[str, bytes, bytearray, dict, \"AudioDecoder\"]) -> dict\r\n\r\nAudio.decode_example(self, value: dict, token_per_repo_id: Optional[dict[str, Union[str, bool, None]]] = None) -> \"AudioDecoder\":\r\n```\r\n\r\n### Video\r\n```python\r\nVideo(decode: bool = True, stream_index: Optional[int] = None, dimension_order: Literal['NCHW', 'NHWC'] = 'NCHW', num_ffmpeg_threads: int = 1, device: Optional[Union[str, \"torch.device\"]] = 'cpu', seek_mode: Literal['exact', 'approximate'] = 'exact')\r\n\r\nVideo.encode_example(self, value: Union[str, bytes, bytearray, Example, np.ndarray, \"VideoDecoder\"]) -> Example:\r\n\r\nVideo.decode_example(self, value: Union[str, Example], token_per_repo_id: Optional[dict[str, Union[bool, str]]] = None, ) -> \"VideoDecoder\":\r\n```\r\n\r\n\r\n\r\n## Notes\r\nAudio features constructor takes in 1 new optional param stream_index which is passed to the AudioDecoder constructor to select the stream index of a file.\r\nAudio feature can now take in torchcodec.decoders.AudioDecoder as input to encode_example()\r\nAudio feature decode_example() returns torchcodec.decoders.AudioDecoder\r\n\r\nVideo feature constructor takes in 5 new optional params stream_index, dimension_order, num_ffmpeg_threads, device, seek_mode all of which are passed to VideoDecoder constructor \r\nVideo feature decode_example() returns torchcodec.decoders.VideoDecoder\r\nVideo feature can now take in torchcodec.decoders.VideoDecoder as input to encode_example() \r\n\r\nAll test cases have been updated to reflect these changes\r\nAll documentation has also been updated to reflect these changes.\r\n\r\nBoth VideoDecoder and AudioDecoder when formatted with (np_formatter, tf_formatter, etc) will ignore the type and return themselves. Formatting test cases were updated accordingly to reflect this. (Pretty simple to make this not the case if we want though)\r\n\r\n## Errors\r\nThis test case from `tests/packaged_modules/test_audiofolder.py`\r\n```python\r\n@require_librosa\r\n@require_sndfile\r\n@pytest.mark.parametrize(\"streaming\", [False, True])\r\ndef test_data_files_with_metadata_and_archives(streaming, cache_dir, data_files_with_zip_archives):\r\n    audiofolder = AudioFolder(data_files=data_files_with_zip_archives, cache_dir=cache_dir)\r\n    audiofolder.download_and_prepare()\r\n    datasets = audiofolder.as_streaming_dataset() if streaming else audiofolder.as_dataset()\r\n    for split, data_files in data_files_with_zip_archives.items():\r\n        num_of_archives = len(data_files)  # the metadata file is inside the archive\r\n        expected_num_of_audios = 2 * num_of_archives\r\n        assert split in datasets\r\n        dataset = list(datasets[split])\r\n        assert len(dataset) == expected_num_of_audios\r\n        # make sure each sample has its own audio (all arrays are different) and metadata\r\n        assert (\r\n            sum(np.array_equal(dataset[0][\"audio\"].get_all_samples().data.numpy(), example[\"audio\"].get_all_samples().data.numpy()) for example in dataset[1:])\r\n            == 0\r\n        )\r\n        assert len({example[\"text\"] for example in dataset}) == expected_num_of_audios\r\n        assert all(example[\"text\"] is not None for example in dataset)\r\n```\r\nFails now because AudioDecoder needs to access the files after the lines below are run, but there seems to be some context issues. The file the decoder is trying to read is closed before the decoder gets the chance to decode it.\r\n```python\r\naudiofolder.download_and_prepare()\r\ndatasets = audiofolder.as_streaming_dataset() if streaming else audiofolder.as_dataset()\r\n```<end_of_turn>\n<start_of_turn>model\nTorchcodec decoding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd a feature to pass the key of the DatasetDict when performing map\n\n### Motivation\n\nI often preprocess using map on DatasetDict. \r\nSometimes, I need to preprocess train and valid data differently depending on the task. \r\nSo, I thought it would be nice to pass the key (like train, valid) when performing map on DatasetDict. \r\n\r\nWhat do you think?\n\n### Your contribution\n\nI can submit a pull request to add the feature to pass the key of the DatasetDict when performing map.<end_of_turn>\n<start_of_turn>model\nHow about adding a feature to pass the key when performing map on DatasetDict?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6412<end_of_turn>\n<start_of_turn>model\nRemove token value from warnings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nhttps://github.com/huggingface/datasets/blob/a91582de288d98e94bcb5ab634ca1cfeeff544c5/src/datasets/utils/file_utils.py#L510C1-L523C14<end_of_turn>\n<start_of_turn>model\nWhy call http_head() when fsspec_head()  succeeds<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nOnce I have locally built a large dataset that I want to push to hub, I use the recommended approach of .push_to_hub to get the dataset on the hub, and after pushing a few shards, it consistently hangs. This has happened over 40 times over the past week, and despite my best efforts to try and catch this happening and kill a process and restart, it seems to be extremely time wasting -- so I came to you to report this and to seek help. \r\n\r\nI already tried installing hf_transfer, but it doesn't support Byte file uploads so I uninstalled it.\n\n### Reproduction\n\n```python\r\nimport multiprocessing as mp\r\nimport pathlib\r\nfrom math import ceil\r\n\r\nimport datasets\r\nimport numpy as np\r\nfrom tqdm.auto import tqdm\r\n\r\nfrom tali.data.data import select_subtitles_between_timestamps\r\nfrom tali.utils import load_json\r\n\r\ntali_dataset_dir = \"/data/\"\r\n\r\nif __name__ == \"__main__\":\r\n    full_dataset = datasets.load_dataset(\r\n        \"Antreas/TALI\", num_proc=mp.cpu_count(), cache_dir=tali_dataset_dir\r\n    )\r\n\r\n    def data_generator(set_name, percentage: float = 1.0):\r\n        dataset = full_dataset[set_name]\r\n\r\n        for item in tqdm(dataset):\r\n            video_list = item[\"youtube_content_video\"]\r\n            video_list = np.random.choice(\r\n                video_list, int(ceil(len(video_list) * percentage))\r\n            )\r\n            if len(video_list) == 0:\r\n                continue\r\n            captions = item[\"youtube_subtitle_text\"]\r\n            captions = select_subtitles_between_timestamps(\r\n                subtitle_dict=load_json(\r\n                    captions.replace(\r\n                        \"/data/\",\r\n                        tali_dataset_dir,\r\n                    )\r\n                ),\r\n                starting_timestamp=0,\r\n                ending_timestamp=100000000,\r\n            )\r\n\r\n            for video_path in video_list:\r\n                temp_path = video_path.replace(\"/data/\", tali_dataset_dir)\r\n                video_path_actual: pathlib.Path = pathlib.Path(temp_path)\r\n\r\n                if video_path_actual.exists():\r\n                    item[\"youtube_content_video\"] = open(video_path_actual, \"rb\").read()\r\n                    item[\"youtube_subtitle_text\"] = captions\r\n                    yield item\r\n\r\n    train_generator = lambda: data_generator(\"train\", percentage=0.1)\r\n    val_generator = lambda: data_generator(\"val\")\r\n    test_generator = lambda: data_generator(\"test\")\r\n\r\n    train_data = datasets.Dataset.from_generator(\r\n        train_generator,\r\n        num_proc=mp.cpu_count(),\r\n        writer_batch_size=5000,\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    val_data = datasets.Dataset.from_generator(\r\n        val_generator,\r\n        writer_batch_size=5000,\r\n        num_proc=mp.cpu_count(),\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    test_data = datasets.Dataset.from_generator(\r\n        test_generator,\r\n        writer_batch_size=5000,\r\n        num_proc=mp.cpu_count(),\r\n        cache_dir=tali_dataset_dir,\r\n    )\r\n\r\n    dataset = datasets.DatasetDict(\r\n        {\r\n            \"train\": train_data,\r\n            \"val\": val_data,\r\n            \"test\": test_data,\r\n        }\r\n    )\r\n    succesful_competion = False\r\n    while not succesful_competion:\r\n        try:\r\n            dataset.push_to_hub(repo_id=\"Antreas/TALI-small\", max_shard_size=\"5GB\")\r\n            succesful_competion = True\r\n        except Exception as e:\r\n            print(e)\r\n```\n\n### Logs\n\n```shell\nPushing dataset shards to the dataset hub:  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                                                                            | 7/21 [24:33<49:06, 210.45s/it]\r\nError while uploading 'data/val-00007-of-00021-6b216a984af1a4c8.parquet' to the Hub.                                                                                                               \r\nPushing split train to the Hub.                                                                                                                                                                    \r\nResuming upload of the dataset shards.                                                                                                                                                             \r\nPushing dataset shards to the dataset hub: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46/46 [42:10<00:00, 55.01s/it]\r\nPushing split val to the Hub.                                                                                                                                                                      \r\nResuming upload of the dataset shards.                                                                                                                                                             \r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01<00:00,  1.55ba/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:23<00:00, 23.51s/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.39ba/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:30<00:00, 30.19s/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.28ba/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:24<00:00, 24.08s/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.42ba/s]\r\nUpload 1 LFS files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:23<00:00, 23.97s/it]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.49ba/s]\r\nCreating parquet from Arrow format: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.54ba/s^\r\nUpload 1 LFS files:   0%|                                                                                                                                                    | 0/1 [04:42<?, ?it/s]\r\nPushing dataset shards to the dataset hub:  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                                                      | 11/21 [17:23<15:48, 94.82s/it]\r\n\r\nThat's where it got stuck\n```\n\n\n### System info\n\n```shell\n- huggingface_hub version: 0.15.1\r\n- Platform: Linux-5.4.0-147-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Token path ?: /root/.cache/huggingface/token\r\n- Has saved token ?: True\r\n- Who am I ?: Antreas\r\n- Configured git credential helpers: store\r\n- FastAI: N/A\r\n- Tensorflow: N/A\r\n- Torch: 2.1.0.dev20230606+cu121\r\n- Jinja2: 3.1.2\r\n- Graphviz: N/A\r\n- Pydot: N/A\r\n- Pillow: 9.5.0\r\n- hf_transfer: N/A\r\n- gradio: N/A\r\n- numpy: 1.24.3\r\n- ENDPOINT: https://huggingface.co\r\n- HUGGINGFACE_HUB_CACHE: /root/.cache/huggingface/hub\r\n- HUGGINGFACE_ASSETS_CACHE: /root/.cache/huggingface/assets\r\n- HF_TOKEN_PATH: /root/.cache/huggingface/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\n```\n<end_of_turn>\n<start_of_turn>model\nPushing a large dataset on the hub consistently hangs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nA few months back I rewrote all docs at [https://albumentations.ai/docs](https://albumentations.ai/docs), and some pages changed their links.\r\n\r\nIn this PR fixed link to the most recent doc in Albumentations about bounding boxes and it's format. \r\n\r\nFix a few typos in the doc as well.<end_of_turn>\n<start_of_turn>model\nFix broken link to albumentations<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSaving a dataset `.to_json()` fails with a `ValueError` since the latest `pandas` [release](https://pandas.pydata.org/docs/dev/whatsnew/v2.1.0.html) (`2.1.0`)\r\n\r\nIn their latest release we have:\r\n\r\n> Improved error handling when using [DataFrame.to_json()](https://pandas.pydata.org/docs/dev/reference/api/pandas.DataFrame.to_json.html#pandas.DataFrame.to_json) with incompatible index and orient arguments ([GH 52143](https://github.com/pandas-dev/pandas/issues/52143))\r\n\r\ni.e. an error is now raised for invalid combinations of `index` and `orient`.\r\n\r\nThis means that unfortunately the custom logic at this line might sometimes lead to contradictions:\r\n\r\nhttps://github.com/huggingface/datasets/blob/029227a116c14720afca71b9b22e78eb2a1c09a6/src/datasets/io/json.py#L96\r\n\r\ne.g. for the default case `orient=records` leads to `index=True`, which now raises a `ValueError`\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\n\r\nif __name__ == '__main__':\r\n\r\n    dataset = datasets.Dataset.from_dict({\"A\": [1, 2, 3], \"B\": [4, 5, 6]})\r\n    dataset.to_json(\"dataset.json\")\r\n```\r\n\r\n```shell\r\n>>>\r\nValueError: 'index=True' is only valid when 'orient' is 'split', 'table', 'index', or 'columns'.\r\n```\n\n### Expected behavior\n\nThe dataset is successfully saved as `.json`\n\n### Environment info\n\n`python >= 3.9`\r\n`pandas >= 2.1.0`<end_of_turn>\n<start_of_turn>model\nValueError: 'index=True' is only valid when 'orient' is 'split', 'table', 'index', or 'columns'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7821\r\n\r\nIn addition to the solution proposed in the issue, I encountered we also need to support 64bit types when writing using the `RecordBatchStreamWriter`. \r\n\r\nNot sure if we want to create such large objects in the CI, but this is the only way to test that the issue is fixed, therefore I added the unit test.<end_of_turn>\n<start_of_turn>model\nFix ArrowInvalid for large variables<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrefactor: remove unnecessary else<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nattempt to fix https://github.com/huggingface/datasets/issues/6371<end_of_turn>\n<start_of_turn>model\ndo not try to download from HF GCS for generator<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix doc generation when `NamedSplit` is used as parameter default value.\r\n\r\nFix #7035.<end_of_turn>\n<start_of_turn>model\nFix doc generation when NamedSplit is used as parameter default value<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nThe dataset viewer feature is very nice. It enables to the user to easily view the dataset. However, when working for private companies we cannot always upload the dataset to the hub. Is there a way to create dataset viewer offline? I.e. to run a code that will open some kind of html or something that makes it easy to view the dataset.\n\n### Motivation\n\nI want to easily view my dataset even when it is hosted locally.\n\n### Your contribution\n\nN.A.<end_of_turn>\n<start_of_turn>model\nOffline dataset viewer<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7433\r\n\r\nThis refactor unifies `num_proc is None or num_proc == 1` and `num_proc > 1`; instead of handling them completely separately where one uses a list of kwargs and shards and the other just uses a single set of kwargs and `self`, by wrapping the `num_proc == 1` case in a list and making the difference just whether or not you use a pool, you set up either case to be able to load each other's cache files just by changing `num_shards`; `num_proc == 1` can sequentially load the shards of a dataset mapped `num_shards > 1` and map any missing shards\r\n\r\nOther than the structural refactor, the main contribution of this PR is `existing_cache_file_map`, which uses a regex of `cache_file_name` and `suffix_template` to find existing cache files, grouped by their `num_shards`; using this data structure, we can reset `num_shards` to an existing set of cache files, and load them accordingly<end_of_turn>\n<start_of_turn>model\nRefactor `Dataset.map` to reuse cache files mapped with different `num_proc`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI'm trying to split `IterableDataset` by `split_dataset_by_node`.\r\nBut when doing split on a already split dataset, the resulting `rank` is greater than `world_size`.\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere is the minimal code for reproduction:\r\n```py\r\n>>> from datasets import load_dataset\r\n>>> from datasets.distributed import split_dataset_by_node\r\n>>> dataset = load_dataset('fla-hub/slimpajama-test', split='train', streaming=True)      \r\n>>> dataset = split_dataset_by_node(dataset, 1, 32)\r\n>>> dataset._distributed\r\nDistributedConfig(rank=1, world_size=32)\r\n>>> dataset = split_dataset_by_node(dataset, 1, 15)\r\n>>> dataset._distributed                           \r\nDistributedConfig(rank=481, world_size=480)\r\n```\r\nAs you can see, the second rank 481 > 480, which is problematic.\r\n\r\n### Expected behavior\r\n\r\nI think this error comes from this line @lhoestq\r\nhttps://github.com/huggingface/datasets/blob/a6ccf944e42c1a84de81bf326accab9999b86c90/src/datasets/iterable_dataset.py#L2943-L2944\r\nWe may need to obtain the rank first. Then the above code gives\r\n```py\r\n>>> dataset._distributed                           \r\nDistributedConfig(rank=16, world_size=480)\r\n```\r\n\r\n### Environment info\r\n\r\ndatasets==2.20.0<end_of_turn>\n<start_of_turn>model\nProblematic rank after calling `split_dataset_by_node` twice<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a bug where calling `IterableDatasetDict.map()` or `IterableDataset.map()` with the default `features=None` argument would overwrite the existing `info.features` attribute with `None`. This, in turn, caused the resulting dataset to lose its schema, breaking downstream usage of attributes like `column_names`.\r\n\r\nWhy\r\nPreviously, the code would always set `info.features = features`, even if `features` was `None`. When mapping with removal of columns or other transformations, this led to the destruction of the schema and caused failures in code that relied on the dataset schema being present.\r\n\r\nHow\r\nWe now update `info.features` only if `features` is not `None`. This preserves the original schema unless the user explicitly provides a new one.\r\n\r\nReference\r\nFixes #7568<end_of_turn>\n<start_of_turn>model\nFix: Prevent loss of info.features and column_names in IterableDatasetDict.map when features is None<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFor datasets with lots of configs defined in YAML\r\n\r\nE.g. `load_dataset(\"uonlp/CulturaX\", \"fr\", revision=\"refs/pr/6\")` from >1min to 15sec<end_of_turn>\n<start_of_turn>model\nParallelize builder configs creation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...for faster dataset download when there are many many small files (e.g. imagefolder, audiofolder)\r\n\r\n### Behcnmark\r\n\r\nfor example on [lhoestq/tmp-images-writer_batch_size](https://hf.co/datasets/lhoestq/tmp-images-writer_batch_size) (128 images)\r\n\r\n|    | duration of the download step in `load_dataset()` |\r\n|--| ----------------------------------------------------------------------|\r\n| Before | 58s |\r\n| Now | 3s |\r\n\r\nThis should fix issues with the Dataset Viewer taking too much time to show up for imagefolder/audiofolder datasets.\r\n\r\n### Implementation details\r\n\r\nThe main change is in the `DownloadManager`:\r\n\r\n```diff\r\n- download_func = partial(self._download, download_config=download_config)\r\n+ download_func = partial(self._download_batched, download_config=download_config)\r\ndownloaded_path_or_paths = map_nested(\r\n    download_func,\r\n    url_or_urls,\r\n    map_tuple=True,\r\n    num_proc=download_config.num_proc,\r\n    desc=\"Downloading data files\",\r\n+   batched=True,\r\n+   batch_size=-1,\r\n)\r\n```\r\n\r\nand `_download_batched` is a multithreaded function.\r\n\r\nI only enable multithreading if there are more than 16 files and files are small though, otherwise the progress bar that counts the number of downloaded files is not fluid (updating when a big batch of big files are done downloading). To do so I simply check if the first file is smaller than 20MB.\r\n\r\nI also had to tweak `map_nested` to support batching. In particular it slices the data correctly if the user also enables multiprocessing.<end_of_turn>\n<start_of_turn>model\nMultithreaded downloads<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm not entirely sure if this should be submitted as a bug in the `datasets` library or the `huggingface_hub` library, given it could be fixed at different levels of the stack.\n\nThe fundamental issue is that the `load_datasets` api doesn't use the `.no_exist` files in the hub cache unlike other wrapper APIs that do. This is because the `utils.file_utils.cached_path` used directly calls `hf_hub_download` instead of using `file_download.try_to_load_from_cache` from `huggingface_hub` (see `transformers` library `utils.hub.cached_files` for one alternate example).\n\nThis results in unnecessary metadata HTTP requests occurring for files that don't exist on every call. It won't generate the .no_exist cache files, nor will it use them.\n\n### Steps to reproduce the bug\n\nRun the following snippet as one example (setting cache dirs to clean paths for clarity)\n`env HF_HOME=~/local_hf_hub python repro.py`\n```\nfrom datasets import load_dataset\n\nimport huggingface_hub\n\n# monkeypatch to print out metadata requests being made\noriginal_get_hf_file_metadata = huggingface_hub.file_download.get_hf_file_metadata\n\ndef get_hf_file_metadata_wrapper(*args, **kwargs):\n    print(\"File metadata request made (get_hf_file_metadata):\", args, kwargs)\n    return original_get_hf_file_metadata(*args, **kwargs)\n\n# Apply the patch\nhuggingface_hub.file_download.get_hf_file_metadata = get_hf_file_metadata_wrapper\n\ndataset = load_dataset(\n    \"Salesforce/wikitext\",\n    \"wikitext-2-v1\",\n    split=\"test\",\n    trust_remote_code=True,\n    cache_dir=\"~/local_datasets\",\n    revision=\"b08601e04326c79dfdd32d625aee71d232d685c3\",\n)\n```\n\nThis may be called over and over again, and you will see the same calls for files that don't exist:\n```\nFile metadata request made (get_hf_file_metadata): () {'url': 'https://huggingface.co/datasets/Salesforce/wikitext/resolve/b08601e04326c79dfdd32d625aee71d232d685c3/wikitext.py', 'proxies': None, 'timeout': 10, 'headers': {'user-agent': 'datasets/3.6.0; hf_hub/0.33.2; python/3.12.11; torch/2.7.0; huggingface_hub/0.33.2; pyarrow/20.0.0; jax/0.5.3'}, 'token': None}\nFile metadata request made (get_hf_file_metadata): () {'url': 'https://huggingface.co/datasets/Salesforce/wikitext/resolve/b08601e04326c79dfdd32d625aee71d232d685c3/.huggingface.yaml', 'proxies': None, 'timeout': 10, 'headers': {'user-agent': 'datasets/3.6.0; hf_hub/0.33.2; python/3.12.11; torch/2.7.0; huggingface_hub/0.33.2; pyarrow/20.0.0; jax/0.5.3'}, 'token': None}\nFile metadata request made (get_hf_file_metadata): () {'url': 'https://huggingface.co/datasets/Salesforce/wikitext/resolve/b08601e04326c79dfdd32d625aee71d232d685c3/dataset_infos.json', 'proxies': None, 'timeout': 10, 'headers': {'user-agent': 'datasets/3.6.0; hf_hub/0.33.2; python/3.12.11; torch/2.7.0; huggingface_hub/0.33.2; pyarrow/20.0.0; jax/0.5.3'}, 'token': None}\n```\n\nAnd you can see that the .no_exist folder is never created\n```\n$ ls ~/local_hf_hub/hub/datasets--Salesforce--wikitext/\nblobs  refs  snapshots\n```\n\n\n### Expected behavior\n\nThe expected behavior is for the print \"File metadata request made\" to stop after the first call, and for .no_exist directory & files to be populated under ~/local_hf_hub/hub/datasets--Salesforce--wikitext/\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-6.5.13-65-650-4141-22041-coreweave-amd64-85c45edc-x86_64-with-glibc2.35\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.33.2\n- PyArrow version: 20.0.0\n- Pandas version: 2.3.1\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nload_dataset does not check .no_exist files in the hub cache<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen pushing a `Dataset` with multiple `Features` (`input`, `output`, `tags`) to Huggingface Hub (private org repo), and later downloading the `Dataset`, only `input` and `output` load - I believe the expected behavior is for all `Features` to be loaded by default?\r\n\r\n### Steps to reproduce the bug\r\n\r\nPushing the data. `data_concat` is a `list` of `dict`s.\r\n```python\r\nfor datum in data_concat:\r\n    datum_tags = {d[\"key\"]: d[\"value\"] for d in datum[\"tags\"]}\r\n    split_fraction = # some logic that generates a train/test split number\r\n    if split_faction < test_fraction:\r\n        data_test.append(datum)\r\n    else:\r\n        data_train.append(datum)\r\n\r\ndataset = DatasetDict(\r\n    {\r\n        \"train\": Dataset.from_list(data_train),\r\n        \"test\": Dataset.from_list(data_test),\r\n        \"full\": Dataset.from_list(data_concat),\r\n    },\r\n)\r\n\r\ndataset_shuffled = dataset.shuffle(seed=shuffle_seed)\r\n\r\ndataset_shuffled.push_to_hub(\r\n    repo_id=hf_repo_id,\r\n    private=True,\r\n    config_name=m,\r\n    revision=revision,\r\n    token=hf_token,\r\n)\r\n```\r\n\r\nLoading it later:\r\n```python\r\n        dataset = datasets.load_dataset(\r\n            path=hf_repo_id,\r\n            name=name,\r\n            token=hf_token,\r\n        )\r\n```\r\n\r\nProduces:\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    full: Dataset({\r\n        features: ['input', 'output'],\r\n        num_rows: <obfuscated>\r\n    })\r\n})\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe expected result is below:\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    test: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n    full: Dataset({\r\n        features: ['input', 'output', 'tags'],\r\n        num_rows: <obfuscated>\r\n    })\r\n})\r\n```\r\n\r\nMy workaround is as follows:\r\n```python\r\ndsinfo = datasets.get_dataset_config_info(\r\n    path=data_files,\r\n    config_name=data_config,\r\n    token=hf_token,\r\n)\r\n\r\nallfeatures = dsinfo.features.copy()\r\nif \"tags\" not in allfeatures:\r\n    allfeatures[\"tags\"] = [{\"key\": Value(dtype=\"string\", id=None), \"value\": Value(dtype=\"string\", id=None)}]\r\n\r\ndataset = datasets.load_dataset(\r\n    path=data_files,\r\n    name=data_config,\r\n    features=allfeatures,\r\n    token=hf_token,\r\n)\r\n```\r\n\r\nInterestingly enough (and perhaps a related bug?), if I don't add the `tags` to `allfeatures` above (i.e. only loading `input` and `output`), it throws an error when executing `load_dataset`:\r\n\r\n```\r\nValueError: Couldn't cast\r\ntags: list<element: struct<key: string, value: string>>\r\n  child 0, element: struct<key: string, value: string>\r\n      child 0, key: string\r\n      child 1, value: string\r\ninput: <obfuscated>\r\noutput: <obfuscated>\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"tags\": [{\"key\": {\"dtype\": \"string\",' + 532\r\nto\r\n{'input': <obfuscated>, 'output': <obfuscated>\r\nbecause column names don't match\r\n```\r\n\r\nTraceback for this:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/bt/github/core/.venv/lib/python3.11/site-packages/datasets/load.py\", line 2152, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/bt/github/core/.venv/lib/python3.11/site-packages/datasets/builder.py\", line 948, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/bt/github/core/.venv/lib/python3.11/site-packages/datasets/builder.py\", line 1043, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Users/bt/github/core/.venv/lib/python3.11/site-packages/datasets/builder.py\", line 1805, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/Users/bt/github/core/.venv/lib/python3.11/site-packages/datasets/builder.py\", line 1950, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: macOS-14.0-arm64-arm-64bit\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nLoading HF Hub Dataset (private org repo) fails to load all features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #7767<end_of_turn>\n<start_of_turn>model\nCustom `dl_manager` in `load_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi,\r\n\r\nFollowing this <https://discuss.huggingface.co/t/how-to-preprocess-a-wikipedia-dataset-using-dataflowrunner/41991/3>, here is a simple PR to pass any additional args to datasets-cli as kwargs in the DatasetBuilder in `run_beam.py`.\r\n\r\nI also took the liberty to add missing setup steps to the `beam.mdx` docs in order to help everyone.\r\n\r\n@lhoestq <end_of_turn>\n<start_of_turn>model\nPass datasets-cli additional args as kwargs to DatasetBuilder in `run_beam.py`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfollowing https://github.com/huggingface/datasets/pull/6658<end_of_turn>\n<start_of_turn>model\nFix resuming arrow format<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHere, an example code is given:\r\nhttps://huggingface.co/docs/transformers/tasks/language_modeling\r\n\r\nThis code + article references the ELI5 dataset.\r\nELI5 is no longer available, as the ELI5 dataset page states: https://huggingface.co/datasets/eli5\r\n\"Defunct: Dataset \"eli5\" is defunct and no longer accessible due to unavailability of the source data.\r\nReddit recently [changed the terms of access](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/) to its API, making the source data for this dataset unavailable.\r\n\"\r\n\r\nPlease change the example code to use a different dataset.<end_of_turn>\n<start_of_turn>model\nELI5 no longer available, but referenced in example code<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nLet's say I have dataset A which has 100k examples, and dataset B which has 100m examples.\r\n\r\nI want to train on an interleaved dataset of A+B, with stopping_strategy='all_exhausted' so dataset B doesn't repeat any examples. But every time A is exhausted I want it to be reshuffled (eg. calling set_epoch)\r\n\r\nOf course I want to interleave as IterableDatasets / streaming mode so B doesn't have to get tokenized completely at the start.\r\n\r\nHow could I achieve this? I was thinking something like, if I wrap dataset A in some new IterableDataset with from_generator() and manually call set_epoch before interleaving it? But I'm not sure how to keep the number of shards in that dataset...\r\n\r\nSomething like\r\n\r\n```\r\ndataset_a = load_dataset(...)\r\ndataset_b = load_dataset(...)\r\n\r\ndef epoch_shuffled_dataset(ds):\r\n  # How to make this maintain the number of shards in ds??\r\n  for epoch in itertools.count():\r\n    ds.set_epoch(epoch)\r\n    yield from iter(ds)\r\n\r\nshuffled_dataset_a = IterableDataset.from_generator(epoch_shuffled_dataset, gen_kwargs={'ds': dataset_a})\r\ninterleaved = interleave_datasets([shuffled_dataset_a, dataset_b], probs, stopping_strategy='all_exhausted')\r\n```<end_of_turn>\n<start_of_turn>model\nHow to set_epoch with interleave_datasets?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsince the dataset is defunct<end_of_turn>\n<start_of_turn>model\nremove eli5 test<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nLoad_dataset from a folder of images doesn't seem to support soft links. It would be nice if it did, especially during methods development where image folders are being curated.\n\n### Motivation\n\nImages are coming from a complex variety of sources and we'd like to be able to soft link directly from the originating folders as opposed to copying. Having a copy of the file ensures that there may be issues with image versioning as well as having double the amount of required disk space.\n\n### Your contribution\n\nN/A<end_of_turn>\n<start_of_turn>model\nSupport soft links for load_datasets imagefolder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollowing up on https://github.com/huggingface/datasets/pull/7840 I asked claude code to check for undocumented parameters for other features and it found:\n- mode parameter on video is documented but unused: https://github.com/huggingface/datasets/blob/main/src/datasets/features/video.py#L48-L49\n- the same goes for the mode parameter on the pdf feature: https://github.com/huggingface/datasets/blob/main/src/datasets/features/pdf.py#L47-L48\n\nI assume checking if these modes can be supported and otherwise removing them is the way to go here.\n<end_of_turn>\n<start_of_turn>model\nDOC: `mode` parameter on pdf and video features unused<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I run my SD training in a Docker image and then listen to logs like `docker logs train -f`, the progress bar is delayed.\r\nIt's updating every few percent.\r\nWhen you have a large dataset that has to be mapped (like 1+ million samples), it's crucial to see the updates in real-time, not every couple hours to make sure nothing got frozen or broken\n\n### Steps to reproduce the bug\n\n1. Run any huge dataset processing as a Docker image\r\n2. `docker logs image_name` to it\n\n### Expected behavior\n\n...\n\n### Environment info\n\n...<end_of_turn>\n<start_of_turn>model\nLogs are delayed when doing .map when `docker logs`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix wrong link. \r\n\r\ncsv kwargs docstring link was pointing to pandas json docs.<end_of_turn>\n<start_of_turn>model\n[MINOR:TYPO] Update arrow_dataset.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nImageFolder with metadata.jsonl error. I downloaded liuhaotian/LLaVA-CC3M-Pretrain-595K locally from Hugging Face. According to the tutorial in https://huggingface.co/docs/datasets/image_dataset#image-captioning, only put images.zip and metadata.jsonl containing information in the same folder. However, after loading, an error was reported: One or several metadata.jsonl were found, but not in the same directory or in a parent directory of.\r\n\r\nThe data in my jsonl file is as follows:\r\n\r\n> {\"id\": \"GCC_train_002448550\", \"file_name\": \"GCC_train_002448550.jpg\", \"conversations\": [{\"from\": \"human\", \"value\": \"<image>\\nProvide a brief description of the given image.\"}, {\"from\": \"gpt\", \"value\": \"a view of a city , where the flyover was proposed to reduce the increasing traffic on thursday .\"}]}\r\n\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\nimage = load_dataset(\"imagefolder\",data_dir='data/opensource_data')\n\n### Expected behavior\n\nsuccess\n\n### Environment info\n\ndatasets==3.2.0<end_of_turn>\n<start_of_turn>model\nOne or several metadata.jsonl were found, but not in the same directory or in a parent directory of<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe audio column remain as non-decoded objects even when accessing them.\n\n```python\ndataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")\ndataset[0] # see that it doesn't show 'array' etc...\n```\n\nWorks fine with `datasets==3.6.0`\n\nFollowed the docs in \n\n- https://huggingface.co/docs/datasets/en/audio_load\n\n### Steps to reproduce the bug\n\n```python\ndataset = load_dataset(\"MrDragonFox/Elise\", split = \"train\")\ndataset[0] # see that it doesn't show 'array' etc...\n```\n\n\n### Expected behavior\n\nIt should decode when accessing the elemenet\n\n### Environment info\n\n4.1.1\nubuntu 22.04\n\n\nRelated\n\n- https://github.com/huggingface/datasets/issues/7707<end_of_turn>\n<start_of_turn>model\nAudio dataset is not decoding on 4.1.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nsee the subject\n\n### Steps to reproduce the bug\n\nregular tests\n\n### Expected behavior\n\nn/a\n\n### Environment info\n\nversion 2.20.0<end_of_turn>\n<start_of_turn>model\nMissing pyspark dependency causes the testsuite to error out, instead of a few tests to be skipped<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi, I use the code below to load local file\r\n```\r\n    def _split_generators(self, dl_manager):\r\n        # TODO: This method is tasked with downloading/extracting the data and defining the splits depending on the configuration\r\n        # If several configurations are possible (listed in BUILDER_CONFIGS), the configuration selected by the user is in self.config.name\r\n\r\n        # dl_manager is a datasets.download.DownloadManager that can be used to download and extract URLS\r\n        # It can accept any type or nested list/dict and will give back the same structure with the url replaced with path to local files.\r\n        # By default the archives will be extracted and a path to a cached folder where they are extracted is returned instead of the archive\r\n        # urls = _URLS[self.config.name]\r\n        data_dir = dl_manager.download_and_extract(_URLs)\r\n        print(data_dir)\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN,\r\n                # These kwargs will be passed to _generate_examples\r\n                gen_kwargs={\r\n                    \"filepath\": os.path.join(data_dir[\"train\"]),\r\n                    \"split\": \"train\",\r\n                },\r\n            ),\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.VALIDATION,\r\n                # These kwargs will be passed to _generate_examples\r\n                gen_kwargs={\r\n                    \"filepath\": os.path.join(data_dir[\"dev\"]),\r\n                    \"split\": \"dev\",\r\n                },\r\n            ),\r\n        ]\r\n```\r\nand error occured\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/zhizhou/data1/zhanghao/huggingface/FineTuning_Transformer/load_local_dataset.py\", line 2, in <module>\r\n    dataset = load_dataset(\"./QA_script.py\",data_files='/home/zhizhou/.cache/huggingface/datasets/conversatiom_corps/part_file.json')\r\n  File \"/home/zhizhou/anaconda3/envs/pytorch/lib/python3.10/site-packages/datasets/load.py\", line 1809, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/zhizhou/anaconda3/envs/pytorch/lib/python3.10/site-packages/datasets/builder.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/zhizhou/anaconda3/envs/pytorch/lib/python3.10/site-packages/datasets/builder.py\", line 1670, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/home/zhizhou/anaconda3/envs/pytorch/lib/python3.10/site-packages/datasets/builder.py\", line 992, in _download_and_prepare\r\n    if str(split_generator.split_info.name).lower() == \"all\":\r\nAttributeError: 'str' object has no attribute 'split_info'. Did you mean: 'splitlines'?\r\n```\r\nCould you help me?<end_of_turn>\n<start_of_turn>model\n  File \"/home/zhizhou/anaconda3/envs/pytorch/lib/python3.10/site-packages/datasets/builder.py\", line 992, in _download_and_prepare     if str(split_generator.split_info.name).lower() == \"all\": AttributeError: 'str' object has no attribute 'split_info'. Did you mean: 'splitlines'?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHey!\r\nFirstly, thanks for maintaining such framework!\r\n\r\nI had a small issue, where I wanted to load a custom dataset of image+text captioning. I had all of my images in a single directory, and one of the images had the name `train.png`. Then, the loaded dataset had only this image.\r\n\r\nI guess it's related to \"train\" as a split name, but it's definitely an unexpected behavior :)\r\nUnfortunately I don't have time to submit a proper PR. I'm attaching a toy example to reproduce the issue.\r\n\r\nThanks,\r\nSagi\n\n### Steps to reproduce the bug\n\nAll of the steps I'm attaching are in a fresh env :)\r\n\r\n```\r\n(base) sagipolaczek@Sagis-MacBook-Pro ~ % conda activate hf_issue_env\r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % python --version\r\nPython 3.10.15\r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % pip list | grep datasets\r\ndatasets           3.0.1\r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % ls -la Documents/hf_datasets_issue          \r\ntotal 352\r\ndrwxr-xr-x   6 sagipolaczek  staff     192 Oct  7 11:59 .\r\ndrwx------@ 23 sagipolaczek  staff     736 Oct  7 11:46 ..\r\n-rw-r--r--@  1 sagipolaczek  staff      72 Oct  7 11:59 metadata.csv\r\n-rw-r--r--@  1 sagipolaczek  staff  160154 Oct  6 18:00 pika.png\r\n-rw-r--r--@  1 sagipolaczek  staff    5495 Oct  6 12:02 pika_pika.png\r\n-rw-r--r--@  1 sagipolaczek  staff    1753 Oct  6 11:50 train.png\r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % cat Documents/hf_datasets_issue/metadata.csv\r\nfile_name,text\r\ntrain.png,A train\r\npika.png,Pika\r\npika_pika.png,Pika Pika!\r\n\r\n\r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % python                                      \r\nPython 3.10.15 (main, Oct  3 2024, 02:33:33) [Clang 14.0.6 ] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"Documents/hf_datasets_issue/\")\r\n>>> dataset\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'text'],\r\n        num_rows: 1\r\n    })\r\n})\r\n>>> dataset[\"train\"][0]\r\n{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=354x84 at 0x10B50FD90>, 'text': 'A train'}\r\n\r\n### DELETING `train.png` sample ###\r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % vim Documents/hf_datasets_issue/metadata.csv\r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % rm Documents/hf_datasets_issue/train.png \r\n(hf_issue_env) sagipolaczek@Sagis-MacBook-Pro ~ % python                                      \r\nPython 3.10.15 (main, Oct  3 2024, 02:33:33) [Clang 14.0.6 ] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"imagefolder\", data_dir=\"Documents/hf_datasets_issue/\")\r\nGenerating train split: 2 examples [00:00, 65.99 examples/s]\r\n>>> dataset\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'text'],\r\n        num_rows: 2\r\n    })\r\n})\r\n>>> dataset[\"train\"]\r\nDataset({\r\n    features: ['image', 'text'],\r\n    num_rows: 2\r\n})\r\n>>> dataset[\"train\"][0],dataset[\"train\"][1]\r\n({'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=2356x1054 at 0x10DD11E70>, 'text': 'Pika'}, {'image': <PIL.PngImagePlugin.PngImageFile image mode=RGBA size=343x154 at 0x10E258C70>, 'text': 'Pika Pika!'})\r\n```\n\n### Expected behavior\n\nMy expected behavior would be to get a dataset with the sample `train.png` in it (along with the others data points).\r\n\r\n\n\n### Environment info\n\nI've attached it in the example:\r\n\r\nPython 3.10.15\r\ndatasets           3.0.1\r\n<end_of_turn>\n<start_of_turn>model\n`load_dataset()` of images from a single directory where `train.png` image exists<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport passing `--num_proc` to CLI test.\r\n\r\nThis was really useful recently to run the command on `pubmed`: https://huggingface.co/datasets/pubmed/discussions/11<end_of_turn>\n<start_of_turn>model\nMake CLI test support multi-processing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/5953\r\n\r\n<img width=\"1292\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/42851186/270fe5bc-1739-4878-b7bc-ab6d35336d4d\">\r\n<end_of_turn>\n<start_of_turn>model\nBetter filenotfound for gated<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes the bug when applying map to an arrow-formatted iterable dataset described here:\r\n\r\nhttps://github.com/huggingface/datasets/issues/6833#issuecomment-2399903885\r\n\r\n```python\r\n\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"rotten_tomatoes\", split=\"train\", streaming=True)\r\nds = ds.with_format(\"arrow\").map(lambda x: x)\r\nfor ex in ds:\r\n    pass\r\n```\r\n\r\n@lhoestq <end_of_turn>\n<start_of_turn>model\nfix unbatched arrow map for iterable datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nCreate battery_analysis.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 4.1.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFor example, https://huggingface.co/datasets/mlfoundations/dclm-baseline-1.0 dataset files have `.zstd` extension which is currently ignored (only `.zst` is registered).<end_of_turn>\n<start_of_turn>model\nRegister `.zstd` extension for zstd-compressed files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nwhen working with a really large dataset it would save us a lot of time (and compute resources) to use either with_transform or the set_transform from the Dataset class instead of waiting for the entire dataset to map\n\n### Motivation\n\ndon't want to wait for a really long dataset to map, this would give IterableDataset an extra advantage over the Dataset class.\r\nreducing time and resources\n\n### Your contribution\n\nI am a little busy with my job search lately, but would post about this feature in my social media.\r\nApologies again (dad going to kick me out soon), if I ever have some free time I will contribute to making this a reality, but that's going to be hard\r\n  \u00a0\u00a0\u00a0 / (\u252c\u252c\ufe4f\u252c\u252c)\\<end_of_turn>\n<start_of_turn>model\nadd `with_transform` and/or `set_transform` to IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen trying to use to_tf_dataset with a custom data_loader collate_fn when I use parallelism I am met with the following error as many times as number of workers there were in ``num_workers``.\r\n\r\n  File \"/opt/miniconda/envs/env/lib/python3.11/site-packages/multiprocess/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/opt/miniconda/envs/env/lib/python3.11/site-packages/multiprocess/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/opt/miniconda/envs/env/lib/python3.11/site-packages/datasets/utils/tf_utils.py\", line 438, in worker_loop\r\n    tf.config.set_visible_devices([], \"GPU\")  # Make sure workers don't try to allocate GPU memory\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/miniconda/envs/env/lib/python3.11/site-packages/tensorflow/python/framework/config.py\", line 566, in set_visible_devices\r\n    context.context().set_visible_devices(devices, device_type)\r\n  File \"/opt/miniconda/envs/env/lib/python3.11/site-packages/tensorflow/python/eager/context.py\", line 1737, in set_visible_devices\r\n    raise RuntimeError(\r\nRuntimeError: Visible devices cannot be modified after being initialized\n\n### Steps to reproduce the bug\n\n1. Download a dataset using HuggingFace load_dataset\r\n2. Define a function that transforms the data in some way to be used in the collate_fn argument\r\n3. Provide a ``batch_size`` and ``num_workers`` value in the ``to_tf_dataset`` function\r\n4. Either retrieve directly or use tfds benchmark to test the dataset\r\n\r\n``` python\r\nfrom datasets import load_datasets\r\nimport tensorflow_datasets as tfds\r\nfrom keras_cv.layers import Resizing\r\n\r\ndef data_loader(examples):\r\n      x = Resizing(examples[0]['image'], 256, 256, crop_to_aspect_ratio=True)\r\n      return {X[0]: x}\r\n\r\nds = load_datasets(\"logasja/FDF\", split=\"test\")\r\n\r\nds = ds.to_tf_dataset(collate_fn=data_loader, batch_size=16, num_workers=2)\r\n\r\ntfds.benchmark(ds)\r\n```\n\n### Expected behavior\n\nUse multiple processes to apply transformations from the collate_fn to the tf dataset on the CPU.\n\n### Environment info\n\n- `datasets` version: 2.19.1\r\n- Platform: Linux-6.5.0-1023-oracle-x86_64-with-glibc2.35\r\n- Python version: 3.11.8\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nto_tf_dataset: Visible devices cannot be modified after being initialized<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe second line starting with the bracket doesn't properly render on huggingface/docs. Added \"> \" to address it.\r\n\r\nIn the client documentation, the markdown 'TIP' paragraph in docs/stream#shuffle is not well executed, not as the other in the same page / while markdown is correctly considering it.\r\n\r\nDocumentation:\r\nhttps://huggingface.co/docs/datasets/v4.3.0/en/stream#shuffle:~:text=%5B!TIP%5D%5BIterableDataset.shuffle()%5D(/docs/datasets/v4.3.0/en/package_reference/main_classes%23datasets.IterableDataset.shuffle)%20will%20also%20shuffle%20the%20order%20of%20the%20shards%20if%20the%20dataset%20is%20sharded%20into%20multiple%20files.\r\n\r\nGithub source:\r\nhttps://github.com/huggingface/datasets/blob/main/docs/source/stream.mdx#:~:text=Casting%20only%20works%20if%20the%20original%20feature%20type%20and%20new%20feature%20type%20are%20compatible.%20For%20example%2C%20you%20can%20cast%20a%20column%20with%20the%20feature%20type%20Value(%27int32%27)%20to%20Value(%27bool%27)%20if%20the%20original%20column%20only%20contains%20ones%20and%20zeros.<end_of_turn>\n<start_of_turn>model\nFix: Properly render [!TIP] block in stream.shuffle documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi team, first off, I love the datasets library! \ud83e\udd70\n\nI'm encountering a potential memory leak / increasing memory usage when training a model on a very large DatasetDict.\n\nSetup: I have a DatasetDict containing 362 distinct datasets, which sum up to ~2.8 billion rows.\n\nTraining Task: I'm performing contrastive learning with SentenceTransformer and Accelerate on a single node with 4 H100, which requires me to sample from only one dataset at a time.\n\nTraining Loop: At each training step, I sample ~16,000 examples from a single dataset, and then switch to a different dataset for the next step. I iterate through all 362 datasets this way. \n\nProblem: The process's memory usage continuously increases over time, eventually causing a stale status where GPUs would stop working. It seems memory from previously sampled datasets isn't being released. I've set num_workers=0 for all experiments.\n\nChart 1: Standard DatasetDict The memory usage grows steadily until it make the training stale (RSS memory) <img width=\"773\" height=\"719\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6606bef5-1153-4f2d-bf08-82da249d6e8d\" />\n\nChart 2: IterableDatasetDict I also tried  to use IterableDatasetDict and IterableDataset. The memory curve is \"smoother,\" but the result is the same: it grows indefinitely and the training become stale. <img width=\"339\" height=\"705\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ee90c1a1-6c3b-4135-9edc-90955cb1695a\" />\n\nAny feedback or guidance on how to manage this memory would be greatly appreciated!\n\n### Steps to reproduce the bug\n\nWIP, I'll add some code that manage to reproduce this error, but not straightforward. \n\n### Expected behavior\n\nThe memory usage should remain relatively constant or plateau after a few steps. Memory used for sampling one dataset should be released before or during the sampling of the next dataset.\n\n### Environment info\n\nPython: 3.12\nDatasets: 4.3.0\nSentenceTransformers: 5.1.1<end_of_turn>\n<start_of_turn>model\nMemory leak / Large memory usage with num_workers = 0 and numerous dataset within DatasetDict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRemove delete doc CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSimilar issue to https://github.com/huggingface/datasets/issues/5985, but across different sessions rather than two calls in the same session.\r\n\r\nUnlike that issue, explicitly calling tokenizer(my_args) before the map() doesn't help, because the tokenizer was created with a different hash to begin with...\r\n\r\nsetup\r\n```\r\nfrom transformers import AutoTokenizer\r\nAutoTokenizer.from_pretrained('bert-base-uncased').save_pretrained(\"tok\")\r\n```\r\n\r\nthis prints different value each time\r\n```\r\nfrom transformers import AutoTokenizer\r\nfrom datasets.utils.py_utils import dumps # Huggingface datasets\r\nprint(hash(dumps(AutoTokenizer.from_pretrained(\"tok\"))))\r\n```<end_of_turn>\n<start_of_turn>model\nMap cache with tokenizer<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am unable to load the wiki_auto_asset_turk dataset. I get a fatal error while trying to access wiki_auto_asset_turk and load it with datasets.load_dataset. The error (TypeError: expected str, bytes or os.PathLike object, not NoneType) is from filenames_for_dataset_split in a os.path.join call \r\n\r\n\r\n>>import datasets\r\n>>print (datasets.__version__)\r\n>>dataset = datasets.load_dataset(\"GEM/wiki_auto_asset_turk\")\r\n\r\nSystem output:\r\n\r\nGenerating train split: 100%|\u2588| 483801/483801 [00:03<00:00, 127164.26 examples/s\r\nGenerating validation split: 100%|\u2588| 20000/20000 [00:00<00:00, 116052.94 example\r\nGenerating test_asset split: 100%|\u2588\u2588| 359/359 [00:00<00:00, 76155.93 examples/s]\r\nGenerating test_turk split: 100%|\u2588\u2588\u2588| 359/359 [00:00<00:00, 87691.76 examples/s]\r\nTraceback (most recent call last):\r\n  File \"/Users/abhinav.sethy/Code/openai_evals/evals/evals/grammarly_tasks/gem_sari.py\", line 3, in <module>\r\n    dataset = datasets.load_dataset(\"GEM/wiki_auto_asset_turk\")\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/load.py\", line 2582, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/builder.py\", line 1005, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/builder.py\", line 1767, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/builder.py\", line 1100, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/builder.py\", line 1565, in _prepare_split\r\n    split_info = self.info.splits[split_generator.name]\r\n                 ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/splits.py\", line 532, in __getitem__\r\n    instructions = make_file_instructions(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/arrow_reader.py\", line 121, in make_file_instructions\r\n    info.name: filenames_for_dataset_split(\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/datasets/naming.py\", line 72, in filenames_for_dataset_split\r\n    prefix = os.path.join(path, prefix)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"<frozen posixpath>\", line 76, in join\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\n\n### Steps to reproduce the bug\n\nimport datasets\r\nprint (datasets.__version__)\r\ndataset = datasets.load_dataset(\"GEM/wiki_auto_asset_turk\")\r\n\n\n### Expected behavior\n\nShould be able to load the dataset without any issues\n\n### Environment info\n\ndatasets version 2.18.0 (was able to reproduce bug with older versions 2.16 and 2.14 also)\r\nPython 3.12.0<end_of_turn>\n<start_of_turn>model\nUnable to load wiki_auto_asset_turk from GEM<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAllow metadata files to have an identifying preface. Specifically, it will recognize files with `-metadata.csv` or `_metadata.csv` as metadata files for the purposes of the dataset viewer functionality.\r\n\r\nResolves #7123.<end_of_turn>\n<start_of_turn>model\nAllow for variation in metadata file names as per issue #7123<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nClose #6690.<end_of_turn>\n<start_of_turn>model\nAdd CLI function to convert script-dataset to Parquet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen a dataset with upper-cases in its name is first loaded using `load_dataset()`, the local cache directory is created with all lowercase letters. \r\n\r\nHowever, upon subsequent loads, the current version attempts to locate the cache directory using the dataset's original name, which includes uppercase letters. This discrepancy can lead to confusion and, particularly in offline mode, results in errors. \r\n\r\n### Reproduce\r\n```bash\r\n~$ python            \r\nPython 3.9.19 (main, Mar 21 2024, 17:11:28) \r\n[GCC 11.2.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"locuslab/TOFU\", \"full\")\r\n>>> quit()\r\n\r\n~$ export HF_DATASETS_OFFLINE=1 \r\n~$ python                      \r\nPython 3.9.19 (main, Mar 21 2024, 17:11:28) \r\n[GCC 11.2.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset(\"locuslab/TOFU\", \"full\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"xxxxxx/anaconda3/envs/llm/lib/python3.9/site-packages/datasets/load.py\", line 2556, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"xxxxxx/anaconda3/envs/llm/lib/python3.9/site-packages/datasets/load.py\", line 2228, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"xxxxxx/anaconda3/envs/llm/lib/python3.9/site-packages/datasets/load.py\", line 1871, in dataset_module_factory\r\n    raise ConnectionError(f\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\") from None\r\nConnectionError: Couldn't reach the Hugging Face Hub for dataset 'locuslab/TOFU': Offline mode is enabled.\r\n>>> \r\n```\r\n\r\nI fix this issue by lowering the dataset name (`.lower()`) when generating cache_dir.<end_of_turn>\n<start_of_turn>model\nFix issue with case sensitivity when loading dataset from local cache<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nhttps://github.com/huggingface/datasets/blame/38d4d0e11e22fdbc4acf373d2421d25abeb43439/src/datasets/arrow_dataset.py#L5925C10-L5925C10\n\nThe example seems to suggest that dataset.add_column() can add column inplace, however, this is wrong -- it cannot. It returns a new dataset with the column added to it.<end_of_turn>\n<start_of_turn>model\nCode example for dataset.add_column() does not reflect correct way to use function<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAllow for `Sequence` of `Image` (or `Audio`) to be embedded inside the shards.\n\n### Motivation\n\nCurrently, thanks to #3685, when `embed_external_files` is set to True (which is the default) in `push_to_hub`, features of type `Image` and `Audio` are embedded inside the arrow/parquet shards, instead of only storing paths to the files.\r\n\r\nI've noticed that this behavior does not extend to `Sequence` of `Image`, when working with a [dataset of timelapse images](https://huggingface.co/datasets/1aurent/Human-Embryo-Timelapse).\n\n### Your contribution\n\nI'll submit a PR if I find a way to add this feature<end_of_turn>\n<start_of_turn>model\n Add support for `Sequence(Audio/Image)` feature in `push_to_hub`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR is a hotfix of:\r\n- #6207\r\n\r\nThat PR introduced the filtering out of `.zip` extensions. This PR reverts that.\r\n\r\nHot fix #6207.\r\n\r\nMaybe we should do patch releases: the bug was introduced in 2.13.1.\r\n\r\nCC: @lhoestq <end_of_turn>\n<start_of_turn>model\nDo not filter out .zip extensions from no-script datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport `ndjson` (Newline Delimited JSON) data files.\r\n\r\nFix #7153.<end_of_turn>\n<start_of_turn>model\nSupport ndjson data files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix export to JSON (`lines=False`) when dataset larger than batch size.\r\n\r\nFix #7037.<end_of_turn>\n<start_of_turn>model\nFix export to JSON when dataset larger than batch size<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis is no longer a pressing issue, but for completeness I am reporting that in August 26th, GET requests to `https://datasets-server.huggingface.co/info\\?dataset\\=livebench/math` were returning 502s when invoked from [github actions](https://github.com/UKGovernmentBEIS/inspect_evals/actions/runs/17241892475/job/48921123754) (that link will expire eventually, [here are the logs](https://github.com/user-attachments/files/22233578/logs_44225296943.zip)). \n\nWhen invoked from actions, it appeared to be consistently failing for ~6 hours. However, these 502s never occurred when the request was invoked from my local machine in that same time period.\n\nI suspect that this is related to how the requests are routed with github actions versus locally. \n\nIts not clear to me if the request even reached huggingface servers or if its the github proxy that stopped it from going through, but I wanted to report it nonetheless in case this is helpful information. I'm curious if huggingface can do anything on their end to confirm cause. \n\nAnd a feature request for if this happens in the future (assuming huggingface has visibilty on it):  A \"datasets status\" page highlighting if 502s occur for  specific individual datasets could be useful for people debugging on the other end of this!\n\n\n<end_of_turn>\n<start_of_turn>model\nComment/feature request: Huggingface 502s from GHA<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWe are trying to load datasets where the image column stores `PIL.PngImagePlugin.PngImageFile` images. However, iterating over these datasets is extremely slow. \nWhat I have found:\n1. It is the presence of the image column that causes the slowdown. Removing the column from the dataset results in blazingly fast (as expected) times\n2. It is ~2x faster to iterate when the column contains a single image as opposed to a list of images i.e., the feature is a Sequence of Image objects. We often need multiple images per sample, so we need to work with a list of images\n3. It is ~17x faster to store paths to PNG files and load them using `PIL.Image.open`, as opposed to iterating over a `Dataset` with an Image column, and ~30x faster compared to `Sequence` of `Image`s. See a simple script below with an openly available dataset.\n\n\nIt would be great to understand the standard practices for storing and loading multimodal datasets (image + text). \n\nhttps://huggingface.co/docs/datasets/en/image_load seems a bit underdeveloped? (e.g., `dataset.decode` only works with `IterableDataset`, but it's not clear from the doc)\n\nThanks!\n\n```python\nfrom datasets import load_dataset, load_from_disk\nfrom PIL import Image\nfrom pathlib import Path\n\nds = load_dataset(\"getomni-ai/ocr-benchmark\")\n\nfor idx, sample in enumerate(ds[\"test\"]):\n    image = sample[\"image\"]\n    image.save(f\"/tmp/ds_files/images/image_{idx}.png\")\n\nds.save_to_disk(\"/tmp/ds_columns\")\n\n# Remove the 'image' column\nds[\"test\"] = ds[\"test\"].remove_columns([\"image\"])\n\n# Create image paths for each sample\nimage_paths = [f\"images/image_{idx}.png\" for idx in range(len(ds[\"test\"]))]\n\n# Add the 'image_path' column to the dataset\nds[\"test\"] = ds[\"test\"].add_column(\"image_path\", image_paths)\n\n# Save the updated dataset\nds.save_to_disk(\"/tmp/ds_files\")\nfiles_path = Path(\"/tmp/ds_files\")\ncolumn_path = Path(\"/tmp/ds_columns\")\n\n# load and benchmark\nds_file = load_from_disk(files_path)\nds_column = load_from_disk(column_path)\nimport time\n\n\nimages_files = []\nstart = time.time()\nfor idx in range(len(ds_file[\"test\"])):\n    image_path = files_path / ds_file[\"test\"][idx][\"image_path\"]\n    image = Image.open(image_path)\n    images_files.append(image)\nend = time.time()\nprint(f\"Time taken to load images from files: {end - start} seconds\")\n\n# Time taken to load images from files: 1.2364635467529297 seconds\n\n\nimages_column = []\nstart = time.time()\nfor idx in range(len(ds_column[\"test\"])):\n    images_column.append(ds_column[\"test\"][idx][\"image\"])\nend = time.time()\nprint(f\"Time taken to load images from columns: {end - start} seconds\")\n\n# Time taken to load images from columns: 20.49347186088562 seconds\n```<end_of_turn>\n<start_of_turn>model\nIterating over Image feature columns is extremely slow<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdding an option to skip corrupted data samples. Currently the datasets behavior is throwing errors if the data sample if corrupted and let user aware and handle the data corruption. When I tried to try-catch the error at user level, the iterator will raise StopIteration when I called next() again. \n\nThe way I try to do error handling is: (This doesn't work, unfortunately)\n\n```\n        # Load the dataset with streaming enabled\n        dataset = load_dataset(\n            \"pixparse/cc12m-wds\", split=\"train\", streaming=True\n        )\n        # Get an iterator from the dataset\n        iterator = iter(dataset)\n\n        while True:\n            try:\n                # Try to get the next example\n                example = next(iterator)\n                \n                # Try to access and process the image\n                image = example[\"jpg\"]\n                pil_image = Image.fromarray(np.array(image))\n                pil_image.verify()  # Verify it's a valid image file\n\n            except StopIteration:  # Code path 1\n                print(\"\\nStopIteration was raised! Reach the end of dataset\")\n                raise StopIteration\n\n            except Exception as e:  # Code path 2\n                errors += 1\n                print(\"Error! Skip this sample\")\n                cotinue\n            else:\n                successful += 1\n```\n\nThis is because the `IterableDataset` already throws an error (reaches Code path 2). And if I continue call next(), it will hit Code path 1. This is because the inner iterator of `IterableDataset`([code](https://github.com/huggingface/datasets/blob/89bd1f971402acb62805ef110bc1059c38b1c8c6/src/datasets/iterable_dataset.py#L2242)) as been stopped, so calling next() on it will raise StopIteration. \n\nSo I can not skip the corrupted data sample in this way. Would also love to hear any suggestions about creating a robust dataloader.\n\nThanks for your help in advance!\n\n\n### Motivation\n\n## Public dataset corruption might be common\nA lot of users would use public dataset, and the public dataset might contains some corrupted data, especially for dataset with image / video etc.  I totally understand it's dataset owner and user's responsibility to ensure the data integrity / run data cleaning or preprocessing, but it would be easier for developers who would use the dataset\n\n## Use cases\nFor example, a robust dataloader would be easy for users who want to try quick tests on different dataset, and chose one dataset which fits their needs. So user could use IterableDataloader with `stream=True` to use the dataset easily without downloading and removing corrupted data samples from the dataset.\n\n\n### Your contribution\n\nThe error handling might not trivial and might need more careful design. <end_of_turn>\n<start_of_turn>model\nProvide an option of robust dataset iterator with error handling<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nMinor fix for metadata files in extension counter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReplace deprecated license_file in `setup.cfg`.\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/6610930650/job/17953825724?pr=6331\r\n```\r\n      /tmp/pip-build-env-a51hls20/overlay/lib/python3.8/site-packages/setuptools/config/setupcfg.py:293: _DeprecatedConfig: Deprecated config in `setup.cfg`\r\n      !!\r\n      \r\n              ********************************************************************************\r\n              The license_file parameter is deprecated, use license_files instead.\r\n      \r\n              By 2023-Oct-30, you need to update your project and remove deprecated calls\r\n              or your builds will no longer be supported.\r\n      \r\n              See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\r\n              ********************************************************************************\r\n      \r\n      !!\r\n```<end_of_turn>\n<start_of_turn>model\nReplace deprecated license_file in setup.cfg<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs discussed in https://github.com/huggingface/datasets/pull/6816, this is needed to support objects that implement `__index__` such as `np.int64` in `Dataset.__getitem__`.<end_of_turn>\n<start_of_turn>model\nSupport indexable objects in `Dataset.__getitem__`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdds `with_rank` to `Dataset.from_generator`.  As for `Dataset.map` and `Dataset.filter`, this is useful when creating cache files using multi-GPU.<end_of_turn>\n<start_of_turn>model\nAdd  with_rank to Dataset.from_generator <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen trying to load a Hub dataset that contains a CSV metadata file, it raises an `ArrowInvalid` error:\r\n```\r\nE   pyarrow.lib.ArrowInvalid: JSON parse error: Invalid value. in row 0\r\n\r\npyarrow/error.pxi:100: ArrowInvalid\r\n```\r\n\r\nSee: https://huggingface.co/datasets/lukarape/public_small_papers/discussions/1<end_of_turn>\n<start_of_turn>model\nHub datasets with CSV metadata raise ArrowInvalid: JSON parse error: Invalid value. in row 0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.17.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsee (internal) https://huggingface.slack.com/archives/C02V51Q3800/p1703885853581679<end_of_turn>\n<start_of_turn>model\nSupport standalone yaml<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ntest1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nLet GeoParquet files with the file extension `*.geoparquet` or `*.gpq` be readable by the default parquet reader.\r\n\r\nThose two file extensions are the ones most commonly used for GeoParquet files, and is included in the `gpq` validator tool at https://github.com/planetlabs/gpq/blob/e5576b4ee7306b4d2259d56c879465a9364dab90/cmd/gpq/command/convert.go#L73-L75\r\n\r\nAddresses https://github.com/huggingface/datasets/issues/6438<end_of_turn>\n<start_of_turn>model\nRead GeoParquet files using parquet reader<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi, this is my code and the error:\r\n```\r\nfrom datasets import load_dataset\r\ngtzan = load_dataset(\"marsyas/gtzan\", \"all\")\r\n```\r\n[error_trace.txt](https://github.com/huggingface/datasets/files/13464397/error_trace.txt)\r\n[audio_yml.txt](https://github.com/huggingface/datasets/files/13464410/audio_yml.txt)\r\nPython 3.11.5\r\nJupyter Notebook 6.5.4\r\nWindows 10\r\n\r\nI'm able to download and work with other datasets, but not this one. For example, both these below work fine:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"facebook/voxpopuli\", \"pl\", split=\"train\", streaming=True)\r\nminds = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\r\n```\r\nThanks for your help\r\n\r\nhttps://huggingface.co/datasets/marsyas/gtzan/tree/main<end_of_turn>\n<start_of_turn>model\nUnable to read \"marsyas/gtzan\" data<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- Implemented TSV-based dataset loaders:\r\n   - WMT21Dataset (local_datasets/wmt21/wmt21_dataset.py)\r\n   - WMT22Dataset (local_datasets/wmt22/wmt22_dataset.py)\r\n   These classes load source-target pairs from .tsv files for train, validation, and test splits.\r\n- Created sample dummy data for both datasets:\r\n   - dummy_data/train.tsv, dummy_data/validation.tsv, dummy_data/test.tsv\r\n   - Includes a few realistic example lines to allow CI and local tests to pass without downloading full datasets.\r\n- Added automated tests for robust validation:\r\n   - tests/test_wmt21.py and tests/test_wmt22.py\r\n   - Checks that all splits load correctly, empty lines are ignored, and the number of examples matches the number of lines in the .tsv files.\r\n   - Edge cases handled: empty lines, malformed lines, extra tabs.\r\n- Added README.md files for both datasets:\r\n   - Provides dataset structure, usage instructions, and placeholders for citation & license information.\r\n   - Ensures that other developers and reviewers can understand dataset usage immediately.\r\n- Ensured easy local testing:\r\n   - Load datasets programmatically using WMT21Dataset / WMT22Dataset.\r\n   - Verified train/validation/test splits are correctly returned as Python dictionaries of Dataset objects.\r\n\r\n\r\n-Provides initial support for WMT21 and WMT22 NLP/translation experiments.\r\n-Allows contributors and reviewers to test dataset loading locally or in CI without downloading large datasets.\r\n-Serves as a template to extend to other WMT datasets in the future.\r\n\r\nTesting instructions:\r\n```\r\n# Activate your environment\r\npytest tests/test_wmt21.py -v\r\npytest tests/test_wmt22.py -v\r\n```\r\n```\r\nfrom local_datasets.wmt21.wmt21_dataset import WMT21Dataset\r\nfrom local_datasets.wmt22.wmt22_dataset import WMT22Dataset\r\n\r\n# WMT21\r\nwmt21 = WMT21Dataset(\"local_datasets/wmt21/dummy_data\")\r\nds21 = wmt21.load()\r\nprint(ds21[\"train\"][0])\r\nprint(ds21[\"validation\"][0])\r\nprint(ds21[\"test\"][0])\r\n\r\n# WMT22\r\nwmt22 = WMT22Dataset(\"local_datasets/wmt22/dummy_data\")\r\nds22 = wmt22.load()\r\nprint(ds22[\"train\"][0])\r\nprint(ds22[\"validation\"][0])\r\nprint(ds22[\"test\"][0])\r\n```<end_of_turn>\n<start_of_turn>model\nDatasets: Add WMT21 & WMT22 loaders (basic TSV loaders, sample data, tests)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR proposes a (slightly hacky) fix for an Issue that can occur when downloading large dataset parts over unstable connections.\r\nThe underlying issue is also being discussed in https://github.com/huggingface/datasets/issues/5594.\r\n\r\nIssue Symptoms & Behaviour:\r\n- Download of a large archive file during dataset download via HTTP-GET fails.\r\n- An silent net exception (which I was unable to identify) is thrown within the `tqdm` download progress.\r\n- Due to missing exception catch code, the above process just continues processing, assuming `http_get` completed successfully.\r\n- Pending Archive file gets renamed to remove the `.incomplete` extension, despite not all data has been downloaded. \r\n- Also, for reasons I did not investigate, there seems to be no real integrity check for the downloaded files; or it does not detect this problem. This is especially problematic, since the downloader script won't retry downloading this archive after CRC-Checking, even if it is being manually restarted / executed again after running into errors on extraction.\r\n\r\nFix proposal: Adding a retry mechanic for HTTP-GET downloads, which adds the following behaviour:\r\n- Download Progress Thread checks for download size validity in case the HTTP connection starves mid download. If the check fails, a RuntimeError is thrown\r\n- Cache Downloader code with retry mechanic monitors for an exception thrown by the download progress thread, and retries download with updated `resume_size`.\r\n- Cache Downloader will not mark incomplete files which have thrown an exception during download, and exceeded retries, as complete.<end_of_turn>\n<start_of_turn>model\nFix for continuation behaviour on broken dataset archives due to starving download connections via HTTP-GET<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIn general, changing the type from string to null is allowed within a dataset \u2014 there are even examples of this in the documentation.\r\n\r\nHowever, if the dataset is large and unevenly distributed, this allowance stops working. The schema gets locked in after reading a chunk. \r\n\r\nConsequently, if all values in the first chunk of a field are, for example, null, the field will be locked as type null, and if a string appears in that field in the second chunk, it will trigger this error:\r\n\r\n<details> \r\n  <summary>Traceback </summary>\r\n\r\n```\r\n   TypeError                                 Traceback (most recent call last)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1868                     try:\r\n-> 1869                         writer.write_table(table)\r\n   1870                     except CastError as cast_error:\r\n\r\n14 frames\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py](https://localhost:8080/#) in write_table(self, pa_table, writer_batch_size)\r\n    579         pa_table = pa_table.combine_chunks()\r\n--> 580         pa_table = table_cast(pa_table, self._schema)\r\n    581         if self.embed_local_files:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in table_cast(table, schema)\r\n   2291     if table.schema != schema:\r\n-> 2292         return cast_table_to_schema(table, schema)\r\n   2293     elif table.schema.metadata != schema.metadata:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in cast_table_to_schema(table, schema)\r\n   2244         )\r\n-> 2245     arrays = [\r\n   2246         cast_array_to_feature(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in <listcomp>(.0)\r\n   2245     arrays = [\r\n-> 2246         cast_array_to_feature(\r\n   2247             table[name] if name in table_column_names else pa.array([None] * len(table), type=schema.field(name).type),\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in wrapper(array, *args, **kwargs)\r\n   1794         if isinstance(array, pa.ChunkedArray):\r\n-> 1795             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1796         else:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in <listcomp>(.0)\r\n   1794         if isinstance(array, pa.ChunkedArray):\r\n-> 1795             return pa.chunked_array([func(chunk, *args, **kwargs) for chunk in array.chunks])\r\n   1796         else:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in cast_array_to_feature(array, feature, allow_primitive_to_str, allow_decimal_to_str)\r\n   2101     elif not isinstance(feature, (Sequence, dict, list, tuple)):\r\n-> 2102         return array_cast(\r\n   2103             array,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in wrapper(array, *args, **kwargs)\r\n   1796         else:\r\n-> 1797             return func(array, *args, **kwargs)\r\n   1798 \r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in array_cast(array, pa_type, allow_primitive_to_str, allow_decimal_to_str)\r\n   1947         if pa.types.is_null(pa_type) and not pa.types.is_null(array.type):\r\n-> 1948             raise TypeError(f\"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}\")\r\n   1949         return array.cast(pa_type)\r\n\r\nTypeError: Couldn't cast array of type string to null\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n\r\n[<ipython-input-353-e02f83980611>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 dd = load_dataset(\"json\", data_files=[\"TEST.json\"])\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2094 \r\n   2095     # Download and prepare data\r\n-> 2096     builder_instance.download_and_prepare(\r\n   2097         download_config=download_config,\r\n   2098         download_mode=download_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    922                     if num_proc is not None:\r\n    923                         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 924                     self._download_and_prepare(\r\n    925                         dl_manager=dl_manager,\r\n    926                         verification_mode=verification_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n    997             try:\r\n    998                 # Prepare split will record examples associated to the split\r\n--> 999                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1000             except OSError as e:\r\n   1001                 raise OSError(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1738             job_id = 0\r\n   1739             with pbar:\r\n-> 1740                 for job_id, done, content in self._prepare_split_single(\r\n   1741                     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1742                 ):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1894             if isinstance(e, DatasetGenerationError):\r\n   1895                 raise\r\n-> 1896             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1897 \r\n   1898         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\n</details>\n\n### Steps to reproduce the bug\n\n```python\r\nimport json\r\nfrom datasets import load_dataset\r\n\r\nwith open(\"TEST.json\", \"w\") as f:\r\n    row = {\"ballast\": \"qwerty\" * 1000, \"b\": None}\r\n    row_str = json.dumps(row) + \"\\n\"\r\n    line_size = len(row_str)\r\n    chunk_size = 10 << 20\r\n    lines_in_chunk = chunk_size // line_size + 1\r\n    print(f\"Writing {lines_in_chunk} lines\")\r\n    for i in range(lines_in_chunk):\r\n        f.write(row_str)\r\n    null_row = {\"ballast\": \"Gotcha\", \"b\": \"Not Null\"}\r\n    f.write(json.dumps(null_row) + \"\\n\")\r\n\r\nload_dataset(\"json\", data_files=[\"TEST.json\"])\r\n```\n\n### Expected behavior\n\nConcatenation of the chunks without errors\n\n### Environment info\n\n- `datasets` version: 3.0.1\r\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.24.7\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nTypeError: Couldn't cast array of type string to null in long json<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdded an optional parameter return_file_name in the load_dataset function. When it is set to True, the function will include the name of the file corresponding to the current line as a feature in the returned output.\r\n\r\nI added this here https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/json/json.py#L92.\r\n\r\nfixes #5806<end_of_turn>\n<start_of_turn>model\nfeat: Return the name of the currently loaded file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\ncannot import name 'DEFAULT_CIPHERS' from 'urllib3.util.ssl_' this is the error i received\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\n\n### Expected behavior\n\nNo errors\n\n### Environment info\n\npython 3.11.5<end_of_turn>\n<start_of_turn>model\nWhile importing load_dataset from datasets <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMake CLI convert_to_parquet not raise error if no rights to create \"script\" branch.\r\n\r\nNot that before this PR, the error was not critical because it was raised at the end of the script, once all the rest of the steps were already performed.\r\n\r\nFix #6901.\r\n\r\nBug introduced in datasets-2.19.0 by:\r\n- #6809<end_of_turn>\n<start_of_turn>model\nMake CLI convert_to_parquet not raise error if no rights to create script branch<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIf the dataset has multiple configs, when using the `datasets-cli convert_to_parquet` command to avoid issues with the data viewer caused by loading scripts, the conversion process only successfully converts the data corresponding to the first config. When it starts converting the second config, it throws an error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/opt/anaconda3/envs/dl/bin/datasets-cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/opt/anaconda3/envs/dl/lib/python3.10/site-packages/datasets/commands/datasets_cli.py\", line 41, in main\r\n    service.run()\r\n  File \"/opt/anaconda3/envs/dl/lib/python3.10/site-packages/datasets/commands/convert_to_parquet.py\", line 83, in run\r\n    dataset.push_to_hub(\r\n  File \"/opt/anaconda3/envs/dl/lib/python3.10/site-packages/datasets/dataset_dict.py\", line 1713, in push_to_hub\r\n    api.create_branch(repo_id, branch=revision, token=token, repo_type=\"dataset\", exist_ok=True)\r\n  File \"/opt/anaconda3/envs/dl/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/opt/anaconda3/envs/dl/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 5503, in create_branch\r\n    hf_raise_for_status(response)\r\n  File \"/opt/anaconda3/envs/dl/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 358, in hf_raise_for_status\r\n    raise BadRequestError(message, response=response) from e\r\nhuggingface_hub.utils._errors.BadRequestError:  (Request ID: Root=1-669fc665-7c2e80d75f4337496ee95402;731fcdc7-0950-4eec-99cf-ce047b8d003f)\r\n\r\nBad request:\r\nInvalid reference for a branch: refs/pr/1\r\n```<end_of_turn>\n<start_of_turn>model\nConvert_to_parquet fails for datasets with multiple configs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://huggingface.co/datasets/PhilEO-community/PhilEO-downstream\r\n\r\nThe dataset viewer gives the following error:\r\n\r\n```\r\nError code:   ConfigNamesError\r\nException:    BadZipFile\r\nMessage:      zipfiles that span multiple disks are not supported\r\nTraceback:    Traceback (most recent call last):\r\n                File \"/src/services/worker/src/worker/job_runners/dataset/config_names.py\", line 67, in compute_config_names_response\r\n                  get_dataset_config_names(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 347, in get_dataset_config_names\r\n                  dataset_module = dataset_module_factory(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1871, in dataset_module_factory\r\n                  raise e1 from None\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1846, in dataset_module_factory\r\n                  return HubDatasetModuleFactoryWithoutScript(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1240, in get_module\r\n                  module_name, default_builder_kwargs = infer_module_for_data_files(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 584, in infer_module_for_data_files\r\n                  split_modules = {\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 585, in <dictcomp>\r\n                  split: infer_module_for_data_files_list(data_files_list, download_config=download_config)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 526, in infer_module_for_data_files_list\r\n                  return infer_module_for_data_files_list_in_archives(data_files_list, download_config=download_config)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 554, in infer_module_for_data_files_list_in_archives\r\n                  for f in xglob(extracted, recursive=True, download_config=download_config)[\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 576, in xglob\r\n                  fs, *_ = fsspec.get_fs_token_paths(urlpath, storage_options=storage_options)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/fsspec/core.py\", line 622, in get_fs_token_paths\r\n                  fs = filesystem(protocol, **inkwargs)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/fsspec/registry.py\", line 290, in filesystem\r\n                  return cls(**storage_options)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/fsspec/spec.py\", line 79, in __call__\r\n                  obj = super().__call__(*args, **kwargs)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/fsspec/implementations/zip.py\", line 57, in __init__\r\n                  self.zip = zipfile.ZipFile(\r\n                File \"/usr/local/lib/python3.9/zipfile.py\", line 1266, in __init__\r\n                  self._RealGetContents()\r\n                File \"/usr/local/lib/python3.9/zipfile.py\", line 1329, in _RealGetContents\r\n                  endrec = _EndRecData(fp)\r\n                File \"/usr/local/lib/python3.9/zipfile.py\", line 286, in _EndRecData\r\n                  return _EndRecData64(fpin, -sizeEndCentDir, endrec)\r\n                File \"/usr/local/lib/python3.9/zipfile.py\", line 232, in _EndRecData64\r\n                  raise BadZipFile(\"zipfiles that span multiple disks are not supported\")\r\n              zipfile.BadZipFile: zipfiles that span multiple disks are not supported\r\n```\r\n\r\nThe files (https://huggingface.co/datasets/PhilEO-community/PhilEO-downstream/tree/main/data) are:\r\n\r\n<img width=\"629\" alt=\"Capture d\u2019e\u0301cran 2024-03-11 a\u0300 22 07 30\" src=\"https://github.com/huggingface/datasets/assets/1676121/0bb15a51-d54f-4d73-8572-e427ea644b36\">\r\n<end_of_turn>\n<start_of_turn>model\nSupport zipfiles that span multiple disks?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #5936 \r\nAlso, a related fix to #5927 <end_of_turn>\n<start_of_turn>model\nFix sequence of array support for most dtype<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently the DIll version is less than 3.9 and now major libraries like Multiprocess, gepa requires Dill version as 0.4.0 and this is making a conflict in installation. So added this small PR to update the DIll. <end_of_turn>\n<start_of_turn>model\nFix: Update Dill Version in Setup py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis small change fixes the documentation to to be compliant with what happens in `encode_example`.\r\nhttps://github.com/huggingface/datasets/blob/e71b0b19d79c7531f9b9bea7c09916b5f6157f42/src/datasets/features/features.py#L1126-L1129<end_of_turn>\n<start_of_turn>model\n`ClassLabel` docs: Correct value for unknown labels<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nFrom what I understand, streaming dataset currently pulls the data, and process the data as it is requested.\r\nThis can introduce significant latency delays when data is loaded into the training process, needing to wait for each segment.\r\n\r\nWhile the delays might be dataset specific (or even mapping instruction/tokenizer specific)\r\n\r\nIs it possible to introduce a `streaming_lookahead` parameter, which is used for predictable workloads (even shuffled dataset with fixed seed). As we can predict in advance what the next few datasamples will be. And fetch them while the current set is being trained.\r\n\r\nWith enough CPU & bandwidth to keep up with the training process, and a sufficiently large lookahead, this will reduce the various latency involved while waiting for the dataset to be ready between batches.\r\n\r\n### Motivation\r\n\r\nFaster streaming performance, while training over extra large TB sized datasets\r\n\r\n### Your contribution\r\n\r\nI currently use HF dataset, with pytorch lightning trainer for RWKV project, and would be able to help test this feature if supported.<end_of_turn>\n<start_of_turn>model\nLookahead streaming support?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAllow returning Pandas DataFrames in `map` transforms.\r\n\r\n(Plus, raise an error in the non-batched mode if a returned PyArrow table/Pandas DataFrame has more than one row)\r\n\r\n<end_of_turn>\n<start_of_turn>model\nSupport returning dataframe in map transform<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix this error when mapping a PDF dataset\r\n\r\n```\r\npyarrow.lib.ArrowInvalid: Could not convert <pdfplumber.pdf.PDF object at 0x13498ee40> with type PDF: did not recognize Python value type when inferring an Arrow data type\r\n```\r\n\r\nand also let map() outputs be lists of images or pdfs<end_of_turn>\n<start_of_turn>model\nWrite pdf in map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAfter investigation, I have found that this copy forces the splits to be sorted alphabetically: https://github.com/huggingface/datasets/blob/029227a116c14720afca71b9b22e78eb2a1c09a6/src/datasets/builder.py#L556\r\n\r\nThis PR removes the alphabetically sort of `DataFilesDict` keys.\r\n- Note that for a `dict`, the order of keys is relevant when hashing:\r\n  ```python\r\n  hash1 = Hasher.hash({'train': 'train.csv', 'test': 'test.csv'})\r\n  hash2 = Hasher.hash({'test': 'test.csv', 'train': 'train.csv'})\r\n  assert hash1 != hash2\r\n  ```\r\n- The `DataFilesDict` is a subclass of `dict`, thus the order should be relevant as well\r\n  ```python\r\n  hash1 = Hasher.hash(DataFilesDict({'train': 'train.csv', 'test': 'test.csv'}))\r\n  hash2 = Hasher.hash(DataFilesDict({'test': 'test.csv', 'train': 'train.csv'}))\r\n  assert hash1 != hash2\r\n  ```\r\n\r\nFix #6196.<end_of_turn>\n<start_of_turn>model\nPreserve split order in DataFilesDict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\nload_dataset(path=\"json\", data_files=\"./test.json\")\r\n```\r\n```\r\nGenerating train split: 0 examples [00:00, ? examples/s]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json/json.py\", line 132, in _generate_tables\r\n    pa_table = paj.read_json(\r\n  File \"pyarrow/_json.pyx\", line 308, in pyarrow._json.read_json\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: JSON parse error: Column() changed from object to array in row 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1997, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json/json.py\", line 155, in _generate_tables\r\n    df = pd.read_json(f, dtype_backend=\"pyarrow\")\r\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 211, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 331, in wrapper\r\n    return func(*args, **kwargs)\r\nTypeError: read_json() got an unexpected keyword argument 'dtype_backend'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/app/t1.py\", line 11, in <module>\r\n    load_dataset(path=data_path, data_files=\"./t2.json\")\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2616, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1029, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1124, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1884, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 2040, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n```\r\nimport pandas as pd\r\nwith open(\"./test.json\", \"r\") as f:\r\n    df = pd.read_json(f, dtype_backend=\"pyarrow\")\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"/app/t3.py\", line 3, in <module>\r\n    df = pd.read_json(f, dtype_backend=\"pyarrow\")\r\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 211, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\", line 331, in wrapper\r\n    return func(*args, **kwargs)\r\nTypeError: read_json() got an unexpected keyword argument 'dtype_backend'\r\n```\n\n### Steps to reproduce the bug\n\n.\n\n### Expected behavior\n\n.\n\n### Environment info\n\n ```\r\ndatasets                  2.20.0\r\npandas                    1.5.3\r\n```<end_of_turn>\n<start_of_turn>model\nload json file error with v2.20.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse content defined chunking by default when writing parquet files.\r\n- [x] set the parameters in `io.parquet.ParquetDatasetReader`\r\n- [x] set the parameters in `arrow_writer.ParquetWriter`\r\n\r\nIt requires a new pyarrow pin \">=21.0.0\" which is released now.<end_of_turn>\n<start_of_turn>model\nfeat: use content defined chunking <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6472.<end_of_turn>\n<start_of_turn>model\nFix CI quality<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen saving out some Audio() data.\r\nThe data is audio recordings with associated 'sentences'.\r\n(They use the audio 'bytes' approach because they're clips within audio files).\r\nCode is below the traceback (I can't upload the voice audio/text (it's not even me)).\r\n\r\n```\r\nTraceback (most recent call last):\r\nFile \"/mnt/ddrive/prj/voice/voice-training-dataset-create/./dataset.py\", line 156, in <module>\r\ncreate_dataset(args)\r\nFile \"/mnt/ddrive/prj/voice/voice-training-dataset-create/./dataset.py\", line 138, in create_dataset\r\nhf_dataset.save_to_disk(args.outds, max_shard_size='50MB')\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 1531, in save_to_disk\r\nfor kwargs in kwargs_per_job:\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 1508, in <genexpr>\r\n\"shard\": self.shard(num_shards=num_shards, index=shard_idx, contiguous=True),\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 4609, in shard\r\nreturn self.select(\r\n^^^^^^^^^^^^\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 556, in wrapper\r\nout: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"/home/j/src/py/datasets/src/datasets/fingerprint.py\", line 511, in wrapper\r\nout = func(dataset, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 3797, in select\r\nreturn self._select_contiguous(start, length, new_fingerprint=new_fingerprint)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 556, in wrapper\r\nout: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"/home/j/src/py/datasets/src/datasets/fingerprint.py\", line 511, in wrapper\r\nout = func(dataset, *args, **kwargs)\r\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 3857, in _select_contiguous\r\n_check_valid_indices_value(start, len(self))\r\nFile \"/home/j/src/py/datasets/src/datasets/arrow_dataset.py\", line 648, in _check_valid_indices_value\r\nraise IndexError(f\"Index {index} out of range for dataset of size {size}.\")\r\nIndexError: Index 339 out of range for dataset of size 339.\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\n(I had to set the default max batch size down due to a different bug... or maybe it's related: https://github.com/huggingface/datasets/issues/5717)\r\n\r\n```python3\r\n#!/usr/bin/env python3\r\nimport argparse\r\nimport os\r\nfrom pathlib import Path\r\nimport soundfile as sf\r\n\r\nimport datasets\r\ndatasets.config.DEFAULT_MAX_BATCH_SIZE=35\r\nfrom datasets import Features, Array2D, Value, Dataset, Sequence, Audio\r\n\r\nimport numpy as np\r\nimport librosa\r\nimport sys\r\nimport soundfile as sf\r\nimport io\r\nimport logging\r\n\r\nlogging.basicConfig(level=logging.DEBUG, filename='debug.log', filemode='w',\r\n                    format='%(name)s - %(levelname)s - %(message)s')\r\n\r\n# Define the arguments for the command-line interface\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser(description=\"Create a Huggingface dataset from labeled audio files.\")\r\n    parser.add_argument(\"--indir_labeled\", action=\"append\", help=\"Directory containing labeled audio files.\", required=True)\r\n    parser.add_argument(\"--outds\", help=\"Path to save the dataset file.\", required=True)\r\n    parser.add_argument(\"--max_clips\", type=int, help=\"Max count of audio samples to add to the dataset.\", default=None)\r\n    parser.add_argument(\"-r\", \"--sr\", type=int, help=\"Sample rate for the audio files.\", default=16000)\r\n    parser.add_argument(\"--no-resample\", action=\"store_true\", help=\"Disable resampling of the audio files.\")\r\n    parser.add_argument(\"--max_clip_secs\", type=float, help=\"Max length of audio clips in seconds.\", default=3.0)\r\n    parser.add_argument(\"-v\", \"--verbose\", action='count', default=1, help=\"Increase verbosity\")\r\n    return parser.parse_args()\r\n\r\n# Convert the NumPy arrays to audio bytes in WAV format\r\ndef numpy_to_bytes(audio_array, sampling_rate=16000):\r\n    with io.BytesIO() as bytes_io:\r\n        sf.write(bytes_io, audio_array, samplerate=sampling_rate,\r\n                 format='wav', subtype='FLOAT') # float32\r\n        return bytes_io.getvalue()\r\n\r\n# Function to find audio and label files in a directory\r\ndef find_audio_label_pairs(indir_labeled):\r\n    audio_label_pairs = []\r\n    for root, _, files in os.walk(indir_labeled):\r\n        for file in files:\r\n            if file.endswith(('.mp3', '.wav', '.aac', '.flac')):\r\n                audio_path = Path(root) / file\r\n                if args.verbose>1:\r\n                    print(f'File: {audio_path}')\r\n                label_path = audio_path.with_suffix('.labels.txt')\r\n                if label_path.exists():\r\n                    if args.verbose>0:\r\n                        print(f'  Pair: {audio_path}')\r\n                    audio_label_pairs.append((audio_path, label_path))\r\n    return audio_label_pairs\r\n\r\ndef process_audio_label_pair(audio_path, label_path, sampling_rate, no_resample, max_clip_secs):\r\n    # Read the label file\r\n    with open(label_path, 'r') as label_file:\r\n        labels = label_file.readlines()\r\n\r\n    # Load the full audio file\r\n    full_audio, current_sr = sf.read(audio_path)\r\n    if not no_resample and current_sr != sampling_rate:\r\n        # You can use librosa.resample here if librosa is available\r\n        full_audio = librosa.resample(full_audio, orig_sr=current_sr, target_sr=sampling_rate)\r\n\r\n    audio_segments = []\r\n    sentences = []\r\n\r\n    # Process each label\r\n    for label in labels:\r\n        start_secs, end_secs, label_text = label.strip().split('\\t')\r\n        start_sample = int(float(start_secs) * sampling_rate)\r\n        end_sample = int(float(end_secs) * sampling_rate)\r\n\r\n        # Extract segment and truncate or pad to max_clip_secs\r\n        audio_segment = full_audio[start_sample:end_sample]\r\n        max_samples = int(max_clip_secs * sampling_rate)\r\n        if len(audio_segment) > max_samples: # Truncate\r\n            audio_segment = audio_segment[:max_samples]\r\n        elif len(audio_segment) < max_samples: # Pad\r\n            padding = np.zeros(max_samples - len(audio_segment), dtype=audio_segment.dtype)\r\n            audio_segment = np.concatenate((audio_segment, padding))\r\n\r\n        audio_segment = numpy_to_bytes(audio_segment)\r\n\r\n        audio_data = {\r\n            'path': str(audio_path),\r\n            'bytes': audio_segment,\r\n        }\r\n\r\n        audio_segments.append(audio_data)\r\n        sentences.append(label_text)\r\n\r\n    return audio_segments, sentences\r\n\r\n# Main function to create the dataset\r\ndef create_dataset(args):\r\n    audio_label_pairs = []\r\n    for indir in args.indir_labeled:\r\n        audio_label_pairs.extend(find_audio_label_pairs(indir))\r\n\r\n    # Initialize our dataset data\r\n    dataset_data = {\r\n        'path': [],        # This will be a list of strings\r\n        'audio': [],       # This will be a list of dictionaries\r\n        'sentence': [],    # This will be a list of strings\r\n    }\r\n\r\n    # Process each audio-label pair and add the data to the dataset\r\n    for audio_path, label_path in audio_label_pairs[:args.max_clips]:\r\n        audio_segments, sentences = process_audio_label_pair(audio_path, label_path, args.sr, args.no_resample, args.max_clip_secs)\r\n        if audio_segments and sentences:\r\n            for audio_data, sentence in zip(audio_segments, sentences):\r\n                if args.verbose>1:\r\n                    print(f'Appending {audio_data[\"path\"]}')\r\n                dataset_data['path'].append(audio_data['path'])\r\n                dataset_data['audio'].append({\r\n                    'path': audio_data['path'],\r\n                    'bytes': audio_data['bytes'],\r\n                })\r\n                dataset_data['sentence'].append(sentence)\r\n\r\n    features = Features({\r\n      'path': Value('string'), # Path is redundant in common voice set also\r\n      'audio': Audio(sampling_rate=16000),\r\n      'sentence': Value('string'),\r\n    })\r\n    hf_dataset = Dataset.from_dict(dataset_data, features=features)\r\n\r\n    for key in dataset_data:\r\n        for i, item in enumerate(dataset_data[key]):\r\n            if item is None or (isinstance(item, bytes) and len(item) == 0):\r\n                logging.error(f\"Invalid {key} at index {i}: {item}\")\r\n                import ipdb; ipdb.set_trace(context=16); pass\r\n\r\n    hf_dataset.save_to_disk(args.outds, max_shard_size='50MB')\r\n    # try:\r\n    #     hf_dataset.save_to_disk(args.outds)\r\n    # except TypeError as e:\r\n    #     # If there's a TypeError, log the exception and the dataset data that might have caused it\r\n    #     logging.exception(\"An error occurred while saving the dataset.\")\r\n    #     import ipdb; ipdb.set_trace(context=16); pass\r\n    #     for key in dataset_data:\r\n    #         logging.debug(f\"{key} length: {len(dataset_data[key])}\")\r\n    #         if key == 'audio':\r\n    #             # Log the first 100 bytes of the audio data to avoid huge log files\r\n    #             for i, audio in enumerate(dataset_data[key]):\r\n    #                 logging.debug(f\"Audio {i}: {audio['bytes'][:100]}\")\r\n    #     raise\r\n\r\n# Run the script\r\nif __name__ == \"__main__\":\r\n    args = parse_args()\r\n    create_dataset(args)\r\n```\n\n### Expected behavior\n\nIt shouldn't fail.\n\n### Environment info\n\n- `datasets` version: 2.14.7.dev0\r\n- Platform: Linux-6.1.0-13-amd64-x86_64-with-glibc2.36\r\n- Python version: 3.11.2\r\n- `huggingface_hub` version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.2\r\n- `fsspec` version: 2023.9.2\r\n\r\n<end_of_turn>\n<start_of_turn>model\nIndex 339 out of range for dataset of size 339 <-- save_to_file()<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRemove README link to deprecated Colab notebook<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen uploading a dataset created locally using `from_dict` with a specified `description` field. It appears before upload, but is missing after upload and re-download.\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nI think the most relevant pattern in the code might be the following lines:\r\n\r\n```\r\ndescription_json_str = json.dumps(\r\n    {\r\n        \"dataset_id\": dataset.spec.dataset_id,\r\n        \"env_name\": dataset.spec.env_spec.id,\r\n        \"action_space\": serialize_space(dataset.spec.action_space),\r\n        \"observation_space\": serialize_space(dataset.spec.observation_space),\r\n    }\r\n)\r\n\r\nhugging_face_dataset = Dataset.from_dict(\r\n    episodes_dict, info=DatasetInfo(description=description_json_str)\r\n)\r\n\r\n```\r\nWhich comes from this function https://github.com/balisujohn/minarai/blob/8e023727f0a8488c4451651d9f7a79b981412c40/minari/integrations/hugging_face.py#L39\r\n\r\n\r\n\r\nTo replicate,\r\nclone this branch of my Minari fork https://github.com/balisujohn/minarai/tree/dev-huggingface then run\r\n\r\n```\r\npython3.8 -m venv env\r\nsource env/bin/activate\r\npython3 -m pip install -e .\r\npython3 -m pip install pytest\r\n```\r\n\r\nThe change the hugging face repo path in the test called  `test_hugging_face_push_and_pull_dataset` in `tests/integrations/test_hugging_face.py` to one you have permissions to write to.\r\n\r\nThen run:\r\n\r\n```\r\npytest tests/integrations/test_hugging_face.py::test_hugging_face_push_and_pull_dataset\r\n```\r\n\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nDATASET INFO BEFORE UPLOADING\r\nDatasetInfo(description='{\"dataset_id\": \"dummy-combo-test-v0\", \"env_name\": \"DummyComboEnv-v0\", \"action_space\": \"{\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, {\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [4.0], \\\\\"high\\\\\": [5.0]}]}\", \"observation_space\": \"{\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, {\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, {\\\\\"type\\\\\": \\\\\"Dict\\\\\", \\\\\"subspaces\\\\\": {\\\\\"component_1\\\\\": {\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [-1.0], \\\\\"high\\\\\": [1.0]}, \\\\\"component_2\\\\\": {\\\\\"type\\\\\": \\\\\"Dict\\\\\", \\\\\"subspaces\\\\\": {\\\\\"subcomponent_1\\\\\": {\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [2.0], \\\\\"high\\\\\": [3.0]}, \\\\\"subcomponent_2\\\\\": {\\\\\"type\\\\\": \\\\\"Tuple\\\\\", \\\\\"subspaces\\\\\": [{\\\\\"type\\\\\": \\\\\"Box\\\\\", \\\\\"dtype\\\\\": \\\\\"float32\\\\\", \\\\\"shape\\\\\": [1], \\\\\"low\\\\\": [4.0], \\\\\"high\\\\\": [5.0]}, {\\\\\"type\\\\\": \\\\\"Discrete\\\\\", \\\\\"dtype\\\\\": \\\\\"int64\\\\\", \\\\\"start\\\\\": 0, \\\\\"n\\\\\": 10}]}}}}}]}]}\"}', citation='', homepage='', license='', features={'observations': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'component_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'component_2': {'subcomponent_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'subcomponent_2': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Value(dtype='int64', id=None)}}}}}, 'actions': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}, 'rewards': Value(dtype='int64', id=None), 'truncations': Value(dtype='bool', id=None), 'terminations': Value(dtype='bool', id=None), 'episode_ids': Value(dtype='int64', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits=None, download_checksums=None, download_size=None, post_processing_size=None, dataset_size=None, size_in_bytes=None)\r\n...\r\nDATASET INFO AFTER UPLOADING AND DOWNLOADING\r\nDatasetInfo(description='', citation='', homepage='', license='', features={'observations': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': {'component_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'component_2': {'subcomponent_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'subcomponent_2': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Value(dtype='int64', id=None)}}}}}, 'actions': {'_index_0': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), '_index_1': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None)}, 'rewards': Value(dtype='int64', id=None), 'truncations': Value(dtype='bool', id=None), 'terminations': Value(dtype='bool', id=None), 'episode_ids': Value(dtype='int64', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name=None, config_name=None, version=None, splits={'train': SplitInfo(name='train', num_bytes=4846, num_examples=60, shard_lengths=None, dataset_name='parquet')}, download_checksums={'https://huggingface.co/datasets/balisujohn/minari_test/resolve/8217b614ff9ba5edc1a30c7df430e92a46f65363/data/train-00000-of-00001-7c5900b93b35745e.parquet': {'num_bytes': 9052, 'checksum': None}}, download_size=9052, post_processing_size=None, dataset_size=4846, size_in_bytes=13898)\r\n...\r\n\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.0-75-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2\r\n<end_of_turn>\n<start_of_turn>model\ndescription disappearing from Info when Uploading a Dataset Created with `from_dict`<end_of_turn>"}
