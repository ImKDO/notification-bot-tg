{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe redirected page encountered 404 not found.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. In this tutorial: https://huggingface.co/learn/nlp-course/chapter5/4?fw=pt \r\noriginal md: https://github.com/huggingface/course/blob/2c733c2246b8b7e0e6f19a9e5d15bb12df43b2a3/chapters/en/chapter5/4.mdx#L49\r\n\r\n```\r\nBy default, \ud83e\udd17 Datasets will decompress the files needed to load a dataset. If you want to preserve hard drive space, you can pass `DownloadConfig(delete_extracted=True)` to the `download_config` argument of `load_dataset()`. See the [documentation](https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig) for more details.\r\n```\r\nThe documentation points to `https://huggingface.co/docs/datasets/package_reference/builder_classes.html?#datasets.utils.DownloadConfig`\r\n\r\nit shows `The documentation page PACKAGE_REFERENCE/BUILDER_CLASSES.HTML doesn\u2019t exist in v2.16.1, but exists on the main version. Click [here](https://huggingface.co/docs/datasets/main/en/package_reference/builder_classes.html) to redirect to the main version of the documentation.`\r\n\r\nBut the redirected website `https://huggingface.co/docs/datasets/main/en/package_reference/builder_classes.html` is 404 not found.\r\n\r\n### Expected behavior\r\n\r\nI Guess the redirected webisite should be \r\n`https://huggingface.co/docs/datasets/main/en/package_reference/builder_classes`    (without `.html`)\r\nor `https://huggingface.co/docs/datasets/main/en/package_reference/builder_classes#datasets.DownloadConfig`.\r\n\r\n### Environment info\r\n\r\nDatasets main<end_of_turn>\n<start_of_turn>model\ndocument page 404 not found after redirection<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nI'm repeatedly finding myself in situations where I want to have a package called `datasets.py` or `evaluate.py` in my code and can't because those names are being taken up by Huggingface packages. While I can understand how (even from the user's perspective) it's aesthetically pleasing to have nice terse library names, ultimately a library hogging simple names like this is something I find short-sighted, impractical and at my most irritable, frankly rude.\r\n\r\nMy preference would be a pattern like what you get with all the other big libraries like numpy or pandas:\r\n\r\n```\r\nimport huggingface as hf\r\n# hf.transformers, hf.datasets, hf.evaluate\r\n```\r\n\r\nor things like\r\n\r\n```\r\nimport huggingface.transformers as tf\r\n# tf.load_model(), etc\r\n```\r\n\r\nIf this isn't possible for some technical reason, at least just call the packages something like `hf_transformers` and so on.\r\n\r\nI realize this is a very big change that's probably been discussed internally already, but I'm making this issue and sister issues on each huggingface project just to start the conversation and begin tracking community feeling on the matter, since I suspect I'm not the only one who feels like this.\r\n\r\nSorry if this has been requested already on this issue tracker, I couldn't find anything looking for terms like \"package name\".\r\n\r\nSister issues:\r\n- [transformers](https://github.com/huggingface/transformers/issues/24934)\r\n- **datasets**\r\n- [evaluate](https://github.com/huggingface/evaluate/issues/476)\r\n\r\n### Motivation\r\n\r\nNot taking up package names the user is likely to want to use.\r\n\r\n### Your contribution\r\n\r\nNo - more a matter of internal discussion among core library authors.<end_of_turn>\n<start_of_turn>model\nChange package name from \"datasets\" to something less generic<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using python3.9 and ```evaluate``` module loading Meteor metric crashes at a non-existent import from ```datasets.config``` in ```datasets v2.14```\n\n### Steps to reproduce the bug\n\n```\r\nfrom evaluate import load\r\nmeteor = load(\"meteor\")\r\n```\r\n\r\nproduces the following error:\r\n  ```\r\n    from datasets.config import importlib_metadata, version\r\nImportError: cannot import name 'importlib_metadata' from 'datasets.config' (<path_to_project>/venv/lib/python3.9/site-packages/datasets/config.py)\r\n```\n\n### Expected behavior\n\n```datasets``` of v2.10 has the following workaround in ```config.py```:\r\n\r\n```\r\nif PY_VERSION < version.parse(\"3.8\"):\r\n    import importlib_metadata\r\nelse:\r\n    import importlib.metadata as importlib_metadata\r\n```\r\n\r\nHowever, it's absent in v2.14 which might be the cause of the issue.\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.5-arm64-arm-64bit\r\n- Python version: 3.9.6\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- Evaluate version: 0.4.0<end_of_turn>\n<start_of_turn>model\nLoading Meteor metric in HF evaluate module crashes due to datasets import issue <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHi, I have a dataset on the hub [here](https://huggingface.co/datasets/manestay/borderlines). It has 1k+ downloads, which I sure is mostly just me and my colleagues working with it. It should have far fewer, since I'm using the same machine with a properly set up HF_HOME variable. However, whenever I run the below function `load_borderlines_hf`, it downloads the entire dataset from the hub and then does the other logic:\r\nhttps://github.com/manestay/borderlines/blob/4e161f444661e2ebfe643f3fe149d9258d63a57d/run_gpt/lib.py#L80\r\n\r\nLet me know what I'm doing wrong here, or if it's a bug with the `datasets` library itself. On the hub I have my data stored in CSVs, but several columns are lists, so that's why I have the code to map splitting on `;`. I looked into dataset loading scripts, but it seemed difficult to set up. I have verified that other `datasets` and `models` on my system are using the cache properly (e.g. I have a 13B parameter model and large datasets, but those are cached and don't redownload).\r\n\r\n__EDIT: __ as pointed out in the discussion below, it may be the `map()` calls that aren't being cached properly. Supposing the `load_dataset()` retrieve from the cache, then it should be the case that the `map()` calls also retrieve from the cached output. But the `map()` commands re-execute sometimes.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Copy and paste the function from [here](https://github.com/manestay/borderlines/blob/4e161f444661e2ebfe643f3fe149d9258d63a57d/run_gpt/lib.py#L80) (lines 80-100)\r\n2. Run it in Python `load_borderlines_hf(None)`\r\n3. It completes successfully, downloading from HF hub, then doing the mapping logic etc.\r\n4. If you run it again after some time, it will re-download, ignoring the cache\r\n\r\n### Expected behavior\r\n\r\nRe-running the code, which calls `datasets.load_dataset('manestay/borderlines', 'territories')`, should use the cached version\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.16.1\r\n- Platform: Linux-5.14.21-150500.55.7-default-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nDataset on Hub re-downloads every time?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nIn [my dataset](https://huggingface.co/datasets/speedcell4/opus-unigram2), there is a column called `lang2`, and there are 94 different classes in total, but the viewer says there are 83 values only. This issue only arises in the `train` split. The total number of values is also 94 in the `test` and `dev` columns, viewer tells the correct number of them.\r\n\r\n<img width=\"177\" alt=\"image\" src=\"https://github.com/user-attachments/assets/78d76ef2-fe0e-4fa3-85e0-fb2552813d1c\">\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python3\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset('speedcell4/opus-unigram2').unique('lang2')\r\nfor key, lang2 in ds.items():\r\n    print(key, len(lang2))\r\n```\r\n\r\nThis script returns the following and tells that the `train` split has 94 values in the `lang2` column.\r\n\r\n```\r\ntrain 94\r\ndev 94\r\ntest 94\r\nzero 5\r\n```\r\n\r\n### Expected behavior\r\n\r\n94 in the reviewer.\r\n\r\n### Environment info\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.4.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux release 8.2.2004 (Core)  (x86_64)\r\nGCC version: (GCC) 8.3.1 20191121 (Red Hat 8.3.1-5)\r\nClang version: Could not collect\r\nCMake version: version 3.11.4\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.9.20 (main, Oct  3 2024, 07:27:41)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-193.28.1.el8_2.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\nGPU 2: NVIDIA A100-SXM4-40GB\r\nGPU 3: NVIDIA A100-SXM4-40GB\r\nGPU 4: NVIDIA A100-SXM4-40GB\r\nGPU 5: NVIDIA A100-SXM4-40GB\r\nGPU 6: NVIDIA A100-SXM4-40GB\r\nGPU 7: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 525.85.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              64\r\nOn-line CPU(s) list: 0-63\r\nThread(s) per core:  1\r\nCore(s) per socket:  32\r\nSocket(s):           2\r\nNUMA node(s):        4\r\nVendor ID:           AuthenticAMD\r\nCPU family:          23\r\nModel:               49\r\nModel name:          AMD EPYC 7542 32-Core Processor\r\nStepping:            0\r\nCPU MHz:             3389.114\r\nBogoMIPS:            5789.40\r\nVirtualization:      AMD-V\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            512K\r\nL3 cache:            16384K\r\nNUMA node0 CPU(s):   0-15\r\nNUMA node1 CPU(s):   16-31\r\nNUMA node2 CPU(s):   32-47\r\nNUMA node3 CPU(s):   48-63\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.4.1+cu121\r\n[pip3] torchaudio==2.4.1+cu121\r\n[pip3] torchdevice==0.1.1\r\n[pip3] torchglyph==0.3.2\r\n[pip3] torchmetrics==1.5.0\r\n[pip3] torchrua==0.5.1\r\n[pip3] torchvision==0.19.1+cu121\r\n[pip3] triton==3.0.0\r\n[pip3] datasets==3.0.1\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.4.1+cu121              pypi_0    pypi\r\n[conda] torchaudio                2.4.1+cu121              pypi_0    pypi\r\n[conda] torchdevice               0.1.1                    pypi_0    pypi\r\n[conda] torchglyph                0.3.2                    pypi_0    pypi\r\n[conda] torchmetrics              1.5.0                    pypi_0    pypi\r\n[conda] torchrua                  0.5.1                    pypi_0    pypi\r\n[conda] torchvision               0.19.1+cu121             pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi<end_of_turn>\n<start_of_turn>model\nDataset viewer displays wrong statists<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I construct a dataset with the following features:\r\n```\r\nfeatures = Features(\r\n    {\r\n        \"pixel_values\": Array3D(dtype=\"float64\", shape=(3, 224, 224)),\r\n        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\r\n        \"attention_mask\": Sequence(Value(dtype=\"int64\")),\r\n        \"tokens\": Sequence(Value(dtype=\"string\")),\r\n        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    }\r\n)\r\n```\r\nand run `push_to_hub`, the individual `*.parquet` files are pushed, but when trying to upload the auto-generated README, I run into the following error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 261, in hf_raise_for_status\r\n    response.raise_for_status()\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/requests/models.py\", line 1021, in raise_for_status\r\n    raise HTTPError(http_error_msg, response=self)\r\nrequests.exceptions.HTTPError: 400 Client Error: Bad Request for url: https://huggingface.co/api/datasets/looppayments/multitask_document_classification_dataset/commit/main\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/kevintee/loop-payments/ml/src/ml/data_scripts/build_document_classification_training_data.py\", line 297, in <module>\r\n    build_dataset()\r\n  File \"/Users/kevintee/loop-payments/ml/src/ml/data_scripts/build_document_classification_training_data.py\", line 290, in build_dataset\r\n    push_to_hub(dataset, \"multitask_document_classification_dataset\")\r\n  File \"/Users/kevintee/loop-payments/ml/src/ml/data_scripts/build_document_classification_training_data.py\", line 135, in push_to_hub\r\n    dataset.push_to_hub(f\"looppayments/{dataset_name}\", private=True)\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 5577, in push_to_hub\r\n    HfApi(endpoint=config.HF_ENDPOINT).upload_file(\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 3221, in upload_file\r\n    commit_info = self.create_commit(\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2728, in create_commit\r\n    hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\r\n  File \"/Users/kevintee/.pyenv/versions/dev2/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py\", line 299, in hf_raise_for_status\r\n    raise BadRequestError(message, response=response) from e\r\nhuggingface_hub.utils._errors.BadRequestError:  (Request ID: Root=1-64ca9c3d-2d2bbef354e102482a9a168e;bc00371c-8549-4859-9f41-43ff140ad36e)\r\n\r\nBad request for commit endpoint:\r\nInvalid YAML in README.md: unknown tag !<tag:yaml.org,2002:python/tuple> (10:9)\r\n\r\n  7 |         - 3\r\n  8 |         - 224\r\n  9 |         - 224\r\n 10 |         dtype: float64\r\n--------------^\r\n 11 |   - name: input_ids\r\n 12 |     sequence: int64\r\n```\r\n\r\nMy guess is that the auto-generated yaml is unable to be parsed for some reason.\n\n### Steps to reproduce the bug\n\nThe description contains most of what's needed to reproduce the issue, but I've added a shortened code snippet:\r\n\r\n```\r\nfrom datasets import Array2D, Array3D, ClassLabel, Dataset, Features, Sequence, Value\r\nfrom PIL import Image\r\nfrom transformers import AutoProcessor\r\n\r\nfeatures = Features(\r\n    {\r\n        \"pixel_values\": Array3D(dtype=\"float64\", shape=(3, 224, 224)),\r\n        \"input_ids\": Sequence(feature=Value(dtype=\"int64\")),\r\n        \"attention_mask\": Sequence(Value(dtype=\"int64\")),\r\n        \"tokens\": Sequence(Value(dtype=\"string\")),\r\n        \"bbox\": Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    }\r\n)\r\n\r\nprocessor = AutoProcessor.from_pretrained(\"microsoft/layoutlmv3-base\", apply_ocr=False)\r\n\r\ndef preprocess_dataset(rows):\r\n    # Get images\r\n    images = [\r\n        Image.open(png_filename).convert(\"RGB\") for png_filename in rows[\"png_filename\"]\r\n    ]\r\n\r\n    encoding = processor(\r\n        images,\r\n        rows[\"tokens\"],\r\n        boxes=rows[\"bbox\"],\r\n        truncation=True,\r\n        padding=\"max_length\",\r\n    )\r\n    encoding[\"tokens\"] = rows[\"tokens\"]\r\n    return encoding\r\n\r\ndataset = dataset.map(\r\n    preprocess_dataset,\r\n    batched=True,\r\n    batch_size=5,\r\n    features=features,\r\n)\r\n```\n\n### Expected behavior\n\nUsing datasets==2.11.0, I'm able to succesfully push_to_hub, no issues, but with datasets==2.14.2, I run into the above error.\n\n### Environment info\n\n- `datasets` version: 2.14.2\r\n- Platform: macOS-12.5-arm64-arm-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nyaml error using push_to_hub with generated README.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe [current documentation](https://huggingface.co/docs/datasets/en/video_dataset) on a creating a video dataset includes only examples with one media file and one json. It would be useful to have examples where multiple files of the same type are included. For example, in a sign language dataset, you may have a base video and a video annotation of the extracted pose. According to the WebDataset documentation, this should be able to be done with period separated filenames. For example:\n\n```e39871fd9fd74f55.base.mp4\ne39871fd9fd74f55.pose.mp4\ne39871fd9fd74f55.json\nf18b91585c4d3f3e.base.mp4\nf18b91585c4d3f3e.pose.mp4\nf18b91585c4d3f3e.json\n... \n```\n\nIf you can confirm that this method of including multiple media files works with huggingface datasets and include an example in the documentation, I'd appreciate it.<end_of_turn>\n<start_of_turn>model\nDocumentation on multiple media files of the same type with WebDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n> Hi  is there any solution for that eror i try to install this one \npip install torch==1.12.1+cpu torchaudio==0.12.1+cpu -f https://download.pytorch.org/whl/torch_stable.html  \nthis is working fine but tell me how to install pytorch version that is fit for gpu <end_of_turn>\n<start_of_turn>model\nOSError: libcudart.so.11.0: cannot open shared object file: No such file or directory<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\npyarrow 13.0.0 just came out\r\n\r\n```\r\nFAILED tests/test_formatting.py::ArrowExtractorTest::test_pandas_extractor - AssertionError: Attributes of Series are different\r\n\r\nAttribute \"dtype\" are different\r\n[left]:  datetime64[us, UTC]\r\n[right]: datetime64[ns, UTC]\r\n```\r\n\r\n```\r\nFAILED tests/test_table.py::test_cast_sliced_fixed_size_array_to_features - TypeError: Couldn't cast array of type\r\nfixed_size_list<item: int32>[3]\r\nto\r\nSequence(feature=Value(dtype='int64', id=None), length=3, id=None)\r\n```\r\n\r\ne.g. in https://github.com/huggingface/datasets/actions/runs/5952253963/job/16143847230\r\n\r\nfirst error may be related to https://github.com/apache/arrow/issues/33321\r\n\r\nsecond one maybe because `feature.length * len(array) == len(array_values)` is not satisfied anymore somehow ?<end_of_turn>\n<start_of_turn>model\nFix CI for pyarrow 13.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen building/releasing conda package, we get this deprecation warning:\r\n```\r\n/usr/share/miniconda/envs/build-datasets/bin/conda-build:11: DeprecationWarning: conda_build.cli.main_build.main is deprecated and will be removed in 4.0.0. Use `conda build` instead.\r\n```\r\n\r\nThis PR fixes the deprecation warning by using `conda build` instead.<end_of_turn>\n<start_of_turn>model\nFix deprecation warning when building conda package<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have been running lm-eval-harness a lot which has results in an API rate limit. This seems strange, since all of the data should be cached locally. I have in fact verified this.\n\n### Steps to reproduce the bug\n\n1. Be Me\r\n2. Run `load_dataset(\"TAUR-Lab/MuSR\")`\r\n3. Hit rate limit error\r\n4. Dataset is in .cache/huggingface/datasets\r\n5. ???\n\n### Expected behavior\n\nWe should not run into API rate limits if we have cached the dataset\n\n### Environment info\n\ndatasets 2.16.0\r\npython 3.10.4<end_of_turn>\n<start_of_turn>model\nload_dataset ignores cached datasets and tries to hit HF Hub, resulting in API rate limit errors<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ndon't include video bytes when running download_and_prepare(format=\"parquet\")\r\n\r\nthis also affects push_to_hub which will just upload the local paths of the videos though<end_of_turn>\n<start_of_turn>model\nDon't embed videos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n[IterableDatasetDict](https://huggingface.co/docs/datasets/v3.2.0/en/package_reference/main_classes#datasets.IterableDatasetDict.map) map function is missing the `desc` parameter. You can see the equivalent map function for [Dataset here](https://huggingface.co/docs/datasets/v3.2.0/en/package_reference/main_classes#datasets.Dataset.map).\n\nThere might be other parameters missing - I haven't checked.\n\n### Steps to reproduce the bug\n\nfrom datasets import Dataset, IterableDataset, IterableDatasetDict\n\nds = IterableDatasetDict({\"train\": Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=3), \n                          \"validate\": Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=3)})\n\nfor d in ds[\"train\"]:\n    print(d)\n\nds = ds.map(lambda x: {k: v+1 for k, v in x.items()}, desc=\"increment\")\n\nfor d in ds[\"train\"]:\n    print(d)\n\n### Expected behavior\n\nThe description parameter should be available for all datasets (or none). \n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-6.1.85+-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 17.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nSynchronize parameters for various datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR uses `split_generator.split_info` as default value for `split_info` if any exception is raised while trying to get `split_generator.name` from `self.info.splits` (this may happen if there is any error in the metadata dataset_info splits).\r\n\r\nPlease note that `split_info` is only used by the logger.\r\n\r\nFix #5895 if passed `verification_mode=\"no_checks\"`:\r\n```python\r\nds = load_dataset(\r\n    \"ArmelR/stack-exchange-instruction\", \r\n    data_dir=\"data/finetune\", \r\n    split=\"train\", \r\n    verification_mode=\"no_checks\", \r\n    revision=\"c609f1caade5cfbf3b9fe9cfa17d7cb000b457bd\",\r\n)\r\n```<end_of_turn>\n<start_of_turn>model\nMake prepare_split more robust if errors in metadata dataset_info splits<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix typos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRemove upper version limit of fsspec[http]<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe test set labels for the RTE and CoLA datasets when loading via datasets load_dataset are all -1.\r\nEdit: It appears this is also the case for every other dataset except for MRPC (stsb, sst2, qqp, mnli (both matched and mismatched), qnli, wnli, ax). Is this intended behavior to safeguard the test set for evaluation purposes?\r\n\r\n### Steps to reproduce the bug\r\n\r\n!pip install datasets\r\nfrom datasets import load_dataset\r\n\r\nrte_data = load_dataset('glue', 'rte')\r\ncola_data = load_dataset('glue', 'cola')\r\nprint(rte_data['test'][0:30]['label'])\r\nprint(cola_data['test'][0:30]['label'])\r\n\r\nOutput:\r\n[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\r\n\r\nThe non-label test data seems to be fine:\r\ne.g. rte_data['test'][1] is:\r\n{'sentence1': \"Authorities in Brazil say that more than 200 people are being held hostage in a prison in the country's remote, Amazonian-jungle state of Rondonia.\",\r\n 'sentence2': 'Authorities in Brazil hold 200 people as hostage.',\r\n 'label': -1,\r\n 'idx': 1}\r\nTraining and validation data are also fine:\r\ne.g. rte_data['train][0] is:\r\n{'sentence1': 'No Weapons of Mass Destruction Found in Iraq Yet.',\r\n 'sentence2': 'Weapons of Mass Destruction Found in Iraq.',\r\n 'label': 1,\r\n 'idx': 0}\r\n\r\n\r\n### Expected behavior\r\n\r\nExpected the labels to be binary 0/1 values; Got all -1s instead\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.0\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0<end_of_turn>\n<start_of_turn>model\nIncorrect test set labels for RTE and CoLA datasets via load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf you call batched=True when calling `filter`, the first item is _always_ filtered out, regardless of the filter condition.\r\n\r\n\n\n### Steps to reproduce the bug\n\nHere's a minimal example:\r\n```python\r\ndef filter_batch_always_true(batch, indices):\r\n    print(\"First index being passed into this filter function: \", indices[0])\r\n    return indices  # Keep all indices\r\n\r\ndata = {\"value\": list(range(10))}\r\ndataset = Dataset.from_dict(data)\r\n\r\nfiltered_dataset = dataset.filter(filter_batch_always_true, with_indices=True, batched=True)\r\nprint(\"Length of original dataset: \", len(dataset))\r\nprint(\"Length of filtered_dataset: \", len(filtered_dataset))\r\nprint(\"Is equal to original? \", len(filtered_dataset) == len(dataset))\r\nprint(\"First item of filtered dataset: \", filtered_dataset[0])\r\nprint(\"Last item of filtered dataset: \", filtered_dataset[-1])\r\n```\r\nprints:\r\n```\r\nFirst index being passed into this filter function:  0\r\nLength of original dataset:  10\r\nLength of filtered_dataset:  9\r\nIs equal to original?  False\r\nFirst item of filtered dataset:  {'value': 1}\r\nLast item of filtered dataset:  {'value': 9}\r\n```\n\n### Expected behavior\n\nFilter should respect the filter condition.\n\n### Environment info\n\n\r\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.5-arm64-arm-64bit\r\n- Python version: 3.9.18\r\n- Huggingface_hub version: 0.17.1\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 2.0.2\r\n<end_of_turn>\n<start_of_turn>model\n`dataset.filter` ALWAYS removes the first item from the dataset when using batched=True<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI ran a package with pyinstaller and got the following error\uff1a\r\n\r\n\n\n### Steps to reproduce the bug\n\n```  \r\n  ...\r\n  File \"datasets\\__init__.py\", line 52, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 499, in exec_module\r\n  File \"datasets\\inspect.py\", line 30, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 499, in exec_module\r\n  File \"datasets\\load.py\", line 58, in <module>\r\n  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\r\n  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\r\n  File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\r\n  File \"PyInstaller\\loader\\pyimod02_importers.py\", line 499, in exec_module\r\n  File \"datasets\\packaged_modules\\__init__.py\", line 31, in <module>\r\n  File \"inspect.py\", line 1147, in getsource\r\n  File \"inspect.py\", line 1129, in getsourcelines\r\n  File \"inspect.py\", line 958, in findsource\r\nOSError: could not get source code\r\n```\n\n### Expected behavior\n\nI have looked up the relevant information, but I can't find a suitable reason\n\n### Environment info\n\n```python\r\npython 3.10\r\ndatasets 2.14.4\r\npyinstaller 5.6.2\r\n```<end_of_turn>\n<start_of_turn>model\npyinstaller : OSError: could not get source code<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix #6588\r\n\r\nxlistdir return name is empty string\r\n\r\nfor example\uff1a\r\n`\r\nfrom datasets.download.streaming_download_manager import xjoin\r\nfrom datasets.download.streaming_download_manager import xlistdir\r\nconfig = DownloadConfig(storage_options=options)\r\nmanger = StreamingDownloadManager(\"ILSVRC2012\",download_config=config)\r\ninput_path = \"lakefs://datalab/main/imagenet/ILSVRC2012.zip\"\r\n\r\n\r\ndownload_files = manger.download_and_extract(input_path)\r\n\r\ncurrent_dir = xjoin(download_files,\"ILSVRC2012/Images/ILSVRC2012_img_train\")\r\nfolder_list = xlistdir(current_dir)\r\n\r\n`\r\n<end_of_turn>\n<start_of_turn>model\nfix os.listdir return name is empty string<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHey there,\r\n\r\nI've just finished adding support to convert and format to `polars.DataFrame`. This was in response to the open issue about integrating Polars [#3334](https://github.com/huggingface/datasets/issues/3334). Datasets can be switched to Polars format via `Dataset.set_format(\"polars\")`. I've also included `to_polars` and `from_polars`. All polars functions are checked via config.POLARS_AVAILABLE. \r\n\r\nA few notes:\r\nThis only supports `DataFrames` and not `LazyFrames`. This probably could be integrated fairly easily via `is_lazy` args in `set_format`, and `to_polars`.\r\n\r\nLet me know your feedbacks.<end_of_turn>\n<start_of_turn>model\nAdd polars compatibility<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen load a large dataset with the following code\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"liwu/MNBVC\", 'news_peoples_daily', split='train')\r\n```\r\n\r\nWe encountered the error: \"OverflowError: Python int too large to convert to C long\"\r\nThe error look something like:\r\n\r\n```\r\nOverflowError: Python int too large to convert to C long\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nOverflowError                             Traceback (most recent call last)\r\n<ipython-input-7-0ed8700e662d> in <module>\r\n----> 1 dataset = load_dataset(\"liwu/MNBVC\", 'news_peoples_daily', split='train', cache_dir='/sfs/MNBVC/.cache/')\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1749         ignore_verifications=ignore_verifications,\r\n   1750         try_from_hf_gcs=try_from_hf_gcs,\r\n-> 1751         use_auth_token=use_auth_token,\r\n   1752     )\r\n   1753 \r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/builder.py in download_and_prepare(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, **download_and_prepare_kwargs)\r\n    703                     if not downloaded_from_gcs:\r\n    704                         self._download_and_prepare(\r\n--> 705                             dl_manager=dl_manager, verify_infos=verify_infos, **download_and_prepare_kwargs\r\n    706                         )\r\n    707                     # Sync info\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos)\r\n   1225 \r\n   1226     def _download_and_prepare(self, dl_manager, verify_infos):\r\n-> 1227         super()._download_and_prepare(dl_manager, verify_infos, check_duplicate_keys=verify_infos)\r\n   1228 \r\n   1229     def _get_examples_iterable_for_split(self, split_generator: SplitGenerator) -> ExamplesIterable:\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/builder.py in _download_and_prepare(self, dl_manager, verify_infos, **prepare_split_kwargs)\r\n    791             try:\r\n    792                 # Prepare split will record examples associated to the split\r\n--> 793                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n    794             except OSError as e:\r\n    795                 raise OSError(\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/builder.py in _prepare_split(self, split_generator, check_duplicate_keys)\r\n   1219                     writer.write(example, key)\r\n   1220             finally:\r\n-> 1221                 num_examples, num_bytes = writer.finalize()\r\n   1222 \r\n   1223         split_generator.split_info.num_examples = num_examples\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/arrow_writer.py in finalize(self, close_stream)\r\n    536             # Re-intializing to empty list for next batch\r\n    537             self.hkey_record = []\r\n--> 538         self.write_examples_on_file()\r\n    539         if self.pa_writer is None:\r\n    540             if self.schema:\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/arrow_writer.py in write_examples_on_file(self)\r\n    407             # Since current_examples contains (example, key) tuples\r\n    408             batch_examples[col] = [row[0][col] for row in self.current_examples]\r\n--> 409         self.write_batch(batch_examples=batch_examples)\r\n    410         self.current_examples = []\r\n    411 \r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/arrow_writer.py in write_batch(self, batch_examples, writer_batch_size)\r\n    506             col_try_type = try_features[col] if try_features is not None and col in try_features else None\r\n    507             typed_sequence = OptimizedTypedSequence(batch_examples[col], type=col_type, try_type=col_try_type, col=col)\r\n--> 508             arrays.append(pa.array(typed_sequence))\r\n    509             inferred_features[col] = typed_sequence.get_inferred_type()\r\n    510         schema = inferred_features.arrow_schema if self.pa_writer is None else self.schema\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib._handle_arrow_array_protocol()\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/datasets/arrow_writer.py in __arrow_array__(self, type)\r\n    180             else:\r\n    181                 trying_cast_to_python_objects = True\r\n--> 182                 out = pa.array(cast_to_python_objects(data, only_1d_for_numpy=True))\r\n    183             # use smaller integer precisions if possible\r\n    184             if self.trying_int_optimization:\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib.array()\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/pyarrow/array.pxi in pyarrow.lib._sequence_to_array()\r\n\r\n/sfs/MNBVC/venv/lib64/python3.6/site-packages/pyarrow/error.pxi in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nOverflowError: Python int too large to convert to C long\r\n```\r\n\r\nHowever, that dataset can be loaded in a streaming manner:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"liwu/MNBVC\", 'news_peoples_daily', split='train', streaming=True)\r\n\r\nfor i in dataset:\r\n    pass  # it work well\r\n```\r\n\r\nAnother issue is reported in our dataset hub:\r\nhttps://huggingface.co/datasets/liwu/MNBVC/discussions/2\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"liwu/MNBVC\", 'news_peoples_daily', split='train')\r\n\r\n### Expected behavior\r\n\r\nthe dataset can be safely loaded\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.4.0\r\n- Platform: Linux-3.10.0-1160.an7.x86_64-x86_64-with-centos-7.9\r\n- Python version: 3.6.8\r\n- PyArrow version: 6.0.1\r\n- Pandas version: 1.1.5<end_of_turn>\n<start_of_turn>model\nGet an error \"OverflowError: Python int too large to convert to C long\" when loading a large dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI met an error when running a notebook provide by Hugging Face,  and met the error.\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[2], line 5\r\n      3 # Load the enhancers dataset from the InstaDeep Hugging Face ressources\r\n      4 dataset_name = \"enhancers_types\"\r\n----> 5 train_dataset_enhancers = load_dataset(\r\n      6         \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n      7         dataset_name,\r\n      8         split=\"train\",\r\n      9         streaming= False,\r\n     10     )\r\n     11 test_dataset_enhancers = load_dataset(\r\n     12         \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n     13         dataset_name,\r\n     14         split=\"test\",\r\n     15         streaming= False,\r\n     16     )\r\n\r\nFile /public/home/hhl/miniconda3/envs/transformer/lib/python3.9/site-packages/datasets/load.py:2129, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2124 verification_mode = VerificationMode(\r\n   2125     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\r\n   2126 )\r\n   2128 # Create a dataset builder\r\n-> 2129 builder_instance = load_dataset_builder(\r\n   2130     path=path,\r\n   2131     name=name,\r\n   2132     data_dir=data_dir,\r\n   2133     data_files=data_files,\r\n   2134     cache_dir=cache_dir,\r\n   2135     features=features,\r\n   2136     download_config=download_config,\r\n   2137     download_mode=download_mode,\r\n   2138     revision=revision,\r\n   2139     token=token,\r\n   2140     storage_options=storage_options,\r\n   2141     trust_remote_code=trust_remote_code,\r\n   2142     _require_default_config_name=name is None,\r\n   2143     **config_kwargs,\r\n   2144 )\r\n   2146 # Return iterable dataset in case of streaming\r\n   2147 if streaming:\r\n\r\nFile /public/home/hhl/miniconda3/envs/transformer/lib/python3.9/site-packages/datasets/load.py:1886, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\r\n   1884 builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\r\n   1885 # Instantiate the dataset builder\r\n-> 1886 builder_instance: DatasetBuilder = builder_cls(\r\n   1887     cache_dir=cache_dir,\r\n   1888     dataset_name=dataset_name,\r\n   1889     config_name=config_name,\r\n   1890     data_dir=data_dir,\r\n   1891     data_files=data_files,\r\n   1892     hash=dataset_module.hash,\r\n   1893     info=info,\r\n   1894     features=features,\r\n   1895     token=token,\r\n   1896     storage_options=storage_options,\r\n   1897     **builder_kwargs,\r\n   1898     **config_kwargs,\r\n   1899 )\r\n   1900 builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\r\n   1902 return builder_instance\r\n\r\nTypeError: 'NoneType' object is not callable\r\n```\r\n\r\nI have checked my internet, it worked well. And the dataset name was just copied from the Hugging Face. \r\nTotally no idea what is wrong! \n\n### Steps to reproduce the bug\n\nTo reproduce the bug you may run\r\n\r\n```\r\nfrom datasets import load_dataset, Dataset\r\n\r\n# Load the enhancers dataset from the InstaDeep Hugging Face ressources\r\ndataset_name = \"enhancers_types\"\r\ntrain_dataset_enhancers = load_dataset(\r\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n        dataset_name,\r\n        split=\"train\",\r\n        streaming= False,\r\n    )\r\ntest_dataset_enhancers = load_dataset(\r\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks_revised\",\r\n        dataset_name,\r\n        split=\"test\",\r\n        streaming= False,\r\n    )\r\n```\n\n### Expected behavior\n\n1. what may be the reasons of the error\r\n2. how can I fine which reason lead to the error\r\n3. how can I save the problem\n\n### Environment info\n\n```\r\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.15.0-117-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.21\r\n- `huggingface_hub` version: 0.27.0\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0\r\n```<end_of_turn>\n<start_of_turn>model\nerror when loading dataset in Hugging Face: NoneType error is not callable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nload_dataset simply hangs. It happens once every ~5 times, and interestingly hangs for a multiple of 5 minutes (hangs for 5/10/15 minutes). Using the profiler in PyCharm shows that it spends the time at <method 'connect' of '_socket.socket' objects>. However, a local cache is available so I am not sure why socket is needed. ([profiler result](https://ibb.co/0Btbbp8))\r\n\r\nIt only happens on WSL for me. It works for native Windows and my MacBook. (cache quickly recognized and loaded within a second).\r\n\r\n\n\n### Steps to reproduce the bug\n\nI am using Ubuntu 22.04.2 LTS (GNU/Linux 5.15.90.1-microsoft-standard-WSL2 x86_64)\r\n\r\nPython 3.10.10 (main, Mar 21 2023, 18:45:11) [GCC 11.2.0] on linux\r\n>>> import datasets\r\n>>> datasets.load_dataset('ai2_arc', 'ARC-Challenge')  # hangs for 5/10/15 minutes\n\n### Expected behavior\n\ncache quickly recognized and loaded within a second\n\n### Environment info\n\nPlease let me know if I should provide more environment information.<end_of_turn>\n<start_of_turn>model\nload_dataset hangs on WSL<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd a `batch` method to the Dataset class, similar to the one recently implemented for `IterableDataset` in PR #7054.\n\n### Motivation\n\nA batched iteration speeds up data loading significantly (see e.g. #6279)\n\n### Your contribution\n\nI plan to open a PR to implement this. <end_of_turn>\n<start_of_turn>model\nAdd `batch` method to `Dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs documented in the [official docs](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/loading_methods#datasets.load_dataset.example-2), I tried to load datasets from local files by\r\n```python\r\n# Load a JSON file\r\nfrom datasets import load_dataset\r\nds = load_dataset('json', data_files='path/to/local/my_dataset.json')\r\n```\r\nBut this failed on a web request because I'm executing the script on a machine without Internet access. Stacktrace shows\r\n```\r\nin PackagedDatasetModuleFactory.__init__(self, name, data_dir, data_files, download_config, download_mode)\r\n    940 self.download_config = download_config\r\n    941 self.download_mode = download_mode\r\n--> 942 increase_load_count(name, resource_type=\"dataset\")\r\n```\r\nI've read from the source code that this can be fixed by setting environment variable to run in offline mode. I'm just wondering that is this an expected behaviour that even loading a LOCAL JSON file requires Internet access by default? And what's the point of requesting to `increase_load_count` on some server when loading just LOCAL data files?<end_of_turn>\n<start_of_turn>model\nLoading local data files initiates web requests<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nClose #6330, close #6333.\r\n\r\n`fsspec.implementations.LocalFilesystem.protocol`\r\nwas changed from `str` \"file\" to `tuple[str,...]` (\"file\", \"local\") in `fsspec>=2023.10.0`\r\n\r\nThis commit supports both styles.<end_of_turn>\n<start_of_turn>model\ndatasets.filesystems: fix is_remote_filesystems<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollowing the conversion guide, copy=False is no longer required and will result in an error: https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword.\r\n\r\nThe following fix should resolve the issue.\r\n\r\nerror found during testing on the MTEB repository e.g. [here](https://github.com/embeddings-benchmark/mteb/pull/938)<end_of_turn>\n<start_of_turn>model\nEnsure compatibility with numpy 2.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt was not reading the feature type from the parquet arrow schema<end_of_turn>\n<start_of_turn>model\nFix streaming parquet with image feature in schema<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIf we write this code: \r\n\r\n```python\r\n\"\"\"Get data and define datasets.\"\"\"\r\n\r\nfrom enum import StrEnum\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\nfrom torchvision import transforms\r\n\r\n\r\nclass Split(StrEnum):\r\n    \"\"\"Describes what type of split to use in the dataloader\"\"\"\r\n\r\n    TRAIN = \"train\"\r\n    TEST = \"test\"\r\n    VAL = \"validation\"\r\n\r\n\r\nclass ImageNetDataLoader(DataLoader):\r\n    \"\"\"Create an ImageNetDataloader\"\"\"\r\n\r\n    _preprocess_transform = transforms.Compose(\r\n        [\r\n            transforms.Resize(256),\r\n            transforms.CenterCrop(224),\r\n        ]\r\n    )\r\n\r\n    def __init__(self, batch_size: int = 4, split: Split = Split.TRAIN):\r\n        dataset = (\r\n            load_dataset(\r\n                \"imagenet-1k\",\r\n                split=split,\r\n                trust_remote_code=True,\r\n                streaming=True,\r\n            )\r\n            .with_format(\"torch\")\r\n            .map(self._preprocess)\r\n        )\r\n\r\n        super().__init__(dataset=dataset, batch_size=batch_size)\r\n\r\n    def _preprocess(self, data):\r\n        if data[\"image\"].shape[0] < 3:\r\n            data[\"image\"] = data[\"image\"].repeat(3, 1, 1)\r\n        data[\"image\"] = self._preprocess_transform(data[\"image\"].float())\r\n        return data\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    dataloader = ImageNetDataLoader(batch_size=2)\r\n    for batch in dataloader:\r\n        print(batch[\"image\"])\r\n        break\r\n```\r\nThis will trigger an user warning : \r\n\r\n```bash\r\ndatasets\\formatting\\torch_formatter.py:85: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\r\n  return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\r\n```\n\n### Motivation\n\nThis happens because the the way the formatted tensor is returned in `TorchFormatter._tensorize`.\r\nThis function handle values of different types, according to some tests it seems that possible value types are `int`, `numpy.ndarray` and `torch.Tensor`.\r\nIn particular this warning is triggered when the value type is `torch.Tensor`, because is not the suggested Pytorch way of doing it:\r\n\r\n- https://stackoverflow.com/questions/55266154/pytorch-preferred-way-to-copy-a-tensor\r\n- https://discuss.pytorch.org/t/it-is-recommended-to-use-source-tensor-clone-detach-or-sourcetensor-clone-detach-requires-grad-true/101218#:~:text=The%20warning%20points%20to%20wrapping%20a%20tensor%20in%20torch.tensor%2C%20which%20is%20not%20recommended.%0AInstead%20of%20torch.tensor(outputs)%20use%20outputs.clone().detach()%20or%20the%20same%20with%20.requires_grad_(True)%2C%20if%20necessary.\r\n\r\n\n\n### Your contribution\n\nA solution that I found to be working is to change the current way of doing it: \r\n\r\n```python\r\nreturn torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\r\n```\r\n\r\nTo:\r\n\r\n```python\r\nif (isinstance(value, torch.Tensor)):\r\n    tensor = value.clone().detach()\r\n    if self.torch_tensor_kwargs.get('requires_grad', False): \r\n        tensor.requires_grad_()\r\n    return tensor\r\nelse:\r\n    return torch.tensor(value, **{**default_dtype, **self.torch_tensor_kwargs})\r\n```<end_of_turn>\n<start_of_turn>model\nDisable warning when using with_format format on tensors<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix `resolve_pattern` for filesystems with tuple protocol.\r\n\r\nFix #6100.\r\n\r\nThe bug code lines were introduced by:\r\n- #6028<end_of_turn>\n<start_of_turn>model\nFix error when loading from GCP bucket<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI used a regex to filter the data files based on their extension for packaged builders.\r\n\r\nI tried and a regex is 10x faster that using `in` to check if the extension is in the list of supported extensions.\r\n\r\nSupersedes https://github.com/huggingface/datasets/pull/5850\r\n\r\nClose https://github.com/huggingface/datasets/issues/5849\r\n\r\nI also did a small change to favor the parquet module in case of a draw in the extension counter.<end_of_turn>\n<start_of_turn>model\nFilter unsupported extensions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nI would like a way to resume training from a checkpoint without waiting for a very long time when using an iterable dataset.\r\n\r\n### Motivation\r\n\r\nI am training models on the speech-recognition task. I have very large datasets that I can't comfortably store on a disk and also quite computationally intensive audio processing to do. As a result I want to load data from my remote when it is needed and perform all processing on the fly.\r\n\r\nI am currently using the iterable dataset feature of _datasets_. It does everything I need with one exception. My issue is that when resuming training at a step n, we have to download all the data and perform the processing of steps < n, just to get the iterable at the right step. In my case it takes almost as long as training for the same steps, which make resuming training from a checkpoint useless in practice.\r\n\r\nI understand that the nature of iterators make it probably nearly impossible to quickly resume training.\r\n\r\nI thought about a possible solution nonetheless : \r\n\r\nI could in fact index my large dataset and make it a mapped dataset. Then I could use set_transform to perform the processing on the fly. Finally, if I'm not mistaken, the _accelerate_ package allows to [skip steps efficiently](https://github.com/huggingface/accelerate/blob/a73898027a211c3f6dc4460351b0ec246aa824aa/src/accelerate/data_loader.py#L827) for a mapped dataset.\r\n\r\nIs it possible to lazily load samples of a mapped dataset ? I'm used to [dataset scripts](https://huggingface.co/docs/datasets/dataset_script), maybe something can be done there.\r\nIf not, I could do it using a plain _Pytorch_ dataset. Then I would need to convert it to a _datasets_' dataset to get all the features of _datasets_. Is it something possible ?\r\n\r\n### Your contribution\r\n\r\nI could provide a PR to allow lazy loading of mapped dataset or the conversion of a mapped _Pytorch_ dataset into a _Datasets_ dataset if you think it is an useful new feature.<end_of_turn>\n<start_of_turn>model\nOffer an alternative to Iterable Dataset that allows lazy loading and processing while skipping batches efficiently<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUntil now, all repos were public by default when created without passing the `private` argument. This meant that passing `private=False` or `private=None` was strictly the same. This is not the case anymore. Enterprise Hub offers organizations to set a default visibility setting for new repos. This is useful for organizations forbidding public repos for security matters. This PR mostly updates docstrings + default values so that `private=None` is always passed when users don't set it manually.\r\n\r\nThis PR doesn't create any breaking change. The real update has been done server-side when introducing the new Enterprise Hub feature. Related to https://github.com/huggingface/huggingface_hub/pull/2679.<end_of_turn>\n<start_of_turn>model\nLet server decide default repo visibility<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.14.6<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nresume from ckpt skips samples if .map is applied\n\nMaybe related: https://github.com/huggingface/datasets/issues/7538\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import Dataset\nfrom datasets.distributed import split_dataset_by_node\n\n\n# Create dataset with map transformation\ndef create_dataset():\n    ds = Dataset.from_dict({\"id\": list(range(100))})\n    ds = ds.to_iterable_dataset(num_shards=4)\n    ds = ds.map(lambda x: x) #comment it out to get desired behavior\n    ds = split_dataset_by_node(ds, rank=0, world_size=2)\n    return ds\n\n\nds = create_dataset()\n\n# Iterate and save checkpoint after 10 samples\nit = iter(ds)\nfor idx, sample in enumerate(it):\n    if idx == 9:  # Checkpoint after 10 samples\n        checkpoint = ds.state_dict()\n        print(f\"Checkpoint saved at sample: {sample['id']}\")\n        break\n\n# Continue with original iterator\noriginal_next_samples = []\nfor idx, sample in enumerate(it):\n    original_next_samples.append(sample[\"id\"])\n    if idx >= 4:\n        break\n\n# Resume from checkpoint\nds_new = create_dataset()\nds_new.load_state_dict(checkpoint)\n\n# Get samples from resumed iterator\nit_new = iter(ds_new)\nresumed_next_samples = []\nfor idx, sample in enumerate(it_new):\n    resumed_next_samples.append(sample[\"id\"])\n    if idx >= 4:\n        break\n\nprint(f\"\\nExpected next samples: {original_next_samples}\")\nprint(f\"Actual next samples:   {resumed_next_samples}\")\nprint(\n    f\"\\n\u274c BUG: {resumed_next_samples[0] - original_next_samples[0]} samples were skipped!\"\n)\n\n```\n\nWith map\n```\nCheckpoint saved at sample: 9\n\nExpected next samples: [10, 11, 12, 13, 14]\nActual next samples:   [50, 51, 52, 53, 54]\n\n\u274c BUG: 40 samples were skipped!\n```\n\n### Expected behavior\n\nwithout map\n```\nExpected next samples: [10, 11, 12, 13, 14]\nActual next samples:   [10, 11, 12, 13, 14]\n\n\u274c BUG: 0 samples were skipped!\n```\n\n### Environment info\n\ndatasets == 3.6.0<end_of_turn>\n<start_of_turn>model\n[bug] resume from ckpt skips samples if .map is applied<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a bug in the `generate_examples` function where `datasets.Value` features with a `float` dtype were incorrectly generated using `np.random.randint`. This resulted in integer values being cast to float, which is not representative of true floating-point data.\r\n\r\n**Key changes include:**\r\n1.  Added explicit handling for float features using `np.random.rand` to generate continuous values.\r\n2.  Introduced fail-fast type checks for unsupported dtypes to improve robustness.\r\n3.  Added validation for sequence features to ensure `seq_shapes` is provided.\r\n\r\n\r\n### Before Fix\r\nFloat features were generated incorrectly as integers cast to float:\r\n\r\n```text\r\nExample 0:\r\nint_feature: 0\r\nfloat_feature: 9.0  <-- Incorrect: An integer disguised as a float\r\nstring_feature: The small grey turtle was surprisingly fast...\r\nseq_feature: [0.3048 0.4291 0.4283]\r\n```\r\n### After Fix\r\nFloat features are now correctly generated as continuous numbers in the range [0, 1):\r\n```text\r\n\r\nExample 0:\r\nint_feature: 0\r\nfloat_feature: 0.0183  <-- Correct: A true random float\r\nstring_feature: The small grey turtle was surprisingly fast...\r\nseq_feature: [0.9237 0.7972 0.8526]<end_of_turn>\n<start_of_turn>model\nFix: Correct float feature generation in `generate_examples`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAvoid calling `http_head` for non-HTTP URLs, by adding and `else` statement.\r\n\r\nCurrently, it makes an unnecessary HTTP call (which adds latency) for non-HTTP protocols, like FTP, S3,...\r\n\r\nI discovered this while working in an unrelated issue.<end_of_turn>\n<start_of_turn>model\nAvoid calling http_head for non-HTTP URLs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen concatenating datasets with `concatenate_datasets`, I would expect the resulting combined dataset to be in the same format as the inputs (assuming it's consistent). This is indeed the behavior when combining `Dataset`, but not when combining `IterableDataset`. Specifically, when applying `concatenate_datasets` to a list of `IterableDataset` in Pytorch format (i.e. using `.with_format(Pytorch)`), the output `IterableDataset` is not in Pytorch format.\n\n### Steps to reproduce the bug\n\n```\nimport datasets\nds = datasets.Dataset.from_dict({\"a\": [1,2,3]})\niterable_ds = ds.to_iterable_dataset()\ndatasets.concatenate_datasets([ds.with_format(\"torch\")]) # <- this preserves Pytorch format\ndatasets.concatenate_datasets([iterable_ds.with_format(\"torch\")]) # <- this does NOT preserves Pytorch format\n```\n\n### Expected behavior\n\nPytorch format should be preserved when combining IterableDataset in Pytorch format.\n\n### Environment info\n\ndatasets==3.5.0, Python 3.11.11, torch==2.2.2<end_of_turn>\n<start_of_turn>model\n`concatenate_datasets` does not preserve Pytorch format for IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6149 <end_of_turn>\n<start_of_turn>model\nFix Parquet loading with `columns`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI try to use a dataset with streaming=True, the issue I have is that the RAM usage becomes higher and higher until it is no longer sustainable.\r\n\r\nI understand that huggingface store data in ram during the streaming, and more worker in dataloader there are, more a lot of shard will be stored in ram, but the issue I have is that the ram usage is not constant. So after each new shard loaded, the ram usage will be higher and higher.\r\n\r\n### Steps to reproduce the bug\r\n\r\nYou can run this code and see you ram usage, after each shard of 255 examples, your ram usage will be extended.\r\n```py\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\n\r\ndataset = load_dataset(\"WaveGenAI/dataset\", streaming=True)\r\n\r\ndataloader = DataLoader(dataset[\"train\"], num_workers=3)\r\n\r\nfor i, data in enumerate(dataloader):\r\n    print(i, end=\"\\r\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe Ram usage should be always the same (just 3 shards loaded in the ram).\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.0.1\r\n- Platform: Linux-6.10.5-arch1-1-x86_64-with-glibc2.40\r\n- Python version: 3.12.4\r\n- `huggingface_hub` version: 0.26.0\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nMemory leak when streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen applying with_format -> map -> with_format(None), array dtypes seem to change, even if features are passed \r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfeatures=Features(**{\"array0\": Array3D((None, 10, 10), dtype=\"float32\")})\r\ndataset = Dataset.from_dict({f\"array0\": [np.zeros((100,10,10), dtype=np.float32)]*25}, features=features)\r\nds = dataset.to_iterable_dataset().with_format(\"numpy\").map(lambda x: x, features=features)\r\nex_0 = next(iter(ds))\r\nds = dataset.to_iterable_dataset().with_format(\"numpy\").map(lambda x: x, features=features).with_format(None)\r\nex_1 = next(iter(ds))\r\nassert ex_1[\"array0\"].dtype == ex_0[\"array0\"].dtype, f\"{ex_1['array0'].dtype} {ex_0['array0'].dtype}\"\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nDtypes should be preserved.\r\n\r\n### Environment info\r\n\r\n3.0.2<end_of_turn>\n<start_of_turn>model\nFormatted map + with_format(None) changes array dtype for iterable datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\n- config_name: some_config\n  data_files:\n  - split: train\n    path:\n    - images/xyz/*.jpg\n```\nwill correctly download but\n\n```\n- config_name: some_config\n  data_files:\n  - split: train\n    path:\n    - ./images/xyz/*.jpg\n```\nwill error with `FileNotFoundError` due to improper url joining. `load_dataset` on the same directory locally works fine.\n\n### Steps to reproduce the bug\n\n1. create a README.md with the front matter of the form\n\n```\n- config_name: some_config\n  data_files:\n  - split: train\n    path:\n    - ./images/xyz/*.jpg\n```\n\n2. `touch ./images/xyz/1.jpg`\n3. Observe this directory loads with `load_dataset(\"filesystem_path\", \"some_config\")` correctly.\n4. Observe exceptions when you load this with `load_dataset(\"repoid/filesystem_path\", \"some_config\")`\n\n### Expected behavior\n\n`./` prefix should be interpreted correctly\n\n### Environment info\n\ndatasets 4.0.0\ndatasets 3.4.0\n\nreproduce<end_of_turn>\n<start_of_turn>model\nconfig paths that start with ./ are not valid as hf:// accessed repos, but are valid when accessed locally<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I use hf datasets on 4 GPU with 40 workers I get some extreme memory bandwidth of constant ~3GB/s. \n\nHowever, if I wrap the dataset in `IterableDataset`, this issue is gone and the data also loads way faster (4x faster training on 1 worker).\n\nIt seems like the workers don't share memory and basically duplicate the data 4x40.\n\n\n\n### Steps to reproduce the bug\n\nTrainer arguments:\n```\n    dataloader_pin_memory=True,\n    dataloader_num_workers=40,\n    dataloader_prefetch_factor=2,\n    dataloader_persistent_workers=True,\n```\nCall trainer:\n```\ntrainer = Trainer(\n    model=model,\n    args=train_args,\n    train_dataset=load_from_disk('..').with_fromat('torch'),\n)\n```\nThe dataset has 600GB and consists of 1225 files.\n\n### Expected behavior\n\nThe optimal bandwidth should be 100MB/s to keep up with GPU.\n\n### Environment info\n\nLinux\nPython 3.11\ndatasets==3.2.0\n\n<end_of_turn>\n<start_of_turn>model\nExtreme memory bandwidth.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ncorrect use with polars example<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I try to load an already downloaded dataset with num_proc=64, the speed is very high for the first 10-20 seconds acheiving 30-40K samples / s, and 100% utilization for all cores but it soon drops to <= 1000 with almost 0% utilization for most cores. \n\n### Steps to reproduce the bug\n\n```\n// download dataset with cli\n!huggingface-cli download --repo-type dataset timm/imagenet-1k-wds --max-workers 32\n\nfrom datasets import load_dataset\nds = load_dataset(\"timm/imagenet-1k-wds\", num_proc=64)\n```\n\n\n### Expected behavior\n\n100% core utilization throughout. \n\n### Environment info\n\nAzure A100-80GB, 16 cores VM\n\n![Image](https://github.com/user-attachments/assets/69d00fe3-d720-4474-9439-21e046d85034)<end_of_turn>\n<start_of_turn>model\nnum_proc parallelization works only for first ~10s.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nadded `token` to the `_datasets_server` functions<end_of_turn>\n<start_of_turn>model\nUse auth to get parquet export<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"finetune.py\", line 103, in <module>\r\n    main(args)\r\n  File \"finetune.py\", line 45, in main\r\n    data_tokenized = data.map(partial(funcs.tokenize_function, tokenizer,\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 868, in map\r\n    {\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 869, in <dictcomp>\r\n    k: dataset.map(\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 592, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3093, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 3432, in _map_single\r\n    arrow_formatted_shard = shard.with_format(\"arrow\")\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 2667, in with_format\r\n    dataset = copy.deepcopy(self)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/copy.py\", line 172, in deepcopy\r\n    y = _reconstruct(x, memo, *rv)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/copy.py\", line 270, in _reconstruct\r\n    state = deepcopy(state, memo)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/copy.py\", line 146, in deepcopy\r\n    y = copier(x, memo)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/copy.py\", line 230, in _deepcopy_dict\r\n    y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/copy.py\", line 153, in deepcopy\r\n    y = copier(memo)\r\n  File \"/opt/conda/envs/ptca/lib/python3.8/site-packages/datasets/table.py\", line 176, in __deepcopy__\r\n    memo[id(self._batches)] = list(self._batches)\r\nAttributeError: 'InMemoryTable' object has no attribute '_batches'\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nI'm running an MLOps flow using AzureML.  \r\nThe error appears when I run the following function in my training script:\r\n\r\n```python\r\ndata_tokenized = data.map(partial(funcs.tokenize_function, tokenizer,\r\n                                      seq_length),\r\n                              batched=True,\r\n                              batch_size=batch_size,\r\n                              remove_columns=['col1', 'col2'])\r\n```\r\n\r\n```python\r\ndef tokenize_function(tok, seq_length, example)\r\n    # Pad so that each batch has the same sequence length\r\n    inp = tok(example['col1'], padding=True, truncation=True)\r\n    outp = tok(example['col2'], padding=\"max_length\", max_length=seq_length)\r\n\r\n    res = {\r\n        'input_ids': inp['input_ids'],\r\n        'attention_mask': inp['attention_mask'],\r\n        'decoder_input_ids': outp['input_ids'],\r\n        'labels': outp['input_ids'],\r\n        'decoder_attention_mask': outp['attention_mask']\r\n    }\r\n    return res\r\n```\r\n\r\n### Expected behavior\r\n\r\nProcessing proceeds without errors.  I ran this same workflow 2 weeks ago without a problem.  I recreated the environment since then but it doesn't appear that datasets versions have changed since Dec. '23.  \r\n\r\n### Environment info\r\n\r\ndatasets 2.16.1\r\ntransformers 4.35.2\r\npyarrow 15.0.0\r\npyarrow-hotfix 0.6\r\ntorch 2.0.1\r\n\r\nI'm not using the latest transformers version because there was an error due to a conflict with Azure mlflow when I tried the last time.  <end_of_turn>\n<start_of_turn>model\nAttributeError: 'InMemoryTable' object has no attribute '_batches'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix the export of a missing method of `Dataset`<end_of_turn>\n<start_of_turn>model\nExport to_iterable_dataset to document<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFor me, with ~40k lines, sorting took 3.5 seconds on a flattened dataset (including the flatten operation) and 22.7 seconds on a mapped dataset (right after sharding), which is about x5 slowdown. Moreover, it seems like it slows down exponentially with bigger datasets (wasn't able to sort 700k lines at all, with flattening takes about a minute).\n\n### Steps to reproduce the bug\n\n```Python\r\nfrom datasets import load_dataset\r\nimport time\r\n\r\ndataset = load_dataset(\"xnli\", \"en\", split=\"train\")\r\n\r\ndataset = dataset.shard(10, 0)\r\n\r\nprint(len(dataset))\r\n\r\nt = time.time()\r\n\r\n# dataset = dataset.flatten_indices()  # uncomment this line and it's fast\r\n\r\ndataset = dataset.sort(\"label\", reverse=True, load_from_cache_file=False)\r\n\r\nprint(f\"finished in {time.time() - t:.4f} seconds\")\r\n\r\n```\n\n### Expected behavior\n\nExpect sorting to take the same or less time than flattening and then sorting.\n\n### Environment info\n\n- `datasets` version: 2.12.1.dev0 (same with 2.12.0 too)\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1<end_of_turn>\n<start_of_turn>model\nUnbearably slow sorting on big mapped datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using load_dataset() from the datasets library (in load.py), specifying a particular split (e.g., split=\"train\") still results in downloading data for all splits when streaming=False. This happens during the builder_instance.download_and_prepare() call.\nThis behavior leads to unnecessary bandwidth usage and longer download times, especially for large datasets, even if the user only intends to use a single split.\n\n### Steps to reproduce the bug\n\ndataset_name = \"skbose/indian-english-nptel-v0\"\ndataset = load_dataset(dataset_name, token=hf_token, split=\"test\")\n\n\n### Expected behavior\n\nOptimize the download logic so that only the required split is downloaded when streaming=False when a specific split is provided.\n\n### Environment info\n\nDataset: skbose/indian-english-nptel-v0\nPlatform: M1 Apple Silicon\nPython verison: 3.12.9\ndatasets>=3.5.0<end_of_turn>\n<start_of_turn>model\nRequesting a specific split (eg: test) still downloads all (train, test, val) data when streaming=False.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCI fails on Windows for test_delete_from_hub after the merge of:\r\n- #6820\r\n\r\nThis is weird because the CI was green in the PR branch before merging to main.\r\n\r\n```\r\nFAILED tests/test_hub.py::test_delete_from_hub - AssertionError: assert [CommitOperat...\\r\\n---\\r\\n')] == [CommitOperat...in/*\\n---\\n')]\r\n  \r\n  At index 1 diff: CommitOperationAdd(path_in_repo='README.md', path_or_fileobj=b'---\\r\\nconfigs:\\r\\n- config_name: cats\\r\\n  data_files:\\r\\n  - split: train\\r\\n    path: cats/train/*\\r\\n---\\r\\n') != CommitOperationAdd(path_in_repo='README.md', path_or_fileobj=b'---\\nconfigs:\\n- config_name: cats\\n  data_files:\\n  - split: train\\n    path: cats/train/*\\n---\\n')\r\n  \r\n  Full diff:\r\n    [\r\n        CommitOperationDelete(\r\n            path_in_repo='dogs/train/0000.csv',\r\n            is_folder=False,\r\n        ),\r\n        CommitOperationAdd(\r\n            path_in_repo='README.md',\r\n  -         path_or_fileobj=b'---\\nconfigs:\\n- config_name: cats\\n  data_files:\\n '\r\n  ?                                                                       --------\r\n  +         path_or_fileobj=b'---\\r\\nconfigs:\\r\\n- config_name: cats\\r\\n  data_f'\r\n  ?                               ++          ++                     ++\r\n  -                         b' - split: train\\n    path: cats/train/*\\n---\\n',\r\n  ?                                                                   ^^^^^^ -\r\n  +                         b'iles:\\r\\n  - split: train\\r\\n    path: cats/train/*\\r'\r\n  ?                           ++++++++++                ++                        ^\r\n  +                         b'\\n---\\r\\n',\r\n        ),\r\n    ]\r\n```<end_of_turn>\n<start_of_turn>model\nCI fails on Windows for test_delete_from_hub and test_xgetsize_private due to new-line character<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRequested in https://discuss.huggingface.co/t/using-set-transform-on-a-dataset-leads-to-an-exception/53036.\r\n\r\nI am not sure if supporting this is the best idea for the following reasons:\r\n\r\n>For this to work, we would have to pickle a custom transform, which means the transform and the objects it references need to be serializable. Also, deserializing these bytes would make `load_from_disk` unsafe, so I'm not sure this is a good idea. \r\n\r\n@lhoestq WDYT?\r\n<end_of_turn>\n<start_of_turn>model\nSupport saving datasets with custom formatting<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```python\r\nfrom datasets import Dataset\r\n\r\ndef worker(example):\r\n    while True:\r\n        continue\r\n    example['a'] = 100\r\n    return example\r\n\r\ndata = Dataset.from_list([{\"a\": 1}, {\"a\": 2}])\r\ndata = data.map(worker)\r\nprint(data[0])\r\n```\r\n\r\nI'm implementing a worker function whose runtime will depend on specific examples (e.g., while most examples take 0.01s in worker, several examples may take 50s).\r\n\r\nTherefore, I would like to know how the current implementation will handle those subprocesses that require a long (e.g., >= 5min) or even infinite time.\r\n\r\nI notice that the current implementation set a timeout of 0.05 second\r\nhttps://github.com/huggingface/datasets/blob/c3ddb1ef00334a6f973679a51e783905fbc9ef0b/src/datasets/utils/py_utils.py#L674\r\n\r\nHowever, this example code still gets stuck.\n\n### Steps to reproduce the bug\n\nrun the example above\n\n### Expected behavior\n\nI want to set a default worker to handle these timeout cases, instead of getting stuck\n\n### Environment info\n\nmain branch version<end_of_turn>\n<start_of_turn>model\nTimeoutError in map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn some file systems (like luster), memory mapping arrow files takes time. This can be accelerated by performing the mmap in parallel on processes or threads.\r\n\r\n- Threads seem to be faster than processes when gathering the list of tables from the workers (see https://github.com/huggingface/datasets/issues/2252).\r\n- I'm not sure if using threads would respect the\u00a0`IN_MEMORY_MAX_SIZE` config.\r\n- I'm not sure if we need to expose\u00a0num_proc\u00a0from\u00a0`BaseReader.read`\u00a0to\u00a0`DatasetBuilder.as_dataset`. Since `\u00a0DatasetBuilder.as_dataset` is used in many places beside `load_dataset`.\r\n\r\n### Tests on luster file system (on a shared partial node):\r\nLoading 1231 shards of ~2GBs.\r\nThe files were pre-loaded in another process before the script runs (couldn't get a fresh node).\r\n```python\r\nimport logging\r\nfrom time import perf_counter\r\n\r\nimport datasets\r\nlogger = datasets.logging.get_logger(__name__)\r\ndatasets.logging.set_verbosity_info()\r\nlogging.basicConfig(level=logging.DEBUG, format=\"%(message)s\")\r\n\r\nclass catchtime:\r\n    # context to measure loading time: https://stackoverflow.com/questions/33987060/python-context-manager-that-measures-time\r\n    def __init__(self, debug_print=\"Time\", logger=logger):\r\n        self.debug_print = debug_print\r\n        self.logger = logger\r\n\r\n    def __enter__(self):\r\n        self.start = perf_counter()\r\n        return self\r\n\r\n    def __exit__(self, type, value, traceback):\r\n        self.time = perf_counter() - self.start\r\n        readout = f\"{self.debug_print}: {self.time:.3f} seconds\"\r\n        self.logger.info(readout)\r\n\r\ndataset_path=\"\"\r\n\r\n# warmup\r\nwith catchtime(\"Loading in parallel\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=16)\r\n\r\n# num_proc=16\r\nwith catchtime(\"Loading in parallel\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=16)\r\n# num_proc=32\r\nwith catchtime(\"Loading in parallel\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=32)\r\n# num_proc=1\r\nwith catchtime(\"Loading in conseq\", logger=logger):\r\n   ds = datasets.load_from_disk(dataset_path,num_proc=1)\r\n\r\n```\r\n\r\n#### Run 1\r\n```\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [01:28<00:00, 13.96shards/s]\r\nLoading in parallel: 88.690 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [01:48<00:00, 11.31shards/s]\r\nLoading in parallel: 109.339 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 32 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [01:06<00:00, 18.56shards/s]\r\nLoading in parallel: 66.931 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [05:09<00:00,  3.98shards/s]\r\nLoading in conseq: 309.792 seconds\r\n```\r\n#### Run 2\r\n\r\n```\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [01:38<00:00, 12.53shards/s]\r\nLoading in parallel: 98.831 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [02:01<00:00, 10.16shards/s]\r\nLoading in parallel: 121.669 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 32 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [01:07<00:00, 18.18shards/s]\r\nLoading in parallel: 68.192 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [05:19<00:00,  3.86shards/s]\r\nLoading in conseq: 319.759 seconds\r\n```\r\n#### Run 3\r\n```\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [01:36<00:00, 12.74shards/s]\r\nLoading in parallel: 96.936 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 16 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [02:00<00:00, 10.24shards/s]\r\nLoading in parallel: 120.761 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk using 32 threads: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [01:08<00:00, 18.04shards/s]\r\nLoading in parallel: 68.666 seconds\r\nopen file: .../dataset_dict.json\r\nLoading the dataset from disk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1231/1231 [05:35<00:00,  3.67shards/s]\r\nLoading in conseq: 335.777 seconds\r\n```\r\n\r\n\r\nfix #2252\r\n<end_of_turn>\n<start_of_turn>model\nAdd concurrent loading of shards to datasets.load_from_disk <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nBetter document defaults of `to_json`: the default format is [JSON-Lines](https://jsonlines.org/).\r\n\r\nRelated to:\r\n- #6891 <end_of_turn>\n<start_of_turn>model\nBetter document defaults of to_json<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nset the right behavior of skip/take depending on whether it's called after or before shuffle/split_by_node<end_of_turn>\n<start_of_turn>model\nImprove skip take shuffling and distributed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd LMDB format support for large memory-mapping files\n\n### Motivation\n\nAdd LMDB format support for large memory-mapping files\n\n### Your contribution\n\nI'm trying to add it<end_of_turn>\n<start_of_turn>model\nAdd LMDB format support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nDear Datasets team,\r\n\r\nWe just have published a dataset on Huggingface:\r\nhttps://huggingface.co/ai4privacy\r\n\r\nHowever, when trying to read it using the Dataset library we get an error. As I understand jsonl files are compatible, could you please clarify how we can solve the issue? Please let me know and we would be more than happy to adapt the structure of the repository or meta data so it works easier:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"ai4privacy/pii-masking-200k\")\r\n```\r\n\r\n```\r\nDownloading readme: 100%\r\n11.8k/11.8k [00:00<00:00, 512kB/s]\r\nDownloading data files: 100%\r\n1/1 [00:11<00:00, 11.16s/it]\r\nDownloading data: 100%\r\n64.3M/64.3M [00:02<00:00, 32.9MB/s]\r\nDownloading data: 100%\r\n113M/113M [00:03<00:00, 35.0MB/s]\r\nDownloading data: 100%\r\n97.7M/97.7M [00:02<00:00, 46.1MB/s]\r\nDownloading data: 100%\r\n90.8M/90.8M [00:02<00:00, 44.9MB/s]\r\nDownloading data: 100%\r\n7.63k/7.63k [00:00<00:00, 41.0kB/s]\r\nDownloading data: 100%\r\n1.03k/1.03k [00:00<00:00, 9.44kB/s]\r\nExtracting data files: 100%\r\n1/1 [00:00<00:00, 29.26it/s]\r\nGenerating train split:\r\n209261/0 [00:05<00:00, 41201.25 examples/s]\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1939                         )\r\n-> 1940                     writer.write_table(table)\r\n   1941                     num_examples_progress_update += len(table)\r\n\r\n8 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py](https://localhost:8080/#) in write_table(self, pa_table, writer_batch_size)\r\n    571         pa_table = pa_table.combine_chunks()\r\n--> 572         pa_table = table_cast(pa_table, self._schema)\r\n    573         if self.embed_local_files:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in table_cast(table, schema)\r\n   2327     if table.schema != schema:\r\n-> 2328         return cast_table_to_schema(table, schema)\r\n   2329     elif table.schema.metadata != schema.metadata:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/table.py](https://localhost:8080/#) in cast_table_to_schema(table, schema)\r\n   2285     if sorted(table.column_names) != sorted(features):\r\n-> 2286         raise ValueError(f\"Couldn't cast\\n{table.schema}\\nto\\n{features}\\nbecause column names don't match\")\r\n   2287     arrays = [cast_array_to_feature(table[name], feature) for name, feature in features.items()]\r\n\r\nValueError: Couldn't cast\r\nJOBTYPE: int64\r\nPHONEIMEI: int64\r\nACCOUNTNAME: int64\r\nVEHICLEVIN: int64\r\nGENDER: int64\r\nCURRENCYCODE: int64\r\nCREDITCARDISSUER: int64\r\nJOBTITLE: int64\r\nSEX: int64\r\nCURRENCYSYMBOL: int64\r\nIP: int64\r\nEYECOLOR: int64\r\nMASKEDNUMBER: int64\r\nSECONDARYADDRESS: int64\r\nJOBAREA: int64\r\nACCOUNTNUMBER: int64\r\nlanguage: string\r\nBITCOINADDRESS: int64\r\nMAC: int64\r\nSSN: int64\r\nEMAIL: int64\r\nETHEREUMADDRESS: int64\r\nDOB: int64\r\nVEHICLEVRM: int64\r\nIPV6: int64\r\nAMOUNT: int64\r\nURL: int64\r\nPHONENUMBER: int64\r\nPIN: int64\r\nTIME: int64\r\nCREDITCARDNUMBER: int64\r\nFIRSTNAME: int64\r\nIBAN: int64\r\nBIC: int64\r\nCOUNTY: int64\r\nSTATE: int64\r\nLASTNAME: int64\r\nZIPCODE: int64\r\nHEIGHT: int64\r\nORDINALDIRECTION: int64\r\nMIDDLENAME: int64\r\nSTREET: int64\r\nUSERNAME: int64\r\nCURRENCY: int64\r\nPREFIX: int64\r\nUSERAGENT: int64\r\nCURRENCYNAME: int64\r\nLITECOINADDRESS: int64\r\nCREDITCARDCVV: int64\r\nAGE: int64\r\nCITY: int64\r\nPASSWORD: int64\r\nBUILDINGNUMBER: int64\r\nIPV4: int64\r\nNEARBYGPSCOORDINATE: int64\r\nDATE: int64\r\nCOMPANYNAME: int64\r\nto\r\n{'masked_text': Value(dtype='string', id=None), 'unmasked_text': Value(dtype='string', id=None), 'privacy_mask': Value(dtype='string', id=None), 'span_labels': Value(dtype='string', id=None), 'bio_labels': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'tokenised_text': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n[<ipython-input-2-f1c6811e9c83>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 dataset = load_dataset(\"ai4privacy/pii-masking-200k\")\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2151 \r\n   2152     # Download and prepare data\r\n-> 2153     builder_instance.download_and_prepare(\r\n   2154         download_config=download_config,\r\n   2155         download_mode=download_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952                         if num_proc is not None:\r\n    953                             prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954                         self._download_and_prepare(\r\n    955                             dl_manager=dl_manager,\r\n    956                             verification_mode=verification_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1047             try:\r\n   1048                 # Prepare split will record examples associated to the split\r\n-> 1049                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1050             except OSError as e:\r\n   1051                 raise OSError(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split(self, split_generator, file_format, num_proc, max_shard_size)\r\n   1811             job_id = 0\r\n   1812             with pbar:\r\n-> 1813                 for job_id, done, content in self._prepare_split_single(\r\n   1814                     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1815                 ):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   1956             if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1957                 e = e.__context__\r\n-> 1958             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1959 \r\n   1960         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\r\nThank you and have a great day ahead\r\n\r\n### Steps to reproduce the bug\r\n\r\nOpen Google Colab Notebook:\r\n\r\nRun command:\r\n!pip3 install datasets\r\n\r\nRun code:\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"ai4privacy/pii-masking-200k\")\r\n\r\n### Expected behavior\r\n\r\nDownload the dataset successfully from HuggingFace to the notebook so that we can start working with it\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.19.1\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nDatasets does not load HuggingFace Repository properly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nPreviously it was possible to access a whole column that was e.g. in numpy format via `with_format` by indexing the column. Now this possibility seems to be gone with the new Column() class. As far as I see, this makes working on a whole column (in-memory) more complex, i.e. normalizing an in-memory dataset for which iterating would be too slow. Is this intended behaviour? I couldn't find much documentation on the intended usage of the new Column class yet.\n\n### Steps to reproduce the bug\n\nSteps to reproduce:\n```\nfrom datasets import load_dataset\ndataset = load_dataset(\"lhoestq/demo1\")\ndataset = dataset.with_format(\"numpy\")\nprint(dataset[\"star\"].ndim)\n```\n\n### Expected behavior\n\nWorking on whole columns should be possible.\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: Linux-6.8.0-63-generic-x86_64-with-glibc2.36\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.34.3\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.1\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nRelease 4.0.0 breaks usage patterns of with_format<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI was trying out polars as an output for a map function and found that it wasn't a valid return type in `validate_function_output`. Thought that we should accommodate this by creating and adding it to the `allowed_processed_input_types` variable.<end_of_turn>\n<start_of_turn>model\nAllow polars as valid output type<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR attempts to keep track of non_nullable pyarrow fields when converting a `pa.Schema` to `Features`. At the same time, when outputting the `arrow_schema`, the original non-nullable fields are restored. This allows for more consistent behavior and avoids breaking behavior as illustrated in #7479.\r\n\r\nI am by no means a pyarrow expert so some logic in `find_non_nullable_fields` may not perfect. Not sure if more logic (type checks) are needed for deep-checking a given schema. Maybe there are other pyarrow structures that need to be covered?\r\n\r\nTests are added, but again, these may not have sufficient coverage in terms of pyarrow structure types.\r\n\r\ncloses #7479 <end_of_turn>\n<start_of_turn>model\nImplement capability to restore non-nullability in Features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nwas causing transformers CI to fail\r\n\r\nhttps://circleci.com/gh/huggingface/transformers/855105<end_of_turn>\n<start_of_turn>model\nFix legacy_dataset_infos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe dependency is only used for catching an exception from other code. That can be done with an import guard.\r\n<end_of_turn>\n<start_of_turn>model\nRemove `aiohttp` from direct dependencies<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nTrying to download only the 'validation' split of my dataset; instead hit the error `datasets.exceptions.ExpectedMoreSplitsError`.\r\nAppears to be the same undesired behavior as reported in [#6939](https://github.com/huggingface/datasets/issues/6939), but with `data_files`, not `data_dir`.\r\n\r\nHere is the Traceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/app/app.py\", line 12, in <module>\r\n    ds = load_dataset('datacomp/imagenet-1k-random0.0', token=GATED_IMAGENET, data_files={'validation': 'data/val*'}, split='validation', trust_remote_code=True)\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/load.py\", line 2154, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/builder.py\", line 924, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/builder.py\", line 1018, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/usr/local/lib/python3.10/site-packages/datasets/utils/info_utils.py\", line 68, in verify_splits\r\n    raise ExpectedMoreSplitsError(str(set(expected_splits) - set(recorded_splits)))\r\ndatasets.exceptions.ExpectedMoreSplitsError: {'train', 'test'}\r\n```\r\n\r\nNote: I am using the `data_files` argument only because I am trying to specify that I only want the 'validation' split, and the whole dataset will be downloaded even when the `split='validation'` argument is specified, unless you also specify `data_files`, as described here: https://discuss.huggingface.co/t/how-can-i-download-a-specific-split-of-a-dataset/79027\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Create a Space with the default blank 'gradio' SDK https://huggingface.co/new-space\r\n2. Create a file 'app.py' that loads a dataset to only extract a 'validation' split:\r\n\r\n`ds = load_dataset('datacomp/imagenet-1k-random0.0', token=GATED_IMAGENET, data_files={'validation': 'data/val*'}, split='validation', trust_remote_code=True)`\r\n\r\n### Expected behavior\r\n\r\nDownloading validation split.\r\n\r\n### Environment info\r\n\r\nDefault environment for creating a new Space. Relevant to this bug, that is:\r\n\r\n```\r\nFROM docker.io/library/python:3.10@sha256:fd0fa50d997eb56ce560c6e5ca6a1f5cf8fdff87572a16ac07fb1f5ca01eb608\r\n\r\n--> RUN pip install --no-cache-dir pip==22.3.1 && \tpip install --no-cache-dir \tdatasets \t\"huggingface-hub>=0.19\" \"hf-transfer>=0.1.4\" \"protobuf<4\" \"click<8.1\" \r\n```<end_of_turn>\n<start_of_turn>model\nFaulty datasets.exceptions.ExpectedMoreSplitsError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nEDIT:\r\n~~Fix test_delete_from_hub on Windows by passing explicit encoding.~~\r\nFix test_delete_from_hub and test_xgetsize_private by uploading the README file content directly (encoding the string), instead of writing a local file and uploading it.\r\n\r\nNote that local files created on Windows will have \"\\r\\n\" line endings, instead of \"\\n\".\r\nThese are no longer transformed to \"\\n\" by the Hub.\r\n\r\nFix #6856.<end_of_turn>\n<start_of_turn>model\nFix line-endings in tests on Windows<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMy dataset is stored on the company's lakefs server. How can I write code to load the dataset? It would be great if I could provide code examples or provide some references\r\n<end_of_turn>\n<start_of_turn>model\nHow to load data from lakefs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNeeded to use those in dataset-viewer:\r\n\r\n- dev version of hfh https://github.com/huggingface/dataset-viewer/pull/2781: don't span the hub with /paths-info requests\r\n- dev version of datasets at https://github.com/huggingface/datasets/pull/6875: don't write too big logs in the viewer\r\n\r\nclose https://github.com/huggingface/datasets/issues/6863<end_of_turn>\n<start_of_turn>model\nUnpin hfh<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes https://github.com/huggingface/datasets/issues/6566\r\n\r\nLet me know if there's any tests I need to clear.<end_of_turn>\n<start_of_turn>model\nUpdate features.py to avoid bfloat16 unsupported error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes link to the builder classes :)<end_of_turn>\n<start_of_turn>model\n[docs] Fix link<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIt would be great if there would be support for high performance, highly scalable text deduplication algorithms as part of the datasets library.\n\n### Motivation\n\nMotivated by this blog post https://huggingface.co/blog/dedup and this library https://github.com/google-research/deduplicate-text-datasets, but slightly frustrated by how its not very easy to work with these tools I am proposing this feature.\n\n### Your contribution\n\nI would be happy to contribute to the development effort of this feature. would love to collaborate with others in the development effort. <end_of_turn>\n<start_of_turn>model\nRequest for text deduplication feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix https://github.com/huggingface/datasets/issues/7147<end_of_turn>\n<start_of_turn>model\nFix zero proba interleave datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to load the 'default' subset of the following dataset which contains lots of files (828 per split): [https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb](https://huggingface.co/datasets/mteb/biblenlp-corpus-mmteb)\r\n\r\nWhen trying to load it using the `load_dataset` function I get the following error\r\n\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> d = load_dataset('mteb/biblenlp-corpus-mmteb')\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 201k/201k [00:00<00:00, 1.07MB/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 1069.15it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 436182.33it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 2228.75it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 646478.73it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 831032.24it/s]\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:00<00:00, 517645.51it/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:33<00:00, 24.87files/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:30<00:00, 27.48files/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 828/828 [00:30<00:00, 26.94files/s]\r\nGenerating train split: 1571592 examples [00:03, 461438.97 examples/s]\r\nGenerating test split: 11163 examples [00:00, 118190.72 examples/s]\r\nTraceback (most recent call last):\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 1995, in _prepare_split_single\r\n    for _, table in generator:\r\n  File \".env/lib/python3.12/site-packages/datasets/packaged_modules/json/json.py\", line 99, in _generate_tables\r\n    with open(file, \"rb\") as f:\r\n         ^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/datasets/streaming.py\", line 75, in wrapper\r\n    return function(*args, download_config=download_config, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/datasets/utils/file_utils.py\", line 1224, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/core.py\", line 135, in open\r\n    return self.__enter__()\r\n           ^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/core.py\", line 103, in __enter__\r\n    f = self.fs.open(self.path, mode=mode)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/spec.py\", line 1293, in open\r\n    f = self._open(\r\n        ^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/datasets/filesystems/compression.py\", line 81, in _open\r\n    return self.file.open()\r\n           ^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/core.py\", line 135, in open\r\n    return self.__enter__()\r\n           ^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/core.py\", line 103, in __enter__\r\n    f = self.fs.open(self.path, mode=mode)\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/spec.py\", line 1293, in open\r\n    f = self._open(\r\n        ^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 197, in _open\r\n    return LocalFileOpener(path, mode, fs=self, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \".env/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 322, in __init__\r\n    self._open()\r\n  File \".env/lib/python3.12/site-packages/fsspec/implementations/local.py\", line 327, in _open\r\n    self.f = open(self.path, mode=self.mode)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nOSError: [Errno 24] Too many open files: '.cache/huggingface/datasets/downloads/3a347186abfc0f9c924dde0221d246db758c7232c0101523f04a87c17d696618'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 981, in incomplete_dir\r\n    yield tmp_dir\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 1027, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 1122, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 1882, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 2038, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \".env/lib/python3.12/site-packages/datasets/load.py\", line 2609, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 1007, in download_and_prepare\r\n    with incomplete_dir(self._output_dir) as tmp_output_dir:\r\n  File \"/usr/lib/python3.12/contextlib.py\", line 158, in __exit__\r\n    self.gen.throw(value)\r\n  File \".env/lib/python3.12/site-packages/datasets/builder.py\", line 988, in incomplete_dir\r\n    shutil.rmtree(tmp_dir)\r\n  File \"/usr/lib/python3.12/shutil.py\", line 785, in rmtree\r\n    _rmtree_safe_fd(fd, path, onexc)\r\n  File \"/usr/lib/python3.12/shutil.py\", line 661, in _rmtree_safe_fd\r\n    onexc(os.scandir, path, err)\r\n  File \"/usr/lib/python3.12/shutil.py\", line 657, in _rmtree_safe_fd\r\n    with os.scandir(topfd) as scandir_it:\r\n         ^^^^^^^^^^^^^^^^^\r\nOSError: [Errno 24] Too many open files: '.cache/huggingface/datasets/mteb___biblenlp-corpus-mmteb/default/0.0.0/3912ed967b0834547f35b2da9470c4976b357c9a.incomplete'\r\n\r\n\r\n```\r\n\r\nI looked for the maximum number of open files on my machine (Ubuntu 24.04) and it seems to be 1024, but even when I try to load a single split (`load_dataset('mteb/biblenlp-corpus-mmteb', split='train')`) I get the same error\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\nd = load_dataset('mteb/biblenlp-corpus-mmteb')\r\n```\n\n### Expected behavior\n\nLoad the dataset without error\n\n### Environment info\n\n- `datasets` version: 2.19.0\r\n- Platform: Linux-6.8.0-31-generic-x86_64-with-glibc2.39\r\n- Python version: 3.12.3\r\n- `huggingface_hub` version: 0.23.0\r\n- PyArrow version: 16.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.3.1\r\n<end_of_turn>\n<start_of_turn>model\nOSError: [Errno 24] Too many open files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix https://github.com/huggingface/datasets/issues/6087\r\n\r\n(Colab installs 2023.6.0, so we should be good) <end_of_turn>\n<start_of_turn>model\nBump fsspec from 2021.11.1 to 2022.3.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHello!\r\n\r\nI'm currently experiencing an issue where I can't concatenate datasets if an inner field of a Feature is Optional.\r\n\r\nI have a column named `speaker`, and this holds some information about a speaker.\r\n\r\n```python\r\n@dataclass\r\nclass Speaker:\r\n   name: str\r\n   email: Optional[str]\r\n```\r\n\r\nIf I have two datasets, one happens to have `email` always None, then I get `The features can't be aligned because the key email of features`\r\n\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nYou can run the following script:\r\n\r\n```python\r\n\r\nds = Dataset.from_dict({'speaker': [{'name': 'Ben', 'email': None}]})\r\nds2 = Dataset.from_dict({'speaker': [{'name': 'Fred', 'email': 'abc@aol.com'}]})\r\nconcatenate_datasets([ds, ds2])\r\n\r\n>>>The features can't be aligned because the key speaker of features {'speaker': {'email': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None)}} has unexpected type - {'email': Value(dtype='string', id=None), 'name': Value(dtype='string', id=None)} (expected either {'email': Value(dtype='null', id=None), 'name': Value(dtype='string', id=None)} or Value(\"null\").\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nI think this should work; if two top-level columns were in the same situation it would properly cast to `string`.\r\n\r\n```python\r\nds = Dataset.from_dict({'email': [None, None]})\r\nds2 = Dataset.from_dict({'email': ['abc@aol.com', 'one@yahoo.com']})\r\nconcatenate_datasets([ds, ds2])\r\n>>> # Works!\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.15.1.dev0\r\n- Platform: Linux-5.15.0-89-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.13\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.4.4\r\n- `fsspec` version: 2023.6.0\r\n\r\nI would be happy to fix this issue.<end_of_turn>\n<start_of_turn>model\nCan't align optional features of struct<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nStreaming datasets, as intended, do not load the entire dataset in memory or disk. However, while querying the next data chunk from the remote, sometimes it is possible that the service is down or there might be other issues that may cause the query to fail. In such a scenario, it would be nice to make these queries retryable (perhaps with a backoff strategy).\n\n### Motivation\n\nI was working on a model and the model checkpoints after every 1000 steps. At step 1800 I got a 504 HTTP status code error from Huggingface hub for my pytorch `dataloader`. Given the size of my model and data, it took around 2 hours to reach 1800 steps and now it will take about an hour to recover the lost 800. It would be better to get a retryable querying strategy.\n\n### Your contribution\n\nIt would be better if someone having experience in this area takes this up as this would require some testing.<end_of_turn>\n<start_of_turn>model\nMake Dataset streaming queries retryable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.14.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am creating a large WebDataset-format dataset for sign language processing research, and a number of the videos are over 2GB. The instant I hit one of the shards with one of those videos, I get a ArrowCapacityError, even with streaming. \n\n\n\nI made a config for the dataset that specifically includes just one problem shard, and the error triggers the instant you even run load_dataset(), even with streaming=True\n```\nds = load_dataset(\"bible-nlp/sign-bibles\", \"ase_chronological_bible_translation_in_american_sign_language_119_introductions_and_passages_debugging_problem_shard\", streaming=True, split=\"train\")\n```\n\nThis gives: \n```\nFile \"/opt/home/cleong/projects/semantic_and_visual_similarity/sign-bibles-dataset/sign_bibles_dataset/tasks/test_iteration.py\", line 13, in iterate_keys\n    ds = load_dataset(\"bible-nlp/sign-bibles\", language_subset, streaming=True, split=\"train\")\n  File \"/opt/home/cleong/envs/sign-bibles-dataset/lib/python3.13/site-packages/datasets/load.py\", line 1409, in load_dataset\n    return builder_instance.as_streaming_dataset(split=split)\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^\n  File \"/opt/home/cleong/envs/sign-bibles-dataset/lib/python3.13/site-packages/datasets/builder.py\", line 1225, in as_streaming_dataset\n    splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}\n                                               ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n  File \"/opt/home/cleong/envs/sign-bibles-dataset/lib/python3.13/site-packages/datasets/packaged_modules/webdataset/webdataset.py\", line 88, in _split_generators\n    pa.Table.from_pylist(cast_to_python_objects([example], only_1d_for_numpy=True))\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"pyarrow/table.pxi\", line 2046, in pyarrow.lib._Tabular.from_pylist\n  File \"pyarrow/table.pxi\", line 6431, in pyarrow.lib._from_pylist\n  File \"pyarrow/table.pxi\", line 4893, in pyarrow.lib.Table.from_arrays\n  File \"pyarrow/table.pxi\", line 1607, in pyarrow.lib._sanitize_arrays\n  File \"pyarrow/table.pxi\", line 1588, in pyarrow.lib._schema_from_arrays\n  File \"pyarrow/array.pxi\", line 375, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 45, in pyarrow.lib._sequence_to_array\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowCapacityError: array cannot contain more than 2147483646 bytes, have 3980158992\n```\n\n### Steps to reproduce the bug\n\n```python\n#!/usr/bin/env python\n\nimport argparse\nfrom datasets import get_dataset_config_names, load_dataset\nfrom tqdm import tqdm\nfrom pyarrow.lib import ArrowCapacityError, ArrowInvalid\n\n\ndef iterate_keys(language_subset: str) -> None:\n    \"\"\"Iterate over all samples in the Sign Bibles dataset and print idx and sample key.\"\"\"\n\n    # https://huggingface.co/docs/datasets/v4.0.0/en/package_reference/loading_methods#datasets.load_dataset\n    ds = load_dataset(\"bible-nlp/sign-bibles\", language_subset, streaming=True, split=\"train\")\n\n    print(f\"\\n==> Loaded dataset config '{language_subset}'\")\n\n    idx = 0\n    estimated_shard_index = 0\n    samples_per_shard = 5\n    with tqdm(desc=f\"{language_subset} samples\") as pbar:\n        iterator = iter(ds)\n        while True:\n            try:\n                if idx % samples_per_shard == 0 and idx > 0:  # 5 samples per shard: 0, 1, 2, 3, 4\n                    print(f\"Estimated Shard idx (starting at 0, {samples_per_shard}/shard): {estimated_shard_index}\")\n                    estimated_shard_index += 1\n                sample = next(iterator)\n                sample_key = sample.get(\"__key__\", \"missing-key\")\n                print(f\"[{language_subset}] idx={idx}, key={sample_key}\")\n                idx += 1\n\n                pbar.update(1)\n\n            except StopIteration:\n                print(f\"Finished iterating through {idx} samples of {language_subset}\")\n                break\n\n            except (ArrowCapacityError, ArrowInvalid) as e:\n                print(f\"PyArrow error on idx={idx}, config={language_subset}: {e}\")\n                idx += 1\n                pbar.update(1)\n                continue\n\n            except KeyError as e:\n                print(f\"Missing key error on idx={idx}, config={language_subset}: {e}\")\n                idx += 1\n                pbar.update(1)\n                continue\n\n\ndef main():\n    configs = get_dataset_config_names(\"bible-nlp/sign-bibles\")\n    print(f\"Available configs: {configs}\")\n    configs = [\n        \"ase_chronological_bible_translation_in_american_sign_language_119_introductions_and_passages_debugging_problem_shard\"\n    ]\n\n    for language_subset in configs:\n        print(f\"TESTING CONFIG {language_subset}\")\n        iterate_keys(language_subset)\n        # try:\n\n        # except (ArrowCapacityError, ArrowInvalid) as e:\n        #     print(f\"PyArrow error at config level for {language_subset}: {e}\")\n        #     continue\n        # except RuntimeError as e:\n        #     print(f\"RuntimeError at config level for {language_subset}: {e}\")\n        #     continue\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Iterate through Sign Bibles dataset and print sample keys.\")\n    args = parser.parse_args()\n\n    main()\n```\n\n\n\n### Expected behavior\n\nI expect, when I load with streaming=True, that there should not be any data loaded or anything like that.\nhttps://huggingface.co/docs/datasets/main/en/package_reference/loading_methods#datasets.load_dataset says that with streaming=true, \n\nI did expect to have some trouble with large files, but that the streaming mode would not actually try to load them unless requested, e.g. with sample[\"mp4\"]\n\n>In the streaming case:\n>    Don\u2019t download or cache anything. Instead, the dataset is lazily loaded and will be streamed on-the-fly when iterating on it.\n\n### Environment info\n\nLocal setup: Conda environment on Ubuntu, pip list includes the following\ndatasets                  4.0.0\npyarrow                   20.0.0\n\nVerified on Colab: https://colab.research.google.com/drive/1HdN8stlROWrLSYXUoNeV0vQ9pClhIVM8?usp=sharing, though there it crashes by using up all available RAM\n<end_of_turn>\n<start_of_turn>model\nLarge WebDataset: pyarrow.lib.ArrowCapacityError on load() even with streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi, \n\nThe canonical `multi_news` dataset is currently broken and fails to load. This is because it points to the [alexfabri/multi_news](https://huggingface.co/datasets/alexfabbri/multi_news) repository, which contains a legacy loading script (`multi_news.py`) that requires the now-removed `trust_remote_code` parameter.\n\nThe original maintainer's GitHub and Hugging Face repositories appear to be inactive, so a community-led fix is needed.\n\nI have created a working fix by converting the dataset to the modern Parquet format, which does not require a loading script. The fixed version is available here and loads correctly:\n\n**[Awesome075/multi_news_parquet](https://huggingface.co/datasets/Awesome075/multi_news_parquet)**\n\nCould the maintainers please guide me or themselves update the official `multi_news` dataset to use this working Parquet version? This would involve updating the canonical pointer for \"multi_news\" to resolve to the new repository.\n\nThis action would fix the dataset for all users and ensure its continued availability.\n\nThank you!<end_of_turn>\n<start_of_turn>model\nFix: Canonical 'multi_news' dataset is broken and should be updated to a Parquet version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix type hint `train_test_split`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen trying to stream a dataset i get the following error after a few minutes of waiting.\r\n\r\n```\r\nFileNotFoundError: https://huggingface.co/datasets/facebook/voxpopuli/resolve/main/data/n_files.json\r\nIf the repo is private or gated, make sure to log in with `huggingface-cli login`.\r\n```\r\n\r\nI have already set the proxy environment variables. Downloading a Dataset without streaming works as expected.\r\nStill i suspect that this is connected to being behind a proxy.\r\n\r\nIs there a way to set the proxy for streaming datasets? Possibly a keyword argument that gets passed to ffspec?\r\n\r\n### Steps to reproduce the bug\r\n\r\nThis is the code i use.\r\n\r\n```\r\nimport os\r\nos.environ['http_proxy'] = \"http://example.com:xxxx\" \r\nos.environ['https_proxy'] = \"http://example.com:xxxx\" \r\n\r\n\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"facebook/voxpopuli\", name=\"de\", streaming=True)\r\n```\r\n\r\n### Expected behavior\r\n\r\nI would expect the streaming functionality to use the set proxy settings.\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.0-73-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.2\r\n<end_of_turn>\n<start_of_turn>model\nStreaming Dataset behind Proxy - FileNotFoundError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\nException has occurred: FileNotFoundError\r\nCouldn't find file at https://huggingface.co/datasets/facebook/voxpopuli/resolve/main/{'en': 'data/en/asr_train.tsv'}\r\n```\r\n\r\nError in logic for link url creation. The link should be  https://huggingface.co/datasets/facebook/voxpopuli/resolve/main/data/en/asr_train.tsv \r\n\r\nBasically there should be links directly under ```metadata[\"train\"]```, not under ```metadata[\"train\"][self.config.languages[0]]```\r\n\r\nsame for audio urls\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"facebook/voxpopuli\",\"en\")\r\n```\n\n### Expected behavior\n\nDataset should be loaded successfully.\n\n### Environment info\n\n- `datasets` version: 2.19.0\r\n- Platform: Linux-5.15.0-1041-aws-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 16.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\nProblem loading voxpopuli dataset <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nbefore the release<end_of_turn>\n<start_of_turn>model\nminor docs changes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n... to make debugging issues easier, as `fsspec`'s releases often introduce breaking changes.<end_of_turn>\n<start_of_turn>model\nAdd `fsspec` version to the `datasets-cli env` command output<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix the order of the columns in dataset.features when the order changes with `dataset.select_columns()`.\r\n\r\nI also fixed the same issue for `dataset.flatten()`\r\n\r\nClose https://github.com/huggingface/datasets/issues/5993<end_of_turn>\n<start_of_turn>model\nFix select_columns columns order<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThis will randomly pop up during training now\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/diffusers/examples/controlnet/train_controlnet.py\", line 1192, in <module>\r\n    main(args)\r\n  File \"/workspace/diffusers/examples/controlnet/train_controlnet.py\", line 1041, in main\r\n    for step, batch in enumerate(train_dataloader):\r\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/data_loader.py\", line 561, in __iter__\r\n    next_batch = next(dataloader_iter)\r\n                 ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\r\n    data = self._next_data()\r\n           ^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 673, in _next_data\r\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\r\n    data = self.dataset.__getitems__(possibly_batched_index)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 2746, in __getitems__\r\n    batch = self.__getitem__(keys)\r\n            ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 2742, in __getitem__\r\n    return self._getitem(key)\r\n           ^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 2727, in _getitem\r\n    formatted_output = format_table(\r\n                       ^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\", line 639, in format_table\r\n    return formatter(pa_table, query_type=query_type)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\", line 407, in __call__\r\n    return self.format_batch(pa_table)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\", line 521, in format_batch\r\n    batch = self.python_features_decoder.decode_batch(batch)\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/formatting/formatting.py\", line 228, in decode_batch\r\n    return self.features.decode_batch(batch) if self.features else batch\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/features/features.py\", line 2084, in decode_batch\r\n    [\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/features/features.py\", line 2085, in <listcomp>\r\n    decode_nested_example(self[column_name], value, token_per_repo_id=token_per_repo_id)\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/features/features.py\", line 1403, in decode_nested_example\r\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/features/image.py\", line 188, in decode_example\r\n    image.load()  # to avoid \"Too many open files\" errors\r\n\r\n    ```\r\n\r\n### Steps to reproduce the bug\r\n\r\nTrain on diffusers sd1.5 controlnet example script\r\n\r\nThis will pop up randomly, you can see in wandb below when i manually resume run everytime this error appears\r\n\r\n![image](https://github.com/user-attachments/assets/87e9a6af-cb3c-4398-82e7-d6a90add8d31)\r\n\r\n\r\n### Expected behavior\r\n\r\nTraining to continue without above error\r\n\r\n### Environment info\r\n\r\n- datasets version: 3.0.0\r\n- Platform: Linux-6.5.0-44-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.9\r\n- huggingface_hub version: 0.25.1\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- fsspec version: 2024.6.1\r\n\r\nTraining on 4090<end_of_turn>\n<start_of_turn>model\nsd1.5 diffusers controlnet training script gives new error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDatasets (such as https://huggingface.co/datasets/MLCommons/peoples_speech) cannot be used on Windows due to the fact that windows does not allow colons \":\" in filenames. These should be converted into alternative strings.\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. Attempt to run load_dataset on MLCommons/peoples_speech\n\n### Expected behavior\n\nDoes not crash during extraction\n\n### Environment info\n\nWindows 11, NTFS filesystem, Python 3.12\r\n<end_of_turn>\n<start_of_turn>model\nDatasets with files with colon : in filenames cannot be used on Windows<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using `.repeat()` on an `IterableDataset`, this error gets thrown. There is [this thread](https://discuss.huggingface.co/t/making-an-infinite-iterabledataset/146192/5) that seems to imply the fix is trivial, but I don't know anything about this codebase, so I'm opening this issue rather than attempting to open a PR.\n\n### Steps to reproduce the bug\n\n1. Create an `IterableDataset`.\n2. Call `.repeat(None)` on it.\n3. Wrap it in a pytorch `DataLoader`\n4. Iterate over it.\n\n### Expected behavior\n\nThis should work normally.\n\n### Environment info\n\ndatasets: 3.5.0<end_of_turn>\n<start_of_turn>model\nNotImplementedError: <class 'datasets.iterable_dataset.RepeatExamplesIterable'> doesn't implement num_shards yet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate CONTRIBUTING.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`joblibspark` doesn't support the latest `joblib` release.\r\n\r\nSee https://github.com/huggingface/datasets/actions/runs/5401870932/jobs/9812337078 for the errors<end_of_turn>\n<start_of_turn>model\nPin `joblib` to avoid `joblibspark` test failures<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7650.\r\n\r\nThe metadata files generated by the `DatasetDict.save_to_file` function are not included in the folder_based_builder's metadata list, causing issues when only 1 actual data file is present, as described in issue #7650.\r\n\r\nThis PR adds these filenames to the builder, allowing correct loading.<end_of_turn>\n<start_of_turn>model\nfix: Extended metadata file names for folder_based_builder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFinishes the `to_iterable_dataset` documentation by adding it to the relevant sections in the tutorial and guide.<end_of_turn>\n<start_of_turn>model\n[docs] Complete `to_iterable_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix typo in about_mapstyle_vs_iterable.mdx<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd streaming support after https://github.com/huggingface/datasets/pull/7690, cc @klamike :)\r\n\r\n## Details\r\n\r\nin `datasets` loaders, `open()` is extended to work with files that are on disk but also on HF. Files on HF are streamed using HTTP range requests using the `HfFileSystem` implementation in the `huggingface_hub` library.<end_of_turn>\n<start_of_turn>model\nAllow streaming hdf5 files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...by making tracked iterables picklable.\r\n\r\nThis is important to make streaming datasets compatible with multiprocessing e.g. for parallel data loading<end_of_turn>\n<start_of_turn>model\nFix webdataset pickling<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\n`push_to_hub` now allows users to upload datasets programmatically. However, based on the latest doc, we still need to open the dataset page to add readme file manually.\r\n\r\nHowever, I do discover snippets to intialize a README for every `push_to_hub`:\r\n```\r\ndataset_card = (\r\n    DatasetCard(\r\n        \"---\\n\"\r\n        + str(dataset_card_data)\r\n        + \"\\n---\\n\"\r\n        + f'# Dataset Card for \"{repo_id.split(\"/\")[-1]}\"\\n\\n[More Information needed](https://github.com/huggingface/datasets/blob/main/CONTRIBUTING.md#how-to-contribute-to-the-dataset-cards)'\r\n    )\r\n    if dataset_card is None\r\n    else dataset_card\r\n)\r\n\r\nHfApi(endpoint=config.HF_ENDPOINT).upload_file(\r\n    path_or_fileobj=str(dataset_card).encode(),\r\n    path_in_repo=\"README.md\",\r\n    repo_id=repo_id,\r\n    token=token,\r\n    repo_type=\"dataset\",\r\n    revision=branch,\r\n)\r\n```\r\n\r\nSo, if we can enable `push_to_hub` to upload a readme file by ourselves instead of using the auto generated ones, it can save ton of time, and will definitely alleviate the current \"lack-of-dataset-card\" situation.\r\n\r\n### Motivation\r\n\r\nas elabrated above.\r\n\r\n### Your contribution\r\n\r\nI might be able to make a pr.<end_of_turn>\n<start_of_turn>model\nUpload README via `push_to_hub`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes an issue with the deprecation of `errors` in `TextConfig` introduced by:\r\n- #5974\r\n\r\n```python\r\nIn [1]: ds = load_dataset(\"text\", data_files=\"test.txt\", errors=\"strict\")\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-701c27131a5d> in <module>\r\n----> 1 ds = load_dataset(\"text\", data_files=\"test.txt\", errors=\"strict\")\r\n\r\n~/huggingface/datasets/src/datasets/load.py in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2107 \r\n   2108     # Create a dataset builder\r\n-> 2109     builder_instance = load_dataset_builder(\r\n   2110         path=path,\r\n   2111         name=name,\r\n\r\n~/huggingface/datasets/src/datasets/load.py in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, **config_kwargs)\r\n   1830     builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\r\n   1831     # Instantiate the dataset builder\r\n-> 1832     builder_instance: DatasetBuilder = builder_cls(\r\n   1833         cache_dir=cache_dir,\r\n   1834         dataset_name=dataset_name,\r\n\r\n~/huggingface/datasets/src/datasets/builder.py in __init__(self, cache_dir, dataset_name, config_name, hash, base_path, info, features, token, use_auth_token, repo_id, data_files, data_dir, storage_options, writer_batch_size, name, **config_kwargs)\r\n    371         if data_dir is not None:\r\n    372             config_kwargs[\"data_dir\"] = data_dir\r\n--> 373         self.config, self.config_id = self._create_builder_config(\r\n    374             config_name=config_name,\r\n    375             custom_features=features,\r\n\r\n~/huggingface/datasets/src/datasets/builder.py in _create_builder_config(self, config_name, custom_features, **config_kwargs)\r\n    550             if \"version\" not in config_kwargs and hasattr(self, \"VERSION\") and self.VERSION:\r\n    551                 config_kwargs[\"version\"] = self.VERSION\r\n--> 552             builder_config = self.BUILDER_CONFIG_CLASS(**config_kwargs)\r\n    553 \r\n    554         # otherwise use the config_kwargs to overwrite the attributes\r\n\r\nTypeError: __init__() got an unexpected keyword argument 'errors'\r\n```\r\n\r\nSimilar to:\r\n- #6094<end_of_turn>\n<start_of_turn>model\nFix deprecation of errors in TextConfig<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- update min python version\r\n- replace canonical dataset names with new names\r\n- avoid examples with trust_remote_code<end_of_turn>\n<start_of_turn>model\nUpdate docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n\nI am trying to finetune qwen2.5-vl on 16 * 80G GPUS, and I use `LLaMA-Factory` and set `preprocessing_num_workers=16`. However, I met the following error and the program seem to got crush. It seems that the error come from `datasets` library\n\nThe error logging is like following:\n```text\nConverting format of dataset (num_proc=16): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 19265/19267 [11:44<00:00,  5.88 examples/s]\nConverting format of dataset (num_proc=16): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 19266/19267 [11:44<00:00,  5.02 examples/s]\nConverting format of dataset (num_proc=16): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19267/19267 [11:44<00:00,  5.44 examples/s]\nConverting format of dataset (num_proc=16): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 19267/19267 [11:44<00:00, 27.34 examples/s]\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [00:00<?, ? examples/s]\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\nInvalid NAL unit size (45405 > 35540).\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (7131 > 3225).\nmissing picture in access unit with size 54860\nInvalid NAL unit size (48042 > 33645).\nmissing picture in access unit with size 3229\nmissing picture in access unit with size 33649\nInvalid NAL unit size (86720 > 54856).\nInvalid NAL unit size (48042 > 33645).\nError splitting the input into NAL units.\nmissing picture in access unit with size 35544\nInvalid NAL unit size (45405 > 35540).\nError splitting the input into NAL units.\nError splitting the input into NAL units.\nInvalid NAL unit size (8187 > 7069).\nmissing picture in access unit with size 7073\nInvalid NAL unit size (8187 > 7069).\nError splitting the input into NAL units.\nInvalid NAL unit size (7131 > 3225).\nError splitting the input into NAL units.\nInvalid NAL unit size (14013 > 5998).\nmissing picture in access unit with size 6002\nInvalid NAL unit size (14013 > 5998).\nError splitting the input into NAL units.\nInvalid NAL unit size (17173 > 7231).\nmissing picture in access unit with size 7235\nInvalid NAL unit size (17173 > 7231).\nError splitting the input into NAL units.\nInvalid NAL unit size (16964 > 6055).\nmissing picture in access unit with size 6059\nInvalid NAL unit size (16964 > 6055).\nException in thread Thread-9 (accepter)Error splitting the input into NAL units.\n:\nTraceback (most recent call last):\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n\nRunning tokenizer on dataset (num_proc=16):   0%|          | 0/19267 [13:22<?, ? examples/s]    self.run()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 953, in run\n    Invalid NAL unit size (7032 > 2927).\nmissing picture in access unit with size 2931\nself._target(*self._args, **self._kwargs)\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/site-packages/multiprocess/managers.py\", line 194, in accepter\nInvalid NAL unit size (7032 > 2927).\nError splitting the input into NAL units.\n    t.start()\n  File \"/opt/conda/envs/python3.10.13/lib/python3.10/threading.py\", line 935, in start\n    Invalid NAL unit size (28973 > 6121).\nmissing picture in access unit with size 6125\n_start_new_thread(self._bootstrap, ())Invalid NAL unit size (28973 > 6121).\n\nRuntimeError: can't start new threadError splitting the input into NAL units.\n\nInvalid NAL unit size (4411 > 296).\nmissing picture in access unit with size 300\nInvalid NAL unit size (4411 > 296).\nError splitting the input into NAL units.\nInvalid NAL unit size (14414 > 1471).\nmissing picture in access unit with size 1475\nInvalid NAL unit size (14414 > 1471).\nError splitting the input into NAL units.\nInvalid NAL unit size (5283 > 1792).\nmissing picture in access unit with size 1796\nInvalid NAL unit size (5283 > 1792).\nError splitting the input into NAL units.\nInvalid NAL unit size (79147 > 10042).\nmissing picture in access unit with size 10046\nInvalid NAL unit size (79147 > 10042).\nError splitting the input into NAL units.\n\n```\n\n\n### Others\n\n_No response_\n\n### Steps to reproduce the bug\n\nNone\n\n### Expected behavior\n\nexcpect to run successfully\n\n### Environment info\n\n```\ntransformers==4.49.0\ndatasets==3.2.0\naccelerate==1.2.1\npeft==0.12.0\ntrl==0.9.6\ntokenizers==0.21.0\ngradio>=4.38.0,<=5.18.0\npandas>=2.0.0\nscipy\neinops\nsentencepiece\ntiktoken\nprotobuf\nuvicorn\npydantic\nfastapi\nsse-starlette\nmatplotlib>=3.7.0\nfire\npackaging\npyyaml\nnumpy<2.0.0\nav\nlibrosa\ntyro<0.9.0\nopenlm-hub\nqwen-vl-utils\n\n```<end_of_turn>\n<start_of_turn>model\nError splitting the input into NAL units.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHello,\r\n\r\nWhen working with bio-data, each feature often has metadata associated with it (e.g. species, lineage, snp position, etc). To store this, I like to use the feature classes with the added `metadata` attribute. However, when saving or loading with custom features, you get an error since that class doesn't exist in the global namespace in `datasets.features.features`. Take for example,\r\n\r\n\r\n\r\n```python\r\nfrom dataclasses import dataclass, field\r\nfrom datasets import Dataset\r\nfrom datasets.features.features import Value, Features\r\n\r\n@dataclass\r\nclass FeatureA(Value):\r\n    metadata: dict = field(default=dict)\r\n    _type: str = field(default=\"FeatureA\", init=False, repr=False)\r\n\r\n@dataclass\r\nclass FeatureB(Value):\r\n    metadata: dict = field(default_factory=dict)\r\n    _type: str = field(default=\"FeatureB\", init=False, repr=False)\r\n\r\ntest_data = {\r\n    \"a\": [1, 2, 3],\r\n    \"b\": [4, 5, 6],\r\n}\r\ntest_data = Dataset.from_dict(\r\n    test_data, \r\n    features=Features({\r\n        \"a\": FeatureA(\"int32\", metadata={\"species\": \"lactobacillus acetotolerans\"}),\r\n        \"b\": FeatureB(\"int32\", metadata={\"species\": \"lactobacillus iners\"}),\r\n    })\r\n)\r\n\r\n# returns an error since FeatureA and FeatureB are not in the global namespace\r\ntest_data.save_to_disk('./test_data')\r\n```\r\n\r\n\r\n    Saving the dataset (0/1 shards):   0%|          | 0/3 [00:00<?, ? examples/s]\r\n\r\n\r\n\r\n    ---------------------------------------------------------------------------\r\n\r\n    KeyError                                  Traceback (most recent call last)\r\n\r\n    Cell In[2], line 28\r\n         19 test_data = Dataset.from_dict(\r\n         20     test_data, \r\n         21     features=Features({\r\n       (...)\r\n         24     })\r\n         25 )\r\n         27 # returns an error since FeatureA and FeatureB are not in the global namespace\r\n    ---> 28 test_data.save_to_disk('./test_data')\r\n    ...\r\n    File ~\\Documents\\datasets\\src\\datasets\\features\\features.py:1361, in generate_from_dict(obj)\r\n       1359     return {key: generate_from_dict(value) for key, value in obj.items()}\r\n       1360 obj = dict(obj)\r\n    -> 1361 class_type = globals()[obj.pop(\"_type\")]\r\n       1363 if class_type == Sequence:\r\n       1364     return Sequence(feature=generate_from_dict(obj[\"feature\"]), length=obj.get(\"length\", -1))\r\n    \r\n\r\n    KeyError: 'FeatureA'\r\n\r\n\r\nWe can avoid this by having a registry (like formatters) and doing\r\n\r\n\r\n```python\r\nfrom datasets.features.features import register_feature\r\nregister_feature(FeatureA, \"FeatureA\")\r\nregister_feature(FeatureB, \"FeatureB\")\r\ntest_data.save_to_disk('./test_data')\r\n```\r\n\r\n\r\n    Saving the dataset (1/1 shards): 100%|------| 3/3 [00:00<00:00, 211.13 examples/s]\r\n\r\n\r\nand loading from disk returns with all metadata information\r\n\r\n\r\n```python\r\nfrom datasets import load_from_disk\r\ntest_data = load_from_disk('./test_data')\r\ntest_data.features\r\n```\r\n\r\n\r\n\r\n\r\n    {'a': FeatureA(dtype='int32', id=None, metadata={'species': 'lactobacillus acetotolerans'}),\r\n     'b': FeatureB(dtype='int32', id=None, metadata={'species': 'lactobacillus iners'})}\r\n\r\n\r\n<end_of_turn>\n<start_of_turn>model\nUsing a registry instead of calling globals for fetching feature types<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt was failing when there's a DatasetInfo with non-None info.features from the YAML (therefore containing columns that should be ignored)\r\n\r\nFix https://github.com/huggingface/datasets/issues/6293<end_of_turn>\n<start_of_turn>model\nFix parquet columns argument in streaming mode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi! I very recently noticed that my training time is dominated by batch formatting. Using Lightning's profilers, I located the bottleneck within `datasets.formatting.formatting` and then narrowed it down with `line-profiler`. It turns out that almost all of the overhead is due to creating new instances of `self.python_arrow_extractor`. I admit I'm confused why that could be the case - as far as I can tell there's no complex `__init__` logic to execute.\r\n\r\n![image](https://github.com/huggingface/datasets/assets/320321/5e022e0b-0d21-43d0-8e6f-9e641142e96b)\n\n### Steps to reproduce the bug\n\n1. Set up a dataset `ds` with potentially several (4+) columns (not sure if this is necessary, but it did at one point of the investigation make overhead worse)\r\n2. Process it using a custom transform, `ds = ds.with_transform(transform_func)`\r\n3. Decorate this function https://github.com/huggingface/datasets/blob/main/src/datasets/formatting/formatting.py#L512 with `@profile` from https://pypi.org/project/line-profiler/\r\n4. Profile with `$ kernprof -l script_to_profile.py`\n\n### Expected behavior\n\nBatch formatting should have acceptable overhead.\n\n### Environment info\n\n```\r\ndatasets=2.14.6\r\npyarrow=14.0.0\r\n```<end_of_turn>\n<start_of_turn>model\nFormatting overhead<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis will be useful for LeRobotDataset (robotics datasets for [lerobot](https://github.com/huggingface/lerobot) based on videos)\r\n\r\nImpacted builders:\r\n- ImageFolder\r\n- AudioFolder\r\n- VideoFolder\r\n\r\nImprovements:\r\n- faster to stream (got a 5x speed up on an image dataset)\r\n- improved RAM usage\r\n- support for metadata.parquet\r\n- allow to link to an image/audio/video multiple times\r\n- support for pyarrow filters (mostly efficient for parquet)\r\n- link to files using fields names `*_file_name` (in addition to the already existing `file_name`)\r\n  - this allows to have multiple image/audio/video per row\r\n  - there is also `file_names` and `*_file_names` for lists of image/audio/video\r\n\r\nChanges:\r\n- the builders iterate on the metadata files instead of the media files\r\n- the builders iterate on chunks of metadata instead of loading them in RAM completely\r\n- metadata files are no longer handled separately in `data_files`\r\n- added the `filters` argument to pass to `load_dataset`\r\n  - either as an [Expression](https://arrow.apache.org/docs/python/generated/pyarrow.dataset.Expression.html)\r\n  - or as tuples like `filters=[('event_name', '=', 'SomeEvent')]`\r\n- small breaking change: you can't add labels to a dataset with`drop_labels=False` if it has a metadata file\r\n- small breaking change: you can't use one metadata file for multiple splits anymore\r\n\r\nExample: `lhoestq/pusht-videofolder` is a video dataset with metadata.parquet where multiple rows can point to the same video\r\n\r\n```python\r\nIn [1]: from datasets import load_dataset\r\n\r\nIn [2]: load_dataset(\"lhoestq/pusht-videofolder\")\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:00<00:00, 1087.32it/s]\r\nOut[2]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['video', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index', 'task_index'],\r\n        num_rows: 25650\r\n    })\r\n})\r\n\r\nIn [3]: load_dataset(\"lhoestq/pusht-videofolder\", filters=[(\"next.reward\", \">\", 0.5)])\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:01<00:00, 183.03it/s]\r\nOut[3]: \r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['video', 'observation.state', 'action', 'episode_index', 'frame_index', 'timestamp', 'next.reward', 'next.done', 'next.success', 'index', 'task_index'],\r\n        num_rows: 5773\r\n    })\r\n})\r\n```\r\n\r\nAdditional change for VideoFolder:\r\n- decord can't be installed in many setups, I switched the backend to torchvision instead\r\n- I also added streaming capability from HF (you can get video frames without downloading the full video from HF)\r\n\r\nExample: load a robotics dataset\r\n\r\n```python\r\nIn [1]: from datasets import load_dataset\r\nds\r\nIn [2]: ds = load_dataset(\"lhoestq/pusht-videofolder\")\r\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 207/207 [00:00<00:00, 624.81it/s]\r\n\r\nIn [3]: ds[\"train\"][0]\r\nOut[3]: \r\n{'video': <torchvision.io.video_reader.VideoReader at 0x1145dc290>,\r\n 'observation.state': [222.0, 97.0],\r\n 'action': [233.0, 71.0],\r\n 'episode_index': 0,\r\n 'frame_index': 0,\r\n 'timestamp': 0.0,\r\n 'next.reward': 0.19029748439788818,\r\n 'next.done': False,\r\n 'next.success': False,\r\n 'index': 0,\r\n 'task_index': 0}\r\n```\r\n\r\nExample: stream frames without downloading full videos\r\n\r\n```python\r\nIn [1]: from datasets import load_dataset\r\n\r\nIn [2]: ds = load_dataset(\"BrianGuo/Tennis_Data\", streaming=True)\r\n\r\nIn [3]: example = next(iter(ds[\"train\"]))\r\n\r\nIn [4]: video = example[\"video\"]\r\n\r\nIn [5]: video.get_metadata()\r\nOut[5]: \r\n{'audio': {'framerate': [44100.0], 'duration': [2027.35]},\r\n 'video': {'fps': [59.00002712894387], 'duration': [2027.355]}}\r\n\r\nIn [6]: video.seek(1800, keyframes_only=True)  # 30min\r\nOut[6]: <torchvision.io.video_reader.VideoReader at 0x148d4d010>\r\n\r\nIn [7]: next(video)\r\nOut[7]: \r\n{'data': tensor([[[ 76,  77,  79,  ...,  41,  39,  38],\r\n          [ 76,  77,  79,  ...,  40,  39,  35],\r\n          [ 76,  77,  79,  ...,  34,  30,  26],\r\n          ...,\r\n          [127, 127, 127,  ..., 125, 125, 125],\r\n          [125, 126, 126,  ..., 125, 125, 125],\r\n          [122, 124, 126,  ..., 125, 125, 125]]], dtype=torch.uint8),\r\n 'pts': 1800.0}\r\n```\r\n\r\nTODO:\r\n- [x] docs\r\n- [x] fix tests<end_of_turn>\n<start_of_turn>model\nFaster  folder based builder + parquet support + allow repeated media + use torchvideo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe page https://huggingface.co/docs/datasets/repository_structure explains how to create a simple repository structure without a dataset script.\r\nIt's the simplest way to create a dataset and should be easier to find, particularly on the docs' first pages.<end_of_turn>\n<start_of_turn>model\nDocs: make \"repository structure\" easier to find<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nintended as fix for #7220 if this kind of extensibility is something that datasets is willing to support!\r\n\r\n\r\n```python\r\nfrom datasets.features.features import CustomFeature\r\n\r\n\r\nclass ListOfStrs(CustomFeature):\r\n    requires_encoding = True\r\n    def _encode_example(self, value):\r\n        if isinstance(value, str):\r\n            return [str]\r\n        else:\r\n            return value\r\nfeats = Features(strlist=ListOfStrs())\r\nfeats.encode_example({\"strlist\": \"a\"})[\"strlist\"] == feats[\"strlist\"].encode_example(\"a\")\r\n```<end_of_turn>\n<start_of_turn>model\nadd CustomFeature base class to support user-defined features with encoding/decoding logic<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to discussion at https://github.com/huggingface/datasets/pull/6255\r\n\r\nthis makes this code run in 2sec instead of >10sec\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"glue\", \"sst2\", streaming=True, trust_remote_code=False)\r\n```\r\n\r\nFor some datasets with many configs and files it can be up to 100x faster.\r\nThis is particularly important now that some datasets will be loaded from the Parquet export instead of the scripts.\r\n\r\nThe data files are only resolved in the builder `__init__`. To do so I added DataFilesPatternsList and DataFilesPatternsDict that have `.resolve()` to return resolved DataFilesList and DataFilesDict<end_of_turn>\n<start_of_turn>model\nLazy data files resolution<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix --repo-type order in cli upload docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThere were a few fixes released recently, some DVC ecosystem packages require newer version of `fsspec`. <end_of_turn>\n<start_of_turn>model\nchore(deps): bump fsspec<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate docs on trust_remote_code defaults to False.\r\n\r\nThe docs needed to be updated due to this PR:\r\n- #6954<end_of_turn>\n<start_of_turn>model\nUpdate docs on trust_remote_code defaults to False<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR corrects a small grammatical issue in the outputs of fingerprint.py:\r\n\r\n```diff\r\n- \"This warning is only showed once. Subsequent hashing failures won't be showed.\"\r\n+ \"This warning is only shown once. Subsequent hashing failures won't be shown.\"\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nGrammar fix: correct \"showed\" to \"shown\" in fingerprint.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to https://huggingface.slack.com/archives/C04RG8YRVB8/p1712643948574129 (interal)\r\n\r\nPositional args must be placed before optional args.\r\n\r\nFeel free to merge whenever it's ready.<end_of_turn>\n<start_of_turn>model\nFix typo in docs (upload CLI)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\npicklable batch_fn<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n# PR Summary\r\nThis small PR resolves the regex library warnings showing starting Python3.11:\r\n```python\r\nDeprecationWarning: 'count' is passed as positional argument\r\n```<end_of_turn>\n<start_of_turn>model\nFix regex library warnings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate GitHub Actions to Node 20.\r\n\r\nFix #6679.<end_of_turn>\n<start_of_turn>model\nUpdate GitHub Actions to Node 20<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCould you unpin [this](https://github.com/huggingface/datasets/blob/main/setup.py#L139) or move it to test requirements? This is a testing library and we also use it for our tests as well. We do not want to use a very outdated version.\n\n### Steps to reproduce the bug\n\ncould not install this library due to dependency conflict.\n\n### Expected behavior\n\ncan install datasets\n\n### Environment info\n\nlinux 64<end_of_turn>\n<start_of_turn>model\nCould you unpin responses version?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm using a dataset with map and multiprocessing to run a function that returned a variable length list of outputs. This output list may be empty. Normally this is handled fine, but there is an edge case that crops up when using multiprocessing. In some cases, an empty list result ends up in a dataset shard consisting of a single item. This results in a `The features can't be aligned` error that is difficult to debug because it depends on the number of processes/shards used.\r\n\r\nI've reproduced a minimal example below. My current workaround is to fill empty results with a dummy value that I filter after, but this was a weird error that took a while to track down.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndataset = datasets.Dataset.from_list([{'idx':i} for i in range(60)])\r\n\r\ndef test_func(row, idx):\r\n    if idx==58:\r\n        return {'output': []}\r\n    else:\r\n        return {'output' : [{'test':1}, {'test':2}]}\r\n\r\n# this works fine\r\ntest1 = dataset.map(lambda row, idx: test_func(row, idx), with_indices=True, num_proc=4)\r\n\r\n# this fails\r\ntest2 = dataset.map(lambda row, idx: test_func(row, idx), with_indices=True, num_proc=32)\r\n>ValueError: The features can't be aligned because the key output of features {'idx': Value(dtype='int64', id=None), 'output': Sequence(feature=Value(dtype='null', id=None), length=-1, id=None)} has unexpected type - Sequence(feature=Value(dtype='null', id=None), length=-1, id=None) (expected either [{'test': Value(dtype='int64', id=None)}] or Value(\"null\").\r\n```\r\n\r\nThe error occurs during the check\r\n\r\n```python\r\n_check_if_features_can_be_aligned([dset.features for dset in dsets])\r\n```\r\n\r\nWhen the multiprocessing splitting lines up just right with the empty return value, one of the `dset` in `dsets` will have a single item with an empty list value, causing the error.\n\n### Expected behavior\n\nExpected behavior is the result would be the same regardless of the `num_proc` value used.\n\n### Environment info\n\nDatasets version 2.11.0\r\nPython 3.9.16<end_of_turn>\n<start_of_turn>model\nInconsistent \"The features can't be aligned\" error when combining map, multiprocessing, and variable length outputs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nThe motivation for removing py.typed no longer seems to apply.  Would a solution like [this one](https://github.com/huggingface/huggingface_hub/pull/2752) work here?\n\n### Motivation\n\nMyPy support is broken.  As more type checkers come out, such as RedKnot, these may also be broken.  It would be good to be PEP 561 compliant as long as it's not too onerous.\n\n### Your contribution\n\nI can re-add py.typed, but I don't know how to make sur all of the `__all__` files are provided (although you may not need to with modern PyRight).<end_of_turn>\n<start_of_turn>model\nRe-add py.typed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix small typo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\ni'm unable to load my dataset for [controlnet training](https://github.com/huggingface/diffusers/blob/074e12358bc17e7dbe111ea4f62f05dbae8a49d5/examples/controlnet/train_controlnet.py#L606) using load_dataset(). however, load_from_disk() seems to work? \r\nwould appreciate if someone can explain why that's the case here\r\n\r\n1. for reference here's the structure of the original training files _before_ dataset creation - \r\n\r\n```\r\n- dir train\r\n     - dir A (illustrations)\r\n     - dir B (SignWriting)\r\n     - prompt.json containing:\r\n       {\"source\": \"B/file.png\", \"target\": \"A/file.png\", \"prompt\": \"...\"}\r\n```\r\n\r\n2. here are features _after_ dataset creation - \r\n```\r\n  \"features\": {\r\n    \"control_image\": {\r\n      \"_type\": \"Image\"\r\n    },\r\n    \"image\": {\r\n      \"_type\": \"Image\"\r\n    },\r\n    \"caption\": {\r\n      \"dtype\": \"string\",\r\n      \"_type\": \"Value\"\r\n    }\r\n```\r\n3. I've also attempted to upload the dataset to huggingface with the same error output \n\n### Steps to reproduce the bug\n\n1. [dataset creation script](https://github.com/sign-language-processing/signwriting-illustration/blob/main/signwriting_illustration/controlnet_huggingface/dataset.py)  \r\n\r\n2. controlnet [training script](examples/controlnet/train_controlnet.py) used \r\n\r\n3. training parameters - \r\n\r\n! accelerate launch diffusers/examples/controlnet/train_controlnet.py \\\r\n --pretrained_model_name_or_path=\"stable-diffusion-v1-5/stable-diffusion-v1-5\" \\\r\n --output_dir=\"$OUTPUT_DIR\" \\\r\n --train_data_dir=\"$HF_DATASET_DIR\" \\\r\n --conditioning_image_column=control_image \\\r\n --image_column=image \\\r\n --caption_column=caption \\\r\n --resolution=512\\\r\n --learning_rate=1e-5 \\\r\n --validation_image \"./validation/0a4b3c71265bb3a726457837428dda78.png\" \"./validation/0a5922fe2c638e6776bd62f623145004.png\" \"./validation/1c9f1a53106f64c682cf5d009ee7156f.png\" \\\r\n --validation_prompt \"An illustration of a man with short hair\" \"An illustration of a woman with short hair\" \"An illustration of Barack Obama\" \\\r\n --train_batch_size=4 \\\r\n --num_train_epochs=500 \\\r\n --tracker_project_name=\"sd-controlnet-signwriting-test\" \\\r\n --hub_model_id=\"sarahahtee/signwriting-illustration-test\" \\\r\n --checkpointing_steps=5000 \\\r\n --validation_steps=1000 \\\r\n --report_to wandb \\\r\n --push_to_hub\r\n \r\n 4. command - \r\n` sbatch --export=HUGGINGFACE_TOKEN=hf_token,WANDB_API_KEY=api_key script.sh`\n\n### Expected behavior\n\n```\r\n11/25/2024 17:12:18 - INFO - __main__ - Initializing controlnet weights from unet\r\nGenerating train split: 1 examples [00:00, 334.85 examples/s]\r\nTraceback (most recent call last):\r\n  File \"/data/user/user/signwriting_illustration/controlnet_huggingface/diffusers/examples/controlnet/train_controlnet.py\", line 1189, in <module>\r\n    main(args)\r\n  File \"/data/user/user/signwriting_illustration/controlnet_huggingface/diffusers/examples/controlnet/train_controlnet.py\", line 923, in main\r\n    train_dataset = make_train_dataset(args, tokenizer, accelerator)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/data/user/user/signwriting_illustration/controlnet_huggingface/diffusers/examples/controlnet/train_controlnet.py\", line 639, in make_train_dataset\r\n    raise ValueError(\r\nValueError: `--image_column` value 'image' not found in dataset columns. Dataset columns are: _data_files, _fingerprint, _format_columns, _format_kwargs, _format_type, _output_all_columns, _split\r\n```\n\n### Environment info\n\naccelerate 1.1.1\r\nhuggingface-hub 0.26.2\r\npython 3.11\r\ntorch 2.5.1\r\ntransformers 4.46.2<end_of_turn>\n<start_of_turn>model\nloading dataset issue with load_dataset() when training controlnet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI added `lock_importable_file` in `get_dataset_builder_class` and `extend_dataset_builder_for_streaming` to fix the issue, and I also added a test\r\n\r\ncc @clefourrier <end_of_turn>\n<start_of_turn>model\nFix concurrent script loading with force_redownload<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n- When reading dataset, a cache will be generated to the ~/. cache/huggingface/datasets directory\r\n- When using .map and .filter operations, runtime cache will be generated to the /tmp/hf_datasets-* directory\r\n- The default is to use the path of tempfile.tempdir\r\n- If I modify this path to the NFS disk, an error will be reported, but the program will continue to run\r\n- https://github.com/huggingface/datasets/blob/main/src/datasets/config.py#L257\r\n\r\n\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/managers.py\", line 616, in _run_server\r\n    server.serve_forever()\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/managers.py\", line 182, in serve_forever\r\n    sys.exit(0)\r\nSystemExit: 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/util.py\", line 300, in _run_finalizers\r\n    finalizer()\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/util.py\", line 224, in __call__\r\n    res = self._callback(*self._args, **self._kwargs)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/util.py\", line 133, in _remove_temp_dir\r\n    rmtree(tempdir)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/shutil.py\", line 718, in rmtree\r\n    _rmtree_safe_fd(fd, path, onerror)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/shutil.py\", line 675, in _rmtree_safe_fd\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/shutil.py\", line 673, in _rmtree_safe_fd\r\n    os.unlink(entry.name, dir_fd=topfd)\r\nOSError: [Errno 16] Device or resource busy: '.nfs000000038330a012000030b4'\r\nTraceback (most recent call last):\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/process.py\", line 315, in _bootstrap\r\n    self.run()\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/managers.py\", line 616, in _run_server\r\n    server.serve_forever()\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/managers.py\", line 182, in serve_forever\r\n    sys.exit(0)\r\nSystemExit: 0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/util.py\", line 300, in _run_finalizers\r\n    finalizer()\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/util.py\", line 224, in __call__\r\n    res = self._callback(*self._args, **self._kwargs)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/site-packages/multiprocess/util.py\", line 133, in _remove_temp_dir\r\n    rmtree(tempdir)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/shutil.py\", line 718, in rmtree\r\n    _rmtree_safe_fd(fd, path, onerror)\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/shutil.py\", line 675, in _rmtree_safe_fd\r\n    onerror(os.unlink, fullname, sys.exc_info())\r\n  File \"/home/wzp/miniconda3/envs/dask/lib/python3.8/shutil.py\", line 673, in _rmtree_safe_fd\r\n    os.unlink(entry.name, dir_fd=topfd)\r\nOSError: [Errno 16] Device or resource busy: '.nfs0000000400064d4a000030e5'\r\n\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nimport os\r\nimport time\r\nimport tempfile\r\n\r\nfrom datasets import load_dataset\r\n\r\ndef add_column(sample):\r\n    # print(type(sample))\r\n    # time.sleep(0.1)\r\n    sample['__ds__stats__'] = {'data': 123}\r\n    return sample\r\n\r\ndef filt_column(sample):\r\n    # print(type(sample))\r\n    if len(sample['content']) > 10:\r\n        return True\r\n    else:\r\n        return False\r\n\r\nif __name__ == '__main__':\r\n\r\n    input_dir = '/mnt/temp/CN/small'  # some json dataset\r\n    dataset = load_dataset('json', data_dir=input_dir)\r\n\r\n   \r\n    temp_dir = '/media/release/release/temp/temp'  # a nfs folder\r\n    os.makedirs(temp_dir, exist_ok=True)\r\n   \r\n    # change huggingface-datasets runtime cache in nfs\uff08default in /tmp\uff09\r\n    tempfile.tempdir = temp_dir\r\n\r\n    aa = dataset.map(add_column, num_proc=64)\r\n\r\n    aa = aa.filter(filt_column, num_proc=64)\r\n    print(aa)\r\n```\r\n\r\n### Expected behavior\r\n\r\nno error occur\r\n\r\n### Environment info\r\n\r\ndatasets==2.18.0\r\nubuntu 20.04<end_of_turn>\n<start_of_turn>model\ncache in nfs error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nContains the following improvements:\r\n\r\n* fixes a \"share dataset\" link in README and modifies the \"hosting\" part in the disclaimer section\r\n* updates `Makefile` to also run the style checks on `utils` and `setup.py`\r\n* deletes a test for GH-hosted datasets (no longer supported)\r\n* deletes `convert_dataset.sh` (outdated)\r\n* aligns `utils/release.py` with `transformers` (the current version is outdated)<end_of_turn>\n<start_of_turn>model\nMisc improvements<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ntesting https://github.com/huggingface/doc-builder/pull/426<end_of_turn>\n<start_of_turn>model\ntesting doc-builder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nInstead of waiting for data files to be extracted in the packaged builders, we can prepend the compression prefix and extract them as they are being read (using `fsspec`). This saves disk space (deleting extracted archives is not set by default) and slightly speeds up dataset generation (less disk reads)<end_of_turn>\n<start_of_turn>model\nExtract data on the fly in packaged builders<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTraceback (most recent call last):\n  File \"/workspace/ctn.py\", line 8, in <module>\n    ds = load_dataset(f\"naver-clova-ix/synthdog-{lang}\")  # \u0438\u043b\u0438 \"synthdog-zh\" \u0434\u043b\u044f \u043a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\u0433\u043e\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1397, in load_dataset\n    builder_instance = load_dataset_builder(\n        path=path,\n    ...<10 lines>...\n        **config_kwargs,\n    )\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1185, in load_dataset_builder\n    builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 612, in _use_legacy_cache_dir_if_possible\n    self._check_legacy_cache2(dataset_module) or self._check_legacy_cache() or None\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 485, in _check_legacy_cache2\n    config_id = self.config.name + \"-\" + Hasher.hash({\"data_files\": self.config.data_files})\n                                         ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/fingerprint.py\", line 188, in hash\n    return cls.hash_bytes(dumps(value))\n                          ~~~~~^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 120, in dumps\n    dump(obj, file)\n    ~~~~^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 114, in dump\n    Pickler(file, recurse=True).dump(obj)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 428, in dump\n    StockPickler.dump(self, obj)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n  File \"/usr/lib/python3.14/pickle.py\", line 498, in dump\n    self.save(obj)\n    ~~~~~~~~~^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 70, in save\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 422, in save\n    StockPickler.save(self, obj, save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.14/pickle.py\", line 572, in save\n    f(self, obj)  # Call unbound method with explicit self\n    ~^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 1262, in save_module_dict\n    StockPickler.save_dict(pickler, obj)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.14/pickle.py\", line 1064, in save_dict\n    self._batch_setitems(obj.items(), obj)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\n\n\n### Steps to reproduce the bug\n\nds_train = ds[\"train\"].map(lambda x: {**x, \"lang\": lang})\n\n\n### Expected behavior\n\nFixed bugs\n\n### Environment info\n\n\n- `datasets` version: 4.2.0\n- Platform: Linux-6.8.0-85-generic-x86_64-with-glibc2.39\n- Python version: 3.14.0\n- `huggingface_hub` version: 0.35.3\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.3\n- `fsspec` version: 2025.9.0\n<end_of_turn>\n<start_of_turn>model\nCaching does not work when using python3.14<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI am using a cluster that does not have access to the internet when given a job. I tried downloading the dataset using the huggingface-cli command and then loading it with load_dataset but I get an error:\r\n\r\n```raise EmptyDatasetError(f\"The directory at {base_path} doesn't contain any data files\") from None```\r\n\r\nThe dataset I'm using is \"lmsys/chatbot_arena_conversations\". The folder structure is\r\n\r\n- README.md\r\n- data\r\n  - train-00000-of-00001-cced8514c7ed782a.parquet\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Download dataset using HuggingFace CLI: ```huggingface-cli download lmsys/chatbot_arena_conversations --local-dir ./lmsys/chatbot_arena_conversations```\r\n2. In Python\r\n```\r\nfrom datasets import load_dataset\r\nload_dataset(\"lmsys/chatbot_arena_conversations\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nShould return a Dataset Dict in the form of\r\n```\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: [...],\r\n        num_rows: 33,000\r\n    })\r\n})\r\n```\r\n### Environment info\r\n\r\nPython 3.11.5\r\nDatasets 2.18.0\r\nTransformers 4.38.2\r\nPytorch 2.2.0\r\nPyarrow 15.0.1\r\nRocky Linux release 8.9 (Green Obsidian)\r\n<end_of_turn>\n<start_of_turn>model\nEmptyDatasetError when loading dataset downloaded with HuggingFace cli<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi everyone :)\r\n\r\nI have two local & custom datasets (1 \"sentence\" per line) which I split along the 95/5 lines for pre-training a Bert model. I use a modified version of `run_mlm.py` in order to be able to make use of `interleave_dataset`:\r\n\r\n- `tokenize()` runs fine\r\n- `group_text()` runs fine\r\n\r\nEverytime, on step 19, I get \r\n\r\n```pytb\r\n  File \"env/lib/python3.9/site-packages/transformers/data/data_collator.py\", line 779, in torch_mask_tokens\r\n    inputs[indices_random] = random_words[indices_random]\r\nRuntimeError: Index put requires the source and destination dtypes match, got Float for the destination and Long for the source.\r\n```\r\n\r\nI tried:\r\n- training without interleave on dataset 1, it runs\r\n- training without interleave on dataset 2, it runs\r\n- training without `.to_iterable_dataset()`, it hangs then crash\r\n- training without group_text() and padding to max_length seemed to fix the issue, but who knows if this was just because it was an issue that would come much later in terms of steps.\r\n\r\nI might have coded something wrong, but I don't get what \n\n### Steps to reproduce the bug\n\nI have this function:\r\n\r\n```py\r\ndef build_dataset(path: str, percent: str):\r\n    dataset = load_dataset(\r\n        \"text\",\r\n        data_files={\"train\": [path]},\r\n        split=f\"train[{percent}]\"\r\n    )\r\n    dataset = dataset.map(\r\n        lambda examples: tokenize(examples[\"text\"]),\r\n        batched=True,\r\n        num_proc=num_proc,\r\n    )\r\n\r\n    dataset = dataset.map(\r\n        group_texts,\r\n        batched=True,\r\n        num_proc=num_proc,\r\n        desc=f\"Grouping texts in chunks of {tokenizer.max_seq_length}\",\r\n        remove_columns=[\"text\"]\r\n    )\r\n\r\n    print(len(dataset))\r\n    return dataset.to_iterable_dataset()\r\n```\r\n\r\nI hardcoded group_text:\r\n```py\r\n    def group_texts(examples):\r\n        # Concatenate all texts.\r\n        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\r\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\r\n        # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.\r\n        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\r\n        total_length = (total_length // 512) * 512\r\n        # Split by chunks of max_len.\r\n        result = {\r\n            k: [t[i: i + 512] for i in range(0, total_length, 512)]\r\n            for k, t in concatenated_examples.items()\r\n        }\r\n        # result = {k: [el for el in elements if el] for k, elements in result.items()}\r\n        return result\r\n```\r\n\r\nAnd then I build datasets using the following code:\r\n\r\n```py\r\ntrain1 = build_dataset(\"d1.txt\", \":95%\")\r\ntrain2 = build_dataset(\"d2.txt\", \":95%\")\r\ndev1 = build_dataset(\"d1.txt\", \"95%:\")\r\ndev2 = build_dataset(\"d2.txt\", \"95%:\")\r\n```\r\n\r\nand finally I run\r\n```py\r\ntrain_dataset = interleave_datasets(\r\n    [train1, train2],\r\n    probabilities=[0.8, 0.2],\r\n    seed=42\r\n)\r\neval_dataset = interleave_datasets(\r\n    [dev1, dev2],\r\n    probabilities=[0.8, 0.2],\r\n    seed=42\r\n)\r\n```\r\n\r\nThen I run the training part which remains mostly untouched:\r\n\r\n> CUDA_VISIBLE_DEVICES=1 python custom_dataset.py --model_type bert --per_device_train_batch_size 32 --do_train --output_dir /var/mlm/training-bert/model --max_seq_length 512 --save_steps 10000 --save_total_limit 3 --auto_find_batch_size --logging_dir ./logs-bert --learning_rate 0.0001 --do_train --num_train_epochs 25 --warmup_steps 10000 --max_step 45000 --fp16\n\n### Expected behavior\n\nThe model should then train normally, but fails every time at the same step (19).\r\n\r\nprinting the variables at `inputs[indices_random] = random_words[indices_random]` shows a magnificient empty tensor (, 32) [if I remember well]\n\n### Environment info\n\ntransformers[torch] 4.30.2\r\nUbuntu\r\nA100 0 CUDA 12\r\nDriver Version: 525.116.04<end_of_turn>\n<start_of_turn>model\ninterleave_datasets & DataCollatorForLanguageModeling having a conflict ?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR updates https://huggingface.co/docs/datasets/loading to also link to the new video loading docs.\r\n\r\ncc @mfarre <end_of_turn>\n<start_of_turn>model\nAdd link to video dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCache only the actual changes to the dataset i.e. changed columns.\n\n### Motivation\n\nI realized that caching actually saves the complete dataset again.\r\nThis is especially problematic for image datasets if one wants to only change another column e.g. some metadata and then has to save 5 TB again.\n\n### Your contribution\n\nIs this even viable in the current architecture of the package?\r\nI quickly looked into it and it seems it would require significant changes.\r\n\r\nI would spend some time looking into this but maybe somebody could help with the feasibility and some plan to implement before spending too much time on it?<end_of_turn>\n<start_of_turn>model\nCache only changed columns?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI want to do:\r\n```\r\ndataset.take(512)\r\n```\r\n\r\nbut it only works with streaming = True\n\n### Motivation\n\nuniform interface to data sets. Really surprising the above only works with streaming = True. \n\n### Your contribution\n\nShould be trivial to copy paste the IterableDataset .take to use the local path in the data (when streaming = False)<end_of_turn>\n<start_of_turn>model\nAllow dataset implement .take<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`dtype: ClassLabel` in the README.md yaml metadata is parsed incorrectly and causes the data viewer to fail.\n\nThis yaml in my metadata ([source](https://huggingface.co/datasets/BrentLab/yeast_genome_resources/blob/main/README.md), though i changed `ClassLabel` to `string` to using different dtype in order to avoid the error):\n\n```yaml\nlicense: mit\npretty_name: BrentLab Yeast Genome Resources\nsize_categories:\n  - 1K<n<10K\nlanguage:\n  - en\ndataset_info:\n  features:\n    - name: start\n      dtype: int32\n      description: Start coordinate (1-based, **inclusive**)\n    - name: end\n      dtype: int32\n      description: End coordinate (1-based, **inclusive**)\n    - name: strand\n      dtype: ClassLabel\n...\n```\n\nis producing the following error in the data viewer:\n\n```\nError code:   ConfigNamesError\nException:    ValueError\nMessage:      Feature type 'Classlabel' not found. Available feature types: ['Value', 'ClassLabel', 'Translation', 'TranslationVariableLanguages', 'LargeList', 'List', 'Array2D', 'Array3D', 'Array4D', 'Array5D', 'Audio', 'Image', 'Video', 'Pdf']\nTraceback:    Traceback (most recent call last):\n                File \"/src/services/worker/src/worker/job_runners/dataset/config_names.py\", line 66, in compute_config_names_response\n                  config_names = get_dataset_config_names(\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 161, in get_dataset_config_names\n                  dataset_module = dataset_module_factory(\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1031, in dataset_module_factory\n                  raise e1 from None\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 996, in dataset_module_factory\n                  return HubDatasetModuleFactory(\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 605, in get_module\n                  dataset_infos = DatasetInfosDict.from_dataset_card_data(dataset_card_data)\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/info.py\", line 386, in from_dataset_card_data\n                  dataset_info = DatasetInfo._from_yaml_dict(dataset_card_data[\"dataset_info\"])\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/info.py\", line 317, in _from_yaml_dict\n                  yaml_data[\"features\"] = Features._from_yaml_list(yaml_data[\"features\"])\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 2027, in _from_yaml_list\n                  return cls.from_dict(from_yaml_inner(yaml_data))\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1872, in from_dict\n                  obj = generate_from_dict(dic)\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1459, in generate_from_dict\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1459, in <dictcomp>\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1465, in generate_from_dict\n                  raise ValueError(f\"Feature type '{_type}' not found. Available feature types: {list(_FEATURE_TYPES.keys())}\")\n              ValueError: Feature type 'Classlabel' not found. Available feature types: ['Value', 'ClassLabel', 'Translation', 'TranslationVariableLanguages', 'LargeList', 'List', 'Array2D', 'Array3D', 'Array4D', 'Array5D', 'Audio', 'Image', 'Video', 'Pdf']\n```\n\nI think that this is caused by this line\n\nhttps://github.com/huggingface/datasets/blob/896616c6cb03d92a33248c3529b0796cda27e955/src/datasets/features/features.py#L2013\n\nReproducible example from [naming.py](https://github.com/huggingface/datasets/blob/896616c6cb03d92a33248c3529b0796cda27e955/src/datasets/naming.py)\n\n```python\nimport itertools\nimport os\nimport re\n\n\n_uppercase_uppercase_re = re.compile(r\"([A-Z]+)([A-Z][a-z])\")\n_lowercase_uppercase_re = re.compile(r\"([a-z\\d])([A-Z])\")\n\n_single_underscore_re = re.compile(r\"(?<!_)_(?!_)\")\n_multiple_underscores_re = re.compile(r\"(_{2,})\")\n\n_split_re = r\"^\\w+(\\.\\w+)*$\"\n\ndef snakecase_to_camelcase(name):\n    \"\"\"Convert snake-case string to camel-case string.\"\"\"\n    name = _single_underscore_re.split(name)\n    name = [_multiple_underscores_re.split(n) for n in name]\n    return \"\".join(n.capitalize() for n in itertools.chain.from_iterable(name) if n != \"\")\n\nsnakecase_to_camelcase(\"ClassLabel\")\n```\n\nResult:\n\n```raw\n'Classlabel'\n```<end_of_turn>\n<start_of_turn>model\ndtype: ClassLabel is not parsed correctly in `features.py`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix https://github.com/huggingface/datasets/issues/7682<end_of_turn>\n<start_of_turn>model\nfix audio cast storage from array + sampling_rate<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe change in https://github.com/huggingface/datasets/pull/3197 introduced redundant list-comprehensions when `obj` is a long sequence of scalars. This becomes a noticeable overhead when loading data from an `IterableDataset` in the function `_apply_feature_types_on_example` and can be eliminated by adding a check for scalars in `encode_nested_example` proposed here. \r\n\r\nIn the following code example\r\n```\r\nimport time\r\nfrom datasets.features import Sequence, Value\r\nfrom datasets.features.features import encode_nested_example\r\n\r\nschema = Sequence(Value(\"int32\"))\r\nobj = list(range(100000))\r\n\r\nstart = time.perf_counter()\r\nresult = encode_nested_example(schema, obj)\r\nstop = time.perf_counter()\r\n\r\nprint(f\"Time spent is {stop-start} sec\")\r\n```\r\n`encode_nested_example` becomes 492x faster (from 0.0769 to 0.0002 sec), respectively 322x (from 0.00814 to 0.00003 sec) for a list of length 10000, on a GH200 system, making it unnoticeable when loading data with tokenization.\r\n\r\nAnother change is made to avoid creating arrays from scalars and afterwards re-extracting them during casting to python (`obj == obj.__array__()[()]` in that case), which avoids a regression in the array write benchmarks.<end_of_turn>\n<start_of_turn>model\nOptimized sequence encoding for scalars<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe `.map` function cannot hash under python 3.9. Tried to use [the solution here](https://github.com/huggingface/datasets/issues/4521#issuecomment-1205166653), but still get the same message: \r\n\r\n\r\n`Parameter 'function'=<function map_to_pred at 0x7fa0b49ead30> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.`\n\n### Steps to reproduce the bug\n\n```python\r\ndef map_to_pred(batch):\r\n    \"\"\"\r\n    Perform inference on an audio batch\r\n\r\n    Parameters:\r\n        batch (dict): A dictionary containing audio data and other related information.\r\n\r\n    Returns:\r\n        dict: The input batch dictionary with added prediction and transcription fields.\r\n    \"\"\"\r\n    audio = batch['audio']\r\n    input_features = processor(\r\n        audio['array'], sampling_rate=audio['sampling_rate'], return_tensors=\"pt\").input_features\r\n    input_features = input_features.to('cuda')\r\n    with torch.no_grad():\r\n        predicted_ids = model.generate(input_features)\r\n    preds = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\r\n    batch['prediction'] = processor.tokenizer._normalize(preds)\r\n    batch[\"transcription\"] = processor.tokenizer._normalize(batch['transcription'])\r\n    return batch\r\n\r\nMODEL_CARD = \"openai/whisper-small\"\r\nMODEL_NAME = MODEL_CARD.rsplit('/', maxsplit=1)[-1]\r\nmodel = WhisperForConditionalGeneration.from_pretrained(MODEL_CARD)\r\nprocessor = AutoProcessor.from_pretrained(\r\nMODEL_CARD, language=\"english\", task=\"transcribe\")\r\nmodel = torch.compile(model)\r\ndt = load_dataset(\"audiofolder\", data_dir=config['DATA']['dataset'], split=\"test\")\r\ndt = dt.cast_column(\"audio\", Audio(sampling_rate=16000))\r\nresult = coraal_dt.map(map_to_pred, num_proc=16)\r\n```\n\n### Expected behavior\n\nHashed and cached dataset starts inferencing\n\n### Environment info\n\n- `transformers` version: 4.35.0\r\n- Platform: Linux-5.14.0-284.30.1.el9_2.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.9.18\r\n- Huggingface_hub version: 0.17.3\r\n- Safetensors version: 0.4.0\r\n- Accelerate version: 0.24.1\r\n- Accelerate config:    not found\r\n- PyTorch version (GPU?): 2.1.0 (True)\r\n- Tensorflow version (GPU?): not installed (NA)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: yes\r\n- Using distributed or parallel set-up in script?: no<end_of_turn>\n<start_of_turn>model\n`.map` not hashing under python 3.9<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCompanion pr to https://github.com/huggingface/doc-builder/pull/379<end_of_turn>\n<start_of_turn>model\n[doc build] Use secrets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport ruff 0.5.0 in CI and revert:\r\n- #7007\r\n\r\nFix #7008.<end_of_turn>\n<start_of_turn>model\nSupport ruff 0.5.0 in CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollowing up on discussion in #6623 and #7198 I thought this would be pretty useful for my case so had a go at implementing.\r\n\r\nMy main motivation is to be able to call iterable_dataset.repeat(None).take(samples_per_epoch) to safely avoid timeout issues in a distributed training setting. This would provide a straightforward workaround for several open issues related to this situation:  https://github.com/huggingface/datasets/issues/6437, https://github.com/huggingface/datasets/issues/6594, https://github.com/huggingface/datasets/issues/6623, https://github.com/huggingface/datasets/issues/6719.\r\n\r\n@lhoestq let me know if this looks on the right track!<end_of_turn>\n<start_of_turn>model\nAdd repeat method to datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm trying to fine tune the openai/whisper model from huggingface using jupyter notebook and i keep getting this error, i'm following the steps in this blog post\r\n\r\nhttps://huggingface.co/blog/fine-tune-whisper\r\n\r\nI tried google collab and it works but because I'm on the free version the training doesn't complete\r\n\r\nthe error comes in jupyter notebook when i run this line\r\n\r\n`common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)`\r\n\r\nhere is the error message\r\n\r\n```\r\nMap (num_proc=4): 0% 0/2506 [00:52<?, ? examples/s]\r\nThe above exception was the direct cause of the following exception:\r\n\r\nNameError Traceback (most recent call last) Cell In[19], line 1 ----> 1 common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\dataset_dict.py:853, in DatasetDict.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, desc) 850 if cache_file_names is None: 851 cache_file_names = {k: None for k in self} 852 return DatasetDict( --> 853 { 854 k: dataset.map( 855 function=function, 856 with_indices=with_indices, 857 with_rank=with_rank, 858 input_columns=input_columns, 859 batched=batched, 860 batch_size=batch_size, 861 drop_last_batch=drop_last_batch, 862 remove_columns=remove_columns, 863 keep_in_memory=keep_in_memory, 864 load_from_cache_file=load_from_cache_file, 865 cache_file_name=cache_file_names[k], 866 writer_batch_size=writer_batch_size, 867 features=features, 868 disable_nullable=disable_nullable, 869 fn_kwargs=fn_kwargs, 870 num_proc=num_proc, 871 desc=desc, 872 ) 873 for k, dataset in self.items() 874 } 875 )\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\dataset_dict.py:854, in <dictcomp>(.0) 850 if cache_file_names is None: 851 cache_file_names = {k: None for k in self} 852 return DatasetDict( 853 { --> 854 k: dataset.map( 855 function=function, 856 with_indices=with_indices, 857 with_rank=with_rank, 858 input_columns=input_columns, 859 batched=batched, 860 batch_size=batch_size, 861 drop_last_batch=drop_last_batch, 862 remove_columns=remove_columns, 863 keep_in_memory=keep_in_memory, 864 load_from_cache_file=load_from_cache_file, 865 cache_file_name=cache_file_names[k], 866 writer_batch_size=writer_batch_size, 867 features=features, 868 disable_nullable=disable_nullable, 869 fn_kwargs=fn_kwargs, 870 num_proc=num_proc, 871 desc=desc, 872 ) 873 for k, dataset in self.items() 874 } 875 )\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:592, in transmit_tasks.<locals>.wrapper(*args, **kwargs) 590 self: \"Dataset\" = kwargs.pop(\"self\") 591 # apply actual function --> 592 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs) 593 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out] 594 for dataset in datasets: 595 # Remove task templates if a column mapping of the template is no longer valid\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs) 550 self_format = { 551 \"type\": self._format_type, 552 \"format_kwargs\": self._format_kwargs, 553 \"columns\": self._format_columns, 554 \"output_all_columns\": self._output_all_columns, 555 } 556 # apply actual function --> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs) 558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out] 559 # re-apply format to the output\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\arrow_dataset.py:3189, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc) 3182 logger.info(f\"Spawning {num_proc} processes\") 3183 with logging.tqdm( 3184 disable=not logging.is_progress_bar_enabled(), 3185 unit=\" examples\", 3186 total=pbar_total, 3187 desc=(desc or \"Map\") + f\" (num_proc={num_proc})\", 3188 ) as pbar: -> 3189 for rank, done, content in iflatmap_unordered( 3190 pool, Dataset._map_single, kwargs_iterable=kwargs_per_job 3191 ): 3192 if done: 3193 shards_done += 1\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1394, in iflatmap_unordered(pool, func, kwargs_iterable) 1391 finally: 1392 if not pool_changed: 1393 # we get the result in case there's an error to raise -> 1394 [async_result.get(timeout=0.05) for async_result in async_results]\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\datasets\\utils\\py_utils.py:1394, in <listcomp>(.0) 1391 finally: 1392 if not pool_changed: 1393 # we get the result in case there's an error to raise -> 1394 [async_result.get(timeout=0.05) for async_result in async_results]\r\n\r\nFile ~\\anaconda\\Lib\\site-packages\\multiprocess\\pool.py:774, in ApplyResult.get(self, timeout) 772 return self._value 773 else: --> 774 raise self._value\r\n\r\nNameError: name 'feature_extractor' is not defined\r\n```\n\n### Steps to reproduce the bug\n\n1. follow the steps in this blog post \r\n     https://huggingface.co/blog/fine-tune-whisper\r\n\r\n2.  run this line of code\r\n    `common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=4)`\r\n\r\n3. I'm using jupyter notebook from anaconda\n\n### Expected behavior\n\nNo error message\n\n### Environment info\n\ndatasets version: 2.8.0\r\nPython version: 3.11\r\nWindows 10<end_of_turn>\n<start_of_turn>model\nI'm trying to fine tune the openai/whisper model from huggingface using jupyter notebook and i keep getting this error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds a complementary example showing the **column-overwriting** pattern, which is both more direct and more flexible for many transformations.\r\n\r\n### Proposed Change\r\n\r\nThe original `remove_columns` example remains untouched. Below it, this PR introduces an alternative approach that overwrites an existing column during batch mapping.\r\n\r\nThis teaches users a core `.map()` capability for in-place transformations without extra intermediate steps.\r\n\r\n**New Example:**\r\n\r\n> ```python\r\n> >>> from datasets import Dataset\r\n> >>> dataset = Dataset.from_dict({\"a\": [0, 1, 2]})\r\n> # Overwrite \"a\" directly to duplicate each value\r\n> >>> duplicated_dataset = dataset.map(\r\n> ...     lambda batch: {\"a\": [x for x in batch[\"a\"] for _ in range(2)]},\r\n> ...     batched=True\r\n> ... )\r\n> >>> duplicated_dataset\r\n> Dataset({\r\n>     features: ['a'],\r\n>     num_rows: 6\r\n> })\r\n> >>> duplicated_dataset[\"a\"]\r\n> [0, 0, 1, 1, 2, 2]\r\n> ```\r\n<end_of_turn>\n<start_of_turn>model\ndocs: Add column overwrite example to batch mapping guide<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...to be aligned with IterableDataset.take and IterableDataset.skip<end_of_turn>\n<start_of_turn>model\nAdd Dataset.take and Dataset.skip<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using the Dataset.to_json() function, an unexpected error occurs if the parameter is set to lines=False. The stored data should be in the form of a list, but it actually turns into multiple lists, which causes an error when reading the data again.\r\nThe reason is that to_json() writes to the file in several segments based on the batch size. This is not a problem when lines=True, but it is incorrect when lines=False, because writing in several times will produce multiple lists(when len(dataset) > batch_size).\n\n### Steps to reproduce the bug\n\ntry this code:\r\n```python\r\nfrom datasets import load_dataset\r\nimport json\r\n\r\ntrain_dataset = load_dataset(\"Anthropic/hh-rlhf\", data_dir=\"harmless-base\")[\"train\"]\r\noutput_path = \"./harmless-base_hftojs.json\"\r\nprint(len(train_dataset))\r\ntrain_dataset.to_json(output_path, lines=False, force_ascii=False, indent=2)\r\n\r\nwith open(output_path, encoding=\"utf-8\") as f:\r\n    data = json.loads(f.read())\r\n\r\n```\r\nit raise error: json.decoder.JSONDecodeError: Extra data: line 4003 column 1 (char 1373709)\r\n\r\nExtra square brackets have appeared here:\r\n<img width=\"265\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/26499566/81492332-386d-42e8-88d1-b6d4ae3682cc\">\r\n\n\n### Expected behavior\n\nThe code runs normally.\n\n### Environment info\n\ndatasets=2.20.0<end_of_turn>\n<start_of_turn>model\nA bug of Dataset.to_json() function<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixed Quickstart Notebook Link in the [Overview notebook](https://github.com/huggingface/datasets/blob/main/notebooks/Overview.ipynb)<end_of_turn>\n<start_of_turn>model\nUpdated Quickstart Notebook link<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHello, when I ran the [code snippet](https://huggingface.co/docs/datasets/v2.14.4/en/loading#json) on the document, I encountered the following problem:\r\n\r\n```\r\nPython 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> base_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"\r\n>>> dataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/datasets/load.py\", line 2112, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/datasets/load.py\", line 1798, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/datasets/load.py\", line 1413, in dataset_module_factory\r\n    ).get_module()\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/datasets/load.py\", line 949, in get_module\r\n    data_files = DataFilesDict.from_patterns(\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/datasets/data_files.py\", line 672, in from_patterns\r\n    DataFilesList.from_patterns(\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/datasets/data_files.py\", line 578, in from_patterns\r\n    resolve_pattern(\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/datasets/data_files.py\", line 340, in resolve_pattern\r\n    for filepath, info in fs.glob(pattern, detail=True).items()\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/fsspec/asyn.py\", line 113, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/fsspec/asyn.py\", line 98, in sync\r\n    raise return_result\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/fsspec/asyn.py\", line 53, in _runner\r\n    result[0] = await coro\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/fsspec/implementations/http.py\", line 449, in _glob\r\n    elif await self._exists(path):\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/fsspec/implementations/http.py\", line 306, in _exists\r\n    r = await session.get(self.encode_url(path), **kw)\r\n  File \"/home/liushuai/anaconda3/lib/python3.10/site-packages/aiohttp/client.py\", line 922, in get\r\n    self._request(hdrs.METH_GET, url, allow_redirects=allow_redirects, **kwargs)\r\nTypeError: ClientSession._request() got an unexpected keyword argument 'https'\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset  \r\nbase_url = \"https://rajpurkar.github.io/SQuAD-explorer/dataset/\"  \r\ndataset = load_dataset(\"json\", data_files={\"train\": base_url + \"train-v1.1.json\", \"validation\": base_url + \"dev-v1.1.json\"}, field=\"data\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nable to load normally\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.4.54-2-x86_64-with-glibc2.27\r\n- Python version: 3.10.9\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nTypeError: ClientSession._request() got an unexpected keyword argument 'https'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`AttributeError: type object 'tqdm' has no attribute '_lock'`\n\nIt occurs when I'm trying to load datasets in thread pool.\n\nIssue https://github.com/huggingface/datasets/issues/6066 and PR https://github.com/huggingface/datasets/pull/6067 https://github.com/huggingface/datasets/pull/6068 tried to fix this. \n\n\n### Steps to reproduce the bug\n\nWill have to try several times to reproduce the error because it is concerned with threads.\n \n1. save some datasets for test\n```pythonfrom datasets import Dataset, DatasetDict\nimport os\n\nos.makedirs(\"test_dataset_shards\", exist_ok=True)\n\nfor i in range(10):\n    data = Dataset.from_dict({\"text\": [f\"example {j}\" for j in range(1000000)]})\n    data = DatasetDict({'train': data})\n    data.save_to_disk(f\"test_dataset_shards/shard_{i}\")\n```\n\n2. load them in a thread pool \n```python\nfrom datasets import load_from_disk\nfrom tqdm import tqdm\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport glob\n\ndatas = glob.glob('test_dataset_shards/shard_*')\nwith ThreadPoolExecutor(max_workers=10) as pool:\n    futures = [pool.submit(load_from_disk, it) for it in datas]\ndatas = []\nfor future in tqdm(as_completed(futures), total=len(futures)):\n    datas.append(future.result())\n```\n\n### Expected behavior\n\nno exception raised\n\n### Environment info\n\ndatasets==2.19.0\npython==3.10<end_of_turn>\n<start_of_turn>model\nAttributeError: type object 'tqdm' has no attribute '_lock'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove dead code for pyarrow < 15.0.0.\r\n\r\nCode is dead since the merge of:\r\n- #6892\r\n\r\nFix #7022.<end_of_turn>\n<start_of_turn>model\nRemove dead code for pyarrow < 15.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nBasic implementation, which writes one shard per input dataset shard.\r\nThis is to be improved later.\r\n\r\nClose https://github.com/huggingface/datasets/issues/5665\r\n\r\nPS: for image/audio datasets structured as actual image/audio files (not parquet), you can sometimes speed it up with `ds.decode(num_threads=...).push_to_hub(...)`<end_of_turn>\n<start_of_turn>model\nAdd `IterableDataset.push_to_hub()`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen trying to filter my custom dataset, the process hangs, regardless of the lambda function used. It appears to be an issue with the way the Images are being handled. The dataset in question is a preprocessed version of https://huggingface.co/datasets/danaaubakirova/patfig where notably, I have converted the data to the Parquet format.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('lcolonn/patfig', split='test')\r\nds_filtered = ds.filter(lambda row: row['cpc_class'] != 'Y')\r\n```\r\n\r\nEventually I ctrl+C and I obtain this stack trace:\r\n\r\n```\r\n>>> ds_filtered = ds.filter(lambda row: row['cpc_class'] != 'Y')\r\nFilter:   0%|                                                                | 0/998 [00:00<?, ? examples/s]Filter:   0%|                                                                | 0/998 [00:35<?, ? examples/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 567, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/fingerprint.py\", line 482, in wrapper\r\n    out = func(dataset, *args, **kwargs)\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3714, in filter\r\n    indices = self.map(\r\n              ^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 602, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 567, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3161, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3552, in _map_single\r\n    batch = apply_function_on_filtered_inputs(\r\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 3421, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 6478, in get_indices_from_mask_function\r\n    num_examples = len(batch[next(iter(batch.keys()))])\r\n                       ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 273, in __getitem__\r\n    value = self.format(key)\r\n            ^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 376, in format\r\n    return self.formatter.format_column(self.pa_table.select([key]))\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 443, in format_column\r\n    column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/formatting/formatting.py\", line 219, in decode_column\r\n    return self.features.decode_column(column, column_name) if self.features else column\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/features/features.py\", line 2008, in decode_column\r\n    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/features/features.py\", line 2008, in <listcomp>\r\n    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/features/features.py\", line 1351, in decode_nested_example\r\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/datasets/features/image.py\", line 188, in decode_example\r\n    image.load()  # to avoid \"Too many open files\" errors\r\n    ^^^^^^^^^^^^\r\n  File \"/home/l-walewski/miniconda3/envs/patentqa/lib/python3.11/site-packages/PIL/ImageFile.py\", line 293, in load\r\n    n, err_code = decoder.decode(b)\r\n                  ^^^^^^^^^^^^^^^^^\r\nKeyboardInterrupt\r\n```\r\n\r\nWarning! This can even seem to cause some computers to crash.\r\n\r\n### Expected behavior\r\n\r\nShould return the filtered dataset\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.20.0\r\n- Platform: Linux-6.5.0-41-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.9\r\n- `huggingface_hub` version: 0.24.0\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\nFilter hangs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis is needed to enable page pruning with DataFusion, which will be useful for the Dataset Viewer.\r\n\r\nIndeed page pruning with DataFusion allows to download only certain pages of a row group, reducing the I/O required to read just a few rows.\r\n\r\nBut while data page v1 generally works, it's not easy with DataFusion to do page pruning on datasets with nested data. This is because rows can span multiple pages in v1, contrary to v2.\r\n\r\ncc @severo for viz<end_of_turn>\n<start_of_turn>model\nParquet: use data page v2 for efficient page pruning<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n# Let's make it faster\r\nFirst, an evidence...\r\n![image](https://github.com/huggingface/datasets/assets/159512661/a703a82c-43a0-426c-9d99-24c563d70965)\r\nFigure 1: CProfile for loading 3 files from cerebras/SlimPajama-627B train split, and 3 files from test split using streaming=True. X axis is 1106 seconds long.\r\n\r\nSee? It's pretty slow.\r\n\r\nWhat is resolve pattern doing?\r\n```\r\nresolve_pattern called with **/train/** and hf://datasets/cerebras/SlimPajama-627B@2d0accdd58c5d5511943ca1f5ff0e3eb5e293543\r\nresolve_pattern took 20.815081119537354 seconds\r\n```\r\nMakes sense. How to improve it?\r\n\r\n## Bigger project, biggest payoff\r\n\r\nDatabricks (and consequently, spark) store a compressed manifest file of the files contained in the remote filesystem.\r\nThen, you download one tiny file, decompress it, and all the operations are local instead of this shenanigans.\r\n\r\nIt seems pretty straightforward to make dataset uploads compute a manifest and upload it alongside their data.\r\n\r\nThis would make resolution time so fast that nobody would ever think about it again.\r\nIt also means you either need to have the uploader compute it _every time_, or have a hook that computes it.\r\n\r\n## Smaller project, immediate payoff: Be diligent in avoiding deepcopy\r\n\r\nRevise the _ls_tree method to avoid deepcopy:\r\n```\r\n    def _ls_tree(\r\n        self,\r\n        path: str,\r\n        recursive: bool = False,\r\n        refresh: bool = False,\r\n        revision: Optional[str] = None,\r\n        expand_info: bool = True,\r\n    ):\r\n ..... omitted .....\r\n            for path_info in tree:\r\n                if isinstance(path_info, RepoFile):\r\n                    cache_path_info = {\r\n                        \"name\": root_path + \"/\" + path_info.path,\r\n                        \"size\": path_info.size,\r\n                        \"type\": \"file\",\r\n                        \"blob_id\": path_info.blob_id,\r\n                        \"lfs\": path_info.lfs,\r\n                        \"last_commit\": path_info.last_commit,\r\n                        \"security\": path_info.security,\r\n                    }\r\n                else:\r\n                    cache_path_info = {\r\n                        \"name\": root_path + \"/\" + path_info.path,\r\n                        \"size\": 0,\r\n                        \"type\": \"directory\",\r\n                        \"tree_id\": path_info.tree_id,\r\n                        \"last_commit\": path_info.last_commit,\r\n                    }\r\n                parent_path = self._parent(cache_path_info[\"name\"])\r\n                self.dircache.setdefault(parent_path, []).append(cache_path_info)\r\n                out.append(cache_path_info)\r\n        return copy.deepcopy(out)  # copy to not let users modify the dircache\r\n```\r\nObserve this deepcopy at the end. It is making a copy of a very simple data structure. We do not need to copy. We can simply generate the data structure twice instead. It will be much faster.\r\n```\r\n    def _ls_tree(\r\n        self,\r\n        path: str,\r\n        recursive: bool = False,\r\n        refresh: bool = False,\r\n        revision: Optional[str] = None,\r\n        expand_info: bool = True,\r\n    ):\r\n ..... omitted .....\r\n            def make_cache_path_info(path_info):\r\n                if isinstance(path_info, RepoFile):\r\n                    return {\r\n                        \"name\": root_path + \"/\" + path_info.path,\r\n                        \"size\": path_info.size,\r\n                        \"type\": \"file\",\r\n                        \"blob_id\": path_info.blob_id,\r\n                        \"lfs\": path_info.lfs,\r\n                        \"last_commit\": path_info.last_commit,\r\n                        \"security\": path_info.security,\r\n                    }\r\n                else:\r\n                    return {\r\n                        \"name\": root_path + \"/\" + path_info.path,\r\n                        \"size\": 0,\r\n                        \"type\": \"directory\",\r\n                        \"tree_id\": path_info.tree_id,\r\n                        \"last_commit\": path_info.last_commit,\r\n                    }\r\n            for path_info in tree:\r\n                cache_path_info = make_cache_path_info(path_info)\r\n                out_cache_path_info = make_cache_path_info(path_info) # copy to not let users modify the dircache\r\n                parent_path = self._parent(cache_path_info[\"name\"])\r\n                self.dircache.setdefault(parent_path, []).append(cache_path_info)\r\n                out.append(out_cache_path_info)\r\n        return out\r\n```\r\nNote there is no longer a deepcopy in this method. We have replaced it with generating the output twice. This is substantially faster. For me, the entire resolution went from 1100s to 360s.\r\n\r\n## Medium project, medium payoff\r\nAfter the above change, we have this profile:\r\n![image](https://github.com/huggingface/datasets/assets/159512661/db7b83da-2dfc-4c2e-abab-0ede9477876c)\r\nFigure 2: x-axis is 355 seconds. Note that globbing and _ls_tree deep copy is gone. No surprise there. It's much faster now, but we still spend ~187seconds in get_fs_token_paths.\r\n\r\nWell get_fs_token_paths is part of fsspec. We don't need to fix that because we can trust their developers to write high performance code. Probably the caller has misconfigured something. Let's take a look at the storage_options being provided to the filesystem that is constructed during this call.\r\n\r\nAh yes, streaming_download_manager::_prepare_single_hop_path_and_storage_options. We know streaming download manager is not compatible with async right now, but we really need this specific part of the code to be async. We're spending so much time checking isDir on the remote filesystem, it's a huge waste.\r\n\r\nWe can make the call easily 20-30x faster by using async, removing this performance bottleneck almost entirely (and reducing the total time of this part of the code to <30s. There is no reason to block async isDir calls for streaming.\r\n\r\nI'm not going to mess w/ this one myself; I didn't write the streaming impl, and I don't know how it works, but I know the isDir check can be async.\n\n### Steps to reproduce the bug\n\n```\r\nwith cProfile.Profile() as pr:\r\n            pr.enable()\r\n            # Begin Data\r\n            if not os.path.exists(data_cache_dir):\r\n                os.makedirs(data_cache_dir, exist_ok=True)\r\n            training_dataset = load_dataset(training_dataset_name, split=training_split, cache_dir=data_cache_dir, streaming=True).take(training_slice)\r\n            eval_dataset = load_dataset(eval_dataset_name, split=eval_split, cache_dir=data_cache_dir, streaming=True).take(eval_slice)\r\n            # End Data\r\n            pr.disable()\r\n            pr.create_stats()\r\n            if not os.path.exists(profiling_path):\r\n                os.makedirs(profiling_path, exist_ok=True)\r\n            pr.dump_stats(os.path.join(profiling_path, \"cprofile.prof\"))\r\n```\r\n\r\nrun this code for  \"cerebras/SlimPajama-627B\" and whatever other params\n\n### Expected behavior\n\nSomething better.\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.21.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nProfiling for HF Filesystem shows there are easy performance gains to be made<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to run some finetunings on 4 A100 GPUs using SLURM using axolotl training framework which in turn uses Huggingface's Trainer and Accelerate on [Fineweb-10BT](https://huggingface.co/datasets/HuggingFaceFW/fineweb), but I end up running into 429 Client Error: Too Many Requests for URL error when I call next(dataloader_iter). Funny is, that I can run some test fine tuning (for just 200 training steps) in 1 A100 GPU using SLURM. Is there any rate limiter set for querying dataset? I could run the fine tuning with the same settings (4 A100 GPUs in SLURM) last month.\n\n### Steps to reproduce the bug\n\nYou would need a server installed with SLURM\n\n1. Create conda environment\n1.1 conda create -n example_env -c conda-forge gxx=11 python=3.10\n1.2 conda activate example_env\n1.3 pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n1.4 conda install nvidia/label/cuda-12.4.0::cuda-toolkit\n1.5 Download flash_attn-2.7.4.post1+cu12torch2.5cxx11abiFALSE-cp310-cp310-linux_x86_64.whl\n1.6 pip3 install packaging\n1.7 pip3 install ninja\n1.8 pip3 install mlflow\n1.9 Clone https://github.com/calvintanama/axolotl.git\n1.10 `cd` to `axolotl`\n1.11 pip3 install -e '.[deepspeed]'\n\n2. Run the training\n2.1. Create a folder called `config_run` in axolotl directory\n2.2. Copy `config/phi3_pruned_extra_pretrain_22_29_bottleneck_residual_8_a100_4.yaml` to `config_run`\n2.3. Change yaml file in the `config_run` accordingly\n2.4. Change directory and conda environment name in `jobs/train_phi3_22_29_bottleneck_residual_8_a100_4_temp.sh`\n2.5. `jobs/train_phi3_22_29_bottleneck_residual_8_a100_4_temp.sh`\n\n### Expected behavior\n\nThis should not cause any error, but gotten\n\n```\nFile \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/accelerate/data_loader.py\", line 552, in __iter__\n[rank3]:     current_batch = next(dataloader_iter)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n[rank3]:     data = self._next_data()\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n[rank3]:     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 33, in fetch\n[rank3]:     data.append(next(self.dataset_iter))\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/accelerate/data_loader.py\", line 338, in __iter__\n[rank3]:     for element in self.dataset:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 2266, in __iter__\n[rank3]:     for key, example in ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1866, in __iter__\n[rank3]:     for key, example in self.ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1084, in __iter__\n[rank3]:     yield from self._iter()\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1263, in _iter\n[rank3]:     for key, transformed_example in outputs:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1258, in <genexpr>\n[rank3]:     outputs = (\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1244, in iter_outputs\n[rank3]:     for i, key_example in inputs_iterator:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1106, in iter_batched_inputs\n[rank3]:     for key, example in iterator:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1866, in __iter__\n[rank3]:     for key, example in self.ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1535, in __iter__\n[rank3]:     for x in self.ex_iterable:\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 374, in __iter__\n[rank3]:     for key, pa_table in self.generate_tables_fn(**gen_kwags):\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 90, in _generate_tables\n[rank3]:     if parquet_fragment.row_groups:\n[rank3]:   File \"pyarrow/_dataset_parquet.pyx\", line 386, in pyarrow._dataset_parquet.ParquetFileFragment.row_groups.__get__\n[rank3]:   File \"pyarrow/_dataset_parquet.pyx\", line 393, in pyarrow._dataset_parquet.ParquetFileFragment.metadata.__get__\n[rank3]:   File \"pyarrow/_dataset_parquet.pyx\", line 382, in pyarrow._dataset_parquet.ParquetFileFragment.ensure_complete_metadata\n[rank3]:   File \"pyarrow/error.pxi\", line 89, in pyarrow.lib.check_status\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 827, in read_with_retries\n[rank3]:     out = read(*args, **kwargs)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 1013, in read\n[rank3]:     return super().read(length)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/fsspec/spec.py\", line 1941, in read\n[rank3]:     out = self.cache._fetch(self.loc, self.loc + length)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/fsspec/caching.py\", line 234, in _fetch\n[rank3]:     self.cache = self.fetcher(start, end)  # new block replaces old\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 976, in _fetch_range\n[rank3]:     hf_raise_for_status(r)\n[rank3]:   File \"/home/hk-project-test-p0023745/cd7437/miniconda3/envs/llmpruning_train_temp/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 482, in hf_raise_for_status\n[rank3]:     raise _format(HfHubHTTPError, str(e), response) from e\n[rank3]: huggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/datasets/HuggingFaceFW/fineweb/resolve/0f039043b23fe1d4eed300b504aa4b4a68f1c7ba/sample/10BT/006_00000.parquet\n```\n\n### Environment info\n\n- datasets 3.5.0\n- torch 2.5.1\n- transformers 4.46.2<end_of_turn>\n<start_of_turn>model\nHfHubHTTPError: 429 Client Error: Too Many Requests for URL when trying to access Fineweb-10BT on 4A100 GPUs using SLURM<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have run the official example of MLM like:\r\n\r\n```bash\r\n  python run_mlm.py \\\r\n      --model_name_or_path roberta-base \\\r\n      --dataset_name togethercomputer/RedPajama-Data-1T \\\r\n      --dataset_config_name arxiv \\\r\n      --per_device_train_batch_size 10 \\\r\n      --preprocessing_num_workers 20 \\\r\n      --validation_split_percentage 0 \\\r\n      --cache_dir /project/huggingface_cache/datasets \\\r\n      --line_by_line \\\r\n      --do_train \\\r\n      --pad_to_max_length \\\r\n      --output_dir /project/huggingface_cache/test-mlm\r\n```\r\nit successfully runs and at my cache folder has `cache-1982fea76aa54a13_00001_of_00020.arrow`.....  `cache-1982fea76aa54a13_00020_of_00020.arrow ` as tokenization cache of `map` method. And the cache works fine every time I run the command above.\r\n\r\nHowever, when I switched to jupyter notebook (since I do not want to load datasets every time when I changed other parameters not related to the dataloading). It is not recognizing the cache files and starts to re-run the entire tokenization process. \r\n\r\nI changed my code to \r\n```python\r\ntokenized_datasets = raw_datasets[\"train\"].map(\r\n                tokenize_function,\r\n                batched=True,\r\n                num_proc=data_args.preprocessing_num_workers,\r\n                remove_columns=[text_column_name],\r\n                load_from_cache_file=True,\r\n                desc=\"Running tokenizer on dataset line_by_line\",\r\n                # cache_file_names= {\"train\": \"cache-1982fea76aa54a13.arrow\"}\r\n                cache_file_name=\"cache-1982fea76aa54a13.arrow\",\r\n                new_fingerprint=\"1982fea76aa54a13\"\r\n            )\r\n```\r\nit still does not recognize the previously cached files and trying to re-run the tokenization process.\n\n### Steps to reproduce the bug\n\nuse jupyter notebook for dataset map function.\n\n### Expected behavior\n\nthe map function accepts the given cache_file_name and new_fingerprint then load the previously cached files.\n\n### Environment info\n\n- `datasets` version: 2.14.4.dev0\r\n- Platform: Linux-3.10.0-1160.59.1.el7.x86_64-x86_64-with-glibc2.10\r\n- Python version: 3.8.8\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nForce to reuse cache at given path<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported on slack:\r\n\r\nThe following code snippet gives an error with v2.19 but not with v2.18:\r\nfrom datasets import load_dataset\r\n```\r\ndataset = load_dataset(\r\n    \"lvwerra/stack-exchange-paired\",\r\n    split=\"train\",\r\n    cache_dir=None,\r\n    data_dir=\"data/rl\",\r\n)\r\n```\r\nand the error is:\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2609, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1027, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 1140, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/utils/info_utils.py\", line 92, in verify_splits\r\n    raise ExpectedMoreSplits(str(set(expected_splits) - set(recorded_splits)))\r\ndatasets.utils.info_utils.ExpectedMoreSplits: {'test'}\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nFix expected splits when passing data_files or dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHi, \r\n\r\ncould you add an implementation of a batched `IterableDataset`. It already support an option to do batch iteration via `.iter(batch_size=...)` but this cannot be used in combination with a torch `DataLoader` since it just returns an iterator.\n\n### Motivation\n\nThe current implementation loads each element of a batch individually which can be very slow in cases of a big batch_size. I did some experiments [here](https://discuss.huggingface.co/t/slow-dataloader-with-big-batch-size/57224) and using a batched iteration would speed up data loading significantly.\n\n### Your contribution\n\nN/A<end_of_turn>\n<start_of_turn>model\nBatched IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLet the dataset contain a column named 'a', which is of the string type.\r\nIf 'a' is converted to a boolean using batched mapping, the mapper automatically casts the boolean to a string (e.g., True -> 'true').\r\nIt only happens when the original column and the mapped column name are identical.\r\n\r\nThank you!\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset\r\n\r\ndset = Dataset.from_dict({'a': ['11', '22']})\r\ndset = dset.map(lambda x: {'a': [True for _ in x['a']]}, batched=True)\r\nprint(dset['a'])\r\n```\r\n```\r\n> ['true', 'true']\r\n```\n\n### Expected behavior\n\n[True, True]\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.4.0-148-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.21.4\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\nBatched mapping of existing string column casts boolean to string<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nAdd an environment variable to replace the default lock file path.\r\n\r\n### Motivation\r\n\r\nUsually, dataset path is a read-only path while the lock file needs to be modified each time. It would be convenient if the path can be reset individually.\r\n\r\n### Your contribution\r\n\r\n```/src/datasets/utils/filelock.py\r\nclass UnixFileLock(BaseFileLock):\r\n    def __init__(self, lock_file, timeout=-1, max_filename_length=None):\r\n        #-------------------\r\n        if os.getenv('DS_TMP_PATH'):\r\n            file_name = str(lock_file).split('/')[-1]\r\n            dataset_tmp_path = os.getenv('DS_TMP_PATH')\r\n            lock_file = os.path.join(dataset_tmp_path, file_name)\r\n        #-------------------\r\n        max_filename_length = os.statvfs(os.path.dirname(lock_file)).f_namemax\r\n        super().__init__(lock_file, timeout=timeout, max_filename_length=max_filename_length)\r\n```\r\nA simple demo is as upper. Thanks.<end_of_turn>\n<start_of_turn>model\nAllow setting the environment variable for the lock file path<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix typo in arrow_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading a dataset from a GCP bucket raises a type error. This bug was introduced recently (either in 2.14 or 2.14.1), and appeared during a migration from 2.13.1.\n\n### Steps to reproduce the bug\n\nLoad any file from a GCP bucket:\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"json\", data_files=[\"gs://...\"])\r\n```\r\n\r\nThe following exception is raised:\r\n\r\n```python\r\nTraceback (most recent call last):\r\n...\r\npackages/datasets/data_files.py\", line 335, in resolve_pattern\r\n    protocol_prefix = fs.protocol + \"://\" if fs.protocol != \"file\" else \"\"\r\nTypeError: can only concatenate tuple (not \"str\") to tuple\r\n```\r\n\r\nWith a `GoogleFileSystem`, the attribute `fs.protocol` is a tuple `('gs', 'gcs')` and hence cannot be concatenated with a string.\r\n\n\n### Expected behavior\n\nThe file should be loaded without exception.\n\n### Environment info\n\n- `datasets` version: 2.14.1\r\n- Platform: macOS-13.2.1-x86_64-i386-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n<end_of_turn>\n<start_of_turn>model\nTypeError when loading from GCP bucket<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7594 \r\nThis PR adds support for filtering specific columns when loading datasets from .json or .jsonl files \u2014 similar to how the columns=... argument works for Parquet.\r\n\r\nAs suggested, support for the `columns=...` argument (previously available for Parquet) has now been extended to **JSON and JSONL** loading via `load_dataset(...)`. You can now load only specific keys/columns and skip the rest \u2014 which should help in cases where some fields are unclean, inconsistent, or just unnecessary.\r\n\r\n### Example:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"json\", data_files=\"your_data.jsonl\", columns=[\"id\", \"title\"])\r\nprint(dataset[\"train\"].column_names)\r\n# Output: ['id', 'title']\r\n```\r\n\r\n### Summary of changes:\r\n\r\n* Added `columns: Optional[List[str]]` to `JsonConfig`\r\n* Updated `_generate_tables()` to filter selected columns\r\n* Forwarded `columns` argument from `load_dataset()` to the config\r\n* Added test for validation(should be fine!)\r\n\r\nLet me know if you'd like the same to be added for CSV or others as a follow-up \u2014 happy to help.<end_of_turn>\n<start_of_turn>model\nAdd columns support to JSON loader for selective key filtering<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport loading a dataset from a JSON file with an array of strings.\r\n\r\nSee: https://huggingface.co/datasets/CausalLM/Refined-Anime-Text/discussions/1<end_of_turn>\n<start_of_turn>model\nSupport JSON file with an array of strings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nRunning .map and filter functions with `num_procs` consecutively instantiates several multiprocessing pools iteratively. \r\n\r\nAs instantiating a Pool is very resource intensive it can be a bottleneck to performing iteratively filtering. \r\n\r\nMy ideas:\r\n1. There should be an option to declare `persistent_workers` similar to pytorch DataLoader. Downside would be that would be complex to determine the correct resource allocation and deallocation of the pool. i.e. the dataset can outlive the utility of the pool. \r\n2. Provide a pool as an argument. Downside would be the expertise required by the user. Upside, is that there is better resource management.\n\n### Motivation\n\nIs really slow to iteratively perform map and filter operations on a dataset.\n\n### Your contribution\n\nIf approved I could integrate it. I would need to know what method would be most suitable to implement from the two options above.<end_of_turn>\n<start_of_turn>model\nPersistent multi-process Pool<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI just added drop_duplicates=True to `.from_patterns`. I used a dict to deduplicate and preserve the order\r\n\r\nclose https://github.com/huggingface/datasets/issues/6259\r\nclose https://github.com/huggingface/datasets/issues/6272\r\n<end_of_turn>\n<start_of_turn>model\nDrop data_files duplicates<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently, there are only several Sign Language labels, I would like to propose adding all the Signed Languages as new labels which are described in this ISO standard: https://www.evertype.com/standards/iso639/sign-language.html\n\n### Motivation\n\nDatasets currently only have labels for several signed languages. There are more signed languages in the world. Furthermore, some signed languages that have a lot of online data cannot be found because of this reason (for instance, German Sign Language, and there is no German Sign Language label on huggingface datasets even though there are a lot of readily available sign language datasets exist for German Sign Language, which are used very frequently in Sign Language Processing papers, and models.)\n\n### Your contribution\n\nI can submit a PR for this as well, adding the ISO codes and languages to the labels in datasets.<end_of_turn>\n<start_of_turn>model\nSign Language Support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using the with_format() function on a dataset containing images, even if the image column is not part of the columns provided in the function, its type will be changed to bytes.\r\nHere is a minimal reproduction of the bug:\r\nhttps://colab.research.google.com/drive/1hyaOspgyhB41oiR1-tXE3k_gJCdJUQCf?usp=sharing\n\n### Steps to reproduce the bug\n\n1. Load the image dataset\r\n2. apply with_format(columns=[\"text\"])\r\n3. Check the type of images in the \"image\" column before and after applying with_format\n\n### Expected behavior\n\nThe type should stay the same, but it does not\n\n### Environment info\n\ndatasets==2.14.6\r\n<end_of_turn>\n<start_of_turn>model\nwith_format() function returns bytes instead of PIL images even when image column is not part of \"columns\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nNo TF in win tests<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm trying to run a rag example, and the dataset is wiki_dpr.\r\nwiki_dpr download and extracting have been completed successfully.\r\nHowever, at the generating train split stage, an error from wiki_dpr.py keeps popping up.\r\n\r\nEspecially in \"_generate_examples\" :\r\n1. The following error occurs in the line **id, text, title = line.strip().split(\"\\t\")**\r\nValueError: not enough values \u200b\u200bto unpack (expected 3, got 2)\r\n-> This part handles exceptions so that even if an error occurs, it passes.\r\n\r\n2. **ID mismatch between lines {id} and vector {vec_id}**\r\nThis error seems to occur at the line \" assert int(id) == int(vec_id),\".\r\nAfter I handled the exception in the split error, generating train split progressed to 80%, but an id mismatch error occurred at about the 16200000th vector id.\r\n\r\nDebugging is even more difficult because it takes a long time to download and split wiki_dpr. I need help. thank you in advance!!\n\n### Steps to reproduce the bug\n\nOccurs in the generating train split step when running the rag example in the transformers repository.\r\nSpecifically, it is an error in wiki_dpr.py.\n\n### Expected behavior\n\n.\n\n### Environment info\n\npython 3.8<end_of_turn>\n<start_of_turn>model\nwiki_dpr.py error (ID mismatch between lines {id} and vector {vec_id}<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe DVC integration seems to be broken. \nFollowed this guide: https://dvc.org/doc/user-guide/integrations/huggingface\n\n### Steps to reproduce the bug\n\n#### Script to reproduce\n~~~python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\n    \"csv\",\n    data_files=\"dvc://workshop/satellite-data/jan_train.csv\",\n    storage_options={\"url\": \"https://github.com/iterative/dataset-registry.git\"},\n)\n\nprint(dataset)\n~~~\n\n#### Error log\n~~~\nTraceback (most recent call last):\n  File \"C:\\tmp\\test\\load.py\", line 3, in <module>\n    dataset = load_dataset(\n              ^^^^^^^^^^^^^\n  File \"C:\\tmp\\test\\.venv\\Lib\\site-packages\\datasets\\load.py\", line 2151, in load_dataset\n    builder_instance.download_and_prepare(\n  File \"C:\\tmp\\test\\.venv\\Lib\\site-packages\\datasets\\builder.py\", line 808, in download_and_prepare\n    fs, output_dir = url_to_fs(output_dir, **(storage_options or {}))\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nTypeError: url_to_fs() got multiple values for argument 'url'\n~~~\n\n\n\n### Expected behavior\n\nIntegration would work and the indicated file is downloaded and opened.\n\n### Environment info\n\n#### Python version\n~~~\npython --version\nPython 3.11.10\n~~~\n\n#### Venv (pip install datasets dvc):\n~~~\nPackage                Version\n---------------------- -----------\naiohappyeyeballs       2.4.6\naiohttp                3.11.13\naiohttp-retry          2.9.1\naiosignal              1.3.2\namqp                   5.3.1\nannotated-types        0.7.0\nantlr4-python3-runtime 4.9.3\nappdirs                1.4.4\nasyncssh               2.20.0\natpublic               5.1\nattrs                  25.1.0\nbilliard               4.2.1\ncelery                 5.4.0\ncertifi                2025.1.31\ncffi                   1.17.1\ncharset-normalizer     3.4.1\nclick                  8.1.8\nclick-didyoumean       0.3.1\nclick-plugins          1.1.1\nclick-repl             0.3.0\ncolorama               0.4.6\nconfigobj              5.0.9\ncryptography           44.0.1\ndatasets               3.3.2\ndictdiffer             0.9.0\ndill                   0.3.8\ndiskcache              5.6.3\ndistro                 1.9.0\ndpath                  2.2.0\ndulwich                0.22.7\ndvc                    3.59.1\ndvc-data               3.16.9\ndvc-http               2.32.0\ndvc-objects            5.1.0\ndvc-render             1.0.2\ndvc-studio-client      0.21.0\ndvc-task               0.40.2\nentrypoints            0.4\nfilelock               3.17.0\nflatten-dict           0.4.2\nflufl-lock             8.1.0\nfrozenlist             1.5.0\nfsspec                 2024.12.0\nfuncy                  2.0\ngitdb                  4.0.12\ngitpython              3.1.44\ngrandalf               0.8\ngto                    1.7.2\nhuggingface-hub        0.29.1\nhydra-core             1.3.2\nidna                   3.10\niterative-telemetry    0.0.10\nkombu                  5.4.2\nmarkdown-it-py         3.0.0\nmdurl                  0.1.2\nmultidict              6.1.0\nmultiprocess           0.70.16\nnetworkx               3.4.2\nnumpy                  2.2.3\nomegaconf              2.3.0\norjson                 3.10.15\npackaging              24.2\npandas                 2.2.3\npathspec               0.12.1\nplatformdirs           4.3.6\nprompt-toolkit         3.0.50\npropcache              0.3.0\npsutil                 7.0.0\npyarrow                19.0.1\npycparser              2.22\npydantic               2.10.6\npydantic-core          2.27.2\npydot                  3.0.4\npygit2                 1.17.0\npygments               2.19.1\npygtrie                2.5.0\npyparsing              3.2.1\npython-dateutil        2.9.0.post0\npytz                   2025.1\npywin32                308\npyyaml                 6.0.2\nrequests               2.32.3\nrich                   13.9.4\nruamel-yaml            0.18.10\nruamel-yaml-clib       0.2.12\nscmrepo                3.3.10\nsemver                 3.0.4\nsetuptools             75.8.0\nshellingham            1.5.4\nshortuuid              1.0.13\nshtab                  1.7.1\nsix                    1.17.0\nsmmap                  5.0.2\nsqltrie                0.11.2\ntabulate               0.9.0\ntomlkit                0.13.2\ntqdm                   4.67.1\ntyper                  0.15.1\ntyping-extensions      4.12.2\ntzdata                 2025.1\nurllib3                2.3.0\nvine                   5.1.0\nvoluptuous             0.15.2\nwcwidth                0.2.13\nxxhash                 3.5.0\nyarl                   1.18.3\nzc-lockfile            3.0.post1\n~~~<end_of_turn>\n<start_of_turn>model\nDVC integration broken<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen we switched to `ruff` linter in PR:\r\n- #5519\r\n\r\nimport sorting was disabled in all files containing the `# flake8: noqa` directive\r\n- https://github.com/astral-sh/ruff/issues/11679\r\n\r\nWe should re-enable import sorting on those files.<end_of_turn>\n<start_of_turn>model\nImport sorting is disabled by flake8 noqa directive after switching to ruff linter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\n\r\nMinimal Example\r\n\r\n```\r\nimport torch\r\nfrom datasets import IterableDataset\r\n\r\nd = IterableDataset.from_file(<file_name>)\r\ndl = torch.utils.data.dataloader.DataLoader(d,num_workers=3)\r\n\r\nfor sample in dl:\r\n    print(sample)\r\n\r\n```\r\n\r\nWarning:\r\nToo many dataloader workers: 2 (max is dataset.n_shards=1). Stopping 1 dataloader workers.\r\nTo parallelize data loading, we give each process some shards (or data sources) to process. Therefore it's unnecessary to have a number of workers greater than dataset.n_shards=1. To enable more parallelism, please split the dataset in more files than 1.\r\n\r\nExpected Behavior:\r\nDataset is sharded each cpu uses subset (contiguously - so you can do checkpoint loading/saving) \n\n### Motivation\n\nI have a lot of unused cpu's and would like to be able to shard iterable datasets with pytorch's dataloader when num_workers > 1. This is for a very large single file. I am aware that we can use the `split_dataset_by_node` to ensure that each node (for distributed) gets different shards, but we should extend it so that this also continues for multiple workers. \n\n### Your contribution\n\nIf someone points me to what needs to change, I can create a PR.<end_of_turn>\n<start_of_turn>model\nAutoSharding IterableDataset's when num_workers > 1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`datasets` downloads and generates all splits, even though a single split is requested.  [This](https://huggingface.co/datasets/jordiae/exebench) is the dataset in question.  It uses a loading script.  I am not 100% sure that this is a bug, because maybe with loading scripts `datasets` must actually process all the splits?  But I thought loading scripts were designed to avoid this.\n\n### Steps to reproduce the bug\n\nSee [this notebook](https://colab.research.google.com/drive/14kcXp_hgcdj-kIzK0bCG6taE-CLZPVvq?usp=sharing)\n\nOr:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset('jordiae/exebench', split='test_synth', trust_remote_code=True)\n```\n\n### Expected behavior\n\nI expected only the `test_synth` split to be downloaded and processed.\n\n### Environment info\n\n- `datasets` version: 3.5.1\n- Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n- Python version: 3.11.12\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\ndatasets downloads and generates all splits, even though a single split is requested (for dataset with loading script)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n### My Code\r\n```\r\nfrom datasets import load_dataset\r\nres=[]\r\nfor i in [0,1]:\r\n    di=load_dataset(\r\n            \"json\", \r\n            data_files='path_to.json', \r\n            split='train',\r\n            streaming=True, \r\n        ).map(lambda x: {\"source\": i})\r\n\r\n    res.append(di)\r\n\r\nfor e in res[0]:\r\n    print(e)\r\n```\r\n\r\n### Unexpected Behavior\r\nData in `res[0]` has `source=1`. However the expected value is 0.\r\n\r\n### FYI\r\nI further switch `streaming` to `False`. And the output value is as expected (0). So there may exist bugs in setting `streaming=True` in a for loop.\r\n\r\n### Environment\r\nPython 3.8.0\r\ndatasets==2.18.0\r\ntransformers==4.28.1\n\n### Steps to reproduce the bug\n\n1. Create a Json file with any content.\r\n2. Run the provided code.\r\n3. Switch `streaming` to `False` and run again to see the expected behavior.\n\n### Expected behavior\n\nThe expected behavior is the data are mapped with its corresponding value in the for loop.\n\n### Environment info\n\nPython 3.8.0\r\n\r\ndatasets==2.18.0\r\ntransformers==4.28.1\r\n\r\nUbuntu 20.04<end_of_turn>\n<start_of_turn>model\nUnexpected behavior when using load_dataset with streaming=True in a for loop<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://huggingface.co/datasets/uonlp/CulturaX/discussions/15#6597e83f185db94370d6bf50 for reference<end_of_turn>\n<start_of_turn>model\nDocument YAML configuration with \"data_dir\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove beam, as part of the 3.0 release.<end_of_turn>\n<start_of_turn>model\nRemove beam<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI use the following code  to download natural_question dataset. Even though I have completely download it, the next time I run this code, the new download procedure will start and cover the original /data/lxy/NQ\r\n\r\nconfig=datasets.DownloadConfig(resume_download=True,max_retries=100,cache_dir=r'/data/lxy/NQ',download_desc='NQ')\r\n\r\ndata=datasets.load_dataset('natural_questions',cache_dir=r'/data/lxy/NQ',download_config=config,download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS)\r\n\r\n---\r\nSince I don't have apache_beam installed, it throw a exception. After I pip install apache_beam ,the download restart..\r\n\r\n![image](https://github.com/huggingface/datasets/assets/88258534/f28ce7fe-29ea-4348-b87f-e69182a8bd41)\r\n\n\n### Steps to reproduce the bug\n\nrun this two line code\r\n\r\nconfig=datasets.DownloadConfig(resume_download=True,max_retries=100,cache_dir=r'/data/lxy/NQ',download_desc='NQ')\r\n\r\ndata=datasets.load_dataset('natural_questions',cache_dir=r'/data/lxy/NQ',download_config=config,download_mode=DownloadMode.REUSE_DATASET_IF_EXISTS)\n\n### Expected behavior\n\nDownload behavior can be correctly follow DownloadMode\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-3.10.0-1160.88.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nREUSE_DATASET_IF_EXISTS don't work <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 4.2.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nBug get_metadata_patterns arg error https://github.com/huggingface/datasets/issues/6517<end_of_turn>\n<start_of_turn>model\nfix get_metadata_patterns function args error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI tried following ways, the cache can't be disabled. \r\n\r\nI got 2T data, but I also got more than 2T cache file. I got pressure on storage. I need to diable cache or cleaned immediately after processed. Following ways are all not working, please give some help!\r\n\r\n```python\r\nfrom datasets import disable_caching\r\nfrom transformers import AutoTokenizer\r\ndisable_caching()\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(args.tokenizer_path)\r\ndef tokenization_fn(examples):\r\n    column_name = 'text' if 'text' in examples else 'data'\r\n    tokenized_inputs = tokenizer(\r\n        examples[column_name], return_special_tokens_mask=True, truncation=False,\r\n        max_length=tokenizer.model_max_length\r\n    )\r\n    return tokenized_inputs\r\n\r\ndata = load_dataset('json', data_files=save_local_path, split='train', cache_dir=None)\r\ndata.cleanup_cache_files()\r\nupdated_dataset = data.map(tokenization_fn, load_from_cache_file=False)\r\nupdated_dataset .cleanup_cache_files()\r\n```\r\n\r\n### Expected behavior\r\n\r\nno cache file generated\r\n\r\n### Environment info\r\n\r\nUbuntu 20.04.6 LTS\r\ndatasets  3.0.2<end_of_turn>\n<start_of_turn>model\ncache can't cleaned or disabled<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I run the code en = load_dataset(\"allenai/c4\", \"en\", streaming=True), I encounter an error: raise ValueError(f\"Couldn't infer the same data file format for all splits. Got {split_modules}\") ValueError: Couldn't infer the same data file format for all splits. Got {'train': ('json', {}), 'validation': (None, {})}.\r\nHowever, running dataset = load_dataset('allenai/c4', streaming=True, data_files={'validation': 'en/c4-validation.00003-of-00008.json.gz'}, split='validation') works fine. What is the issue here?\n\n### Steps to reproduce the bug\n\nrun code\uff1a\r\nimport os\r\nos.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\r\nfrom datasets import load_dataset\r\n\r\nen = load_dataset(\"allenai/c4\", \"en\", streaming=True)\n\n### Expected behavior\n\nSuccessfully loaded the dataset.\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-6.5.0-28-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.19\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2024.2.0\r\n<end_of_turn>\n<start_of_turn>model\nValueError: Couldn't infer the same data file format for all splits. Got {'train': ('json', {}), 'validation': (None, {})}<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTemporarily pin fsspec < 2023.10.0 until permanent solution is found.\r\n\r\nHot fix #6330.\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/6610904287/job/17953774987\r\n```\r\n...\r\nERROR tests/test_iterable_dataset.py::test_iterable_dataset_from_file - NotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\r\n= 373 failed, 2055 passed, 17 skipped, 8 warnings, 6 errors in 228.14s (0:03:48) =\r\n```<end_of_turn>\n<start_of_turn>model\nTemporarily pin fsspec < 2023.10.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi, \n\nThe canonical `multi_news` dataset is currently broken and fails to load. This is because it points to the [alexfabri/multi_news](https://huggingface.co/datasets/alexfabbri/multi_news) repository, which contains a legacy loading script (`multi_news.py`) that requires the now-removed `trust_remote_code` parameter.\n\nThe original maintainer's GitHub and Hugging Face repositories appear to be inactive, so a community-led fix is needed.\n\nI have created a working fix by converting the dataset to the modern Parquet format, which does not require a loading script. The fixed version is available here and loads correctly:\n\n**[Awesome075/multi_news_parquet](https://huggingface.co/datasets/Awesome075/multi_news_parquet)**\n\nCould the maintainers please guide me or themselves update the official `multi_news` dataset to use this working Parquet version? This would involve updating the canonical pointer for \"multi_news\" to resolve to the new repository.\n\nThis action would fix the dataset for all users and ensure its continued availability.\n\nThank you!<end_of_turn>\n<start_of_turn>model\nFix: Canonical 'multi_news' dataset is broken and should be updated to a Parquet version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI get a RuntimeError from the following code:\r\n\r\n```python\r\n audio_dataset = Dataset.from_dict({\"audio\": [\"/kaggle/input/bengaliai-speech/train_mp3s/000005f3362c.mp3\"]}).cast_column(\"audio\", Audio())\r\n audio_dataset[0]\r\n ```\r\n \r\n ### Traceback\r\n\r\n<details>\r\n\r\n```python\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[33], line 1\r\n----> 1 train_dataset[0]\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1764, in Dataset.__getitem__(self, key)\r\n   1762 def __getitem__(self, key):  # noqa: F811\r\n   1763     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 1764     return self._getitem(\r\n   1765         key,\r\n   1766     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1749, in Dataset._getitem(self, key, decoded, **kwargs)\r\n   1747 formatter = get_formatter(format_type, features=self.features, decoded=decoded, **format_kwargs)\r\n   1748 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 1749 formatted_output = format_table(\r\n   1750     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   1751 )\r\n   1752 return formatted_output\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:532, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    530 python_formatter = PythonFormatter(features=None)\r\n    531 if format_columns is None:\r\n--> 532     return formatter(pa_table, query_type=query_type)\r\n    533 elif query_type == \"column\":\r\n    534     if key in format_columns:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:281, in Formatter.__call__(self, pa_table, query_type)\r\n    279 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    280     if query_type == \"row\":\r\n--> 281         return self.format_row(pa_table)\r\n    282     elif query_type == \"column\":\r\n    283         return self.format_column(pa_table)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:312, in PythonFormatter.format_row(self, pa_table)\r\n    310 row = self.python_arrow_extractor().extract_row(pa_table)\r\n    311 if self.decoded:\r\n--> 312     row = self.python_features_decoder.decode_row(row)\r\n    313 return row\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:221, in PythonFeaturesDecoder.decode_row(self, row)\r\n    220 def decode_row(self, row: dict) -> dict:\r\n--> 221     return self.features.decode_example(row) if self.features else row\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1386, in Features.decode_example(self, example)\r\n   1376 def decode_example(self, example: dict):\r\n   1377     \"\"\"Decode example with custom feature decoding.\r\n   1378 \r\n   1379     Args:\r\n   (...)\r\n   1383         :obj:`dict[str, Any]`\r\n   1384     \"\"\"\r\n-> 1386     return {\r\n   1387         column_name: decode_nested_example(feature, value)\r\n   1388         if self._column_requires_decoding[column_name]\r\n   1389         else value\r\n   1390         for column_name, (feature, value) in zip_dict(\r\n   1391             {key: value for key, value in self.items() if key in example}, example\r\n   1392         )\r\n   1393     }\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1387, in <dictcomp>(.0)\r\n   1376 def decode_example(self, example: dict):\r\n   1377     \"\"\"Decode example with custom feature decoding.\r\n   1378 \r\n   1379     Args:\r\n   (...)\r\n   1383         :obj:`dict[str, Any]`\r\n   1384     \"\"\"\r\n   1386     return {\r\n-> 1387         column_name: decode_nested_example(feature, value)\r\n   1388         if self._column_requires_decoding[column_name]\r\n   1389         else value\r\n   1390         for column_name, (feature, value) in zip_dict(\r\n   1391             {key: value for key, value in self.items() if key in example}, example\r\n   1392         )\r\n   1393     }\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/features/features.py:1087, in decode_nested_example(schema, obj)\r\n   1085 # Object with special decoding:\r\n   1086 elif isinstance(schema, (Audio, Image)):\r\n-> 1087     return schema.decode_example(obj) if obj is not None else None\r\n   1088 return obj\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/features/audio.py:103, in Audio.decode_example(self, value)\r\n    101     raise ValueError(f\"An audio sample should have one of 'path' or 'bytes' but both are None in {value}.\")\r\n    102 elif path is not None and path.endswith(\"mp3\"):\r\n--> 103     array, sampling_rate = self._decode_mp3(file if file else path)\r\n    104 elif path is not None and path.endswith(\"opus\"):\r\n    105     if file:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/features/audio.py:241, in Audio._decode_mp3(self, path_or_file)\r\n    238 except RuntimeError as err:\r\n    239     raise ImportError(\"To support decoding 'mp3' audio files, please install 'sox'.\") from err\r\n--> 241 array, sampling_rate = torchaudio.load(path_or_file, format=\"mp3\")\r\n    242 if self.sampling_rate and self.sampling_rate != sampling_rate:\r\n    243     if not hasattr(self, \"_resampler\") or self._resampler.orig_freq != sampling_rate:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torchaudio/backend/sox_io_backend.py:256, in load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n    254 if ret is not None:\r\n    255     return ret\r\n--> 256 return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torchaudio/backend/sox_io_backend.py:30, in _fail_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\r\n     22 def _fail_load(\r\n     23     filepath: str,\r\n     24     frame_offset: int = 0,\r\n   (...)\r\n     28     format: Optional[str] = None,\r\n     29 ) -> Tuple[torch.Tensor, int]:\r\n---> 30     raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\r\n\r\nRuntimeError: Failed to load audio from /kaggle/input/bengaliai-speech/train_mp3s/000005f3362c.mp3\r\n```\r\n\r\n</details>\r\n\n\n### Steps to reproduce the bug\n\n1. - Create a custom dataset using Local files of type mp3.\r\n3. - Try to read the first audio item.\n\n### Expected behavior\n\nExpected output\r\n```python\r\naudio_dataset[0][\"audio\"]\r\n{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,\r\n         0.        ,  0.        ], dtype=float32),\r\n 'path': 'path/to/audio_1',\r\n 'sampling_rate': 16000}\r\n ```\n\n### Environment info\n\nN/A<end_of_turn>\n<start_of_turn>model\nLoad local audio data doesn't work<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCan't seem to load the JourneyDB dataset.\r\n\r\nIt throws the following error:\r\n\r\n ```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nCell In[15], line 2\r\n      1 # If the dataset is gated/private, make sure you have run huggingface-cli login\r\n----> 2 dataset = load_dataset(\"JourneyDB/JourneyDB\", data_files=\"data\", use_auth_token=True)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:1664, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\r\n   1661 ignore_verifications = ignore_verifications or save_infos\r\n   1663 # Create a dataset builder\r\n-> 1664 builder_instance = load_dataset_builder(\r\n   1665     path=path,\r\n   1666     name=name,\r\n   1667     data_dir=data_dir,\r\n   1668     data_files=data_files,\r\n   1669     cache_dir=cache_dir,\r\n   1670     features=features,\r\n   1671     download_config=download_config,\r\n   1672     download_mode=download_mode,\r\n   1673     revision=revision,\r\n   1674     use_auth_token=use_auth_token,\r\n   1675     **config_kwargs,\r\n   1676 )\r\n   1678 # Return iterable dataset in case of streaming\r\n   1679 if streaming:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:1490, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, use_auth_token, **config_kwargs)\r\n   1488     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   1489     download_config.use_auth_token = use_auth_token\r\n-> 1490 dataset_module = dataset_module_factory(\r\n   1491     path,\r\n   1492     revision=revision,\r\n   1493     download_config=download_config,\r\n   1494     download_mode=download_mode,\r\n   1495     data_dir=data_dir,\r\n   1496     data_files=data_files,\r\n   1497 )\r\n   1499 # Get dataset builder class from the processing script\r\n   1500 builder_cls = import_main_class(dataset_module.module_path)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/load.py:1238, in dataset_module_factory(path, revision, download_config, download_mode, force_local_path, dynamic_modules_path, data_dir, data_files, **download_kwargs)\r\n   1236                 raise ConnectionError(f\"Couln't reach the Hugging Face Hub for dataset '{path}': {e1}\") from None\r\n   1237             if isinstance(e1, FileNotFoundError):\r\n-> 1238                 raise FileNotFoundError(\r\n   1239                     f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   1240                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   1241                 ) from None\r\n   1242             raise e1 from None\r\n   1243 else:\r\n\r\nFileNotFoundError: Couldn't find a dataset script at /kaggle/working/JourneyDB/JourneyDB/JourneyDB.py or any data file in the same directory. Couldn't find 'JourneyDB/JourneyDB' on the Hugging Face Hub either: FileNotFoundError: Unable to find data in dataset repository JourneyDB/JourneyDB with any supported extension ['csv', 'tsv', 'json', 'jsonl', 'parquet', 'txt', 'blp', 'bmp', 'dib', 'bufr', 'cur', 'pcx', 'dcx', 'dds', 'ps', 'eps', 'fit', 'fits', 'fli', 'flc', 'ftc', 'ftu', 'gbr', 'gif', 'grib', 'h5', 'hdf', 'png', 'apng', 'jp2', 'j2k', 'jpc', 'jpf', 'jpx', 'j2c', 'icns', 'ico', 'im', 'iim', 'tif', 'tiff', 'jfif', 'jpe', 'jpg', 'jpeg', 'mpg', 'mpeg', 'msp', 'pcd', 'pxr', 'pbm', 'pgm', 'ppm', 'pnm', 'psd', 'bw', 'rgb', 'rgba', 'sgi', 'ras', 'tga', 'icb', 'vda', 'vst', 'webp', 'wmf', 'emf', 'xbm', 'xpm', 'zip']\r\n\r\n```\n\n### Steps to reproduce the bug\n\n1) \r\n```\r\nfrom huggingface_hub import notebook_login\r\nnotebook_login()\r\n```\r\n\r\n2)\r\n```\r\n!pip install -q datasets\r\nfrom datasets import load_dataset\r\n```\r\n\r\n3)\r\n`dataset = load_dataset(\"JourneyDB/JourneyDB\", data_files=\"data\", use_auth_token=True)`\n\n### Expected behavior\n\nLoad the dataset\n\n### Environment info\n\nNotebook<end_of_turn>\n<start_of_turn>model\nCan't load a dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi I have the following directory structure:\r\n|--dataset\r\n| |-- images\r\n| |-- metadata1000.csv\r\n| |-- metadata1000.jsonl\r\n| |-- padded_images\r\n\r\nExample of metadata1000.jsonl file\r\n{\"caption\": \"a drawing depicts a full shot of a black t-shirt with a triangular pattern on the front there is a white label on the left side of the triangle\", \"image\": \"images/212734.png\", \"gaussian_padded_image\": \"padded_images/p_212734.png\"}\r\n{\"caption\": \"an eye-level full shot of a large elephant and a baby elephant standing in a watering hole on the left side is a small elephant with its head turned to the right of dry land, trees, and bushes\", \"image\": \"images/212735.png\", \"gaussian_padded_image\": \"padded_images/p_212735.png\"}\r\n.\r\n.\r\n.\r\n\r\nI'm trying to use dataset = load_dataset(\"imagefolder\", data_dir='/dataset/', split='train') to load the the dataset, however it is not able to load according to the fields in the metadata1000.jsonl .\r\nplease assist to load the data properly\r\n\r\nalso getting \r\n\r\n```\r\n  File \"/workspace/train_trans_vae.py\", line 1089, in <module>\r\n    print(get_metadata_patterns('/dataset/'))\r\n  File \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 499, in get_metadata_patterns\r\n    raise FileNotFoundError(f\"The directory at {base_path} doesn't contain any metadata file\") from None\r\nFileNotFoundError: The directory at /dataset/ doesn't contain any metadata file\r\n```\r\n\r\nwhen trying \r\n\r\n```\r\n    from datasets.data_files import get_metadata_patterns\r\n    print(get_metadata_patterns('/dataset/'))\r\n```\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\ndataset Version: 2.18.0\r\nmake a similar jsonl and similar directory format\n\n### Expected behavior\n\ncreates a dataset object with the column names, caption,image,gaussian_padded_image\n\n### Environment info\n\ndataset Version: 2.18.0<end_of_turn>\n<start_of_turn>model\n.Jsonl metadata not detected<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis warning message was shown every time you pass num_proc to `load_dataset` because of `map_nested`\r\n\r\n```\r\nparallel_map is experimental and might be subject to breaking changes in the future\r\n```\r\n\r\nThis PR removes it for `map_nested`. If someone uses another parallel backend they're already warned when `parallel_backend` is called anyway<end_of_turn>\n<start_of_turn>model\nIgnore parallel warning in map_nested<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`datasets.Features` seems to be unable to handle json file that contains fields of `list[dict]`.\n\n### Steps to reproduce the bug\n\n```json\n// test.json\n{\"a\": 1, \"b\": [{\"c\": 2, \"d\": 3}, {\"c\": 4, \"d\": 5}]}\n{\"a\": 5, \"b\": [{\"c\": 7, \"d\": 8}, {\"c\": 9, \"d\": 10}]}\n```\n\n```python\nimport json\nfrom datasets import Dataset, Features, Value, Sequence, load_dataset\n\nannotation_feature = Features({\n    \"a\": Value(\"int32\"),\n    \"b\": Sequence({\n        \"c\": Value(\"int32\"),\n        \"d\": Value(\"int32\"),\n    }),\n})\nannotation_dataset = load_dataset(\n    \"json\", \n    data_files=\"test.json\",\n    features=annotation_feature\n)\n```\n\n```\nArrowNotImplementedError: Unsupported cast from list<item: struct<c: int32, d: int32>> to struct using function cast_struct\n\nThe above exception was the direct cause of the following exception:\n\nDatasetGenerationError                    Traceback (most recent call last)\nCell In[46], line 11\n      2 from datasets import Dataset, Features, Value, Sequence, load_dataset\n      4 annotation_feature = Features({\n      5     \"a\": Value(\"int32\"),\n      6     \"b\": Sequence({\n   (...)      9     }),\n     10 })\n---> 11 annotation_dataset = load_dataset(\n     12     \"json\", \n     13     data_files=\"test.json\",\n     14     features=annotation_feature\n     15 )\n```\n\n### Expected behavior\n\nA `datasets.Datasets` instance should be initialized.\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: Linux-6.11.0-21-generic-x86_64-with-glibc2.39\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nNested Feature raises ArrowNotImplementedError: Unsupported cast using function cast_struct<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading a `.env` file after an `import datasets` call does not correctly use the environment variables. \n\nThis is due the fact that environment variables are read at import time:\nhttps://github.com/huggingface/datasets/blob/de062f0552a810c52077543c1169c38c1f0c53fc/src/datasets/config.py#L155C1-L155C80\n\n### Steps to reproduce the bug\n\n```bash\n# make tmp dir\nmkdir -p /tmp/debug-env\n# make .env file\necho HF_HOME=/tmp/debug-env/data > /tmp/debug-env/.env\n# first load dotenv, downloads to /tmp/debug-env/data\nuv run --with datasets,python-dotenv python3 -c \\\n'import dotenv; dotenv.load_dotenv(\"/tmp/debug-env/.env\"); import datasets; datasets.load_dataset(\"Anthropic/hh-rlhf\")' \n# first import datasets, downloads to `~/.cache/huggingface`\nuv run --with datasets,python-dotenv python3 -c \\\n'import datasets; import dotenv; dotenv.load_dotenv(\"/tmp/debug-env/.env\"); datasets.load_dataset(\"Anthropic/hh-rlhf\")' \n\n```\n\n### Expected behavior\n\nI expect that setting environment variables with something like this:\n\n```python3\nif __name__ == \"__main__\":\n    load_dotenv()\n    main()\n```\nworks correctly. \n\n### Environment info\n\n    \"datasets>=3.3.0\",\n<end_of_turn>\n<start_of_turn>model\nLazy loading of environment variables<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDocumentation link: https://huggingface.co/docs/datasets/en/package_reference/main_classes\n\nLink to method (docstring present): https://github.com/huggingface/datasets/blob/6f2502c5a026caa89839713f6f7c8b958e5e83eb/src/datasets/arrow_dataset.py#L1029\n\nThe docstring is present for the function, but seems missing from the official documentation for the `Dataset` class on HuggingFace.\n\nThe method in question:\n```python\n    @classmethod\n    def from_dict(\n        cls,\n        mapping: dict,\n        features: Optional[Features] = None,\n        info: Optional[DatasetInfo] = None,\n        split: Optional[NamedSplit] = None,\n    ) -> \"Dataset\":\n        \"\"\"\n        Convert `dict` to a `pyarrow.Table` to create a [`Dataset`].\n\n        Important: a dataset created with from_dict() lives in memory\n        and therefore doesn't have an associated cache directory.\n        This may change in the future, but in the meantime if you\n        want to reduce memory usage you should write it back on disk\n        and reload using e.g. save_to_disk / load_from_disk.\n\n        Args:\n            mapping (`Mapping`):\n                Mapping of strings to Arrays or Python lists.\n            features ([`Features`], *optional*):\n                Dataset features.\n            info (`DatasetInfo`, *optional*):\n                Dataset information, like description, citation, etc.\n            split (`NamedSplit`, *optional*):\n                Name of the dataset split.\n\n        Returns:\n            [`Dataset`]\n        \"\"\"\n```<end_of_turn>\n<start_of_turn>model\n[Docs] Missing documentation for `Dataset.from_dict`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently, the reading of the metadata file infers the file extension (.jsonl or .csv) from the passed filename. However, downloaded files from the Hub don't have file extension. For example:\r\n- the original file: `hf://datasets/__DUMMY_TRANSFORMERS_USER__/test-dataset-5916a4-16977085077831/metadata.jsonl`\r\n- corresponds to the downloaded path: `/tmp/pytest-of-username/pytest-46/cache/datasets/downloads/9f5374dbb470f711f6b89d66a5eec1f19cc96324b26bcbebe29138bda6cb20e6`, which does not have extension\r\n\r\nIn the case where the metadata file does not have an extension, the reader assumes it is a JSONL file, thus the reported error when trying to read a CSV file as a JSONL one: `ArrowInvalid: JSON parse error: Invalid value. in row 0`\r\n\r\nThis behavior was introduced by:\r\n- #4837\r\n\r\nThis PR extracts the metadata file extension from the original filename (instead of the downloaded one) and passes it as a parameter to the read_metadata function.\r\n\r\nFix #6315.<end_of_turn>\n<start_of_turn>model\nFix loading Hub datasets with CSV metadata file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen trying to import datasets, I get a pyarrow ValueError:\r\n\r\nTraceback (most recent call last):\r\n  File \"/Users/edward/test/test.py\", line 1, in <module>\r\n    import datasets\r\n  File \"/Users/edward/opt/anaconda3/envs/cs235/lib/python3.9/site-packages/datasets/__init__.py\", line 43, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"/Users/edward/opt/anaconda3/envs/cs235/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 65, in <module>\r\n    from .arrow_reader import ArrowReader\r\n  File \"/Users/edward/opt/anaconda3/envs/cs235/lib/python3.9/site-packages/datasets/arrow_reader.py\", line 28, in <module>\r\n    import pyarrow.parquet as pq\r\n  File \"/Users/edward/opt/anaconda3/envs/cs235/lib/python3.9/site-packages/pyarrow/parquet/__init__.py\", line 20, in <module>\r\n    from .core import *\r\n  File \"/Users/edward/opt/anaconda3/envs/cs235/lib/python3.9/site-packages/pyarrow/parquet/core.py\", line 45, in <module>\r\n    from pyarrow.fs import (LocalFileSystem, FileSystem, FileType,\r\n  File \"/Users/edward/opt/anaconda3/envs/cs235/lib/python3.9/site-packages/pyarrow/fs.py\", line 49, in <module>\r\n    from pyarrow._gcsfs import GcsFileSystem  # noqa\r\n  File \"pyarrow/_gcsfs.pyx\", line 1, in init pyarrow._gcsfs\r\nValueError: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject\r\n\r\n### Steps to reproduce the bug\r\n\r\n`import datasets`\r\n\r\n### Expected behavior\r\n\r\nSuccessful import\r\n\r\n### Environment info\r\n\r\nConda environment, MacOS\r\npython 3.9.12\r\ndatasets 2.12.0\r\n\r\n<end_of_turn>\n<start_of_turn>model\nCannot import datasets - ValueError: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix issues with the deprecation of `use_auth_token`  introduced by:\r\n- #5996\r\n\r\nin functions:\r\n- `get_authentication_headers_for_url`\r\n- `request_etag`\r\n- `get_from_cache`\r\n\r\nCurrently, `TypeError` is raised: https://github.com/huggingface/datasets-server/actions/runs/5711650666/job/15484685570?pr=1588\r\n```\r\nFAILED tests/job_runners/config/test_parquet_and_info.py::test__is_too_big_external_files[None-None-False] - TypeError: get_authentication_headers_for_url() got an unexpected keyword argument 'use_auth_token'\r\n\r\nFAILED tests/job_runners/config/test_parquet_and_info.py::test_fill_builder_info[None-False] - libcommon.exceptions.FileSystemError: Could not read the parquet files: get_authentication_headers_for_url() got an unexpected keyword argument 'use_auth_token'\r\n```\r\n\r\nRelated to:\r\n- #6094<end_of_turn>\n<start_of_turn>model\nFix deprecation of use_auth_token in file_utils<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading some jsonl from redpajama-data-1T github source [togethercomputer/RedPajama-Data-1T](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T) fails due to one row of the file containing an extra field called **symlink_target: string>**. \r\nWhen deleting that line the loading is successful. \r\n\r\nWe also tried loading this file with the discrepancy using this function and it is successful\r\n```python\r\nos.environ[\"RED_PAJAMA_DATA_DIR\"] =\"/path_to_local_copy_of_RedPajama-Data-1T\"\r\nds = load_dataset('togethercomputer/RedPajama-Data-1T', 'github',cache_dir=\"/path_to_folder_with_jsonl\",streaming=True)['train']\r\n```\n\n### Steps to reproduce the bug\n\nSteps to reproduce the behavior:\r\n1. Load one jsonl from the redpajama-data-1T\r\n```bash \r\nwget https://data.together.xyz/redpajama-data-1T/v1.0.0/github/filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl\r\n```\r\n\r\n2.Load dataset will give error:\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('json', data_files='/path_to/filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl')\r\n```\r\n\r\n_TypeError: Couldn't cast array of type\r\nStruct\r\n<content_hash: string, \r\ntimestamp: string, \r\nsource: string, \r\nline_count: int64, \r\nmax_line_length: int64,\r\navg_line_length: double, \r\nalnum_prop: double, \r\nrepo_name: string, \r\nid: string, \r\nsize: string, \r\nbinary: bool, \r\ncopies: string, \r\nref: string, \r\npath: string, \r\nmode: string, \r\nlicense: string, \r\nlanguage: list<item: struct<name: string, bytes: string>>, **symlink_target: string>**\r\nto\r\n{'content_hash': Value(dtype='string', id=None), \r\n'timestamp': Value(dtype='string', id=None), \r\n'source': Value(dtype='string', id=None), \r\n'line_count': Value(dtype='int64', id=None), \r\n'max_line_length': Value(dtype='int64', id=None), \r\n'avg_line_length': Value(dtype='float64', id=None), \r\n'alnum_prop': Value(dtype='float64', id=None), \r\n'repo_name': Value(dtype='string', id=None), \r\n'id': Value(dtype='string', id=None), \r\n'size': Value(dtype='string', id=None), \r\n'binary': Value(dtype='bool', id=None), \r\n'copies': Value(dtype='string', id=None), \r\n'ref': Value(dtype='string', id=None), \r\n'path': Value(dtype='string', id=None), \r\n'mode': Value(dtype='string', id=None), \r\n'license': Value(dtype='string', id=None), \r\n'language': [{'name': Value(dtype='string', id=None), 'bytes': Value(dtype='string', id=None)}]}_\r\n\r\n3. To remove the line causing the problem that includes the **symlink_target: string>** do:\r\n```bash \r\nsed -i '112252d' filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl\r\n```\r\n4. Rerun the loading function now is succesful:\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset('json', data_files='/path_to/filtered_27f05c041a1c401783f90b9415e40e4b.sampled.jsonl')\r\n```\n\n### Expected behavior\n\nHave a clean dataset without discrepancies on the jsonl fields or have the load_dataset('json',...) method not error out. \r\n\n\n### Environment info\n\n- `datasets` version: 2.14.1\r\n- Platform: Linux-4.18.0-425.13.1.el8_7.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nload_dataset('json',...) from togethercomputer/RedPajama-Data-1T errors when jsonl rows contains different data fields<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen a user access requests are enabled, and new requests are set to Manual Review, the dataset owner should be notified of the pending requests. However, instead, currently nothing happens, and so the dataset request can go unanswered for quite some time until the owner happens to check that particular dataset's Settings pane.\n\n### Steps to reproduce the bug\n\n1. Enable a dataset's user access requests\r\n2. Set to Manual Review\r\n3. Ask another HF user to request access to the dataset\r\n4. Dataset owner is not notified\n\n### Expected behavior\n\nThe dataset owner should receive some kind of notification, perhaps in their HF site inbox, or by email, when a dataset access request is made and manual review is enabled.\n\n### Environment info\n\nn/a<end_of_turn>\n<start_of_turn>model\nUser access requests with manual review do not notify the dataset owner<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi,\nThanks for your wonderful work! I'm wondering is there a way to dynamically adjust the sampling weight of each data in the dataset during training? Looking forward to your reply, thanks again.<end_of_turn>\n<start_of_turn>model\nDynamic adjusting dataloader sampling weight<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI notice that when loading data instances with feature type of python dictionary, the dictionary keys would be broadcast so that every instance has the same set of keys. Please see an example in the Motivation section.\r\n\r\nIt is possible to avoid this behavior, i.e., load dictionary features as it is and do not broadcast the keys among instances? Please note that these dictionaries would have to be processed dynamically at each training iteration into strings (and tokenized).\n\n### Motivation\n\nI am trying to load a dataset from a json file. Each instance of the dataset has a feature that is a dictionary but its keys depend on the instance. Every two instances may have different keys. For example, imagine a dataset that contains a set of math expressions from a bunch of mutually redundant expressions:\r\n```\r\n{\r\n    \"index\": 0,\r\n    \"feature\": {\r\n        \"2 * x + y >= 3\": [\"2 * x + y >= 3\", \"4 * x + 2 * y >= 6\"],\r\n        ...\r\n    }\r\n},\r\n...\r\n{\r\n    \"index\": 9999,\r\n    \"feature\": {\r\n        \"x >= 6\": [\"x >= 6\", \"x >= 0\", \"x >= -1\"],\r\n        ...\r\n    }\r\n},\r\n...\r\n```\r\nWhen directly loading the dataset using `data = load_dataset(\"json\", data_files=file_paths, split='train')`, each instance would have all the keys from other instances and None as values. That is, instance of index 0 becomes:\r\n```\r\n{\r\n    \"index\": 0,\r\n    \"feature\": {\r\n        \"2 * x + y >= 3\": [\"2 * x + y >= 3\", \"4 * x + 2 * y >= 6\"],\r\n        ...\r\n        \"x >= 6\": None,  # keys from other instances\r\n        ...\r\n    }\r\n},\r\n```\r\nThis is not desirable. Moreover, issue would be raised if I attempt to combine two such datasets using `data = concatenate_datasets(multi_datasets)`, perhaps because their dictionary features contain different keys.\r\n\r\nA solution I can think of is to store the dictionary features as a long string, and evaluate it later. Please kindly suggest any other solution using existing methods of datasets.\n\n### Your contribution\n\nN/A<end_of_turn>\n<start_of_turn>model\nSupport for data with instance-wise dictionary as features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.21.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix Args section of feature docstrings.\r\n\r\nCurrently, some args do not appear in the docs because they are not properly parsed due to the lack of their type (between parentheses).<end_of_turn>\n<start_of_turn>model\nFix args of feature docstrings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\na fix for #7223 in case datasets is happy to support this kind of extensibility! seems cool / powerful for allowing sharing of datasets with potentially different feature types<end_of_turn>\n<start_of_turn>model\nfallback to default feature casting in case custom features not available during dataset loading<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n              There are two ways an iterable dataset can be split by node:\r\n1. if the number of shards is a factor of number of GPUs: in that case the shards are evenly distributed per GPU\r\n2. otherwise, each GPU iterate on the data and at the end keeps 1 sample out of n(GPUs) - skipping the others.\r\n\r\nIn case 2. it's therefore possible to have the same examples passed to `prepare_dataset` for each GPU.\r\n\r\nThis doesn't sound optimized though, because it runs the preprocessing on samples that won't be used in the end.\r\n\r\nCould you open a new issue so that we can discuss about this and find a solution ?\r\n\r\n_Originally posted by @lhoestq in https://github.com/huggingface/datasets/issues/5360#issuecomment-1592729051_\r\n            <end_of_turn>\n<start_of_turn>model\nIterableDataset: split by node and map may preprocess samples that will be skipped anyway<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix `NonMatchingSplitsSizesError` or `ExpectedMoreSplits` error for no-code Hub datasets if the user passes:\r\n- `data_dir`\r\n- `data_files`\r\n\r\nThe proposed solution is to avoid using exported dataset info (from Parquet exports) in these cases.\r\nAdditionally, also if the user passes `revision` other than \"main\" (so that no network requests are made).\r\n\r\nThis PR fixes a bug introduced by:\r\n- #6714\r\n\r\nFix #6918, fix #6939.<end_of_turn>\n<start_of_turn>model\nFix NonMatchingSplitsSizesError/ExpectedMoreSplits when passing data_dir/data_files in no-code Hub datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix type annotation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn Python 3.14, there's a change in the signature of `_Pickler._batch_setitems`.\r\n\r\nIt's changed to:\r\n\r\n```python\r\n# pickle.py\r\n\r\ndef _batch_setitems(self, items, obj):\r\n    # Helper to batch up SETITEMS sequences; proto >= 1 only\r\n    save = self.save\r\n    write = self.write\r\n```\r\n\r\nTo accomodate this, in `dill`, we have this compatibility code:\r\n\r\n```python\r\nif sys.hexversion < 0x30E00A1:\r\n    pickler._batch_setitems(iter(source.items()))\r\nelse:\r\n    pickler._batch_setitems(iter(source.items()), obj=obj)\r\n```\r\n\r\nThus, the datasets package will emit this error\r\n\r\n```\r\n\u2502 /Users/sghuang/mamba/envs/ds/lib/python3.14/site-packages/dill/_dill.py:1262 in save_module_dict \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1259 \u2502   \u2502   if is_dill(pickler, child=False) and pickler._session:                            \u2502\r\n\u2502   1260 \u2502   \u2502   \u2502   # we only care about session the first pass thru                              \u2502\r\n\u2502   1261 \u2502   \u2502   \u2502   pickler._first_pass = False                                                   \u2502\r\n\u2502 \u2771 1262 \u2502   \u2502   StockPickler.save_dict(pickler, obj)                                              \u2502\r\n\u2502   1263 \u2502   \u2502   logger.trace(pickler, \"# D2\")                                                     \u2502\r\n\u2502   1264 \u2502   return                                                                                \u2502\r\n\u2502   1265                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /Users/sghuang/mamba/envs/ds/lib/python3.14/pickle.py:1133 in save_dict                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1130 \u2502   \u2502   print(f\"Line number: {inspect.getsourcelines(method)[1]}\")                        \u2502\r\n\u2502   1131 \u2502   \u2502   print(f\"Full path: {inspect.getmodule(method)}\")                                  \u2502\r\n\u2502   1132 \u2502   \u2502   print(f\"Class: {method.__qualname__}\")                                            \u2502\r\n\u2502 \u2771 1133 \u2502   \u2502   self._batch_setitems(obj.items(), obj)                                            \u2502\r\n\u2502   1134 \u2502                                                                                         \u2502\r\n\u2502   1135 \u2502   dispatch[dict] = save_dict                                                            \u2502\r\n\u2502   1136                                                                                           \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\r\n[NOTE] when serializing datasets.table.InMemoryTable state\r\n[NOTE] when serializing datasets.table.InMemoryTable object\r\n```\r\n\r\nTo fix it, we update the signature of the `_batch_setitems` method defined in `utils/_dill.py`.\r\n\r\nThis fix should be backward compatible, since the compatibility is handled by `dill`.\r\n\r\nThis should close #7813.\r\n\r\nSimilar to https://github.com/joblib/joblib/issues/1658.\r\n\r\nRelated to https://github.com/uqfoundation/dill/pull/724.<end_of_turn>\n<start_of_turn>model\nfix: better args passthrough for `_batch_setitems()`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport `huggingface-hub` pre-releases.\r\n\r\nThis way we will have our CI green when testing `huggingface-hub` release candidates. See: https://github.com/huggingface/datasets/tree/ci-test-huggingface-hub-v0.20.0.rc1\r\n\r\nClose #6513.<end_of_turn>\n<start_of_turn>model\nSupport huggingface-hub pre-releases<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nPraveen_repo_pull_req<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf you:\r\n\r\n1. Interleave two iterable datasets together with the interleave_datasets function, or shuffle an iterable dataset\r\n2. Set the output format to torch tensors with .with_format('torch')\r\n\r\nThen iterating through the dataset becomes over 100x slower than it is if you don't apply the torch formatting.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\nimport torch\r\nfrom tqdm import tqdm\r\n\r\nrand_a = torch.randn(3,224,224)\r\nrand_b = torch.randn(3,224,224)\r\n\r\na = torch.stack([rand_a] * 1000)\r\nb = torch.stack([rand_b] * 1000)\r\n\r\nfeatures = datasets.Features({\"tensor\": datasets.Array3D(shape=(3,224,224), dtype=\"float32\")})\r\nds_a = datasets.Dataset.from_dict({\"tensor\": a}, features=features).to_iterable_dataset()\r\nds_b = datasets.Dataset.from_dict({\"tensor\": b}, features=features).to_iterable_dataset()\r\n\r\n# Iterating through either dataset with torch formatting is really fast (2000it/s on my machine)\r\nfor example in tqdm(ds_a.with_format('torch')):\r\n    pass\r\n\r\n# Iterating through either dataset shuffled is also pretty fast (100it/s on my machine)\r\nfor example in tqdm(ds_a.shuffle()):\r\n    pass\r\n\r\n# Iterating through this interleaved dataset is pretty fast (200it/s on my machine)\r\nds_fast = datasets.interleave_datasets([ds_a, ds_b])\r\nfor example in tqdm(ds_fast):\r\n    pass\r\n\r\n# Iterating through either dataset with torch formatting *after shuffling* is really slow... (<2it/s on my machine)\r\nfor example in tqdm(ds_a.shuffle().with_format('torch')):\r\n    pass\r\n\r\n# Iterating through this torch formatted interleaved dataset is also really slow (<2it/s on my machine)...\r\nds_slow = datasets.interleave_datasets([ds_a, ds_b]).with_format('torch')\r\nfor example in tqdm(ds_slow):\r\n    pass\r\n\r\n# Even doing this is way faster!! (70it/s on my machine)\r\nfor example in tqdm(ds_fast):\r\n    test = torch.tensor(example['tensor'])\r\n\r\n```\n\n### Expected behavior\n\nApplying torch formatting to the interleaved dataset shouldn't increase the time taken to iterate through the dataset by very much, since even explicitly converting every example is over 70x faster than calling .with_format('torch').\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-6.5.0-15-generic-x86_64-with-glibc2.38\r\n- Python version: 3.11.6\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\n'with_format' is extremely slow when used together with 'interleave_datasets' or 'shuffle' on IterableDatasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello,\n\nI'm trying to follow the [Hugging Face Pipelines tutorial](https://huggingface.co/docs/transformers/main_classes/pipelines) but the tutorial seems to work only on old datasets versions.\n\nI then get the error :\n\n```\n--------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[65], [line 1](vscode-notebook-cell:?execution_count=65&line=1)\n----> [1](vscode-notebook-cell:?execution_count=65&line=1) dataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n      3 # KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n      4 # as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\n      5 for out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n\nFile ~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:1392, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\n   1387 verification_mode = VerificationMode(\n   1388     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\n   1389 )\n   1391 # Create a dataset builder\n-> [1392](https://file+.vscode-resource.vscode-cdn.net/home/edwin/Desktop/debug/llm_course/~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:1392) builder_instance = load_dataset_builder(\n   1393     path=path,\n   1394     name=name,\n   1395     data_dir=data_dir,\n   1396     data_files=data_files,\n   1397     cache_dir=cache_dir,\n   1398     features=features,\n   1399     download_config=download_config,\n   1400     download_mode=download_mode,\n   1401     revision=revision,\n   1402     token=token,\n   1403     storage_options=storage_options,\n   1404     **config_kwargs,\n   1405 )\n   1407 # Return iterable dataset in case of streaming\n   1408 if streaming:\n\nFile ~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:1132, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\n   1130 if features is not None:\n   1131     features = _fix_for_backward_compatible_features(features)\n-> [1132](https://file+.vscode-resource.vscode-cdn.net/home/edwin/Desktop/debug/llm_course/~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:1132) dataset_module = dataset_module_factory(\n   1133     path,\n   1134     revision=revision,\n   1135     download_config=download_config,\n   1136     download_mode=download_mode,\n   1137     data_dir=data_dir,\n   1138     data_files=data_files,\n   1139     cache_dir=cache_dir,\n   1140 )\n   1141 # Get dataset builder class\n   1142 builder_kwargs = dataset_module.builder_kwargs\n\nFile ~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:1031, in dataset_module_factory(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\n   1026             if isinstance(e1, FileNotFoundError):\n   1027                 raise FileNotFoundError(\n   1028                     f\"Couldn't find any data file at {relative_to_absolute_path(path)}. \"\n   1029                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\n   1030                 ) from None\n-> [1031](https://file+.vscode-resource.vscode-cdn.net/home/edwin/Desktop/debug/llm_course/~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:1031)             raise e1 from None\n   1032 else:\n   1033     raise FileNotFoundError(f\"Couldn't find any data file at {relative_to_absolute_path(path)}.\")\n\nFile ~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:989, in dataset_module_factory(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\n    981 try:\n    982     api.hf_hub_download(\n    983         repo_id=path,\n    984         filename=filename,\n   (...)    987         proxies=download_config.proxies,\n    988     )\n--> [989](https://file+.vscode-resource.vscode-cdn.net/home/edwin/Desktop/debug/llm_course/~/Desktop/debug/llm_course/.venv/lib/python3.11/site-packages/datasets/load.py:989)     raise RuntimeError(f\"Dataset scripts are no longer supported, but found {filename}\")\n    990 except EntryNotFoundError:\n    991     # Use the infos from the parquet export except in some cases:\n    992     if data_dir or data_files or (revision and revision != \"main\"):\n\nRuntimeError: Dataset scripts are no longer supported, but found superb.py\n```\n\nNB : I tried to replace \"superb\" by  \"anton-l/superb_demo\" but I get a 'torchcodec' importing error. Maybe I misunderstood something.\n\n### Steps to reproduce the bug\n\n```\nimport datasets\nfrom transformers import pipeline\nfrom transformers.pipelines.pt_utils import KeyDataset\nfrom tqdm.auto import tqdm\n\npipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\ndataset = datasets.load_dataset(\"superb\", name=\"asr\", split=\"test\")\n\n# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\nfor out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n    print(out)\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n    # {\"text\": ....}\n    # ....\n```\n\n### Expected behavior\n\nGet the tutorial expected results\n\n### Environment info\n\n--- SYSTEM INFO ---\nOperating System: Ubuntu 24.10                              \nKernel: Linux 6.11.0-29-generic\nArchitecture: x86-64\n\n--- PYTHON ---\nPython 3.11.13\n\n--- VENV INFO ----\ndatasets=4.0.0\ntransformers=4.53\ntqdm=4.67.1<end_of_turn>\n<start_of_turn>model\nDataset scripts are no longer supported, but found superb.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes the root cause of the issue:\r\n- #6197\r\n\r\nThis PR also removes the temporary pin of `pandas` introduced by:\r\n- #6200\r\n\r\n\r\nNote that for orient in ['records', 'values'], index value is ignored but\r\n- in `pandas` < 2.1.0, a ValueError is raised if not index and orient not in ['split', 'table']\r\n  - for orient = 'records', we need index = True\r\n  - default index value is True\r\n- in `pandas` = 2.1.0, a ValueError is raised if index is True and orient in ['records', 'values']\r\n  - for orient = 'records', we need index = False or None\r\n  - default index value is None\r\n\r\nThis PR fixes the issue by not passing index and thus using default index value (valid for all pandas versions), unless orient is 'split' or 'table' (where we pass index = False, as it was done before this fix).<end_of_turn>\n<start_of_turn>model\nFix to_json ValueError and remove pandas pin<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nafter discussions in https://github.com/huggingface/datasets/pull/6415<end_of_turn>\n<start_of_turn>model\nMulti gpu docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm working on a dataset which comprises other datasets on the hub.\r\nURL: https://huggingface.co/datasets/open-asr-leaderboard/datasets-test-only\r\n\r\nNote: Some of the sub-datasets in this metadataset require explicit access.\r\n\r\nAll the other datasets work fine, except, `common_voice`.\n\n### Steps to reproduce the bug\n\nhttps://github.com/Vaibhavs10/scratchpad/blob/main/cv_datasets_bug_repro.ipynb\n\n### Expected behavior\n\nIt should work if the provided access token is valid (as it does for all the other datasets)\n\n### Environment info\n\ndatasets version -> 2.14.4<end_of_turn>\n<start_of_turn>model\n`Invalid user token` even when correct user token is passed!<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi all,\r\n\r\nI have been running into a pretty persistent issue recently when trying to load datasets. \r\n```python\r\n    train_dataset = load_dataset(\r\n        'llama-2-7b-tokenized', \r\n        split = 'train'\r\n    )\r\n```\r\nI receive a KeyError which crashes the runs.\r\n```\r\nTraceback (most recent call last):\r\n    main()\r\n\r\n    train_dataset = load_dataset(\r\n                    ^^^^^^^^^^^^^\r\n    builder_instance = load_dataset_builder(\r\n                       ^^^^^^^^^^^^^^^^^^^^^\r\n    dataset_module = dataset_module_factory(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^\r\n    raise e1 from None\r\n\r\n    ).get_module()\r\n      ^^^^^^^^^^^^\r\n    else get_data_patterns(base_path, download_config=self.download_config)\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    return _get_data_files_patterns(resolver)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    data_files = pattern_resolver(pattern)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    fs, _, _ = get_fs_token_paths(pattern, storage_options=storage_options)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    paths = [f for f in sorted(fs.glob(paths)) if not fs.isdir(f)]\r\n                               ^^^^^^^^^^^^^^\r\n\r\n    allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    for _, dirs, files in self.walk(path, maxdepth, detail=True, **kwargs):\r\n\r\n    listing = self.ls(path, detail=True, **kwargs)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    \"last_modified\": parse_datetime(tree_item[\"lastCommit\"][\"date\"]),\r\n                                    ~~~~~~~~~^^^^^^^^^^^^^^\r\nKeyError: 'lastCommit'\r\n```\r\nAny help would be greatly appreciated.\r\n\r\nThank you,\r\n\r\nEnrico\n\n### Steps to reproduce the bug\n\nLoad the dataset from the Huggingface hub.\r\n```python\r\n    train_dataset = load_dataset(\r\n        'llama-2-7b-tokenized', \r\n        split = 'train'\r\n    )\r\n```\n\n### Expected behavior\n\nLoads the dataset.\n\n### Environment info\n\ndatasets-2.14.3\r\nCUDA 11.8\r\nPython 3.11<end_of_turn>\n<start_of_turn>model\nDatasets crashing runs due to KeyError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nChange dill version to >=0.3.9,<0.4.5 and check for errors<end_of_turn>\n<start_of_turn>model\nChange dill version in requirements<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nAssuming there is a dataset with 52000 sentences, each with a length of 500, it takes 20 seconds to extract a sentence from the dataset\u2026\u2026\uff1fIs there something wrong with my iteration?\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport datasets\r\nimport time\r\nimport random\r\n\r\nnum_rows = 52000\r\nnum_cols = 500\r\n\r\nrandom_input = [[random.randint(1, 100) for _ in range(num_cols)] for _ in range(num_rows)]\r\nrandom_output = [[random.randint(1, 100) for _ in range(num_cols)] for _ in range(num_rows)]\r\n\r\n\r\ns=time.time()\r\nd={'random_input':random_input,'random_output':random_output}\r\ndataset=datasets.Dataset.from_dict(d)\r\nprint('from dict',time.time()-s)\r\nprint(dataset)\r\n\r\n\r\nfor i in range(len(dataset)):\r\n    aa=time.time()\r\n    a,b=dataset['random_input'][i],dataset['random_output'][i]\r\n    print(time.time()-aa)\r\n\r\n```\r\n\r\ncorresponding output\r\n```bash\r\nfrom dict 9.215498685836792\r\nDataset({\r\n    features: ['random_input', 'random_output'],\r\n    num_rows: 52000\r\n})\r\n19.129778146743774\r\n19.329464197158813\r\n19.27668261528015\r\n19.28557538986206\r\n19.247620582580566\r\n19.624247074127197\r\n19.28673791885376\r\n19.301053047180176\r\n19.290496110916138\r\n19.291821718215942\r\n19.357765197753906\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nUnder normal circumstances, iteration should be very rapid as it does not involve the main tasks other than getting items\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.19.0\r\n- Platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.21.4\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nUnimaginable super slow iteration<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ndataset: https://huggingface.co/datasets/NortheasternUniversity/big_patent\n\nWhen I try to load it with the datasets library, it fails with:\n\nRuntimeError: Dataset scripts are no longer supported, but found big_patent.py\n\n\nCould you please publish a Parquet/Arrow export of BIGPATENT on the Hugging Face so that it can be accessed with datasets>=4.x.\n\n<end_of_turn>\n<start_of_turn>model\nBIGPATENT dataset inaccessible (deprecated script loader)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ngoogle colab ex<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsupports #7804\r\nAdd support for the dicom file format. \r\n\r\nThis PR follows PR #7815 and PR #7325 closely.\r\nRemarkable differences:\r\nI made sure that we can load all of pydicom's test data, and encountered the `force=True` parameter that we explicitly support here. This allows to trying to load corrupted dicom files, we explicitly test this!\r\n\r\nThere is one dataset with all of dicom's test data on huggingface which can be loaded using this branch with the following script:\r\n```python\r\nfrom datasets import load_dataset\r\nfrom datasets import Features, ClassLabel\r\nfrom datasets.features import Dicom\r\n\r\nfeatures = Features({\r\n    \"dicom\": Dicom(force=True),  # necessary to be able to load one corrupted file\r\n    \"label\": ClassLabel(num_classes=2)\r\n})\r\n\r\nds = load_dataset(\"TobiasPitters/dicom-sample-dataset\",\r\n                  features=features)\r\n\r\nerror_count = 0\r\n\r\nfor idx, item in enumerate(ds[\"test\"]):\r\n    dicom = item[\"dicom\"]\r\n\r\n    try:\r\n        print(f\"Type: {type(dicom)}\")\r\n        if hasattr(dicom, 'PatientID'):\r\n            print(f\"PatientID: {dicom.PatientID}\")\r\n        if hasattr(dicom, 'StudyInstanceUID'):\r\n            print(f\"StudyInstanceUID: {dicom.StudyInstanceUID}\")\r\n        if hasattr(dicom, 'Modality'):\r\n            print(f\"Modality: {dicom.Modality}\")\r\n    except Exception as e:\r\n        error_count += 1\r\n        print(e)\r\n\r\nprint(f\"Finished processing with {error_count} errors.\")\r\n```\r\n\r\ntodo: \r\n- [ ] add docs (will do so soon)<end_of_turn>\n<start_of_turn>model\nAdd DICOM support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading dataset, the info specified by `data_files` did not overwrite the original info.\n\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import load_dataset\n\ntraindata = load_dataset(\n            \"allenai/c4\",\n            \"en\",\n            data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\", \n                        \"validation\": \"en/c4-validation.00000-of-00008.json.gz\"},\n        )\n```\n\n\n```log\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=828589180707, num_examples=364868892, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=809262831, num_examples=356317, shard_lengths=[223006, 133311], dataset_name='c4')}, {'expected': SplitInfo(name='validation', num_bytes=825767266, num_examples=364608, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='validation', num_bytes=102199431, num_examples=45576, shard_lengths=None, dataset_name='c4')}]\n```\n\n```python\nfrom datasets import load_dataset\n\ntraindata = load_dataset(\n            \"allenai/c4\",\n            \"en\",\n            data_files={\"train\": \"en/c4-train.00000-of-01024.json.gz\"},\n            split=\"train\"\n        )\n```\n\n```log\nExpectedMoreSplitsError: {'validation'}\n```\n\n### Expected behavior\n\nNo error\n\n### Environment info\n\ndatasets 4.0.0<end_of_turn>\n<start_of_turn>model\nNonMatchingSplitsSizesError and ExpectedMoreSplitsError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### System Info\n\n- `transformers` version: 4.32.1\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.11.5\r\n- Huggingface_hub version: 0.15.1\r\n- Safetensors version: 0.3.2\r\n- Accelerate version: not installed\r\n- Accelerate config: not found\r\n- PyTorch version (GPU?): 2.1.1+cpu (False)\r\n- Tensorflow version (GPU?): 2.15.0 (False)\r\n- Flax version (CPU?/GPU?/TPU?): not installed (NA)\r\n- Jax version: not installed\r\n- JaxLib version: not installed\r\n- Using GPU in script?: no\r\n- Using distributed or parallel set-up in script?: no\n\n### Who can help?\n\n@marios\n\n### Information\n\n- [X] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [X] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\n[Source](https://huggingface.co/docs/datasets/v2.2.1/en/access)\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset.info\r\n```\r\n\r\nDatasetInfo(description='GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https://www.microsoft.com/en-us/download/details.aspx?id=52398', license='', features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(names=['not_equivalent', 'equivalent'], id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, task_templates=None, builder_name='glue', dataset_name=None, config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943843, num_examples=3668, shard_lengths=None, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105879, num_examples=408, shard_lengths=None, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442410, num_examples=1725, shard_lengths=None, dataset_name='glue')}, download_checksums={'https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': None}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': None}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': None}}, download_size=1494541, post_processing_size=None, dataset_size=1492132, size_in_bytes=2986673)\n\n### Expected behavior\n\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('glue', 'mrpc', split='train')\r\ndataset.info\r\n```\r\nDatasetInfo(\r\n    description='GLUE, the General Language Understanding Evaluation benchmark\\n(https://gluebenchmark.com/) is a collection of resources for training,\\nevaluating, and analyzing natural language understanding systems.\\n\\n', \r\n    citation='@inproceedings{dolan2005automatically,\\n  title={Automatically constructing a corpus of sentential paraphrases},\\n  author={Dolan, William B and Brockett, Chris},\\n  booktitle={Proceedings of the Third International Workshop on Paraphrasing (IWP2005)},\\n  year={2005}\\n}\\n@inproceedings{wang2019glue,\\n  title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\\n  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\\n  note={In the Proceedings of ICLR.},\\n  year={2019}\\n}\\n', homepage='https://www.microsoft.com/en-us/download/details.aspx?id=52398', \r\n    license='', \r\n    features={'sentence1': Value(dtype='string', id=None), 'sentence2': Value(dtype='string', id=None), 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None), 'idx': Value(dtype='int32', id=None)}, post_processed=None, supervised_keys=None, builder_name='glue', config_name='mrpc', version=1.0.0, splits={'train': SplitInfo(name='train', num_bytes=943851, num_examples=3668, dataset_name='glue'), 'validation': SplitInfo(name='validation', num_bytes=105887, num_examples=408, dataset_name='glue'), 'test': SplitInfo(name='test', num_bytes=442418, num_examples=1725, dataset_name='glue')}, \r\n    download_checksums={'https://dl.fbaipublicfiles.com/glue/data/mrpc_dev_ids.tsv': {'num_bytes': 6222, 'checksum': '971d7767d81b997fd9060ade0ec23c4fc31cbb226a55d1bd4a1bac474eb81dc7'}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_train.txt': {'num_bytes': 1047044, 'checksum': '60a9b09084528f0673eedee2b69cb941920f0b8cd0eeccefc464a98768457f89'}, 'https://dl.fbaipublicfiles.com/senteval/senteval_data/msr_paraphrase_test.txt': {'num_bytes': 441275, 'checksum': 'a04e271090879aaba6423d65b94950c089298587d9c084bf9cd7439bd785f784'}}, \r\n    download_size=1494541, \r\n    post_processing_size=None, \r\n    dataset_size=1492156, \r\n    size_in_bytes=2986697\r\n)<end_of_turn>\n<start_of_turn>model\nNewline characters don't behave as expected when calling dataset.info<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMaking this change, as encouraged here:\r\n\r\n* https://github.com/huggingface/datasets/pull/7434#discussion_r1979933054\r\n\r\ninstead of having the pattern of using `try`-`except` to handle when there is no match, we can instead check if the return value is `None`; we can also assert that the return value should not be `None` if we know that should be true<end_of_turn>\n<start_of_turn>model\nRefactor `string_to_dict` to return `None` if there is no match instead of raising `ValueError`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI want to play around with different hyperparameters when training but don't want to re-map my dataset with 3 million samples each time for tens of hours when I [fully fine-tune SDXL](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py).\r\n\r\nAfter I do the mapping like this:\r\n```\r\ntrain_dataset = train_dataset.map(compute_embeddings_fn, batched=True)\r\ntrain_dataset = train_dataset.map(\r\n    compute_vae_encodings_fn,\r\n    batched=True,\r\n    batch_size=16,\r\n)\r\n```\r\nand try to save it like this:\r\n`train_dataset.save_to_disk(\"test\")`\r\ni get this error ([full traceback](https://pastebin.com/kq3vt739)):\r\n```\r\nTypeError: Object of type function is not JSON serializable\r\nThe format kwargs must be JSON serializable, but key 'transform' isn't.\r\n```\r\n\r\nBut what is interesting is that pushing to hub works like that:\r\n`train_dataset.push_to_hub(\"kopyl/mapped-833-icons-sdxl-1024-dataset\", token=True)`\r\nHere is the link of the pushed dataset: https://huggingface.co/datasets/kopyl/mapped-833-icons-sdxl-1024-dataset\r\n\r\n### Steps to reproduce the bug\r\n\r\nHere is the self-contained notebook:\r\n\r\nhttps://colab.research.google.com/drive/1RtCsEMVcwWcMwlWURk_cj_9xUBHz065M?usp=sharing\r\n\r\n### Expected behavior\r\n\r\nIt should be easily saved to disk\r\n\r\n### Environment info\r\n\r\nNVIDIA A100, Linux (NC24ads A100 v4 from Azure), CUDA 12.2.\r\n\r\n[pip freeze](https://pastebin.com/QTNb6iru)<end_of_turn>\n<start_of_turn>model\nImpossible to save a mapped dataset to disk<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nModify `is_remote_filesystem` to return True for FUSE-mounted paths<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7821\r\n\r\nIn addition to the solution proposed in the issue, I encountered we also need to support 64bit types when writing using the `RecordBatchStreamWriter`. \r\n\r\nNot sure if we want to create such large objects in the CI, but this is the only way to test that the issue is fixed, therefore I added the unit test.<end_of_turn>\n<start_of_turn>model\nFix ArrowInvalid for large variables<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWe push changes to our datasets programmatically. Our git client jGit reports that the hf git server returns null as Content-Type after a push.\n\n### Steps to reproduce the bug\n\nA basic kotlin application:\r\n```\r\n   val person = PersonIdent(\r\n        \"padmalcom\",\r\n        \"padmalcom@sth.com\"\r\n    )\r\n\r\n    val cp = UsernamePasswordCredentialsProvider(\r\n        \"padmalcom\",\r\n        \"mysecrettoken\"\r\n    )\r\n\r\n    val git =\r\n        KGit.cloneRepository {\r\n            setURI(\"https://huggingface.co/datasets/sth/images\")\r\n            setTimeout(60)\r\n            setProgressMonitor(TextProgressMonitor())\r\n            setCredentialsProvider(cp)\r\n        }\r\n\r\n    FileOutputStream(\"./images/images.csv\").apply { writeCsv(images) }\r\n    git.add {\r\n        addFilepattern(\"images.csv\")\r\n    }\r\n\r\n    for (i in images) {\r\n        FileUtils.copyFile(\r\n            File(\"./files/${i.id}\"),\r\n            File(\"./images/${i.id + File(i.fileName).extension }\")\r\n        )\r\n        git.add {\r\n            addFilepattern(\"${i.id + File(i.fileName).extension }\")\r\n        }\r\n    }\r\n\r\n    val revCommit = git.commit {\r\n        author = person\r\n        message = \"Uploading images at \" + LocalDateTime.now()\r\n            .format(DateTimeFormatter.ISO_DATE_TIME)\r\n        setCredentialsProvider(cp)\r\n    }\r\n\r\n    val push = git.push {\r\n        setCredentialsProvider(cp)\r\n    }\r\n```\n\n### Expected behavior\n\nThe git server is expected to return the Content-Type _application/x-git-receive-pack-result_.\n\n### Environment info\n\nIt is independent from the datasets library.<end_of_turn>\n<start_of_turn>model\nHuggingface GIT returns null as Content-Type instead of application/x-git-receive-pack-result<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nEnsure PyTorch images are converted to (C, H, W) instead of (H, W, C). See #6394 for motivation. <end_of_turn>\n<start_of_turn>model\nUpdate torch_formatter.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading a simple webdataset takes ~45 minutes.\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"PixArt-alpha/SAM-LLaVA-Captions10M\")\r\n```\n\n### Expected behavior\n\nThe dataset should load immediately as it does when loaded through a normal indexed WebDataset loader. Generating splits should be optional and there should be a message showing how to disable it. \n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-4.18.0-372.32.1.el8_6.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.24.1\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\nGenerating train split takes a long time<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCreate a dataset. Save it to disk. Load from disk. Shuffle, usning a `np.random.Generator`. Iterate. Shuffle again. Iterate. The iterates are different since the supplied np.random.Generator has progressed between the shuffles.\r\n\r\nLoad dataset from disk again. Shuffle and Iterate. See same result as before. Shuffle and iterate, and this time it does not have the same shuffling as ion previous run.\r\n\r\nThe motivation is I have a deep learning loop with \r\n\r\n```\r\nfor epoch in range(10):\r\n    for batch in dataset.shuffle(generator=generator).iter(batch_size=32):\r\n        .... # do stuff\r\n```\r\nwhere I want a new shuffling at every epoch. Instead I get the same shuffling.\n\n### Steps to reproduce the bug\n\nRun the code below two times. \r\n\r\n```python\r\nimport datasets\r\nimport numpy as np\r\n\r\ngenerator = np.random.default_rng(0)\r\nds = datasets.Dataset.from_dict(mapping={\"X\":range(1000)})\r\nds.save_to_disk(\"tmp\")\r\nprint(\"First loop: \", end=\"\")\r\nfor _ in range(10):\r\n    print(next(ds.shuffle(generator=generator).iter(batch_size=1))['X'], end=\", \")\r\nprint(\"\")\r\n\r\nprint(\"Second loop: \", end=\"\")\r\nds = datasets.Dataset.load_from_disk(\"tmp\")\r\nfor _ in range(10):\r\n    print(next(ds.shuffle(generator=generator).iter(batch_size=1))['X'], end=\", \")\r\nprint(\"\")\r\n```\r\n\r\nThe output is:\r\n\r\n```\r\n$ python main.py \r\nSaving the dataset (1/1 shards): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00<00:00, 495019.95 examples/s]\r\nFirst loop: 459, 739, 72, 943, 241, 181, 845, 830, 896, 334, \r\nSecond loop: 741, 847, 944, 795, 483, 842, 717, 865, 231, 840,\r\n$ python main.py \r\nSaving the dataset (1/1 shards): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [00:00<00:00, 22243.40 examples/s]\r\nFirst loop: 459, 739, 72, 943, 241, 181, 845, 830, 896, 334, \r\nSecond loop: 741, 741, 741, 741, 741, 741, 741, 741, 741, 741, \r\n```\r\n\r\nThe second loop, on the second run, only spits out \"741, 741, 741....\" which is *not* the desired output\n\n### Expected behavior\n\nI want the dataset to shuffle at every epoch since I provide it with a generator for shuffling. \n\n### Environment info\n\nDatasets version 2.21.0\r\nUbuntu linux.<end_of_turn>\n<start_of_turn>model\nCaching shuffles by np.random.Generator results in unintiutive behavior<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAfter jax-0.4.27 release (https://github.com/google/jax/releases/tag/jax-v0.4.27), our CI is broken with the error:\r\n```Python traceback\r\nAttributeError: 'jaxlib.xla_extension.DeviceList' object has no attribute 'device'. Did you mean: 'devices'?\r\n```\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/8997488610/job/24715736153\r\n```Python traceback\r\n___________________ FormatterTest.test_jax_formatter_device ____________________\r\n[gw1] linux -- Python 3.10.14 /opt/hostedtoolcache/Python/3.10.14/x64/bin/python\r\n\r\nself = <tests.test_formatting.FormatterTest testMethod=test_jax_formatter_device>\r\n\r\n    @require_jax\r\n    def test_jax_formatter_device(self):\r\n        import jax\r\n    \r\n        from datasets.formatting import JaxFormatter\r\n    \r\n        pa_table = self._create_dummy_table()\r\n        device = jax.devices()[0]\r\n        formatter = JaxFormatter(device=str(device))\r\n        row = formatter.format_row(pa_table)\r\n>       assert row[\"a\"].device() == device\r\nE       AttributeError: 'jaxlib.xla_extension.DeviceList' object has no attribute 'device'. Did you mean: 'devices'?\r\n\r\ntests/test_formatting.py:630: AttributeError\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken after jax-0.4.27 release: AttributeError: 'jaxlib.xla_extension.DeviceList' object has no attribute 'device'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nhttps://huggingface.co/docs/datasets/v4.0.0/en/package_reference/main_classes#datasets.Dataset.map.num_proc\n```\nnum_proc (int, optional, defaults to None) \u2014 Max number of processes when generating cache. Already cached\nshards are loaded sequentially.\n```\n\nfor batch:\n```\nnum_proc (int, optional, defaults to None): The number of processes to use for multiprocessing. If None, no \nmultiprocessing is used. This can significantly speed up batching for large datasets.\n```\n\nSo what happens to `map.num_proc` - is it the same behavior as `batch.num_proc` - so only if `num_proc=None` then no multiprocessing is used?\n\nLet's update the doc to be unambiguous.\n\n**bonus**: we could make all of these behave similarly to `DataLoader.num_workers` - where `num_workers==0` implies no multiprocessing. I think that's the most intuitive, IMHO. 0 workers - the main process has to do all the work. `None` could be the same as `0`.\n\ncontext: debugging a failing `map`\n\nThank you!<end_of_turn>\n<start_of_turn>model\n[doc] map.num_proc needs clarification<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nhttps://huggingface.co/docs/datasets/en/semantic_segmentation shows wrong example with torchvision transforms.\r\nSpecifically, as one can see in screenshot below, the object boundaries have weird colors. \r\n\r\n<img width=\"689\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/4803565/59aa0e2c-2e3e-415b-9d42-2314044c5aee\">\r\n\r\n\r\nOriginal example with `albumentations` is correct \r\n<img width=\"705\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/4803565/27dbd725-cea5-4e48-ba59-7050c3ce17b3\">\r\n\r\n\r\nThat is because `torch vision.transforms.Resize` interpolates with bilinear everything which is wrong when used for segmentation labels - you just cannot mix them. Overall, `torchvision.transforms` is designed for classification only and cannot be used to images and masks together, unless you write two separate branches of augmentations.\r\n\r\n\r\nThe correct way would be to use `v2` version of transforms and convert the segmentation labels to https://pytorch.org/vision/main/generated/torchvision.tv_tensors.Mask.html#torchvision.tv_tensors.Mask object\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\nGo to the website.\r\n<img width=\"689\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/4803565/ea1276d0-d69a-48cf-b9c2-cd61217815ef\">\r\n\r\nhttps://huggingface.co/docs/datasets/en/semantic_segmentation\n\n### Expected behavior\n\nResults, similar to `albumentation`. Or remove the torch vision part altogether. Or use `kornia` instead.\n\n### Environment info\n\nIrrelevant<end_of_turn>\n<start_of_turn>model\nExample on Semantic segmentation contains bug<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix https://github.com/huggingface/datasets/issues/6750#issuecomment-2016678729\r\n\r\nI didn't find a guideline on how to run the tests, so i just run the following steps to make sure that this bug is fixed.\r\n1. `python test.py`,\r\n2. then `HF_DATASETS_OFFLINE=1 python test.py`\r\n\r\nThe `test.py` is\r\n```\r\n  import datasets\r\n\r\n  datasets.utils.logging.set_verbosity_info()\r\n\r\n  ds = datasets.load_dataset('izhx/STS17-debug')\r\n  print(ds)\r\n\r\n  ds = datasets.load_dataset('C-MTEB/AFQMC', revision='b44c3b011063adb25877c13823db83bb193913c4')\r\n  print(ds)\r\n```\r\n\r\n\r\n<end_of_turn>\n<start_of_turn>model\nFix cache path to snakecase for `CachedDatasetModuleFactory` and `Cache`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate README.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCreate a dataset composed of sequence of array fails for most dtypes (see code below).\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Sequence, Array2D, Features, Dataset\r\n\r\nimport numpy as np\r\n\r\nfor dtype in [\r\n    \"bool\",  # ok\r\n    \"int8\",  # failed\r\n    \"int16\",  # failed\r\n    \"int32\",  # failed\r\n    \"int64\",  # ok\r\n    \"uint8\",  # failed\r\n    \"uint16\",  # failed\r\n    \"uint32\",  # failed\r\n    \"uint64\",  # failed\r\n    \"float16\",  # failed\r\n    \"float32\",  # failed\r\n    \"float64\",  # ok\r\n]:\r\n    features = Features({\"foo\": Sequence(Array2D(dtype=dtype, shape=(2, 2)))})\r\n    sequence = [\r\n        [[1.0, 2.0], [3.0, 4.0]],\r\n        [[5.0, 6.0], [7.0, 8.0]],\r\n    ]\r\n    array = np.array(sequence, dtype=dtype)\r\n    try:\r\n        dataset = Dataset.from_dict({\"foo\": [array]}, features=features)\r\n    except Exception as e:\r\n        print(f\"Failed for dtype={dtype}\")\r\n```\r\n\r\nTraceback for `dtype=\"int8\"`:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/qgallouedec/datasets/a.py\", line 29, in <module>\r\n    raise e\r\n  File \"/home/qgallouedec/datasets/a.py\", line 26, in <module>\r\n    dataset = Dataset.from_dict({\"foo\": [array]}, features=features)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 899, in from_dict\r\n    pa_table = InMemoryTable.from_pydict(mapping=mapping)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/table.py\", line 799, in from_pydict\r\n    return cls(pa.Table.from_pydict(*args, **kwargs))\r\n  File \"pyarrow/table.pxi\", line 3725, in pyarrow.lib.Table.from_pydict\r\n  File \"pyarrow/table.pxi\", line 5254, in pyarrow.lib._from_pydict\r\n  File \"pyarrow/array.pxi\", line 350, in pyarrow.lib.asarray\r\n  File \"pyarrow/array.pxi\", line 236, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 204, in __arrow_array__\r\n    out = cast_array_to_feature(out, type, allow_number_to_str=not self.trying_type)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/table.py\", line 2091, in cast_array_to_feature\r\n    casted_values = _c(array.values, feature.feature)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/table.py\", line 2139, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/table.py\", line 1833, in wrapper\r\n    return func(array, *args, **kwargs)\r\n  File \"/home/qgallouedec/env/lib/python3.10/site-packages/datasets/table.py\", line 1967, in array_cast\r\n    return pa_type.wrap_array(array)\r\n  File \"pyarrow/types.pxi\", line 879, in pyarrow.lib.BaseExtensionType.wrap_array\r\nTypeError: Incompatible storage type for extension<arrow.py_extension_type<Array2DExtensionType>>: expected list<item: list<item: int8>>, got list<item: list<item: int64>>\r\n```\n\n### Expected behavior\n\nNot to fail.\n\n### Environment info\n\n\r\n- Python 3.10.6\r\n- datasets: master branch\r\n- Numpy: 1.23.4<end_of_turn>\n<start_of_turn>model\nSequence of array not supported for most dtype<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://github.com/huggingface/dataset-viewer/issues/2650\r\n\r\nTell me if it's OK, or if it's a breaking change that must be handled differently.\r\n\r\nAlso note that the docs page is still https://huggingface.co/docs/datasets-server/, so I didn't change it.\r\n\r\nAnd the API URL is still https://datasets-server.huggingface.co/ (and [might always be](https://github.com/huggingface/dataset-viewer/issues/2666)), so I let it too.<end_of_turn>\n<start_of_turn>model\nrename datasets-server to dataset-viewer<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR addresses Issue #7472\r\n\r\ncc: @lhoestq <end_of_turn>\n<start_of_turn>model\nSupport skip_trying_type <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAccording to the docs, there is a \"mono\" parameter to the Audio feature, which turns any stereo into mono. In practice the signal is not touched and the mono parameter, even though documented, does not exist.\n\n\nhttps://github.com/huggingface/datasets/blob/41c05299348a499807432ab476e1cdc4143c8772/src/datasets/features/audio.py#L52C1-L54C22<end_of_turn>\n<start_of_turn>model\nmono parameter to the Audio feature is missing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen I tried to download opus_books using:\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"Helsinki-NLP/opus_books\")\n\nI got the following errors:\nFileNotFoundError: Couldn't find any data file at /workspace/Helsinki-NLP/opus_books. Couldn't find 'Helsinki-NLP/opus_books' on the Hugging Face Hub either: LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n\nI also tried:\ndataset = load_dataset(\"opus_books\", \"en-zh\")\n\nand the errors remain the same. However, I can download \"mlabonne/FineTome-100k\" successfully. \nMy datasets is version 4.2.0\n\nAny clues? Big thanks.<end_of_turn>\n<start_of_turn>model\nCannot download opus dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Issue Description\n\nI've encountered confusion when using `load_dataset` and `load_from_disk` in the datasets library. Specifically, when working offline with the gsm8k dataset, I can load it using a local path:\n\n```python\nimport datasets\nds = datasets.load_dataset('/root/xxx/datasets/gsm8k', 'main')\n```\noutput:\n```text\nDatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 7473\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 1319\n    })\n})\n```\n\nThis works as expected. However, after processing the dataset (converting answer format from #### to \\boxed{})\n```python\nimport datasets\nds = datasets.load_dataset('/root/xxx/datasets/gsm8k', 'main')\nds_train = ds['train']\nds_test = ds['test']\nimport re\ndef convert(sample):\n    solution = sample['answer']\n    solution = re.sub(r'####\\s*(\\S+)', r'\\\\boxed{\\1}', solution)\n    sample = {\n        'problem': sample['question'],\n        'solution': solution\n    }\n    return sample\n\nds_train = ds_train.map(convert, remove_columns=['question', 'answer'])\nds_test = ds_test.map(convert,remove_columns=['question', 'answer'])\n```\n\n I saved it using save_to_disk:\n```python\nfrom datasets.dataset_dict import DatasetDict\ndata_dict = DatasetDict({\n    'train': ds_train,\n    'test': ds_test\n})\ndata_dict.save_to_disk('/root/xxx/datasets/gsm8k-new')\n```\nBut now I can only load it using load_from_disk:\n\n```python\nnew_ds = load_from_disk('/root/xxx/datasets/gsm8k-new')\n```\noutput:\n```text\nDatasetDict({\n    train: Dataset({\n        features: ['problem', 'solution'],\n        num_rows: 7473\n    })\n    test: Dataset({\n        features: ['problem', 'solution'],\n        num_rows: 1319\n    })\n})\n```\n\nAttempting to use load_dataset produces unexpected results:\n```python\nnew_ds = load_dataset('/root/xxx/datasets/gsm8k-new')\n```\noutput:\n```text\nDatasetDict({\n    train: Dataset({\n        features: ['_data_files', '_fingerprint', '_format_columns', '_format_kwargs', '_format_type', '_output_all_columns', '_split'],\n        num_rows: 1\n    })\n    test: Dataset({\n        features: ['_data_files', '_fingerprint', '_format_columns', '_format_kwargs', '_format_type', '_output_all_columns', '_split'],\n        num_rows: 1\n    })\n})\n```\nQuestions\n1. Why is it designed such that after using `save_to_disk`, the dataset cannot be loaded with `load_dataset`? For small projects with limited code, it might be relatively easy to change all instances of `load_dataset` to `load_from_disk`. However, for complex frameworks like TRL or lighteval, diving into the framework code to change `load_dataset` to `load_from_disk` is extremely tedious and error-prone.\nAdditionally, `load_from_disk` cannot load datasets directly downloaded from the hub, which means that if you need to modify a dataset, you have to choose between using `load_from_disk` or `load_dataset`. This creates an unnecessary dichotomy in the API and complicates workflow when working with modified datasets.\n2. What's the recommended approach for this use case? Should I manually process my gsm8k-new dataset to make it compatible with load_dataset? Is there a standard way to convert between these formats?\n\nthanks~<end_of_turn>\n<start_of_turn>model\nInconsistency between load_dataset and load_from_disk functionality<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have updated my dataset by adding a new feature, and push it to the hub. When I want to download it on my machine which contain the old version by using `datasets.load_dataset(\"your_dataset_name\", download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)` I get an error (paste bellow).\r\n\r\nSeems that the load_dataset function still use the old features schema instead of downloading everything new from the HUB.\r\n\r\nI find a way to go around this issue by manually deleting the old dataset cache. But from my understanding of `datasets.DownloadMode.FORCE_REDOWNLOAD` option, the dataset cache should be ignored.\n\n### Steps to reproduce the bug\n\n1. Download your dataset in your machine using `datasets.load_dataset`\r\n2. Create a new feature in your dataset and push it to the hub\r\n3. On the same machine redownload your dataset using `datasets.load_dataset(\"your_dataset_name\", download_mode=datasets.DownloadMode.FORCE_REDOWNLOAD)`\n\n### Expected behavior\n\n`\r\nValueError: Couldn't cast\r\nid: string\r\nlevel: string\r\ncontext: list<element: string>\r\n  child 0, element: string\r\ntype: string\r\nanswer: string\r\nquestion: string\r\nsupporting_facts: list<element: string>\r\n  child 0, element: string\r\nfra_answer: string\r\nfra_question: string\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"id\": {\"dtype\": \"string\", \"_type\": \"' + 490\r\nto\r\n{'id': Value(dtype='string', id=None), 'level': Value(dtype='string', id=None), 'context': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'type': Value(dtype='string', id=None), 'answer': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'supporting_facts': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\r\nbecause column names don't match\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError\r\n...\r\nDatasetGenerationError: An error occurred while generating the dataset`\n\n### Environment info\n\ndatasets-2.16.1 huggingface-hub-0.20.2<end_of_turn>\n<start_of_turn>model\ndatasets.DownloadMode.FORCE_REDOWNLOAD use cache to download dataset features with load_dataset function<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe `Build documentation / build / build_main_documentation (push)` job is consistently failing during the \"Syncing repository\" step. The error occurs when attempting to determine the default branch name, resulting in \"Bad credentials\" errors.\n\n### Steps to reproduce the bug\n\n1. Trigger the `build_main_documentation` job.\r\n2. Observe the logs during the \"Syncing repository\" step.\n\n### Expected behavior\n\nThe workflow should be able to retrieve the default branch name without encountering credential issues.\n\n### Environment info\n\n```plaintext\r\nSyncing repository: huggingface/notebooks\r\nGetting Git version info\r\nTemporarily overriding HOME='/home/runner/work/_temp/00e62748-9940-4a4f-bbbc-eb2cda6d7ed6' before making global git config changes\r\nAdding repository directory to the temporary git global config as a safe directory\r\n/usr/bin/git config --global --add safe.directory /home/runner/work/datasets/datasets/notebooks\r\nInitializing the repository\r\nDisabling automatic garbage collection\r\nSetting up auth\r\nDetermining the default branch\r\n  Retrieving the default branch name\r\n  Bad credentials - https://docs.github.com/rest\r\n  Waiting 20 seconds before trying again\r\n  Retrieving the default branch name\r\n  Bad credentials - https://docs.github.com/rest\r\n  Waiting 19 seconds before trying again\r\n  Retrieving the default branch name\r\n  Error: Bad credentials - https://docs.github.com/rest\r\n```<end_of_turn>\n<start_of_turn>model\n Build Documentation Test Fails Due to \"Bad Credentials\" Error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI encountered an ImportError while trying to import `load_dataset` from the `datasets` module in Hugging Face. The error message indicates a problem with importing 'CommitInfo' from 'huggingface_hub'.\n\n### Steps to reproduce the bug\n\n1. pip install git+https://github.com/huggingface/datasets\r\n2. from datasets import load_dataset\n\n### Expected behavior\n\nImportError                               Traceback (most recent call last)\r\nCell In[7], [line 1](vscode-notebook-cell:?execution_count=7&line=1)\r\n----> [1](vscode-notebook-cell:?execution_count=7&line=1) from datasets import load_dataset\r\n      [3](vscode-notebook-cell:?execution_count=7&line=3) train_set = load_dataset(\"mispeech/speechocean762\", split=\"train\")\r\n      [4](vscode-notebook-cell:?execution_count=7&line=4) test_set = load_dataset(\"mispeech/speechocean762\", split=\"test\")\r\n\r\nFile d:\\Anaconda3\\envs\\CS224S\\Lib\\site-packages\\datasets\\__init__.py:[1](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:1)7\r\n      1 # Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\r\n      [2](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:2) #\r\n      [3](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:3) # Licensed under the Apache License, Version 2.0 (the \"License\");\r\n   (...)\r\n     [12](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:12) # See the License for the specific language governing permissions and\r\n     [13](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:13) # limitations under the License.\r\n     [15](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:15) __version__ = \"2.20.1.dev0\"\r\n---> [17](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:17) from .arrow_dataset import Dataset\r\n     [18](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:18) from .arrow_reader import ReadInstruction\r\n     [19](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/__init__.py:19) from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\nFile d:\\Anaconda3\\envs\\CS224S\\Lib\\site-packages\\datasets\\arrow_dataset.py:63\r\n     [61](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:61) import pyarrow.compute as pc\r\n     [62](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:62) from fsspec.core import url_to_fs\r\n---> [63](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:63) from huggingface_hub import (\r\n     [64](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:64)     CommitInfo,\r\n     [65](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:65)     CommitOperationAdd,\r\n...\r\n     [70](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:70) )\r\n     [71](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:71) from huggingface_hub.hf_api import RepoFile\r\n     [72](file:///D:/Anaconda3/envs/CS224S/Lib/site-packages/datasets/arrow_dataset.py:72) from multiprocess import Pool\r\n\r\nImportError: cannot import name 'CommitInfo' from 'huggingface_hub' (d:\\Anaconda3\\envs\\CS224S\\Lib\\site-packages\\huggingface_hub\\__init__.py)\r\nOutput is truncated. View as a [scrollable element](command:cellOutput.enableScrolling?580889ab-0f61-4f37-9214-eaa2b3807f85) or open in a [text editor](command:workbench.action.openLargeOutput?580889ab-0f61-4f37-9214-eaa2b3807f85). Adjust cell output [settings](command:workbench.action.openSettings?%5B%22%40tag%3AnotebookOutputLayout%22%5D)...\n\n### Environment info\n\nLeo@DESKTOP-9NHUAMI MSYS /d/Anaconda3/envs/CS224S/Lib/site-packages/huggingface_hub\r\n$ datasets-cli env\r\nTraceback (most recent call last):\r\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n  File \"<frozen runpy>\", line 88, in _run_code\r\n  File \"D:\\Anaconda3\\envs\\CS224S\\Scripts\\datasets-cli.exe\\__main__.py\", line 4, in <module>\r\n  File \"D:\\Anaconda3\\envs\\CS224S\\Lib\\site-packages\\datasets\\__init__.py\", line 17, in <module>\r\n    from .arrow_dataset import Dataset\r\n  File \"D:\\Anaconda3\\envs\\CS224S\\Lib\\site-packages\\datasets\\arrow_dataset.py\", line 63, in <module>\r\n    from huggingface_hub import (\r\nImportError: cannot import name 'CommitInfo' from 'huggingface_hub' (D:\\Anaconda3\\envs\\CS224S\\Lib\\site-packages\\huggingface_hub\\__init__.py)\r\n(CS224S)<end_of_turn>\n<start_of_turn>model\nImportError when importing datasets.load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, I'm encountered a small issue when saving datasets that led to the saving taking up to multiple hours.\r\nSpecifically, [`Dataset.save_to_disk`](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.save_to_disk) is a lot slower when using `num_proc>1` than when using `num_proc=1`\r\n\r\nThe documentation mentions that \"Multiprocessing is disabled by default.\", but there is no explanation on how to enable it.\n\n### Steps to reproduce the bug\n\n```\r\nimport numpy as np\r\nfrom datasets import Dataset\r\n\r\nn_samples = int(4e6)\r\nn_tokens_sample = 100\r\ndata_dict = {\r\n    'tokens' : np.random.randint(0, 100, (n_samples, n_tokens_sample)),\r\n}\r\n\r\ndataset = Dataset.from_dict(data_dict)\r\ndataset.save_to_disk('test_dataset', num_proc=1)\r\ndataset.save_to_disk('test_dataset', num_proc=4)\r\ndataset.save_to_disk('test_dataset', num_proc=8)\r\n```\r\n\r\nThis results in:\r\n```\r\n>>> dataset.save_to_disk('test_dataset', num_proc=1)\r\nSaving the dataset (7/7 shards): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000000/4000000 [00:17<00:00, 228075.15 examples/s]\r\n>>> dataset.save_to_disk('test_dataset', num_proc=4)\r\nSaving the dataset (7/7 shards): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000000/4000000 [01:49<00:00, 36583.75 examples/s]\r\n>>> dataset.save_to_disk('test_dataset', num_proc=8)\r\nSaving the dataset (8/8 shards): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4000000/4000000 [02:11<00:00, 30518.43 examples/s]\r\n```\r\n\r\nWith larger datasets it can take hours, but I didn't benchmark that for this bug report.\n\n### Expected behavior\n\nI would expect using `num_proc>1` to be faster instead of slower than `num_proc=1`. \n\n### Environment info\n\n- `datasets` version: 3.1.0\r\n- Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.26.2\r\n- PyArrow version: 18.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\n`Dataset.save_to_disk` hangs when using num_proc > 1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n!pip install accelerate>=0.16.0 torchvision transformers>=4.25.1 datasets>=2.19.1 ftfy tensorboard Jinja2 peft==0.7.0\r\n\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\r\nibis-framework 8.0.0 requires pyarrow<16,>=2, but you have pyarrow 17.0.0 which is incompatible.\r\n\r\nto solve above error\r\n!pip install pyarrow==14.0.1\r\n\r\nERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ndatasets 2.21.0 requires pyarrow>=15.0.0, but you have pyarrow 14.0.1 which is incompatible.\n\n### Steps to reproduce the bug\n\n!pip install datasets>=2.19.1\n\n### Expected behavior\n\nrun without dependency error\n\n### Environment info\n\nDiffusers version: 0.31.0.dev0\r\n\r\nPlatform: Linux-6.1.85+-x86_64-with-glibc2.35\r\nRunning on Google Colab?: Yes\r\nPython version: 3.10.12\r\nPyTorch version (GPU?): 2.3.1+cu121 (True)\r\nFlax version (CPU?/GPU?/TPU?): 0.8.4 (gpu)\r\nJax version: 0.4.26\r\nJaxLib version: 0.4.26\r\nHuggingface_hub version: 0.23.5\r\nTransformers version: 4.42.4\r\nAccelerate version: 0.32.1\r\nPEFT version: 0.7.0\r\nBitsandbytes version: not installed\r\nSafetensors version: 0.4.4\r\nxFormers version: not installed\r\nAccelerator: Tesla T4, 15360 MiB\r\nUsing GPU in script?:\r\nUsing distributed or parallel set-up in script?:<end_of_turn>\n<start_of_turn>model\ncudf-cu12 24.4.1, ibis-framework 8.0.0 requires pyarrow<15.0.0a0,>=14.0.1,pyarrow<16,>=2 and datasets 2.21.0 requires pyarrow>=15.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### What does this PR do?\r\n\r\nAdding a GH action to scan for leaked secrets on each commit.\r\n<end_of_turn>\n<start_of_turn>model\nfeat(ci): add trufflehog secrets detection<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAllow `storage_options` to be passed in\n1. `cast` related operations (e.g., `cast_columns, cast`)\n2. `info` related reading (e.g., `from_dict, from_pandas, from_polars`) together with `info.features`\n```python3\nimport datasets\n\nimage_path = \"s3://bucket/sample.png\"\ndataset = datasets.Dataset.from_dict({\"image_path\": [image_path]})\n\n# dataset = dataset.cast_column(\"image_path\", datasets.Image())  # now works without `storage_options`\n\n# expected behavior\ndataset = dataset.cast_column(\"image_path\", datasets.Image(), storage_options={\"anon\": True})\n```\n\n### Motivation\n\nI'm using my own registered fsspec filesystem (s3 with customized local cache support). I need to pass cache folder paths `cache_dirs: list[str]` to the filesystem when I read the remote images (cast from file_paths).\n\n### Your contribution\n\nCould help with a PR at weekends<end_of_turn>\n<start_of_turn>model\ncast columns to Image/Audio/Video with `storage_options`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to https://github.com/huggingface/doc-builder/pull/487 and [internal slack thread](https://huggingface.slack.com/archives/C04F8N7FQNL/p1711384899462349?thread_ts=1711041424.720769&cid=C04F8N7FQNL). There is now a `custom_container` option when building docs in CI. When set to `\"\"` (instead of `\"huggingface/transformers-doc-builder\"` by default), we don't run the CI inside a container, therefore saving ~2min of download time. The plan is to test disabling the transformers container on a few \"big\" repo and if everything works correctly, we will stop making it the default container. More details on https://github.com/huggingface/doc-builder/pull/487.\r\n\r\ncc @mishig25 <end_of_turn>\n<start_of_turn>model\nTest disabling transformers containers in docs CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using the huggingface datasets.Audio feature to decode a local or remote (public HF dataset) audio file inside Google Colab, the notebook kernel crashes with std::bad_alloc (C++ memory allocation failure).\nThe crash happens even with a minimal code example and valid .wav file that can be read successfully using soundfile.\n\nHere is a sample Collab notebook to reproduce the problem.\nhttps://colab.research.google.com/drive/1nnb-GC5748Tux3xcYRussCGp2x-zM9Id?usp=sharing\n\ncode sample:\n```\n...\naudio_dataset = audio_dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n\n# Accessing the first element crashes the Colab kernel\nprint(audio_dataset[0][\"audio\"])\n```\n\nError log \n```\nWARNING what(): std::bad_alloc\nterminate called after throwing an instance of 'std::bad_alloc'\n```\n\nEnvironment\n\nPlatform: Google Colab (Python 3.12.12)\ndatasets Version: 4.3.0\nsoundfile Version: 0.13.1\ntorchaudio Version: 2.8.0+cu126\n\nThanks in advance to help me on this error I get approx two weeks now after it was working before.\n\nRegards\n\n### Steps to reproduce the bug\n\nhttps://colab.research.google.com/drive/1nnb-GC5748Tux3xcYRussCGp2x-zM9Id?usp=sharing\n\n### Expected behavior\n\nLoading the audio and decode it.\nIt should safely return:\n\n{\n  \"path\": \"path/filaname.wav\",\n  \"array\": np.ndarray([...]),\n  \"sampling_rate\": 16000\n}\n\n### Environment info\n\nEnvironment\n\nPlatform: Google Colab (Python 3.12.12)\ndatasets Version: 4.3.0\nsoundfile Version: 0.13.1\ntorchaudio Version: 2.8.0+cu126<end_of_turn>\n<start_of_turn>model\nAudio.cast_column() or Audio.decode_example() causes Colab kernel crash (std::bad_alloc)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe idea is to make this code work for datasets with scripts if they have a Parquet export\r\n\r\n```python\r\nds = load_dataset(\"squad\", trust_remote_code=False)\r\n```\r\n\r\nAnd more generally, it means we use the Parquet export whenever it's possible (it's safer and faster than dataset scripts).\r\n\r\nI also added a `config.USE_PARQUET_EXPORT` variable to use in the datasets-server parquet conversion job\r\n\r\n- [x] Needs https://github.com/huggingface/datasets/pull/6429 to be merged first\r\n\r\ncc @severo I use the /parquet and /info endpoints from datasets-server<end_of_turn>\n<start_of_turn>model\nUse parquet export if possible<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes issue #7745.\r\n\r\nWho can review:\r\n@lhoestq <end_of_turn>\n<start_of_turn>model\nFix Audio docstring by removing unsupported mono argument<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am getting the following error while I am trying to upload the CSV sheet to train a model. My CSV sheet content is exactly same as shown in the example CSV file in the Auto Train page. Attaching screenshot of error for reference. I have also tried converting the index of the answer that are integer into string by placing inverted commas and also without inverted commas. \r\nCan anyone please help me out? \r\nFYI : I am using Chrome browser.\r\n\r\nError type: ArrowInvalid\r\nDetails: Failed to parse string: '[254,254]' as a scalar of type int32\r\n\r\n![Screenshot 2023-08-19 165827](https://github.com/huggingface/datasets/assets/90616801/95fad96e-7dce-4bb5-9f83-9f1659a32891)\r\n\n\n### Steps to reproduce the bug\n\nKindly let me know how to fix this?\n\n### Expected behavior\n\nKindly let me know how to fix this?\n\n### Environment info\n\nKindly let me know how to fix this?<end_of_turn>\n<start_of_turn>model\nError type: ArrowInvalid Details: Failed to parse string: '[254,254]' as a scalar of type int32<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nContext: issue #5990\r\n\r\nIn order to implement the checkpointing, I introduce a metadata folder that keeps one yaml file for each set that one is uploading. This yaml keeps track of what shards have already been uploaded, and which one the idx of the latest one was. Using this information I am then able to easily get the push_to_hub function to retrieve on demand past history of uploads and continue mapping and uploading from where it was left off. <end_of_turn>\n<start_of_turn>model\nImplement proper checkpointing for dataset uploading with resume function that does not require remapping shards that have already been uploaded<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently using a `map` call processes and duplicates the whole dataset, which takes both time and disk space.\r\n\r\nThe solution is to allow lazy mapping, which is essentially a saved chain of transforms that are applied on the fly whenever a slice of the dataset is requested.\r\n\r\nThe API should look like `map`, as `set_transform` changes the current dataset while `map` returns another dataset.\n\n### Motivation\n\nLazy processing allows lower disk usage and faster experimentation.\n\n### Your contribution\n\n_<end_of_turn>\n<start_of_turn>model\n[FR] Transform Chaining, Lazy Mapping<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollow up to huggingface/huggingface.js#296<end_of_turn>\n<start_of_turn>model\nUpdate hub-docs reference<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse `hf-internal-testing` for hosting instead of the maintainers' dataset repos.<end_of_turn>\n<start_of_turn>model\nUse `hf-internal-testing` repos for hosting test dataset repos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n```python\nfrom datasets import Dataset\n\ndataset = Dataset.from_dict({\n    \"messages\": [\n        [\n            {\"a\": \"...\",},\n            {\"b\": \"...\",},\n        ],\n    ]\n})\nprint(dataset[0])\n```\n\nWhat I get:\n```\n{'messages': [{'a': '...', 'b': None}, {'a': None, 'b': '...'}]}\n```\n\nWhat I want:\n\n```\n{'messages': [{'a': '...'}, {'b': '...'}]}\n```\n\nIs there an easy way to automatically remove these auto-filled null/none values?\n\nIf not, I probably need a recursive none exclusion function, don't I?\n\nDatasets 3.6.0<end_of_turn>\n<start_of_turn>model\nUnwanted column padding in nested lists of dicts<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI've noticed a strange behaviour with Iterable state_dict: the value of shard_example_idx is always equal to the amount of samples in a shard.\n\n\n### Steps to reproduce the bug\n\nI am reusing the example from the doc\n```python\nfrom datasets import Dataset\n\nds = Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=1)\nstate_dict = None\n# Iterate through the dataset and print examples\nfor idx, example in enumerate(ds):\n    print(example)\n    if idx == 2:\n        state_dict = ds.state_dict()\n        print(\"checkpoint\")\n        break\nprint(state_dict)\n```\nReturns:\n```\n{'a': 0}\n{'a': 1}\ncheckpoint\n{'examples_iterable': {'shard_idx': 0, 'shard_example_idx': 6, 'type': 'ArrowExamplesIterable'}, 'epoch': 0}\n```\n\n\n### Expected behavior\n\nshard_example_idx should be 2 instead of 6\nIf we run with num_shards=2, then shard_example_idx is 3 instead of 2 and so on.\n\n### Environment info\n\n- `datasets` version: 3.4.1\n- Platform: macOS-14.6.1-arm64-arm-64bit\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.29.3\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nIterableDataset's state_dict shard_example_idx is always equal to the number of samples in a shard<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIt seems that when saving a data frame with a categorical column inside the size can grow exponentially.\r\n\r\nThis seems to happen because when we save the categorical data to parquet, we are saving the data + all the categories existing in the original data. This happens even when the categories are not present in the original data.\r\n\n\n### Steps to reproduce the bug\n\nTo reproduce the bug, it is enough to run this script:\r\n\r\n```\r\nimport pandas as pd\r\nimport os\r\n\r\nif __name__ == \"__main__\":\r\n    for n in [10, 1e2, 1e3, 1e4, 1e5]:\r\n        for n_col in [1, 10, 100, 1000, 10000]:\r\n            input = pd.DataFrame([{\"{i}\": f\"{i}_cat\" for col in range(n_col)} for i in range(int(n))])\r\n            input.iloc[0:100].to_parquet(\"a.parquet\")\r\n            for col in input.columns:\r\n                input[col] = input[col].astype(\"category\")\r\n            input.iloc[0:100].to_parquet(\"b.parquet\")\r\n            a_size_mb = os.stat(\"a.parquet\").st_size / (1024 * 1024)\r\n            b_size_mb = os.stat(\"b.parquet\").st_size / (1024 * 1024)\r\n            print(f\"{n} {n_col} {a_size_mb} {b_size_mb} {100*b_size_mb/a_size_mb:.2f}\")\r\n```\r\n\r\nThat produces this output:\r\n\r\n<img width=\"464\" alt=\"Screenshot 2023-10-31 at 11 25 25\" src=\"https://github.com/huggingface/datasets/assets/82567957/2b8a9284-7f9e-4c10-a006-0a27236ebd15\">\r\n\r\n\n\n### Expected behavior\n\nIn my opinion either:\r\n\r\n1. The two file should have (almost) the same size\r\n2. There should be warning telling the user that such difference in size is possible\n\n### Environment info\n\nPython 3.8.18\r\npandas==2.0.3\r\nnumpy==1.24.4<end_of_turn>\n<start_of_turn>model\nParquet size grows exponential for categorical data<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix type of code example of load imagefolder dataset<end_of_turn>\n<start_of_turn>model\nSmall typo in the code example of create imagefolder dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPin multiprocess<0.70.1 to align with dill<0.3.9.\r\n\r\nNote that multiprocess-0.70.1 requires dill-0.3.9: https://github.com/uqfoundation/multiprocess/releases/tag/0.70.17\r\n\r\nFix #7186.<end_of_turn>\n<start_of_turn>model\nPin multiprocess<0.70.1 to align with dill<0.3.9<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIn dataset.save_to_disk('/path/to/save/dataset'),\r\n\r\nadd the option to save in parquet format\r\n\r\ndataset.save_to_disk('/path/to/save/dataset', format=\"parquet\"),\r\n\r\nbecause arrow is not used for Production Big data.... (only parquet)\r\n\r\n\n\n### Motivation\n\nbecause arrow is not used for Production Big data.... (only parquet)\n\n### Your contribution\n\nI can do the testing !<end_of_turn>\n<start_of_turn>model\nAdd the option of saving in parquet instead of arrow <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nCreating a datasets cache and mounting this into, for example, a docker container, renders the data unreadable due to absolute paths written into the cache.\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Create a datasets cache by downloading some data\r\n2. Mount the dataset folder into a docker container or remote system.\r\n3. (Edit) Set `HF_HOME` or `HF_DATASET_CACHE` to point to the mounted cache.\r\n4. Attempt to access the data from within the docker container.\r\n5. An error is thrown saying no file exists at \\<absolute path to original cache location\\>\r\n\r\n### Expected behavior\r\n\r\nThe data is loaded without error\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.4.0-162-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nMounting datasets cache fails due to absolute paths.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWe should not use the parquet export when the user is passing config_kwargs\r\n\r\nI also fixed a regression that would disallow creating a custom config when a dataset has multiple predefined configs\r\n\r\nfix https://github.com/huggingface/datasets/issues/6533<end_of_turn>\n<start_of_turn>model\nFix custom configs from script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nhttps://github.com/huggingface/datasets/blob/a8a797cc92e860c8d0df71e0aa826f4d2690713e/src/datasets/load.py#L1809\r\n\r\nWhat I can to is break the `load_dataset` and use `load_datset_builder` + `download_and_prepare` instead.\n\n### Steps to reproduce the bug\n\nhttps://github.com/huggingface/datasets/blob/a8a797cc92e860c8d0df71e0aa826f4d2690713e/src/datasets/load.py#L1809\n\n### Expected behavior\n\nUsers can define the max shard size.\n\n### Environment info\n\ndatasets==2.13.1<end_of_turn>\n<start_of_turn>model\nWhy max_shard_size is not supported in load_dataset and passed to download_and_prepare<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix https://github.com/huggingface/datasets-server/issues/1812\r\n\r\nthis was causing this issue:\r\n\r\n```ipython\r\nIn [1]: from datasets import *\r\n\r\nIn [2]: inspect.get_dataset_config_names(\"aakanksha/udpos\")\r\nOut[2]: ['default']\r\n\r\nIn [3]: load_dataset_builder(\"aakanksha/udpos\").config.name\r\nOut[3]: 'en'\r\n```<end_of_turn>\n<start_of_turn>model\nCheck builder cls default config name in inspect<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis should fix the `test_py311 (windows latest, deps-latest` errors.\r\n\r\n```\r\n=========================== short test summary info ===========================\r\nERROR tests/test_search.py - AttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\r\nERROR tests/test_search.py - AttributeError: `np.float_` was removed in the NumPy 2.0 release. Use `np.float64` instead.\r\n===== 2822 passed, 54 skipped, 10 warnings, 2 errors in 373.36s (0:06:13) =====\r\nError: Process completed with exit code 1.\r\n```\r\n\r\nThe elasticsearch version used is `elasticsearch==7.9.1`, which is 4 years old and uses the removed `numpy.float_`.\r\n\r\nelasticsearch fixed this in [https://github.com/elastic/elasticsearch-py/pull/2551](https://github.com/elastic/elasticsearch-py/pull/2551) and released in 8.15.0 (August 2024) and 7.17.12 (September 2024).\r\n<end_of_turn>\n<start_of_turn>model\nfix: update elasticsearch version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nHi thanks for the library! I would like to have a huggingface Dataset, and one of its column is custom (non-serializable) Python objects. For example, a minimal code:\r\n\r\n```\r\nclass MyClass:\r\n    pass\r\n\r\ndataset = datasets.Dataset.from_list([\r\n    dict(a=MyClass(), b='hello'),\r\n])\r\n```\r\n\r\nIt gives error:\r\n\r\n```\r\nArrowInvalid: Could not convert <__main__.MyClass object at 0x7a852830d050> with type MyClass: did not recognize Python value type when inferring an Arrow data type\r\n```\r\n\r\nI guess it is because Dataset forces to convert everything into arrow format. However, is there any ways to make the scenario work? Thanks!\r\n\r\n### Motivation\r\n\r\n(see above)\r\n\r\n### Your contribution\r\n\r\nYes, I am happy to PR!\r\n\r\nCross-posted: https://discuss.huggingface.co/t/datasets-with-custom-python-objects/79050?u=fzyzcjy\r\n\r\nEDIT: possibly related https://github.com/huggingface/datasets/issues/5766<end_of_turn>\n<start_of_turn>model\n(Willing to PR) Datasets with custom python objects<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix tests using hf-internal-testing/librispeech_asr_dummy once that dataset has been converted to Parquet.\r\n\r\nFix #6997.<end_of_turn>\n<start_of_turn>model\nFix tests using hf-internal-testing/librispeech_asr_dummy<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have been struggling with this for two days, any help would be appreciated. Python 3.10\r\n\r\n```\r\nfrom setfit import SetFitModel\r\nfrom huggingface_hub import login\r\n\r\naccess_token_read = \"cccxxxccc\"\r\n\r\n# Authenticate with the Hugging Face Hub\r\nlogin(token=access_token_read)\r\n\r\n# Load the models from the Hugging Face Hub\r\ntrainer_relv = SetFitModel.from_pretrained(\"snowdere/trainer_relevance\")\r\ntrainer_trust = SetFitModel.from_pretrained(\"snowdere/trainer_trust\")\r\ntrainer_sent = SetFitModel.from_pretrained(\"snowdere/trainer_sent\")\r\ntrainer_topic = SetFitModel.from_pretrained(\"snowdere/trainer_topic\")\r\n\r\n```\r\n```\r\n\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[6], line 1\r\n----> 1 from setfit import SetFitModel\r\n      2 from huggingface_hub import login\r\n      4 access_token_read = \"ccsddsds\"\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/setfit/__init__.py:7\r\n      4 import os\r\n      5 import warnings\r\n----> 7 from .data import get_templated_dataset, sample_dataset\r\n      8 from .model_card import SetFitModelCardData\r\n      9 from .modeling import SetFitHead, SetFitModel\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/setfit/data.py:5\r\n      3 import pandas as pd\r\n      4 import torch\r\n----> 5 from datasets import Dataset, DatasetDict, load_dataset\r\n      6 from torch.utils.data import Dataset as TorchDataset\r\n      8 from . import logging\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/__init__.py:18\r\n      1 # ruff: noqa\r\n      2 # Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\r\n      3 #\r\n   (...)\r\n     13 # See the License for the specific language governing permissions and\r\n     14 # limitations under the License.\r\n     16 __version__ = \"2.19.0\"\r\n---> 18 from .arrow_dataset import Dataset\r\n     19 from .arrow_reader import ReadInstruction\r\n     20 from .builder import ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:76\r\n     73 from tqdm.contrib.concurrent import thread_map\r\n     75 from . import config\r\n---> 76 from .arrow_reader import ArrowReader\r\n     77 from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n     78 from .data_files import sanitize_patterns\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_reader.py:29\r\n     26 from typing import TYPE_CHECKING, List, Optional, Union\r\n     28 import pyarrow as pa\r\n---> 29 import pyarrow.parquet as pq\r\n     30 from tqdm.contrib.concurrent import thread_map\r\n     32 from .download.download_config import DownloadConfig\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/parquet/__init__.py:20\r\n      1 # Licensed to the Apache Software Foundation (ASF) under one\r\n      2 # or more contributor license agreements.  See the NOTICE file\r\n      3 # distributed with this work for additional information\r\n   (...)\r\n     17 \r\n     18 # flake8: noqa\r\n---> 20 from .core import *\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/parquet/core.py:33\r\n     30 import pyarrow as pa\r\n     32 try:\r\n---> 33     import pyarrow._parquet as _parquet\r\n     34 except ImportError as exc:\r\n     35     raise ImportError(\r\n     36         \"The pyarrow installation is not built with support \"\r\n     37         f\"for the Parquet file format ({str(exc)})\"\r\n     38     ) from None\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/pyarrow/_parquet.pyx:1, in init pyarrow._parquet()\r\n\r\nAttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'\r\n```\r\n\r\nsetfit: 1.0.3\r\ntransformers: 4.41.2\r\nlingua-language-detector: 2.0.2\r\npolars: 0.20.31\r\nlightning: None\r\ngoogle-cloud-bigquery: 3.24.0\r\nshapely: 2.0.4\r\npyarrow: 16.0.0\n\n### Steps to reproduce the bug\n\nI have tried all version combinations for Dataset and Pyarrow, the all have the same error since a few days ago. This is accross multiple scripts I have. \n\n### Expected behavior\n\nJust ron normally. \n\n### Environment info\n\n3.10<end_of_turn>\n<start_of_turn>model\nAttributeError: module 'pyarrow.lib' has no attribute 'ListViewType'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ndataset: https://huggingface.co/datasets/NortheasternUniversity/big_patent\n\nWhen I try to load it with the datasets library, it fails with:\n\nRuntimeError: Dataset scripts are no longer supported, but found big_patent.py\n\n\nCould you please publish a Parquet/Arrow export of BIGPATENT on the Hugging Face so that it can be accessed with datasets>=4.x.\n\n<end_of_turn>\n<start_of_turn>model\nBIGPATENT dataset inaccessible (deprecated script loader)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen resuming from a checkpoint, `IterableDataset` will drop samples if `num_shards % world_size == 0` and the underlying example supports `iter_arrow` and needs to be formatted. \n\nIn that case, the `FormattedExamplesIterable` fetches a batch of samples from the child iterable's `iter_arrow` and yields them one by one (after formatting). However, the child increments the `shard_example_idx` counter (in its `iter_arrow`) before returning the batch for the whole batch size, which leads to a portion of samples being skipped if the iteration (of the parent iterable) is stopped mid-batch. \n\nPerhaps one way to avoid this would be by signalling the child iterable which samples (within the chunk) are processed by the parent and which are not, so that it can adjust the `shard_example_idx` counter accordingly. This would also mean the chunk needs to be sliced when resuming, but this is straightforward to implement.\n\nThe following is a minimal reproducer of the bug:\n```python\nfrom datasets import Dataset\nfrom datasets.distributed import split_dataset_by_node\n\nds = Dataset.from_dict({\"n\": list(range(24))})\nds = ds.to_iterable_dataset(num_shards=4)\n\nworld_size = 4\nrank = 0    \nds_rank = split_dataset_by_node(ds, rank, world_size)\n\nit = iter(ds_rank)\nexamples = []\nfor idx, example in enumerate(it):\n    examples.append(example)\n    if idx == 2:\n        state_dict = ds_rank.state_dict()\n        break\n\nds_rank.load_state_dict(state_dict)\nit_resumed = iter(ds_rank)\nexamples_resumed = examples[:]\n\nfor example in it:\n    examples.append(example)\n\nfor example in it_resumed:\n    examples_resumed.append(example)\n\nprint(\"ORIGINAL ITER EXAMPLES:\", examples)\nprint(\"RESUMED ITER EXAMPLES:\", examples_resumed)\n```<end_of_turn>\n<start_of_turn>model\n`IterableDataset` drops samples when resuming from a checkpoint<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nAs of this morning (PST) 2/1/2024, seeing the wmt16 dataset is missing from opus , could you suggest an alternative?\r\n\r\n```\r\nDownloading data files:   0%|                                                                                                                     | 0/4 [00:00<?, ?it/s]Traceback (most recent call last):\r\n  File \"test.py\", line 2, in <module>\r\n    raw_datasets = load_dataset(\"wmt16\",\"ro-en\",split=\"train\")\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/load.py\", line 2153, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/builder.py\", line 1717, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/builder.py\", line 1027, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/root/.cache/huggingface/modules/datasets_modules/datasets/wmt16/746749a11d25c02058042da7502d973ff410e73457f3d305fc1177dc0e8c4227/wmt_utils.py\", line 754, in _split_generators\r\n    downloaded_files = dl_manager.download_and_extract(urls_to_download)\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/download/download_manager.py\", line 565, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/download/download_manager.py\", line 428, in download\r\n    downloaded_path_or_paths = map_nested(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/py_utils.py\", line 464, in map_nested\r\n    mapped = [\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/py_utils.py\", line 465, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/py_utils.py\", line 384, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/py_utils.py\", line 384, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/py_utils.py\", line 367, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/download/download_manager.py\", line 454, in _download\r\n    return cached_path(url_or_filename, download_config=download_config)\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/file_utils.py\", line 182, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/usr/local/lib/python3.8/dist-packages/datasets/utils/file_utils.py\", line 596, in get_from_cache\r\n    raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\nFileNotFoundError: Couldn't find file at https://opus.nlpl.eu/download.php?f=SETIMES/v2/tmx/en-ro.tmx.gz\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\nraw_datasets = load_dataset(\"wmt16\",\"ro-en\",split=\"train\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpect the dataset to be downloaded/ at least a clean exit with error explaining dataset is missing and a suggestion for next steps\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.7\r\n- Platform: Linux-5.15.0-92-generic-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.0.1\r\n<end_of_turn>\n<start_of_turn>model\nCannot download wmt16 dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTrying to run the following fine-tuning script (based on this page [here](https://github.com/huggingface/instruction-tuned-sd)):\n\n```\n! accelerate launch /content/instruction-tuned-sd/finetune_instruct_pix2pix.py \\\n    --pretrained_model_name_or_path=${MODEL_ID} \\\n    --dataset_name=${DATASET_NAME} \\\n    --use_ema \\\n    --enable_xformers_memory_efficient_attention \\\n    --resolution=512 --random_flip \\\n    --train_batch_size=2 --gradient_accumulation_steps=4 --gradient_checkpointing \\\n    --max_train_steps=500 \\\n    --checkpointing_steps=25 --checkpoints_total_limit=1 \\\n    --learning_rate=5e-05 --max_grad_norm=1 --lr_warmup_steps=20 \\\n    --conditioning_dropout_prob=0.1 \\\n    --mixed_precision=fp16 \\\n    --seed=42 \\\n    --output_dir=${OUTPUT_DIR} \\\n    --original_image_column=before \\\n    --edit_prompt=prompt \\\n    --edited_image=after\n```\n\nbut I keep getting the following error:\n\n```\nTraceback (most recent call last):\n  File \"/content/instruction-tuned-sd/finetune_instruct_pix2pix.py\", line 1137, in <module>\n    main()\n  File \"/content/instruction-tuned-sd/finetune_instruct_pix2pix.py\", line 652, in main\n    dataset = load_dataset(\n              ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 2129, in load_dataset\n    builder_instance = load_dataset_builder(\n                       ^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/load.py\", line 1886, in load_dataset_builder\n    builder_instance: DatasetBuilder = builder_cls(\n                                       ^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 342, in __init__\n    self.config, self.config_id = self._create_builder_config(\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/builder.py\", line 590, in _create_builder_config\n    raise ValueError(f\"BuilderConfig {builder_config} doesn't have a '{key}' key.\")\nValueError: BuilderConfig ParquetConfig(name='default', version=0.0.0, data_dir=None, data_files={'train': ['data/train-*']}, description=None, batch_size=None, columns=None, features=None, filters=None) doesn't have a 'use_auth_token' key.\nTraceback (most recent call last):\n  File \"/usr/local/bin/accelerate\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n```\n\nAny ideas? `datasets` version should be `3.2.0`.\n\n### Steps to reproduce the bug\n\nJust running the script above.\n\n### Expected behavior\n\nNo errors\n\n### Environment info\n\nPython 3.11.11\n\ndatasets==3.2.0<end_of_turn>\n<start_of_turn>model\nBuilderConfig ParquetConfig(...) doesn't have a 'use_auth_token' key.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI tried to add input_ids to dataset with map(), and I used the return_tensors='pt', but why I got the callback with the type of List\uff1f\r\n![image](https://github.com/user-attachments/assets/ab046e20-2174-4e91-9cd6-4a296a43e83c)\r\n\n\n### Steps to reproduce the bug\n\n![image](https://github.com/user-attachments/assets/5d504d4c-22c7-4742-99a1-9cab78739b17)\n\n### Expected behavior\n\nSorry for this silly question, I'm noob on using this tool. But I think it should return a tensor value as I have used the protocol\uff1f\r\nWhen I tokenize only one sentence using tokenized_input=tokenizer(input, return_tensors='pt' )\uff0cit does return in tensor type. Why doesn't it work in map()\uff1f\n\n### Environment info\n\ntransformers>=4.41.2,<=4.45.0\r\ndatasets>=2.16.0,<=2.21.0\r\naccelerate>=0.30.1,<=0.34.2\r\npeft>=0.11.1,<=0.12.0\r\ntrl>=0.8.6,<=0.9.6\r\ngradio>=4.0.0\r\npandas>=2.0.0\r\nscipy\r\neinops\r\nsentencepiece\r\ntiktoken\r\nprotobuf\r\nuvicorn\r\npydantic\r\nfastapi\r\nsse-starlette\r\nmatplotlib>=3.7.0\r\nfire\r\npackaging\r\npyyaml\r\nnumpy<2.0.0\r\n<end_of_turn>\n<start_of_turn>model\nWhy return_tensors='pt' doesn't work\uff1f<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using [`Dataset.from_generator`](https://github.com/huggingface/datasets/blob/c9c1166e1cf81d38534020f9c167b326585339e5/src/datasets/arrow_dataset.py#L1072) with `streaming=False`, the internal logic will call [`download_and_prepare`](https://github.com/huggingface/datasets/blob/main/src/datasets/io/generator.py#L47) which will attempt to download from HF GCS which is redundant, because user has already provided the generator from which the data should be drawn. \r\n\r\nIf someone attempts to call `Dataset.from_generator` from an environment that doesn't have external internet access (for example internal production machine) and doesn't set `HF_DATASETS_OFFLINE=1`, this will result in process being stuck at building connection.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\ndef gen():\r\n    for _ in range(100):\r\n        yield {\"text\": \"dummy text\"}\r\n\r\ndataset = datasets.Dataset.from_generator(gen)\r\n```\r\n\r\nA minimum example executed on any environment that doesn't have access to HF GCS can result in the error\n\n### Expected behavior\n\n`try_from_hf_gcs` should be set to False here https://github.com/huggingface/datasets/blob/c9c1166e1cf81d38534020f9c167b326585339e5/src/datasets/io/generator.py#L51\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-3.10.0-1160.90.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\n`Dataset.from_generator` should not try to download from HF GCS<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrent multiprocessing for download/extract is not done nestedly. For example, when processing SlimPajama, there is only 3 processes (for train/test/val), while there are many files inside these 3 folders\r\n\r\n```\r\nDownloading data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]\r\nDownloading data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]\r\nDownloading data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]\r\n\r\nExtracting data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]\r\nExtracting data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]\u001b[A\r\nExtracting data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]\u001b[A\u001b[A\r\n```\n\n### Motivation\n\nspeedup dataset loading\n\n### Your contribution\n\nI can help test the feature<end_of_turn>\n<start_of_turn>model\nSupport multiprocessing for download/extract nestedly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nnm<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.2.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6691<end_of_turn>\n<start_of_turn>model\nEnhancement: Enable loading TSV files in load_dataset()<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nValueError: Invalid pattern: '**' can only be an entire path component\r\nwhen loading any dataset\r\n\n\n### Steps to reproduce the bug\n\nimport datasets\r\nds = datasets.load_dataset(\"TokenBender/code_instructions_122k_alpaca_style\")\n\n### Expected behavior\n\nloading the dataset successfully \n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Windows-10-10.0.22631-SP0\r\n- Python version: 3.11.7\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\nInvalid pattern: '**' can only be an entire path component<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd a command-line option, e.g. `--auto-merge-pull-request` that enables automatic merging of the commits created by the `convert-to-parquet` tool.\n\n### Motivation\n\nLarge datasets may result in dozens of PRs due to the splitting mechanism. Each of these has to be manually accepted via the website.\n\n### Your contribution\n\nHappy to look into submitting a PR if this is of interest to maintainers.<end_of_turn>\n<start_of_turn>model\nAuto-merge option for `convert-to-parquet`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate load_hub.mdx<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix typo in `Dataset.map` docstring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nPerforming a `.map` method on a dataset loses it's config name / split name only if run with multiproc\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Audio, load_dataset\r\nfrom transformers import AutoFeatureExtractor\r\nimport numpy as np\r\n\r\n# load dummy dataset\r\nlibri = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\")\r\n\r\n# make train / test splits\r\nlibri = libri[\"validation\"].train_test_split(seed=42, shuffle=True, test_size=0.1)\r\n\r\n# example feature extractor\r\nmodel_id = \"ntu-spml/distilhubert\"\r\nfeature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True, return_attention_mask=True)\r\n\r\nsampling_rate = feature_extractor.sampling_rate\r\n\r\nlibri = libri.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\r\n\r\nmax_duration = 30.0\r\n\r\ndef preprocess_function(examples):\r\n    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\r\n    inputs = feature_extractor(\r\n        audio_arrays,\r\n        sampling_rate=feature_extractor.sampling_rate,\r\n        max_length=int(feature_extractor.sampling_rate * max_duration),\r\n        truncation=True,\r\n        return_attention_mask=True,\r\n    )\r\n    return inputs\r\n\r\n# single proc map\r\nlibri_encoded = libri.map(\r\n    preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True, num_proc=1\r\n)\r\n\r\nprint(10 * \"=\" ,\"Single processing\", 10 * \"=\")\r\nprint(\"Config name before: \", libri[\"train\"].config_name, \" Split name before: \", libri[\"train\"].split)\r\nprint(\"Config name after: \", libri_encoded[\"train\"].config_name, \" Split name after: \", libri_encoded[\"train\"].split)\r\n\r\n# multi proc map\r\nlibri_encoded = libri.map(\r\n    preprocess_function, remove_columns=[\"audio\", \"file\"], batched=True, num_proc=2\r\n)\r\n\r\nprint(10 * \"=\" ,\"Multi processing\", 10 * \"=\")\r\nprint(\"Config name before: \", libri[\"train\"].config_name, \" Split name before: \", libri[\"train\"].split)\r\nprint(\"Config name after: \", libri_encoded[\"train\"].config_name, \" Split name after: \", libri_encoded[\"train\"].split)\r\n```\r\n\r\n**Print Output:**\r\n```\r\n========== Single processing ==========\r\nConfig name before:  clean  Split name before:  validation\r\nConfig name after:  clean  Split name after:  validation\r\n========== Multi processing ==========\r\nConfig name before:  clean  Split name before:  validation\r\nConfig name after:  None  Split name after:  None\r\n```\r\n\r\n=> we can see that the config/split names are lost in the multiprocessing setting\r\n\r\n\r\n\n\n### Expected behavior\n\nShould retain both config / split names in the multiproc setting\n\n### Environment info\n\n- `datasets` version: 2.13.1.dev0\r\n- Platform: Linux-5.15.0-67-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2<end_of_turn>\n<start_of_turn>model\nConfig name / split name lost after map with multiproc<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove dead code for non-dict data_files from packaged modules.\r\n\r\nSince the merge of this PR:\r\n- #2986\r\n\r\nthe builders' variable self.config.data_files is always a dict, which makes the condition on (str, list, tuple) dead code.<end_of_turn>\n<start_of_turn>model\nRemove dead code for non-dict data_files from packaged modules<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nAllow setting file lock type, maybe from an environment variable\r\n\r\nCurrently, it only depends on whether fnctl is available:\r\n\r\nhttps://github.com/huggingface/datasets/blob/12ebe695b4748c5a26e08b44ed51955f74f5801d/src/datasets/utils/filelock.py#L463-L470C16\r\n\r\n### Motivation\r\n\r\nIn my environment, flock isn't supported on a network attached drive\r\n\r\n### Your contribution\r\n\r\nI'll be happy to submit a pr.<end_of_turn>\n<start_of_turn>model\nAdd ability to set lock type<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nRegardless of how many cores I used, I have 16 or 32 threads, map slows down to a crawl at around 80% done, lingers maybe until 97% extremely slowly and NEVER finishes the job. It just hangs.\r\n\r\nAfter watching this for 27 hours I control-C out of it. Until the end one process appears to be doing something, but it never ends.\r\n\r\nI saw some comments about fast tokenizers using Rust and all and tried different variations. NOTHING works.\r\n\r\n\n\n### Steps to reproduce the bug\n\nRunning it without breaking the dataset into parts results in the same behavior. The loop was an attempt to see if this was a RAM issue. \r\n\r\nfor idx in range(100):\r\n    dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir=cache_dir, split=f'train[{idx}%:{idx+1}%]')\r\n    dataset = dataset.map(partial(tokenize_fn, tokenizer), batched=False, num_proc=1, remove_columns=[\"text\", \"meta\"])\r\n    dataset.save_to_disk(training_args.cache_dir + f\"/training_data_{idx}\")\r\n\n\n### Expected behavior\n\nI expect map to run at more or less the same speed it starts with and FINISH its processing. \n\n### Environment info\n\nPython 3.8, same with 3.10 makes no difference.\r\nUbuntu 20.04, <end_of_turn>\n<start_of_turn>model\nDatasets.map is severely broken<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen trying to load an image dataset in an old Python environment (with Pillow-8.4.0), an error is raised:\r\n```Python traceback\r\nAttributeError: module 'PIL.Image' has no attribute 'ExifTags'\r\n```\r\nThe error traceback:\r\n```Python traceback\r\n~/huggingface/datasets/src/datasets/iterable_dataset.py in __iter__(self)\r\n   1391                 # `IterableDataset` automatically fills missing columns with None.\r\n   1392                 # This is done with `_apply_feature_types_on_example`.\r\n-> 1393                 example = _apply_feature_types_on_example(\r\n   1394                     example, self.features, token_per_repo_id=self._token_per_repo_id\r\n   1395                 )\r\n\r\n~/huggingface/datasets/src/datasets/iterable_dataset.py in _apply_feature_types_on_example(example, features, token_per_repo_id)\r\n   1080     encoded_example = features.encode_example(example)\r\n   1081     # Decode example for Audio feature, e.g.\r\n-> 1082     decoded_example = features.decode_example(encoded_example, token_per_repo_id=token_per_repo_id)\r\n   1083     return decoded_example\r\n   1084 \r\n\r\n~/huggingface/datasets/src/datasets/features/features.py in decode_example(self, example, token_per_repo_id)\r\n   1974 \r\n-> 1975         return {\r\n   1976             column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1977             if self._column_requires_decoding[column_name]\r\n\r\n~/huggingface/datasets/src/datasets/features/features.py in <dictcomp>(.0)\r\n   1974 \r\n   1975         return {\r\n-> 1976             column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1977             if self._column_requires_decoding[column_name]\r\n   1978             else value\r\n\r\n~/huggingface/datasets/src/datasets/features/features.py in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1339         # we pass the token to read and decode files from private repositories in streaming mode\r\n   1340         if obj is not None and schema.decode:\r\n-> 1341             return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1342     return obj\r\n   1343 \r\n\r\n~/huggingface/datasets/src/datasets/features/image.py in decode_example(self, value, token_per_repo_id)\r\n    187             image = PIL.Image.open(BytesIO(bytes_))\r\n    188         image.load()  # to avoid \"Too many open files\" errors\r\n--> 189         if image.getexif().get(PIL.Image.ExifTags.Base.Orientation) is not None:\r\n    190             image = PIL.ImageOps.exif_transpose(image)\r\n    191         if self.mode and self.mode != image.mode:\r\n\r\n~/huggingface/datasets/venv/lib/python3.9/site-packages/PIL/Image.py in __getattr__(name)\r\n     75                 )\r\n     76                 return categories[name]\r\n---> 77         raise AttributeError(f\"module '{__name__}' has no attribute '{name}'\")\r\n     78 \r\n     79 \r\n\r\nAttributeError: module 'PIL.Image' has no attribute 'ExifTags'\r\n```\r\n\r\n### Environment info\r\n\r\nSince datasets 2.19.0<end_of_turn>\n<start_of_turn>model\nAttributeError: module 'PIL.Image' has no attribute 'ExifTags'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDeprecate `Dataset.export` that generates a TFRecord file from a dataset as this method is undocumented, and the usage seems low. Users should use [TFRecordWriter](https://www.tensorflow.org/api_docs/python/tf/io/TFRecordWriter#write) or the official [TFRecord](https://www.tensorflow.org/tutorials/load_data/tfrecord) tutorial (on which this method is based) to write TFRecord files instead.<end_of_turn>\n<start_of_turn>model\nDeprecate `Dataset.export`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I try to load the opus_book from hugging face (following the [guide on the website](https://huggingface.co/docs/transformers/main/en/tasks/translation))\r\n\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\n\r\ndataset = load_dataset(\"Helsinki-NLP/opus_books\", \"en-fr\", features=[\"id\", \"translation\"])\r\n```\r\n\r\nI get an error:\r\n`PermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:/Users/Me/.cache/huggingface/datasets/Helsinki-NLP___parquet/ca-de-a39f1ef185b9b73b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec.incomplete\\\\parquet-train-00000-00000-of-NNNNN.arrow'\r\n`\r\n\r\n\r\n\r\n<details><summary>Full stacktrace</summary>\r\n<p>\r\n\r\n```python\r\n\r\nAttributeError                            Traceback (most recent call last)\r\nFile c:\\Users\\Me\\.conda\\envs\\ia\\lib\\site-packages\\datasets\\builder.py:1858, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   [1857](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/builder.py:1857) _time = time.time()\r\n-> [1858](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/builder.py:1858) for _, table in generator:\r\n   [1859](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/builder.py:1859)     if max_shard_size is not None and writer._num_bytes > max_shard_size:\r\n\r\nFile c:\\Users\\Me\\.conda\\envs\\ia\\lib\\site-packages\\datasets\\packaged_modules\\parquet\\parquet.py:59, in Parquet._generate_tables(self, files)\r\n     [58](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/packaged_modules/parquet/parquet.py:58) def _generate_tables(self, files):\r\n---> [59](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/packaged_modules/parquet/parquet.py:59)     schema = self.config.features.arrow_schema if self.config.features is not None else None\r\n     [60](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/packaged_modules/parquet/parquet.py:60)     if self.config.features is not None and self.config.columns is not None:\r\n\r\nAttributeError: 'list' object has no attribute 'arrow_schema'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nAttributeError                            Traceback (most recent call last)\r\nFile c:\\Users\\Me\\.conda\\envs\\ia\\lib\\site-packages\\datasets\\builder.py:1882, in ArrowBasedBuilder._prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\r\n   [1881](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/builder.py:1881) num_shards = shard_id + 1\r\n-> [1882](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/builder.py:1882) num_examples, num_bytes = writer.finalize()\r\n   [1883](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/builder.py:1883) writer.close()\r\n\r\nFile c:\\Users\\Me\\.conda\\envs\\ia\\lib\\site-packages\\datasets\\arrow_writer.py:584, in ArrowWriter.finalize(self, close_stream)\r\n    [583](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/arrow_writer.py:583) # If schema is known, infer features even if no examples were written\r\n--> [584](file:///C:/Users/Me/.conda/envs/ia/lib/site-packages/datasets/arrow_writer.py:584) if self.pa_writer is None and self.schema:\r\n...\r\n--> [627](file:///C:/Users/Me/.conda/envs/ia/lib/shutil.py:627)         os.unlink(fullname)\r\n    [628](file:///C:/Users/Me/.conda/envs/ia/lib/shutil.py:628)     except OSError:\r\n    [629](file:///C:/Users/Me/.conda/envs/ia/lib/shutil.py:629)         onerror(os.unlink, fullname, sys.exc_info())\r\n\r\nPermissionError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:/Users/Me/.cache/huggingface/datasets/Helsinki-NLP___parquet/ca-de-a39f1ef185b9b73b/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec.incomplete\\\\parquet-train-00000-00000-of-NNNNN.arrow'\r\n``` \r\n\r\n\r\n</p>\r\n</details> \n\n### Steps to reproduce the bug\n\nSteps to reproduce:\r\n\r\nJust execute these lines\r\n\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\n\r\ndataset = load_dataset(\"Helsinki-NLP/opus_books\", \"en-fr\", features=[\"id\", \"translation\"])\r\n```\r\n\n\n### Expected behavior\n\nI expect the dataset to be loaded without any errors.\n\n### Environment info\n\n| Package| Version|\r\n|--------|--------|\r\n| transformers| 4.37.2|\r\n| python| 3.9.19|\r\n| pytorch| 2.3.0|\r\n| datasets|2.12.0 | \r\n| arrow  |  1.2.3| \r\n                 \r\nI am using Conda on Windows 11.<end_of_turn>\n<start_of_turn>model\nWinError 32 The process cannot access the file during load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nEncounter pyarrow.lib.arrowinvalid error with map function in some example when loading the dataset\n\n### Steps to reproduce the bug\n\n```\nfrom datasets import load_dataset\nfrom PIL import Image, PngImagePlugin\n\ndataset = load_dataset(\"leonardPKU/GEOQA_R1V_Train_8K\")\nsystem_prompt=\"You are a helpful AI Assistant\"\ndef make_conversation(example):\n    prompt = []\n\n    prompt.append({\"role\": \"system\", \"content\": system_prompt})\n    prompt.append(\n            {\n                \"role\": \"user\", \n                \"content\": [\n                        {\"type\": \"image\"},\n                        {\"type\": \"text\", \"text\": example[\"problem\"]},\n                        ]\n            }\n        )\n    return {\"prompt\": prompt}\n\ndef check_data_types(example):\n    for key, value in example.items():\n        if key == 'image':\n            if not isinstance(value, PngImagePlugin.PngImageFile):\n                print(value)\n        if key == \"problem\" or key == \"solution\":\n            if not isinstance(value, str):\n                print(value)\n\n    return example\n\ndataset = dataset.map(check_data_types)\ndataset = dataset.map(make_conversation)\n```\n\n### Expected behavior\n\nSuccessfully process the dataset with map\n\n### Environment info\n\ndatasets==3.3.1<end_of_turn>\n<start_of_turn>model\npyarrow.lib.arrowinvalid: cannot mix list and non-list, non-null values with map function<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix length for ci<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHi,\r\n\r\nI have been trying to import the datasets library but I keep gtting this error.\r\n\r\n`Traceback (most recent call last):\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/IPython/core/interactiveshell.py:3508 in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n\r\n  Cell In[2], line 1\r\n    import datasets\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/datasets/__init__.py:22\r\n    from .arrow_dataset import Dataset\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/datasets/arrow_dataset.py:67\r\n    from .arrow_writer import ArrowWriter, OptimizedTypedSequence\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/datasets/arrow_writer.py:27\r\n    from .features import Features, Image, Value\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/datasets/features/__init__.py:17\r\n    from .audio import Audio\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/datasets/features/audio.py:11\r\n    from ..download.streaming_download_manager import xopen, xsplitext\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/datasets/download/__init__.py:10\r\n    from .streaming_download_manager import StreamingDownloadManager\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/datasets/download/streaming_download_manager.py:18\r\n    from aiohttp.client_exceptions import ClientError\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/aiohttp/__init__.py:7\r\n    from .connector import *  # noqa\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/aiohttp/connector.py:12\r\n    from .client import ClientRequest\r\n\r\n  File /opt/local/jupyterhub/lib64/python3.9/site-packages/aiohttp/client.py:144\r\n    yield from asyncio.async(resp.release(), loop=loop)\r\n                       ^\r\nSyntaxError: invalid syntax`\r\n\r\nI have simply used these commands:\r\n`import datasets`\r\nand\r\n`from datasets import load_dataset`\r\n\r\n### Environment info\r\n\r\nThe library has been installed a virtual machine on JupyterHub. Although I have used this library multiple times (on the same VM) before, to train/test an ASR or other ML models, I had never encountered this error. <end_of_turn>\n<start_of_turn>model\n'import datasets' throws \"invalid syntax error\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUsing PyVista inside a .map() produces a crash with `objc[78796]: +[NSResponder initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.`\n\n### Steps to reproduce the bug\n\nRun \n\n```python\nimport numpy as np\nimport pyvista as pv\nimport datasets\n\ndata = [{\"coords\": np.random.rand(5, 3)} for _ in range(3)]\n\n\ndef render_point(example):\n  plotter = pv.Plotter(off_screen=True)\n  cloud = pv.PolyData(example[\"coords\"])\n  plotter.add_mesh(cloud)\n  img = plotter.screenshot(return_img=True)\n  return {\"image\": img}\n\n\n# breaks if num_proc>1\nds = datasets.Dataset.from_list(data).map(render_point, num_proc=2)\n\n```\n\n### Expected behavior\n\nIt should work. Just like when I use a process pool to make it work.\n\n```python\nimport numpy as np\nimport pyvista as pv\nimport multiprocessing\n\ndata = [{\"coords\": np.random.rand(5, 3)} for _ in range(3)]\n\n\ndef render_point(example):\n  plotter = pv.Plotter(off_screen=True)\n  cloud = pv.PolyData(example[\"coords\"])\n  plotter.add_mesh(cloud)\n  img = plotter.screenshot(return_img=True)\n  return {\"image\": img}\n\n\nif __name__ == \"__main__\":\n  with multiprocessing.Pool(processes=2) as pool:\n    results = pool.map(render_point, data)\n  print(results[0][\"image\"].shape)\n```\n\n### Environment info\n\n\n- `datasets` version: 3.3.2\n- Platform: macOS-15.3.2-arm64-arm-64bit\n- Python version: 3.11.10\n- `huggingface_hub` version: 0.28.1\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.10.0\n\nAnd then I suppose pyvista info is good to have.\n\n```python\nimport pyvista as pv\nprint(pv.Report())\n```\ngives \n--------------------------------------------------------------------------------\n  Date: Mon Apr 14 21:42:08 2025 CEST\n\n                OS : Darwin (macOS 15.3.2)\n            CPU(s) : 10\n           Machine : arm64\n      Architecture : 64bit\n               RAM : 32.0 GiB\n       Environment : IPython\n       File system : apfs\n        GPU Vendor : Apple\n      GPU Renderer : Apple M1 Max\n       GPU Version : 4.1 Metal - 89.3\n  MathText Support : True\n\n  Python 3.11.10 (main, Oct  7 2024, 23:25:27) [Clang 18.1.8 ]\n\n           pyvista : 0.44.2\n               vtk : 9.4.0\n             numpy : 2.2.2\n        matplotlib : 3.10.0\n            scooby : 0.10.0\n             pooch : 1.8.2\n            pillow : 11.1.0\n           imageio : 2.36.1\n             PyQt5 : 5.15.11\n           IPython : 8.30.0\n             scipy : 1.14.1\n              tqdm : 4.67.1\n        jupyterlab : 4.3.5\n      nest_asyncio : 1.6.0\n--------------------------------------------------------------------------------<end_of_turn>\n<start_of_turn>model\n.map() fails if function uses pyvista<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nClose #6333.<end_of_turn>\n<start_of_turn>model\nunpin-fsspec<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDeprecate `DownloadManager.download_custom`. Users should use `fsspec` URLs (cacheable) or make direct requests with `fsspec`/`requests` (not cacheable) instead.\r\n\r\nWe should deprecate this method as it's not compatible with streaming, and implementing the streaming version of it is hard/impossible. There have been requests to implement the streaming version of this method on the forum, but the reason for this seems to be a tip in the docs that \"promotes\" this method (this PR removes it). \r\n\r\n<end_of_turn>\n<start_of_turn>model\nDeprecate `download_custom`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Task\r\nSupport bytes-like objects (bytes and bytearray) in Features classes\r\n\r\n### Description\r\nThe `Features` classes only accept `bytes` objects for binary data, but not `bytearray`. This leads to errors when using `IterableDataset.from_spark()` with Spark DataFrames as they contain `bytearray` objects, even though both `bytes` and `bytearray` are valid [*bytes-like objects* in Python](https://docs.python.org/3/glossary.html#term-bytes-like-object).\r\n\r\n### Changes\r\n- Updated `Features` classes to accept both `bytes` and `bytearray` types for binary data fields.\r\n\r\n### Reasoning\r\n- `bytes` and `bytearray` serve the same purpose for binary data, with the only difference being mutability.\r\n- Modifying the Spark iterator to convert `bytearray` to `bytes` would be a workaround, not a true fix. I think the correct solution is to accept all bytes-like objects as input.\r\n- This approach is more robust and future-proof since Python 3.12+ provides a [standard way to check for buffer protocol](https://docs.python.org/3/c-api/buffer.html#bufferobjects).\r\n\r\n### Testing\r\n- Added tests to cover `bytearray` inputs for image features.\r\n\r\n### Related Issues\r\n- Fixes: #7517 <end_of_turn>\n<start_of_turn>model\nfix: Image Feature in Datasets Library Fails to Handle bytearray Objects from Spark DataFrames (#7517)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRe-enable import sorting that was wrongly disabled by `flake8: noqa` directive after switching to `ruff` linter in datasets-2.10.0 PR:\r\n- #5519\r\n\r\nNote that after the linter switch, we wrongly replaced `flake8: noqa` with `ruff: noqa` in datasets-2.17.0 PR:\r\n- #6619\r\n\r\nThat replacement was wrong because we kept the `isort: skip` directives although they were indeed disabled by `flake8: noqa` first and by `ruff: noqa` afterwards. See for example `__init__.py` file after the linter switch:\r\n- We kept the `flake8: noqa` directive\r\nhttps://github.com/huggingface/datasets/blob/06ae3f678651bfbb3ca7dd3274ee2f38e0e0237e/src/datasets/__init__.py#L1\r\n- Whereas we also kept the `isort: skip` directives (that were disabled)\r\nhttps://github.com/huggingface/datasets/blob/06ae3f678651bfbb3ca7dd3274ee2f38e0e0237e/src/datasets/__init__.py#L82-L84\r\n\r\nFix #6942.<end_of_turn>\n<start_of_turn>model\nRe-enable import sorting disabled by flake8:noqa directive when using ruff linter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nnetCDF (*.nc) is a file format for storing multidimensional scientific data, which is used by packages like `xarray` (labelled multi-dimensional arrays in Python). It would be nice to have native support for netCDF in `datasets`.\n\n### Motivation\n\nWhen uploading *.nc files onto Huggingface Hub through the `datasets` API, I would like to be able to preview the dataset without converting it to another format.\n\n### Your contribution\n\nI can submit a PR, provided I have the time.<end_of_turn>\n<start_of_turn>model\nAdding support for netCDF (*.nc) files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I attempt to download a model from the Hub that is gated without being logged in, I get a nice error message. E.g.:\r\n\r\nE.g.\r\n```sh\r\nRepository Not Found for url: https://huggingface.co/api/models/DeepFloyd/IF-I-XL-v1.0.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\nInvalid username or password..\r\nWill try to load from local cache.\r\n```\r\n\r\nIf I do the same for a gated dataset on the Hub, I'm not gated a nice error message IMO:\r\n\r\n```sh\r\nFile ~/hf/lib/python3.10/site-packages/fsspec/implementations/http.py:430, in HTTPFileSystem._info(self, url, **kwargs)\r\n    427     except Exception as exc:\r\n    428         if policy == \"get\":\r\n    429             # If get failed, then raise a FileNotFoundError\r\n--> 430             raise FileNotFoundError(url) from exc\r\n    431         logger.debug(str(exc))\r\n    433 return {\"name\": url, \"size\": None, **info, \"type\": \"file\"}\r\n\r\nFileNotFoundError: https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0/resolve/main/n_shards.json\r\n```\n\n### Steps to reproduce the bug\n\n```\r\nhuggingface-cli logout\r\n```\r\n\r\nand then:\r\n\r\n```py\r\nfrom datasets import load_dataset, Audio\r\n\r\n# English\r\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", split=\"test\", streaming=True)\r\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\r\nen_sample = next(iter(stream_data))[\"audio\"][\"array\"]\r\n\r\n# Swahili\r\nstream_data = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"sw\", split=\"test\", streaming=True)\r\nstream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\r\nsw_sample = next(iter(stream_data))[\"audio\"][\"array\"]\r\n```\n\n### Expected behavior\n\nBetter error message\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.16.0.dev0\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n<end_of_turn>\n<start_of_turn>model\nBad error message when trying to download gated dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsee [internal discussion](https://huggingface.slack.com/archives/C02EK7C3SHW/p1703097195609209)\r\n\r\nDraft to get some feedback on a possible `BBox` feature type that can be used to get object detection bounding boxes data in one format or another.\r\n\r\n```python\r\n>>> from datasets import load_dataset, BBox\r\n>>> ds = load_dataset(\"svhn\", \"full_numbers\", split=\"train\")\r\n>>> ds[0]\r\n{\r\n  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=107x46 at 0x126409BE0>,\r\n  'digits': {'bbox': [[38, 1, 21, 40], [57, 3, 16, 40]], 'label': [4, 6]}\r\n}\r\n>>> ds = ds.rename_column(\"digits\", \"annotations\").cast_column(\"annotations\", BBox(format=\"coco\"))\r\n>>> ds[0]\r\n{\r\n  'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=107x46 at 0x147730070>,\r\n  'annotations': [{'bbox': [38, 1, 21, 40], 'category_id': 4}, {'bbox': [57, 3, 16, 40], 'category_id': 6}]\r\n}\r\n```\r\n\r\nnote that it's a type for a list of bounding boxes, not just one - which would be needed to switch from a format to another using type casting.<end_of_turn>\n<start_of_turn>model\nBBox type<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n```python\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"lhoestq/demo1\", split=\"train\")\r\nds = ds.map(lambda x: {}, num_proc=2).filter(lambda x: True).remove_columns([\"id\"])\r\nprint(ds.repo_id)\r\n# lhoestq/demo1\r\n```\r\n\r\n- repo_id is None when the dataset doesn't come from the Hub, e.g. from Dataset.from_dict\r\n- repo_id is set to None when concatenating datasets with different repo ids\r\n\r\nrelated to https://github.com/huggingface/datasets/issues/4129\r\n\r\nTODO:\r\n- [ ] discuss if it's ok for now\r\n- [ ] tests<end_of_turn>\n<start_of_turn>model\nAdd repo_id to DatasetInfo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nload_dataset() ignores the column_names kwarg when loading a CSV file.  Instead, it uses whatever values are on the first line of the file.\r\n\n\n### Steps to reproduce the bug\n\nCall `load_dataset` to load data from a CSV file and specify `column_names` kwarg.\n\n### Expected behavior\n\nThe resulting dataset should have the specified column names **and** the first line of the file should be considered as data values.\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-5.10.0-30-cloud-amd64-x86_64-with-glibc2.31\r\n- Python version: 3.9.2\r\n- `huggingface_hub` version: 0.24.2\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\ncolumn_names ignored by load_dataset() when loading CSV file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n              python 3.9.2\r\nGot an error  _pickle.PicklingError use Dataset.from_spark.\r\n\r\nDid the dataset import load data from spark dataframe using multi-node Spark cluster\r\ndf = spark.read.parquet(args.input_data).repartition(50)\r\nds = Dataset.from_spark(df, keep_in_memory=True,\r\n                        cache_dir=\"/pnc-data/data/nuplan/t5_spark/cache_data\")\r\nds.save_to_disk(args.output_data)\r\n\r\nError : \r\n_pickle.PicklingError: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transforma\r\ntion. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\r\n23/06/16 21:17:20 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\r\n\r\n_Originally posted by @yanzia12138 in https://github.com/huggingface/datasets/issues/5701#issuecomment-1594674306_\r\n \r\nW\r\nTraceback (most recent call last):\r\n  File \"/home/work/main.py\", line 100, in <module>\r\n    run(args)\r\n  File \"/home/work/main.py\", line 80, in run\r\n    ds = Dataset.from_spark(df1, keep_in_memory=True,\r\n  File \"/home/work/.local/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 1281, in from_spark\r\n    return SparkDatasetReader(\r\n  File \"/home/work/.local/lib/python3.9/site-packages/datasets/io/spark.py\", line 53, in read\r\n    self.builder.download_and_prepare(\r\n  File \"/home/work/.local/lib/python3.9/site-packages/datasets/builder.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/work/.local/lib/python3.9/site-packages/datasets/builder.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/work/.local/lib/python3.9/site-packages/datasets/packaged_modules/spark/spark.py\", line 254, in _prepare_split\r\n    self._validate_cache_dir()\r\n  File \"/home/work/.local/lib/python3.9/site-packages/datasets/packaged_modules/spark/spark.py\", line 122, in _validate_cache_dir\r\n    self._spark.sparkContext.parallelize(range(1), 1).mapPartitions(create_cache_and_write_probe).collect()\r\n  File \"/home/work/.local/lib/python3.9/site-packages/pyspark/rdd.py\", line 950, in collect\r\n    sock_info = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\r\n  File \"/home/work/.local/lib/python3.9/site-packages/pyspark/rdd.py\", line 2951, in _jrdd\r\n    wrapped_func = _wrap_function(self.ctx, self.func, self._prev_jrdd_deserializer,\r\n  File \"/home/work/.local/lib/python3.9/site-packages/pyspark/rdd.py\", line 2830, in _wrap_function\r\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\r\n  File \"/home/work/.local/lib/python3.9/site-packages/pyspark/rdd.py\", line 2816, in _prepare_for_python_RDD\r\n    pickled_command = ser.dumps(command)\r\n  File \"/home/work/.local/lib/python3.9/site-packages/pyspark/serializers.py\", line 447, in dumps\r\n    raise pickle.PicklingError(msg)\r\n_pickle.PicklingError: Could not serialize object: RuntimeError: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. S\r\nparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\r\n23/06/19 13:51:21 WARN ExecutorPodsWatchSnapshotSource: Kubernetes client has been closed (this is expected if the application is shutting down.)\r\n<end_of_turn>\n<start_of_turn>model\nGot an error  _pickle.PicklingError use Dataset.from_spark.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nA call to `.filter` occasionally hangs (after the filter is complete, according to tqdm)\r\n\r\nThere is a trace produced\r\n```\r\nException ignored in: <function Dataset.__del__ at 0x7efb48130c10>\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/datasets/arrow_dataset.py\", line 1366, in __del__\r\n    if hasattr(self, \"_indices\"):\r\n  File \"/usr/lib/python3/dist-packages/composer/core/engine.py\", line 123, in sigterm_handler\r\n    sys.exit(128 + signal)\r\nSystemExit: 143\r\n```\r\n\r\nbut I'm not sure if the trace is actually from `datasets`, or from surrounding code that is trying to clean up after datasets gets stuck.\r\n\r\nUnfortunately I can't reproduce this issue anywhere close to reliably. It happens infrequently when using `num_procs > 1`. Anecdotally I started seeing it when using larger datasets (~10M samples).\r\n\r\n### Steps to reproduce the bug\r\n\r\nN/A see description\r\n\r\n### Expected behavior\r\n\r\nmap/filter calls always complete sucessfully\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.4.0-137-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.2<end_of_turn>\n<start_of_turn>model\nFilter occasionally hangs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm building train, dev, and test using `from_generator`; however, in all three cases, the logger prints `Generating train split:`\r\nIt's not possible to change the split name since it seems to be hardcoded: https://github.com/huggingface/datasets/blob/main/src/datasets/packaged_modules/generator/generator.py\n\n### Steps to reproduce the bug\n\n```\r\nIn [1]: from datasets import Dataset\r\n\r\nIn [2]: def gen():\r\n   ...:     yield {\"pokemon\": \"bulbasaur\", \"type\": \"grass\"}\r\n   ...: \r\n\r\nIn [3]: ds = Dataset.from_generator(gen)\r\nGenerating train split: 1 examples [00:00, 133.89 examples/s]\r\n```\n\n### Expected behavior\n\nIt should be possible to specify any split name\n\n### Environment info\n\n- `datasets` version: 2.19.2\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.8.5\r\n- `huggingface_hub` version: 0.23.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\n`from_generator` does not allow to specify the split name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhile importing an audiofolder dataset, where the names of the audiofiles don't correspond to the filenames in the metadata.csv, we get an unclear error message that is not helpful for the debugging, i.e. \n```\nValueError: Instruction \"train\" corresponds to no data!\n```\n\n### Steps to reproduce the bug\n\nAssume an audiofolder with audiofiles, filename1.mp3, filename2.mp3 etc and a file metadata.csv which contains the columns file_name and sentence. The file_names are formatted like filename1.mp3, filename2.mp3 etc. \n\nLoad the audio\n```\nfrom datasets import load_dataset\nload_dataset(\"audiofolder\", data_dir='/path/to/audiofolder')\n```\nWhen the file_names in the csv are not in sync with the filenames in the audiofolder, then we get an Error message:\n```\nFile /opt/conda/lib/python3.12/site-packages/datasets/arrow_reader.py:251, in BaseReader.read(self, name, instructions, split_infos, in_memory)\n    249 if not files:\n    250     msg = f'Instruction \"{instructions}\" corresponds to no data!'\n--> 251     raise ValueError(msg)\n    252 return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\n\nValueError: Instruction \"train\" corresponds to no data!\n```\nload_dataset has a default value for the argument split = 'train'. \n\n\n### Expected behavior\n\nIt would be better to get an error report something like:\n```\nThe metadata.csv file has different filenames than the files in the datadirectory. \n```\nIt would have saved me 4 hours of debugging. \n\n### Environment info\n\n- `datasets` version: 3.2.0\n- Platform: Linux-5.14.0-427.40.1.el9_4.x86_64-x86_64-with-glibc2.39\n- Python version: 3.12.8\n- `huggingface_hub` version: 0.27.0\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\nImporting dataset gives unhelpful error message when filenames in metadata.csv are not found in the directory<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported in https://github.com/huggingface/datasets/pull/6482<end_of_turn>\n<start_of_turn>model\nFix max lock length on unix<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs reported by @ragavsachdeva (see discussion here: https://github.com/huggingface/datasets/pull/7144#issuecomment-2348307792), our webdataset loader is not aligned with the `webdataset` library when splitting keys from filenames.\r\n\r\nFor example, we get a different key splitting for filename `/some/path/22.0/1.1.png`:\r\n- datasets library: `/some/path/22` and `0/1.1.png`\r\n- webdataset library: `/some/path/22.0/1`, `1.png`\r\n\r\n```python\r\nimport webdataset as wds\r\n\r\nwds.tariterators.base_plus_ext(\"/some/path/22.0/1.1.png\")\r\n# ('/some/path/22.0/1', '1.png')\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nWebDataset loader splits keys differently than WebDataset library<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis is a very simple change that improves `to_parquet` to use a more reasonable row group size for image and audio datasets.\r\n\r\nThis is especially useful for `push_to_hub` and will provide a better experience with the dataset viewer on HF<end_of_turn>\n<start_of_turn>model\nBetter row group size in push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nParquetFragment.to_batches uses a buffered stream to read parquet data, which makes streaming faster (x2 on my laptop).\r\n\r\nI also added the `filters` config parameter to support filtering with predicate pushdown, e.g.\r\n\r\n```python\r\nfrom datasets import load_dataset\r\n\r\nfilters = [('problem_source', '==', 'math')]\r\nds = load_dataset(\"nvidia/OpenMathInstruct-2\", streaming=True, filters=filters)\r\nfirst_example = next(iter(ds[\"train\"]))\r\nprint(first_example[\"problem_source\"])\r\n# 'math'\r\n```\r\n\r\ncc @allisonwang-db this is a nice plus for usage in spark<end_of_turn>\n<start_of_turn>model\nFaster parquet streaming + filters with predicate pushdown<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAnalogously with `delete_from_hub`, this PR:\r\n- creates the Python function `convert_to_parquet`\r\n- makes the corresponding CLI command use that function.\r\n\r\nThis way, the functionality can be used both from a terminal and from a Python console.\r\n\r\nThis PR also implements a test for convert_to_parquet function.<end_of_turn>\n<start_of_turn>model\nCreate function to convert to parquet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPin upper version of `fsspec` to avoid disruptions introduced by breaking changes (and the need of urgent patch releases with hotfixes) on each release on their side. See:\r\n- #6331\r\n- #6210 \r\n- #5731\r\n- #5617\r\n- #5447\r\n\r\nI propose that we explicitly test, introduce fixes and support each new `fsspec` version release.\r\n\r\nCC: @LysandreJik <end_of_turn>\n<start_of_turn>model\nPin supported upper version of fsspec<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHave a metadata.csv file with multiple columns that point to relative image or audio files.\n\n### Motivation\n\nCurrently, ImageFolder allows one column, called `file_name`, pointing to relative image files. On the same model, AudioFolder allows one column, called `file_name`, pointing to relative audio files.\r\n\r\nBut it's not possible to have two image columns, or to have two audio column, or to have one audio column and one image column.\n\n### Your contribution\n\nno specific contribution<end_of_turn>\n<start_of_turn>model\nSupport multiple image/audio columns in ImageFolder/AudioFolder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn Python 3.14, there's a change in the signature of `_Pickler._batch_setitems`.\r\n\r\nIt's changed to:\r\n\r\n```python\r\n# pickle.py\r\n\r\ndef _batch_setitems(self, items, obj):\r\n    # Helper to batch up SETITEMS sequences; proto >= 1 only\r\n    save = self.save\r\n    write = self.write\r\n```\r\n\r\nTo accomodate this, in `dill`, we have this compatibility code:\r\n\r\n```python\r\nif sys.hexversion < 0x30E00A1:\r\n    pickler._batch_setitems(iter(source.items()))\r\nelse:\r\n    pickler._batch_setitems(iter(source.items()), obj=obj)\r\n```\r\n\r\nThus, the datasets package will emit this error\r\n\r\n```\r\n\u2502 /Users/sghuang/mamba/envs/ds/lib/python3.14/site-packages/dill/_dill.py:1262 in save_module_dict \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1259 \u2502   \u2502   if is_dill(pickler, child=False) and pickler._session:                            \u2502\r\n\u2502   1260 \u2502   \u2502   \u2502   # we only care about session the first pass thru                              \u2502\r\n\u2502   1261 \u2502   \u2502   \u2502   pickler._first_pass = False                                                   \u2502\r\n\u2502 \u2771 1262 \u2502   \u2502   StockPickler.save_dict(pickler, obj)                                              \u2502\r\n\u2502   1263 \u2502   \u2502   logger.trace(pickler, \"# D2\")                                                     \u2502\r\n\u2502   1264 \u2502   return                                                                                \u2502\r\n\u2502   1265                                                                                           \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502 /Users/sghuang/mamba/envs/ds/lib/python3.14/pickle.py:1133 in save_dict                          \u2502\r\n\u2502                                                                                                  \u2502\r\n\u2502   1130 \u2502   \u2502   print(f\"Line number: {inspect.getsourcelines(method)[1]}\")                        \u2502\r\n\u2502   1131 \u2502   \u2502   print(f\"Full path: {inspect.getmodule(method)}\")                                  \u2502\r\n\u2502   1132 \u2502   \u2502   print(f\"Class: {method.__qualname__}\")                                            \u2502\r\n\u2502 \u2771 1133 \u2502   \u2502   self._batch_setitems(obj.items(), obj)                                            \u2502\r\n\u2502   1134 \u2502                                                                                         \u2502\r\n\u2502   1135 \u2502   dispatch[dict] = save_dict                                                            \u2502\r\n\u2502   1136                                                                                           \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\r\n[NOTE] when serializing datasets.table.InMemoryTable state\r\n[NOTE] when serializing datasets.table.InMemoryTable object\r\n```\r\n\r\nTo fix it, we update the signature of the `_batch_setitems` method defined in `utils/_dill.py`.\r\n\r\nThis fix should be backward compatible, since the compatibility is handled by `dill`.\r\n\r\nThis should close #7813.\r\n\r\nSimilar to https://github.com/joblib/joblib/issues/1658.\r\n\r\nRelated to https://github.com/uqfoundation/dill/pull/724.<end_of_turn>\n<start_of_turn>model\nfix: better args passthrough for `_batch_setitems()`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn the Preprocess tutorial, the to \"the beans dataset\" is incorrect. Fixed.<end_of_turn>\n<start_of_turn>model\nUpdate the beans dataset link in Preprocess<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI set a breakpoint in \"load_dataset.py\" and try to debug my data load codes, but it does not stop at any breakpoints, so \"load_dataset.py\" can not be stepped into ?\n\n<!-- Failed to upload \"\u622a\u56fe 2025-08-05 17-25-18.png\" --><end_of_turn>\n<start_of_turn>model\nCan not stepinto load_dataset.py?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\"in the feature\" should be \"in the future\"<end_of_turn>\n<start_of_turn>model\nFix a typo in arrow_dataset.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- remove canonical dataset name\r\n- remove dataset script logic\r\n- add streaming info\r\n- clearer download and prepare steps<end_of_turn>\n<start_of_turn>model\nupdate load_dataset doctring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI would like to be able to update items in my dataset without affecting all rows. At least if there was a range option, I would be able to process those items, save the dataset, and then continue.\n\nIf I am supposed to split the dataset first, that is not clear, since the docs suggest that any of those functions returns a new object, so I don't think I can do that.\n\n### Motivation\n\nI am applying an extremely time-consuming function to each item in my `Dataset`. Unfortunately, datasets only supports updating values via `map`, so if my computer dies in the middle of this long-running process, I lose all progress. This is far from ideal. I would like to use `datasets` throughout this processing, but this limitation is now forcing me to write my own dataset format just to do this intermediary operation.\n\nIt would be less intuitive but I suppose I could split and then concatenate the dataset before saving? But this feels very inefficient.\n\n### Your contribution\n\nI can test the feature.<end_of_turn>\n<start_of_turn>model\nUpdate items in the dataset without `map`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n---------------------------------------------------------------------------\r\nModuleNotFoundError                       Traceback (most recent call last)\r\n[<ipython-input-9-13b5f31bd391>](https://bcb6shpazyu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20241022-060119_RC00_688494744#) in <cell line: 1>()\r\n----> 1 dataset = load_dataset('knowledgator/events_classification_biotech')\r\n\r\n11 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://bcb6shpazyu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20241022-060119_RC00_688494744#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2130 \r\n   2131     # Create a dataset builder\r\n-> 2132     builder_instance = load_dataset_builder(\r\n   2133         path=path,\r\n   2134         name=name,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://bcb6shpazyu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20241022-060119_RC00_688494744#) in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\r\n   1886         raise ValueError(error_msg)\r\n   1887 \r\n-> 1888     builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\r\n   1889     # Instantiate the dataset builder\r\n   1890     builder_instance: DatasetBuilder = builder_cls(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://bcb6shpazyu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20241022-060119_RC00_688494744#) in get_dataset_builder_class(dataset_module, dataset_name)\r\n    246         dataset_module.importable_file_path\r\n    247     ) if dataset_module.importable_file_path else nullcontext():\r\n--> 248         builder_cls = import_main_class(dataset_module.module_path)\r\n    249     if dataset_module.builder_configs_parameters.builder_configs:\r\n    250         dataset_name = dataset_name or dataset_module.builder_kwargs.get(\"dataset_name\")\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://bcb6shpazyu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20241022-060119_RC00_688494744#) in import_main_class(module_path)\r\n    167 def import_main_class(module_path) -> Optional[Type[DatasetBuilder]]:\r\n    168     \"\"\"Import a module at module_path and return its main class: a DatasetBuilder\"\"\"\r\n--> 169     module = importlib.import_module(module_path)\r\n    170     # Find the main class in our imported module\r\n    171     module_main_cls = None\r\n\r\n[/usr/lib/python3.10/importlib/__init__.py](https://bcb6shpazyu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20241022-060119_RC00_688494744#) in import_module(name, package)\r\n    124                 break\r\n    125             level += 1\r\n--> 126     return _bootstrap._gcd_import(name[level:], package, level)\r\n    127 \r\n    128 \r\n\r\n/usr/lib/python3.10/importlib/_bootstrap.py in _gcd_import(name, package, level)\r\n\r\n/usr/lib/python3.10/importlib/_bootstrap.py in _find_and_load(name, import_)\r\n\r\n/usr/lib/python3.10/importlib/_bootstrap.py in _find_and_load_unlocked(name, import_)\r\n\r\n/usr/lib/python3.10/importlib/_bootstrap.py in _load_unlocked(spec)\r\n\r\n/usr/lib/python3.10/importlib/_bootstrap_external.py in exec_module(self, module)\r\n\r\n/usr/lib/python3.10/importlib/_bootstrap.py in _call_with_frames_removed(f, *args, **kwds)\r\n\r\n[~/.cache/huggingface/modules/datasets_modules/datasets/knowledgator--events_classification_biotech/9c8086d498c3104de3a3c5b6640837e18ccd829dcaca49f1cdffe3eb5c4a6361/events_classification_biotech.py](https://bcb6shpazyu-496ff2e9c6d22116-0-colab.googleusercontent.com/outputframe.html?vrz=colab_20241022-060119_RC00_688494744#) in <module>\r\n      1 import datasets\r\n      2 from datasets import load_dataset\r\n----> 3 from datasets.tasks import TextClassification\r\n      4 \r\n      5 DESCRIPTION = \"\"\"\r\n\r\nModuleNotFoundError: No module named 'datasets.tasks'\r\n\r\n---------------------------------------------------------------------------\r\nNOTE: If your import is failing due to a missing package, you can\r\nmanually install dependencies using either !pip or !apt.\r\n\r\nTo view examples of installing some common dependencies, click the\r\n\"Open Examples\" button below.\r\n---------------------------------------------------------------------------\n\n### Steps to reproduce the bug\n\n!pip install datasets\r\n\r\nfrom datasets import load_dataset\r\n    \r\ndataset = load_dataset('knowledgator/events_classification_biotech') \n\n### Expected behavior\n\nno ModuleNotFoundError \n\n### Environment info\n\ngoogle colab<end_of_turn>\n<start_of_turn>model\nModuleNotFoundError: No module named 'datasets.tasks'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\nimport datasets\nf = datasets.Features({'x': list[datasets.Value(dtype='int32')]})\nf.arrow_schema\n\nTraceback (most recent call last):\n  File \"datasets/features/features.py\", line 1826, in arrow_schema\n    return pa.schema(self.type).with_metadata({\"huggingface\": json.dumps(hf_metadata)})\n                     ^^^^^^^^^\n  File \"datasets/features/features.py\", line 1815, in type\n    return get_nested_type(self)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"datasets/features/features.py\", line 1252, in get_nested_type\n    return pa.struct(\n           ^^^^^^^^^^\n  File \"pyarrow/types.pxi\", line 5406, in pyarrow.lib.struct\n  File \"pyarrow/types.pxi\", line 3890, in pyarrow.lib.field\n  File \"pyarrow/types.pxi\", line 5918, in pyarrow.lib.ensure_type\nTypeError: DataType expected, got <class 'list'>\n```\n\nThe following works\n```\nf = datasets.Features({'x': datasets.LargeList(datasets.Value(dtype='int32'))})\n```\n\n### Expected behavior\n\naccording to https://github.com/huggingface/datasets/blob/458f45a22c3cc9aea5f442f6f519333dcfeae9b9/src/datasets/features/features.py#L1765 python list should be a valid type specification for features\n\n### Environment info\n\n- `datasets` version: 3.5.1\n- Platform: macOS-15.5-arm64-arm-64bit\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\narrow_schema is not compatible with list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR ensures that the temporary filename created is the same as the one that is locked, while writing to the cache.\r\n\r\nThis PR stops using `tempfile` to generate the temporary filename.\r\n\r\nAdditionally, the behavior now is aligned for both `resume_download` `True` and `False`.\r\n\r\nRefactor temp_file_manager so that it uses the filename that is locked: \r\n- Use: `cache_path + \".incomplete\"`, when the locked one is `cache_path + \".lock\"`\r\n\r\nBefore it was using `tempfile` inside `cache_dir`, which was not locked: although very improbable name collision (8 random characters), this was not impossible when huge number of multiple processes.\r\n\r\nMaybe related to \"Stale file handle\" issues caused by `tempfile`: \r\n- [ ] https://huggingface.co/datasets/tapaco/discussions/4\r\n- [ ] https://huggingface.co/datasets/xcsr/discussions/1\r\n- [ ] https://huggingface.co/datasets/covost2/discussions/3\r\n```\r\nError code:   ConfigNamesError\r\nException:    OSError\r\nMessage:      [Errno 116] Stale file handle\r\nTraceback:    Traceback (most recent call last):\r\n                File \"/src/services/worker/src/worker/job_runners/dataset/config_names.py\", line 61, in compute_config_names_response\r\n                  for config in sorted(get_dataset_config_names(path=dataset, use_auth_token=use_auth_token))\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 323, in get_dataset_config_names\r\n                  dataset_module = dataset_module_factory(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1219, in dataset_module_factory\r\n                  raise e1 from None\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1188, in dataset_module_factory\r\n                  return HubDatasetModuleFactoryWithScript(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 907, in get_module\r\n                  dataset_readme_path = self.download_dataset_readme_file()\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 896, in download_dataset_readme_file\r\n                  return cached_path(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/utils/file_utils.py\", line 183, in cached_path\r\n                  output_path = get_from_cache(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/utils/file_utils.py\", line 611, in get_from_cache\r\n                  http_get(\r\n                File \"/usr/local/lib/python3.9/tempfile.py\", line 496, in __exit__\r\n                  result = self.file.__exit__(exc, value, tb)\r\n              OSError: [Errno 116] Stale file handle\r\n```\r\n- the stale file handle error can be raised when `tempfile` tries to close (when exiting its context manager) a filename that has been already closed by other process\r\n  - note that `tempfile` filenames are randomly generated but not locked in our code\r\n\r\nCC: @severo <end_of_turn>\n<start_of_turn>model\nMake get_from_cache use custom temp filename that is locked<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n_FEATURES = datasets.Features(\r\n    {\r\n        \"image\": datasets.Image(),\r\n        \"text\": datasets.Value(\"string\"),\r\n    },\r\n)\r\nThe datasets builder seems only support the unit8 data. How to load the float dtype data? <end_of_turn>\n<start_of_turn>model\nhow to load the image of dtype float32 or float64<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI get the following error when simply running `load_dataset('winogrande','winogrande_xl')`.\r\n\r\nI do not have such an issue in the 1.17.0 version.\r\n\r\n```Traceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2556, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/load.py\", line 2265, in load_dataset_builder\r\n    builder_instance: DatasetBuilder = builder_cls(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 371, in __init__\r\n    self.config, self.config_id = self._create_builder_config(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 620, in _create_builder_config\r\n    builder_config._resolve_data_files(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/builder.py\", line 211, in _resolve_data_files\r\n    self.data_files = self.data_files.resolve(base_path, download_config)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\", line 799, in resolve\r\n    out[key] = data_files_patterns_list.resolve(base_path, download_config)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\", line 752, in resolve\r\n    resolve_pattern(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/data_files.py\", line 393, in resolve_pattern\r\n    raise FileNotFoundError(error_msg)\r\nFileNotFoundError: Unable to find 'hf://datasets/winogrande@ebf71e3c7b5880d019ecf6099c0b09311b1084f5/winogrande_xl/train/0000.parquet' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.geoparquet', '.gpq', '.arrow', '.txt', '.tar', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']```\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\r\ndatasets = load_dataset('winogrande','winogrande_xl')\n\n### Expected behavior\n\n```Downloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.06M/2.06M [00:00<00:00, 5.16MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 118k/118k [00:00<00:00, 360kB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 85.9k/85.9k [00:00<00:00, 242kB/s]\r\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40398/40398 [00:00<00:00, 845491.12 examples/s]\r\nGenerating test split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1767/1767 [00:00<00:00, 362501.11 examples/s]\r\nGenerating validation split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1267/1267 [00:00<00:00, 318768.11 examples/s]```\n\n### Environment info\n\ndatasets version: 1.18.0\r\n<end_of_turn>\n<start_of_turn>model\nWinogrande does not seem to be compatible with datasets version of 1.18.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen `torchdata.stateful_dataloader.StatefulDataloader(persistent_workers=True)` the epochs after resuming only iterate through the examples that were left in the epoch when the training was interrupted. For example, in the script below training is interrupted on step 124 (epoch 1) when 3 batches are left. Then after resuming, the rest of epochs (2 and 3) only iterate through these 3 batches.\n\n### Steps to reproduce the bug\n\nRun the following script with and with PERSISTENT_WORKERS=true.\n\n```python\n# !/usr/bin/env python3\n# torch==2.5.1\n# datasets==3.3.2\n# torchdata>=0.9.0\nimport datasets\nimport pprint\nfrom torchdata.stateful_dataloader import StatefulDataLoader\n\nimport os\n\nPERSISTENT_WORKERS = (\n    os.environ.get(\"PERSISTENT_WORKERS\", \"False\").lower() == \"true\"\n)\n\n# PERSISTENT_WORKERS = True  # Incorrect resume\n\n\n# ds = datasets.load_from_disk(\"dataset\").to_iterable_dataset(num_shards=4)\ndef generator():\n    for i in range(128):\n        yield {\"x\": i}\n\n\nds = datasets.Dataset.from_generator(\n    generator, features=datasets.Features({\"x\": datasets.Value(\"int32\")})\n).to_iterable_dataset(num_shards=4)\n\ndl = StatefulDataLoader(\n    ds, batch_size=2, num_workers=2, persistent_workers=PERSISTENT_WORKERS\n)\nglobal_step = 0\nepoch = 0\nds_state_dict = None\nstate_dict = None\nresumed = False\nwhile True:\n    if epoch >= 3:\n        break\n    if state_dict is not None:\n        dl.load_state_dict(state_dict)\n        state_dict = None\n        ds_state_dict = None\n        resumed = True\n        print(\"resumed\")\n    for i, batch in enumerate(dl):\n        print(f\"epoch: {epoch}, global_step: {global_step}, batch: {batch}\")\n        global_step += 1  # consume datapoint\n        # simulate error\n        if global_step == 124 and not resumed:\n            ds_state_dict = ds.state_dict()\n            state_dict = dl.state_dict()\n            print(\"checkpoint\")\n            print(\"ds_state_dict\")\n            pprint.pprint(ds_state_dict)\n            print(\"dl_state_dict\")\n            pprint.pprint(state_dict)\n            break\n\n    if state_dict is None:\n        ds.set_epoch(epoch)\n        epoch += 1\n```\n\nThe script checkpoints when there are three batches left in the second epoch. After resuming, only the last three batches are repeated in the rest of the epochs.\n\nIf it helps, following are the two state_dicts for the dataloader save at the same step with the two settings. The left one is for `PERSISTENT_WORKERS=False`\n\n![Image](https://github.com/user-attachments/assets/c97d6502-d7bd-4ef4-ae2d-66fe1a9732b1)\n\n### Expected behavior\n\nAll the elements in the dataset should be iterated through in the epochs following the one where we resumed. The expected behavior can be seen by setting `PERSISTENT_WORKERS=False`. \n\n### Environment info\n\ntorch==2.5.1\ndatasets==3.3.2\ntorchdata>=0.9.0<end_of_turn>\n<start_of_turn>model\nEpochs shortened after resuming mid-epoch with Iterable dataset+StatefulDataloader(persistent_workers=True)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWe had a regression and the order of the splits is not preserved. They are alphabetically sorted, instead of preserving original \"train\", \"validation\", \"test\" order.\r\n\r\nCheck: In branch \"main\"\r\n```python\r\nIn [9]: dataset = load_dataset(\"adversarial_qa\", '\"adversarialQA\")\r\n\r\nIn [10]: dataset\r\nOut[10]: \r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n    train: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 30000\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n})\r\n```\r\n\r\nBefore (2.15.0) it was:\r\n```python\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 30000\r\n    })\r\n    validation: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n    test: Dataset({\r\n        features: ['id', 'title', 'context', 'question', 'answers', 'metadata'],\r\n        num_rows: 3000\r\n    })\r\n})\r\n```\r\n\r\nSee issues: \r\n- https://huggingface.co/datasets/adversarial_qa/discussions/3\r\n- https://huggingface.co/datasets/beans/discussions/4\r\n\r\nThis is a regression because it was previously fixed. See:\r\n- #6196\r\n- #5728<end_of_turn>\n<start_of_turn>model\nThe order of the splits is not preserved<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThis is dificult, I cant confirm email because I'm not get any email!\nI cant post forum because I cant confirm email!\nI can send help desk because... no exist on web page.\n\nparagraph 44\n\n### Steps to reproduce the bug\n\nrthjrtrt\n\n### Expected behavior\n\newtgfwetgf\n\n### Environment info\n\nsdgfswdegfwe<end_of_turn>\n<start_of_turn>model\ni cant confirm email<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm training the flux controlnet. The train_dataset.map() takes long time to finish. However, when I killed one training process and want to restart a new training with the same dataset. I can't reuse the mapped result even I defined the cache dir for the dataset.\r\n\r\nwith accelerator.main_process_first():\r\n        from datasets.fingerprint import Hasher\r\n\r\n        # fingerprint used by the cache for the other processes to load the result\r\n        # details: https://github.com/huggingface/diffusers/pull/4038#discussion_r1266078401\r\n        new_fingerprint = Hasher.hash(args)\r\n        train_dataset = train_dataset.map(\r\n            compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint, batch_size=10,\r\n        )\n\n### Steps to reproduce the bug\n\ntrain flux controlnet and start again\r\n\r\n\n\n### Expected behavior\n\nwill not map again\n\n### Environment info\n\nlatest diffusers\r\n\r\n<end_of_turn>\n<start_of_turn>model\nCannot load the cache when mapping the dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhile using `Dataset.push_to_hub` of a canonical dataset, an additional dataset was created under my user namespace.\r\n\r\n## Steps to reproduce the bug\r\nThe command:\r\n```python\r\ncommit_info = ds.push_to_hub(\r\n    \"caner\",\r\n    config_name=\"default\",\r\n    commit_message=\"Convert dataset to Parquet\",\r\n    commit_description=\"Convert dataset to Parquet.\",\r\n    create_pr=True,\r\n    token=token,\r\n)\r\n```\r\ncreates the additional dataset `albertvillanova/caner`.<end_of_turn>\n<start_of_turn>model\nDataset.push_to_hub of a canonical dataset creates an additional dataset under the user namespace<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.13.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading a streaming dataset with this version of fsspec fails with the following error:\r\n\r\n`NotImplementedError: Loading a streaming dataset cached in a LocalFileSystem is not supported yet.`\r\n\r\nI suspect the issue is with this PR \r\n\r\nhttps://github.com/fsspec/filesystem_spec/pull/1381\n\n### Steps to reproduce the bug\n\n1. Upgrade fsspec to version `2023.10.0`\r\n2. Attempt to load a streaming dataset e.g. `load_dataset(\"laion/gpt4v-emotion-dataset\", split=\"train\", streaming=True)`\r\n3. Observe the following exception:\r\n\r\n```\r\n  File \"/opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages/datasets/load.py\", line 2146, in load_dataset\r\n    return builder_instance.as_streaming_dataset(split=split)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/hostedtoolcache/Python/3.11.6/x64/lib/python3.11/site-packages/datasets/builder.py\", line 1318, in as_streaming_dataset\r\n    raise NotImplementedError(\r\nNotImplementedError: Loading a streaming dataset cached in a LocalFileSystem is not supported yet.\r\n```\n\n### Expected behavior\n\nShould stream the dataset as normal.\n\n### Environment info\n\ndatasets@main\r\nfsspec==2023.10.0<end_of_turn>\n<start_of_turn>model\nLatest fsspec==2023.10.0 issue with streaming datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, I am preprocessing a large custom dataset with numpy arrays. I am running into this TypeError during writing in a dataset.map() function. I've tried decreasing writer batch size, but this error persists. This error does not occur for smaller datasets. \r\n\r\n\r\n\r\nThank you!\r\n\r\n\n\n### Steps to reproduce the bug\n\nTraceback (most recent call last):\r\n  File \"/n/home12/yhwang/.conda/envs/lib/python3.10/site-packages/multiprocess/pool.py\", line 125, in worker\r\n    result = (True, func(*args, **kwds))\r\n  File \"/n/home12/yhwang/.conda/envs/lib/python3.10/site-packages/datasets/utils/py_utils.py\", line 1354, in _write_generator_to_queue\r\n    for i, result in enumerate(func(**kwargs)):\r\n  File \"/n/home12/yhwang/.conda/envs/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 3493, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"/n/home12/yhwang/.conda/envs/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 555, in write_batch\r\n    arrays.append(pa.array(typed_sequence))\r\n  File \"pyarrow/array.pxi\", line 243, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 110, in pyarrow.lib._handle_arrow_array_protocol\r\n  File \"/n/home12/yhwang/.conda/envs/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 184, in __arrow_array__\r\n    out = numpy_to_pyarrow_listarray(data)\r\n  File \"/n/home12/yhwang/.conda/envs/lib/python3.10/site-packages/datasets/features/features.py\", line 1394, in numpy_to_pyarrow_listarray\r\n    values = pa.ListArray.from_arrays(offsets, values)\r\n  File \"pyarrow/array.pxi\", line 2004, in pyarrow.lib.ListArray.from_arrays\r\nTypeError: Cannot convert pyarrow.lib.ChunkedArray to pyarrow.lib.Array\n\n### Expected behavior\n\nType should not be a ChunkedArray\n\n### Environment info\n\ndatasets v2.14.5\r\narrow v1.2.3\r\npyarrow v12.0.1<end_of_turn>\n<start_of_turn>model\nTypeError: Cannot convert pyarrow.lib.ChunkedArray to pyarrow.lib.Array<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR ignores the violation of the lint rule E721 in `Pickler.memoize`.\r\n\r\nThe lint rule violation was introduced in this PR:\r\n- #3182\r\n\r\n@lhoestq is there a reason you did not use `isinstance` instead?\r\n\r\nAs a hotfix, we just ignore the violation of the lint rule.\r\n\r\nFix #6136.<end_of_turn>\n<start_of_turn>model\nIgnore CI lint rule violation in Pickler.memoize<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd function to convert a script-dataset to Parquet and push it to the Hub, analogously to the Space: \"Convert a Hugging Face dataset to Parquet\"<end_of_turn>\n<start_of_turn>model\nAdd function to convert a script-dataset to Parquet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHaving `None` values in a `Sequence` of `ArrayND` fails.\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Array2D, Dataset, Features, Sequence\r\n\r\ndata = [\r\n    [\r\n        [[0]],\r\n        None,\r\n        None,\r\n    ]\r\n]\r\nfeature = Sequence(Array2D((1, 1), dtype=\"int64\"))\r\ndataset = Dataset.from_dict({\"a\": data}, features=Features({\"a\": feature}))\r\n\r\ndataset[0]  # error raised only when indexing\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/Users/quentingallouedec/gia/c.py\", line 13, in <module>\r\n    dataset[0]  # error raised only when indexing\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2658, in __getitem__\r\n    return self._getitem(key)\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 2643, in _getitem\r\n    formatted_output = format_table(\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 634, in format_table\r\n    return formatter(pa_table, query_type=query_type)\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 406, in __call__\r\n    return self.format_row(pa_table)\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 441, in format_row\r\n    row = self.python_arrow_extractor().extract_row(pa_table)\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/formatting/formatting.py\", line 144, in extract_row\r\n    return _unnest(pa_table.to_pydict())\r\n  File \"pyarrow/table.pxi\", line 4146, in pyarrow.lib.Table.to_pydict\r\n  File \"pyarrow/table.pxi\", line 1312, in pyarrow.lib.ChunkedArray.to_pylist\r\n  File \"pyarrow/array.pxi\", line 1521, in pyarrow.lib.Array.to_pylist\r\n  File \"pyarrow/scalar.pxi\", line 675, in pyarrow.lib.ListScalar.as_py\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/features/features.py\", line 760, in to_pylist\r\n    return self.to_numpy(zero_copy_only=zero_copy_only).tolist()\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/datasets/features/features.py\", line 725, in to_numpy\r\n    numpy_arr = np.insert(numpy_arr.astype(np.float64), null_indices, np.nan, axis=0)\r\n  File \"<__array_function__ internals>\", line 200, in insert\r\n  File \"/Users/quentingallouedec/gia/env/lib/python3.10/site-packages/numpy/lib/function_base.py\", line 5426, in insert\r\n    old_mask[indices] = False\r\nIndexError: index 3 is out of bounds for axis 0 with size 3\r\n```\r\n\r\nAFAIK, the problem only occurs when you use a `Sequence` of `ArrayND`.\r\n\r\nI strongly suspect that the problem comes from this line, or `np.insert` is misused:\r\n\r\nhttps://github.com/huggingface/datasets/blob/02ee418831aba68d0be93227bce8b3f42ef8980f/src/datasets/features/features.py#L729\r\n\r\nTo put t simply, you want something that do that:\r\n\r\n```python\r\nimport numpy as np\r\nnumpy_arr = np.zeros((1, 1, 1))\r\nnull_indices = np.array([1, 2])\r\nnp.insert(numpy_arr, null_indices, np.nan, axis=0)\r\n# raise an error, instead of outputting \r\n# array([[[ 0.]],\r\n#        [[nan]],\r\n#        [[nan]]])\r\n```\r\n\r\n\n\n### Expected behavior\n\nThe previous code should not raise an error.\n\n### Environment info\n\n- Python 3.10.11\r\n- datasets 2.10.0\r\n- pyarrow 12.0.0<end_of_turn>\n<start_of_turn>model\n`IndexError` when indexing `Sequence` of `Array2D` with `None` values<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport WebDataset containing file basenames with dots.\r\n\r\nFix #6880.<end_of_turn>\n<start_of_turn>model\nSupport WebDataset containing file basenames with dots<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses #7741. Followup to #7690\r\n\r\n- Recursive parsing and feature inference, to preserve the tree structure of the file. Note this means we now visit all links in the file. It also means we have to call` combine_chunks` on any large non-root datasets.\r\n- Support for `complex64` (two `float32`s, used to be converted to two `float64`s)\r\n- Support for ndim complex, compound, more field types for compound (due to reusing the main parser, compound types are treated like groups)\r\n- Cleaned up varlen support\r\n- Always do feature inference and always cast to features (used to cast to schema)\r\n- Updated tests to use `load_dataset` instead of internal APIs\r\n- Removed `columns` in config. Have to give Features (i.e., must specify types) if filtering<end_of_turn>\n<start_of_turn>model\nRefactor HDF5 and preserve tree structure<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have data saved with save_to_disk. The data is big (700Gb). When I try loading it, the only option is load_from_disk, and this function copies the data to a tmp directory, causing me to run out of disk space. Is there an alternative solution to that?\n\n### Steps to reproduce the bug\n\nwhen trying to load data using load_From_disk after being saved using save_to_disk \n\n### Expected behavior\n\nrun out of disk space\n\n### Environment info\n\nlateest version<end_of_turn>\n<start_of_turn>model\nload_from_disk<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI'm using AWS lambda to run a python application. I run the `load_dataset` function with cache_dir=\"/tmp\" and is still throws the OSError(30, 'Read-only file system') error. Is even updated all the HF envs to point to /tmp dir but the issue still persists. I can confirm that the I can write to /tmp directory. \r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nd = load_dataset(\r\n      path=hugging_face_link,\r\n      split=split,\r\n      token=token,\r\n      cache_dir=\"/tmp/hugging_face_cache\",\r\n )\r\n```\r\n\r\n### Expected behavior\r\n\r\nEverything written to the file system as part of the load_datasets function should be in the /tmp directory. \r\n\r\n### Environment info\r\n\r\ndatasets version: 2.16.1\r\nPlatform: Linux-5.10.216-225.855.amzn2.x86_64-x86_64-with-glibc2.26\r\nPython version: 3.11.9\r\nhuggingface_hub version: 0.19.4\r\nPyArrow version: 16.1.0\r\nPandas version: 2.2.2\r\nfsspec version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nload_dataset on AWS lambda throws OSError(30, 'Read-only file system') error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI wanted to use my own script to handle the processing, and followed the tutorial documentation by rewriting the MyDatasetConfig and MyDatasetBuilder (which contains the _info,_split_generators and _generate_examples methods) classes. Testing with simple data was able to output the results of the processing, but when I wished to do more complex processing, I found that I was unable to debug (even the simple samples were inaccessible). There are no errors reported, and I am able to print the _info,_split_generators and _generate_examples messages, but I am unable to access the breakpoints.\n\n### Steps to reproduce the bug\n\n# my_dataset.py\r\nimport json\r\nimport datasets\r\n\r\n\r\nclass MyDatasetConfig(datasets.BuilderConfig):\r\n    def __init__(self, **kwargs):\r\n        super(MyDatasetConfig, self).__init__(**kwargs)\r\n\r\n\r\nclass MyDataset(datasets.GeneratorBasedBuilder):\r\n    VERSION = datasets.Version(\"1.0.0\")\r\n\r\n    BUILDER_CONFIGS = [\r\n        MyDatasetConfig(\r\n            name=\"default\",\r\n            version=VERSION,\r\n            description=\"myDATASET\"\r\n        ),\r\n    ]\r\n\r\n    def _info(self):\r\n        print(\"info\")  # breakpoints\r\n        return datasets.DatasetInfo(\r\n            description=\"myDATASET\",\r\n            features=datasets.Features(\r\n                {\r\n                    \"id\": datasets.Value(\"int32\"),\r\n                    \"text\": datasets.Value(\"string\"),\r\n                    \"label\": datasets.ClassLabel(names=[\"negative\", \"positive\"]),\r\n                }\r\n            ),\r\n            supervised_keys=(\"text\", \"label\"),\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n \r\n        print(\"generate\")  # breakpoints\r\n        data_file = \"data.json\"  \r\n\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN, gen_kwargs={\"filepath\": data_file}\r\n            ),\r\n        ]\r\n\r\n    def _generate_examples(self, filepath):\r\n        print(\"example\")  # breakpoints\r\n        with open(filepath, encoding=\"utf-8\") as f:\r\n            data = json.load(f)\r\n            for idx, sample in enumerate(data):\r\n                yield idx, {\r\n                    \"id\": sample[\"id\"],\r\n                    \"text\": sample[\"text\"],\r\n                    \"label\": sample[\"label\"],\r\n                }\r\n\r\n#main.py\r\nimport os\r\nos.environ[\"TRANSFORMERS_NO_MULTIPROCESSING\"] = \"1\" \r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"my_dataset.py\", split=\"train\", cache_dir=None)\r\n\r\nprint(dataset[:5])\n\n### Expected behavior\n\nPause at breakpoints while running debugging\n\n### Environment info\n\npycharm\r\n<end_of_turn>\n<start_of_turn>model\nHow to debugging<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nin `map()` function, I want to add a new column with a dict structure.\r\n```\r\ndef map_fn(example):\r\n  example['text'] = {'user': ..., 'assistant': ...}\r\n  return example\r\n```\r\nHowever this leads to a wrong order `{'assistant':..., 'user':...}` in the dataset.\r\nThus I can't concatenate two datasets due to the different feature structures.\r\n[Here](https://colab.research.google.com/drive/1zeaWq9Ith4DKWP_EiBNyLfc8S8I68LyY?usp=sharing) is a minimal reproducible example\r\nThis seems an issue in low level pyarrow library instead of datasets, however, I think datasets should allow concatenate two datasets actually in the same structure.\n\n### Steps to reproduce the bug\n\n[Here](https://colab.research.google.com/drive/1zeaWq9Ith4DKWP_EiBNyLfc8S8I68LyY?usp=sharing) is a minimal reproducible example\n\n### Expected behavior\n\ntwo datasets could be concatenated.\n\n### Environment info\n\nN/A<end_of_turn>\n<start_of_turn>model\nAdding column with dict struction when mapping lead to wrong order<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI would like to be able to concatenate multiple `IterableDataset` with possibly different features. I would like to then be able to stream the results in parallel (both using DDP and multiple workers in the pytorch DataLoader). I want the merge of datasets to be well balanced between the different processes.\n\n### Motivation\n\nI want to train a model on a combination of datasets, which I can convert to a single representation. This applies to converting different datasets items to the same Python class, as using a tokenizer on multiple modalities.\n\nAssuming that my original datasets are not necessarily well balanced as they may have different size and thus different number of shards, I would like the merged dataset to be distributed evenly over the multiple processes. I don't mind if it's not perfectly balanced, and as result, some workers of the torch DataLoader do nothing, as long as the DDP is properly handled causing no deadlock.\n\n### What I've tried\n\nI've tried the two functions already provided in datasets, namely `interleave_datasets` and `concatenate_datasets`. \n\n- Interleave seems to be the best approach of what I'm trying to do. However, it doesn't suit my purpose because as I understand it, it stops as soon as one of the dataset source is exhausted, or repeat the smallest source items until the largest is exhausted. I would like something in-between, similarly to what [roundrobin does](https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.roundrobin).\n- Concatenate does not mix the data enough and one dataset may be overrepresented in some early batches.\n\nLet's consider we have 3 datasets composed of different number of shards as follow [[s0_0, s0_1], [s1_0], [s2_0, s2_1, s2_3]], where s denotes the underlying shard, the first index the dataset and the second the shard number.\nIf we request 3 shards in the `shard_data_source` we should obtain the following:\nindex 0 gets s0_0 s2_0\nindex 1 gets s0_1 s2_1\nindex 2 gets s1_0 s2_3\n\nI started implementing the following, but I'm afraid my sharding logic is incorrect.\n\n```python\nfrom copy import deepcopy\nfrom itertools import chain, islice\n\nimport datasets\nimport numpy as np\nfrom datasets import IterableDataset\nfrom datasets.iterable_dataset import _BaseExamplesIterable\nfrom more_itertools import roundrobin\n\n\nclass MixMultiSourcesExampleIterable(_BaseExamplesIterable):\n    def __init__(self, ex_iterables: list[_BaseExamplesIterable]):\n        super().__init__()\n        self.ex_iterables = ex_iterables\n\n    def _init_state_dict(self) -> dict:\n        self._state_dict = {\n            \"ex_iterables\": [ex_iterable._init_state_dict() for ex_iterable in self.ex_iterables],\n            \"type\": self.__class__.__name__,\n        }\n        return self._state_dict\n\n    @property\n    def num_shards(self) -> int:\n        return sum(ex_iterable.num_shards for ex_iterable in self.ex_iterables)\n\n    def __iter__(self):\n        yield from roundrobin(*self.ex_iterables)\n\n    def shuffle_data_sources(self, generator: np.random.Generator) -> \"MixMultiSourcesExampleIterable\":\n        \"\"\"Shuffle the list of examples iterable, as well as each underlying examples iterable.\"\"\"\n        rng = deepcopy(generator)\n        ex_iterables = list(self.ex_iterables)\n        rng.shuffle(ex_iterables)\n        ex_iterables = [ex_iterable.shuffle_data_sources(generator) for ex_iterable in ex_iterables]\n        return MixMultiSourcesExampleIterable(ex_iterables)\n\n    def shard_data_sources(self, num_shards: int, index: int, contiguous=True) -> \"MixMultiSourceExampleIterable\":\n        \"\"\"Shard the underlying iterables in a roundrobin manner.\n\n        Let's consider we have our iterables as [[s0_0, s0_1], [s1_0], [s2_0, s2_1, s2_3]],\n        and we request 3 shards.\n        index 0 gets s0_0 s2_0\n        index 1 gets s0_1 s2_1\n        index 2 gets s1_0 s2_3\n        \"\"\"\n        return MixMultiSourcesExampleIterable(\n            list(\n                islice(\n                    # flatten all underlying iterables\n                    chain.from_iterable([ex_iterable.shard_data_sources(1, 0) for ex_iterable in self.ex_iterables]),\n                    # offset the starting point by the index\n                    index,\n                    # take over the full list, so exhaust the iterators\n                    None,\n                    # step by the number of shards requested\n                    num_shards,\n                )\n            )\n        )\n\n\ndef mix_dataset(iterable_datasets: list[datasets.IterableDataset]) -> IterableDataset:\n    ex_iterable = MixMultiSourcesExampleIterable([ds._ex_iterable for ds in iterable_datasets])\n    return IterableDataset(\n        ex_iterable, distributed=iterable_datasets[0]._distributed, formatting=iterable_datasets[0]._formatting\n    )\n\n```\n\n### Questions\n\n- Am I missing something? Is there a way to use `interleave_datasets` or `concatenate_datasets` to fit my purpose?\n- Would it be the right approach to spread the maximum number of underlying shards across my different processes?\n\n### Your contribution\n\nAs much as I can.<end_of_turn>\n<start_of_turn>model\nConcatenate IterableDataset instances and distribute underlying shards in a RoundRobin manner<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am using the ArrowWriter from datasets.arrow_writer to save a json-style file as arrow files. Within the dictionary, it contains a feature called \"image\" which is a list of PIL.Image objects.\r\nI am saving the json using the following script:\r\n```\r\ndef save_to_arrow(path,temp):\r\n    with ArrowWriter(path=path,writer_batch_size=20) as writer: \r\n        writer.write_batch(temp) \r\n        writer.finalize() \r\n```\r\nHowever, when I attempt to restore the dataset and use the ```Dataset.from_file(path)``` function to load the arrow file, there seems to be an issue with the PIL.Image object in the dataset. The list of PIL.Images appears as follows rather than a normal PIL.Image object:\r\n![1693051705440](https://github.com/huggingface/datasets/assets/14247682/03b204c2-d0fa-4d19-beff-6f4d7b83c848)\r\n\n\n### Steps to reproduce the bug\n\n1. Storing the data json into arrow files:\r\n```\r\ndef save_to_arrow(path,temp):\r\n    with ArrowWriter(path=path,writer_batch_size=20) as writer: \r\n        writer.write_batch(temp) \r\n        writer.finalize() \r\nsave_to_arrow( path, json_file )\r\n```\r\n2.  try to load the arrow file into the Dataset object using the ```Dataset.from_file(path)```\r\n\n\n### Expected behavior\n\nExcept to saving the contained \"image\" feature as a list PIL.Image objects as the arrow file. And I can restore the dataset from the file.\n\n### Environment info\n\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.4.0-150-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.4.4<end_of_turn>\n<start_of_turn>model\nError in saving the PIL image into *.arrow files using datasets.arrow_writer<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhat I did:\r\n- Fixed a small typo in README to improve clarity\r\n- Fixed repeated word \"frameworks frameworks\"\r\n- Split long paragraphs into shorter sentences for readability\r\n- Added # Example comments before code blocks for clarity\r\n\r\nWhy:\r\n- Helps new users avoid confusion\r\n\r\nHow I tested:\r\n- Checked locally in Markdown preview<end_of_turn>\n<start_of_turn>model\nDocs: fix typo, improve readability, add code comments<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt appears there have been two releases of fsspec since this dependency was last updated, it would be great if Datasets could be updated so that it didn't hold back the usage of newer fsspec versions in consuming projects.\r\n\r\nPR based on https://github.com/huggingface/datasets/pull/7352<end_of_turn>\n<start_of_turn>model\nupdate fsspec 2025.3.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have 26,000,000 3-tuples. When I use Dataset.from_dict() to load, neither. py nor Jupiter notebook can run successfully. This is my code:\r\n```\r\n    # len(example_data) is 26,000,000, 'diff' is a text\r\n    diff1_list = [example_data[i].texts[0] for i in range(len(example_data))]\r\n    diff2_list = [example_data[i].texts[1] for i in range(len(example_data))]\r\n    label_list = [example_data[i].label for i in range(len(example_data))]\r\n\r\n    embedding_dataset = Dataset.from_dict({\r\n        \"diff1\": diff1_list,\r\n        \"diff2\": diff2_list,\r\n        \"label\": label_list\r\n    })\r\n```\n\n### Steps to reproduce the bug\n\n1. Initialize a large 3-tuple, e.g. 26,000,000\r\n2. Use Dataset.from_dict() to load\n\n### Expected behavior\n\nDataset.from_dict() run successfully\n\n### Environment info\n\nsentence-transformers                   3.3.1<end_of_turn>\n<start_of_turn>model\nDataset.from_dict() can't handle large dict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCI fails after latest huggingface_hub-0.23.0 release: https://github.com/huggingface/huggingface_hub/releases/tag/v0.23.0\r\n\r\n```\r\nFAILED tests/test_metric_common.py::LocalMetricTest::test_load_metric_bertscore - FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\nFAILED tests/test_metric_common.py::LocalMetricTest::test_load_metric_frugalscore - FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\nFAILED tests/test_metric_common.py::LocalMetricTest::test_load_metric_perplexity - FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\nFAILED tests/test_fingerprint.py::TokenizersHashTest::test_hash_tokenizer - FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\nFAILED tests/test_fingerprint.py::TokenizersHashTest::test_hash_tokenizer_with_cache - FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\nFAILED tests/test_arrow_dataset.py::MiscellaneousDatasetTest::test_set_format_encode - FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n```<end_of_turn>\n<start_of_turn>model\nCI fails after huggingface_hub-0.23.0 release: FutureWarning: \"resume_download\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nAdd support for prefetching the next n batches through iterabledataset to reduce batch loading bottleneck in training loop.\r\n\r\n### Motivation\r\n\r\nThe primary motivation behind this is to use hardware accelerators alongside a streaming dataset. This is required when you are in a low ram or low disk space setting as well as quick iteration where you're iterating though different accelerator environments (e.x changing ec2 instances quickly to figure out batch/sec for a particular architecture). \r\n\r\nCurrently, using the IterableDataset results in accelerators becoming basically useless due to the massive bottleneck induced by the dataset lazy loading/transform/mapping.\r\n\r\nI've considered two alternatives:\r\nPyTorch dataloader that handles this. However, I'm using jax, and I believe this is a piece of functionality that should live in the stream class.\r\n\r\nReplicating the \"num_workers\" part of the PyTorch DataLoader to eagerly load batches and apply the transform so Arrow caching will automatically cache results and make them accessible.\r\n\r\n### Your contribution\r\n\r\nI may or may not have time to do this. Currently, I've written the basic multiprocessor approach to handle the eager DataLoader for my own use case with code that's not integrated to datasets. I'd definitely see this as being the default over the regular Dataset for most people given that they wouldn't have to wait on the datasets while also not worrying about performance.<end_of_turn>\n<start_of_turn>model\nPrefetching for IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nSplitting off https://github.com/huggingface/huggingface_hub/issues/1997 - currently `huggingface-cli delete-cache` doesn't take care of cleaning `datasets` temp files\r\n\r\ne.g. I discovered having millions of files under `datasets/downloads` cache, I had to do:\r\n\r\n```\r\nsudo find /data/huggingface/datasets/downloads -type f -mtime +3 -exec rm {} \\+\r\nsudo find /data/huggingface/datasets/downloads -type d -empty -delete\r\n```\r\n \r\ncould the cleanup be integrated into `huggingface-cli` or a different tool provided to keep the folders tidy and not consume inodes and space \r\n\r\ne.g. there were tens of thousands of `.lock` files - I don't know why they never get removed - lock files should be temporary for the duration of the operation requiring the lock and not remain after the operation finished, IMHO.\r\n\r\nAlso I think one should be able to nuke `datasets/downloads` w/o hurting the cache, but I think there are some datasets that rely on files extracted under this dir - or at least they did in the past - which is very difficult to manage since one has no idea what is safe to delete and what not.\r\n\r\nThank you\r\n\r\n@Wauplin (requested to be tagged)<end_of_turn>\n<start_of_turn>model\n`datasets/downloads` cleanup tool<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nTraceback (most recent call last):\n  File \"/workspace/ctn.py\", line 8, in <module>\n    ds = load_dataset(f\"naver-clova-ix/synthdog-{lang}\")  # \u0438\u043b\u0438 \"synthdog-zh\" \u0434\u043b\u044f \u043a\u0438\u0442\u0430\u0439\u0441\u043a\u043e\u0433\u043e\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1397, in load_dataset\n    builder_instance = load_dataset_builder(\n        path=path,\n    ...<10 lines>...\n        **config_kwargs,\n    )\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1185, in load_dataset_builder\n    builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 612, in _use_legacy_cache_dir_if_possible\n    self._check_legacy_cache2(dataset_module) or self._check_legacy_cache() or None\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 485, in _check_legacy_cache2\n    config_id = self.config.name + \"-\" + Hasher.hash({\"data_files\": self.config.data_files})\n                                         ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/fingerprint.py\", line 188, in hash\n    return cls.hash_bytes(dumps(value))\n                          ~~~~~^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 120, in dumps\n    dump(obj, file)\n    ~~~~^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 114, in dump\n    Pickler(file, recurse=True).dump(obj)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 428, in dump\n    StockPickler.dump(self, obj)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^\n  File \"/usr/lib/python3.14/pickle.py\", line 498, in dump\n    self.save(obj)\n    ~~~~~~~~~^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 70, in save\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 422, in save\n    StockPickler.save(self, obj, save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.14/pickle.py\", line 572, in save\n    f(self, obj)  # Call unbound method with explicit self\n    ~^^^^^^^^^^^\n  File \"/workspace/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 1262, in save_module_dict\n    StockPickler.save_dict(pickler, obj)\n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/usr/lib/python3.14/pickle.py\", line 1064, in save_dict\n    self._batch_setitems(obj.items(), obj)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\n\n\n### Steps to reproduce the bug\n\nds_train = ds[\"train\"].map(lambda x: {**x, \"lang\": lang})\n\n\n### Expected behavior\n\nFixed bugs\n\n### Environment info\n\n\n- `datasets` version: 4.2.0\n- Platform: Linux-6.8.0-85-generic-x86_64-with-glibc2.39\n- Python version: 3.14.0\n- `huggingface_hub` version: 0.35.3\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.3\n- `fsspec` version: 2025.9.0\n<end_of_turn>\n<start_of_turn>model\nCaching does not work when using python3.14<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n> should we actually allow characters like spaces? maybe it's better to add validation for whitespace symbols and directly in datasets and raise\r\n\r\nhttps://github.com/huggingface/datasets-server/issues/853\r\n<end_of_turn>\n<start_of_turn>model\nSet a rule on the config and split names<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse pandas ujson in JSON loader to improve performance.\r\n\r\nNote that `datasets` has `pandas` as required dependency. And `pandas` includes `ujson` in `pd.io.json.ujson_loads`.\r\n\r\nFix #6867.\r\n\r\nCC: @natolambert<end_of_turn>\n<start_of_turn>model\nUse pandas ujson in JSON loader to improve performance<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(and also reorder the hffs args to improve caching)\r\n\r\nWhen using `DataLoader(iterable_dataset, num_workers=...)` the dataset is pickled and passed to the worker. However previously the resulting dataset would be in a process with an empty hffs cache. By keeping the cache attached to `IterableDataset`, the cached hffs instances are pickled with the dataset and re-populates the cache in the DataLoader workers\r\n\r\nthis requires https://github.com/huggingface/huggingface_hub/pull/3443 to work effectively though, otherwise the unpickled hffs cache would start empty\r\n\r\n\r\ncc @andimarafioti @ltmeyer<end_of_turn>\n<start_of_turn>model\nKeep hffs cache in workers when streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFor researchers in some countries or regions, it is usually the case that the download ability of `load_dataset` is disabled due to the complex network environment. People in these regions often prefer to use git clone or other programming tricks to manually download the files to the disk (for example, [How to elegantly download hf models, zhihu zhuanlan](https://zhuanlan.zhihu.com/p/475260268) proposed a crawlder based solution,  and [Is there any mirror for hf_hub, zhihu answer](https://www.zhihu.com/question/371644077) provided some cloud based solutions, and [How to avoid pitfalls on Hugging face downloading, zhihu zhuanlan] gave some useful suggestions), and then use `load_from_disk` to get the dataset object. \r\nHowever, when one finally has the local files on the disk, it is still buggy when trying to load the files into objects. \n\n### Steps to reproduce the bug\n\nSteps to reproduce the bug:\r\n1. Found CIFAR dataset in hugging face: https://huggingface.co/datasets/cifar100/tree/main\r\n2. Click \":\" button to show \"Clone repository\" option, and then follow the prompts on the box:\r\n ```bash\r\n  cd my_directory_absolute\r\n  git lfs install\r\n  git clone https://huggingface.co/datasets/cifar100\r\n  ls  my_directory_absolute/cifar100 # confirm that the directory exists and it is OK. \r\n  ```\r\n3. Write A python file to try to load the dataset\r\n```python\r\nfrom datasets import load_dataset, load_from_disk\r\ndataset = load_from_disk(\"my_directory_absolute/cifar100\")\r\n```\r\nNotice that according to issue #3700 , it is wrong to use load_dataset(\"my_directory_absolute/cifar100\"), so we must use load_from_disk instead. \r\n\r\n4. Then you will see the error reported:\r\n```log\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nCell In[5], line 9\r\n      1 from datasets import load_dataset, load_from_disk\r\n----> 9 dataset = load_from_disk(\"my_directory_absolute/cifar100\")\r\n\r\nFile [~/miniconda3/envs/ai/lib/python3.10/site-packages/datasets/load.py:2232), in load_from_disk(dataset_path, fs, keep_in_memory, storage_options)\r\n   2230     return DatasetDict.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\r\n   2231 else:\r\n-> 2232     raise FileNotFoundError(\r\n   2233         f\"Directory {dataset_path} is neither a `Dataset` directory nor a `DatasetDict` directory.\"\r\n   2234     )\r\n\r\nFileNotFoundError: Directory my_directory_absolute/cifar100 is neither a `Dataset` directory nor a `DatasetDict` directory.\r\n```\n\n### Expected behavior\n\nThe dataset should be load successfully. \n\n### Environment info\n\n```bash\r\ndatasets-cli env\r\n```\r\n-> results:\r\n```txt\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.14.2\r\n- Platform: Linux-4.18.0-372.32.1.el8_6.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n```<end_of_turn>\n<start_of_turn>model\nraise FileNotFoundError(\"Directory {dataset_path} is neither a `Dataset` directory nor a `DatasetDict` directory.\" )<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfor https://huggingface.co/datasets/allenai/olmo-mix-1124<end_of_turn>\n<start_of_turn>model\nConvert to string when needed + faster .zstd<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\njust added torch.no_grad and eval()<end_of_turn>\n<start_of_turn>model\nMinor multi gpu doc improvement<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes a case when concatenating or interleaving datasets with `with_format(...)` call was slower.\r\nDetails here: https://github.com/huggingface/datasets/issues/6637\r\n\r\n@lhoestq I tried to minimize the duplication between iter and iter_arrow methods, not sure if this is against the design, can separate those if needed.<end_of_turn>\n<start_of_turn>model\nAdd support for arrow iterable when concatenating or interleaving<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAccording to the docs of Datasets.from_generator:\r\n```\r\n        gen_kwargs(`dict`, *optional*):\r\n            Keyword arguments to be passed to the `generator` callable.\r\n            You can define a sharded dataset by passing the list of shards in `gen_kwargs`.\r\n```\r\nSo I'd expect that if gen_kwargs was a list, then my generator would be called once for each element in the list with the dict in the list for that element.\r\nIt doesn't work that way though.\n\n### Steps to reproduce the bug\n\n```python\r\n#!/usr/bin/python\r\n\r\nfrom pathlib import Path\r\nimport datasets\r\n\r\ndef process_yaml(file):\r\n    yield dict(example=42)\r\n\r\n\r\nif __name__ == '__main__':\r\n    import sys\r\n    dir = Path(sys.argv[0]).parent\r\n    ds = datasets.Dataset.from_generator(process_yaml, gen_kwargs=[{'file':f} for f in dir.glob('*.yml')],\r\n        )\r\n    ds.to_json('training.jsonl')\r\n    \r\n\r\n```\r\n```\r\nGenerating train split: 0 examples [00:00, ? examples/s]\r\nTraceback (most recent call last):\r\n  File \"/tmp/dataset_bug.py\", line 13, in <module>\r\n    ds = datasets.Dataset.from_generator(process_yaml, gen_kwargs=[{'file':f} for f in dir.glob('*.yml')],\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/hartmans/ai/venv/lib/python3.11/site-packages/datasets/arrow_dataset.py\", line 1072, in from_generator\r\n    ).read()\r\n      ^^^^^^\r\n  File \"/home/hartmans/ai/venv/lib/python3.11/site-packages/datasets/io/generator.py\", line 47, in read\r\n    self.builder.download_and_prepare(\r\n  File \"/home/hartmans/ai/venv/lib/python3.11/site-packages/datasets/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/hartmans/ai/venv/lib/python3.11/site-packages/datasets/builder.py\", line 1717, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/home/hartmans/ai/venv/lib/python3.11/site-packages/datasets/builder.py\", line 1049, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/hartmans/ai/venv/lib/python3.11/site-packages/datasets/builder.py\", line 1555, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/home/hartmans/ai/venv/lib/python3.11/site-packages/datasets/builder.py\", line 1656, in _prepare_split_single\r\n    generator = self._generate_examples(**gen_kwargs)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nTypeError: datasets.packaged_modules.generator.generator.Generator._generate_examples() argument after ** must be a ```\r\nmapping, not list\r\n\n\n### Expected behavior\n\nI would expect that process_yaml would be called once for each yaml file in the directory where the script is run.\r\nI also tried with the list being in gen_kwargs, but in that case process_yaml gets called with a list.\r\n\n\n### Environment info\n\n- `datasets` version: 2.14.6.dev0 (git commit 0cc77d7f45c7369; also tested with 2.14.0)\r\n- Platform: Linux-6.1.0-10-amd64-x86_64-with-glibc2.36\r\n- Python version: 3.11.2\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n<end_of_turn>\n<start_of_turn>model\nDataset.from_generator raises with sharded gen_args<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsee https://github.com/huggingface/datasets-server/pull/2554#discussion_r1516516867<end_of_turn>\n<start_of_turn>model\nAdd details in docstring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi to whoever is reading this! \ud83e\udd17 (Most likely @mariosasko)\r\n\r\n## What's in this PR?\r\n\r\nThis PR replaces the `open` from Python with `fsspec.open` and adds the argument `storage_options` for the methods `to_json`, `to_csv`, and `to_parquet`, to allow users to export any \ud83e\udd17`Dataset` into a file in a file-system as requested at #6086.\r\n\r\n## What's missing in this PR?\r\n\r\nAs per `to_json`, `to_csv`, and `to_parquet` docstrings for the recently included `storage_options` arg, I've scoped it to 2.15.0, so we should check that before merging in case we want to scope that for 2.14.2 instead.\r\n\r\nAdditionally, should we also add `fsspec` support for the `from_csv`, `from_json`, and `from_parquet` methods? If you want me to do so @mariosasko just let me know and I'll create another PR to support that too!\r\n\r\nFix #6086.<end_of_turn>\n<start_of_turn>model\nAdd `fsspec` support for `to_json`, `to_csv`, and `to_parquet`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate required `soxr` version from pre-release to release 0.4.0: https://github.com/dofuuz/python-soxr/releases/tag/v0.4.0<end_of_turn>\n<start_of_turn>model\nUpdate required soxr version from pre-release to release<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDiscussion in https://github.com/huggingface/datasets/issues/5798<end_of_turn>\n<start_of_turn>model\nAdd parallel module using joblib for Spark<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes:\r\n* bumps the PyArrow version check in the `cast_array_to_feature` to avoid the offset bug (still not fixed)\r\n* aligns the Pandas formatting tests with the Numpy ones (the current test fails due to https://github.com/apache/arrow/pull/35656, which requires `.to_pandas(coerce_temporal_nanoseconds=True)` to always return `datetime [ns]` objects)\r\n\r\nFix #6173 \r\n<end_of_turn>\n<start_of_turn>model\nPyArrow 13 CI fixes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse acronyms consistently across document_dataset docs.<end_of_turn>\n<start_of_turn>model\ndocs: document_dataset PDFs & OCR<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSince Aug 30, 2024, CI benchmarks are broken: https://github.com/huggingface/datasets/actions/runs/11108421214/job/30861323975\r\n```\r\n{\"level\":\"error\",\"message\":\"Resource not accessible by integration\",\"name\":\"HttpError\",\"request\":{\"body\":\"{\\\"body\\\":\\\"<details>\\\\n<summary>Show benchmarks</summary>\\\\n\\\\nPyArrow==8.0.0\\\\n\\\\n<details>\\\\n<summary>Show updated benchmarks!</summary>\\\\n\\\\n### Benchmark: benchmark_array_xd.json\\\\n\\\\n| metric | read_batch_formatted_as_numpy after write_array2d |\r\n...\r\n\"headers\":{\"accept\":\"application/vnd.github.v3+json\",\"authorization\":\"token [REDACTED]\",\"content-type\":\"application/json; charset=utf-8\",\"user-agent\":\"octokit-rest.js/18.0.0 octokit-core.js/3.6.0 Node.js/16.20.2 (linux; x64)\"},\"method\":\"POST\",\"request\":{\"agent\":{\"_events\":{},\"_eventsCount\":2,\"cache\":\r\n...\r\n\"response\":{\"data\":{\"documentation_url\":\"https://docs.github.com/rest/issues/comments#create-an-issue-comment\",\"message\":\"Resource not accessible by integration\",\"status\":\"403\"},\r\n...\r\n\"stack\":\"HttpError: Resource not accessible by integration\\n    at /usr/lib/node_modules/@dvcorg/cml/node_modules/@octokit/request/dist-node/index.js:86:21\\n    at processTicksAndRejections (node:internal/process/task_queues:96:5)\\n    at async Job.doExecute (/usr/lib/node_modules/@dvcorg/cml/node_modules/bottleneck/light.js:405:18)\",\"status\":403}\r\n```<end_of_turn>\n<start_of_turn>model\nCI benchmarks are broken<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIterating over a [`Column`](https://github.com/huggingface/datasets/blob/8b1bd4ec1cc9e9ce022f749abb6485ef984ae7c0/src/datasets/arrow_dataset.py#L633-L692) iterates through the parent [`Dataset`](https://github.com/huggingface/datasets/blob/8b1bd4ec1cc9e9ce022f749abb6485ef984ae7c0/src/datasets/arrow_dataset.py#L695) and applies all formatting/transforms on each row, regardless of which column is being accessed. This causes an error when transforms depend on columns not present in the projection.\n\n### Steps to reproduce the bug\n\n### Load a dataset with multiple columns\n\n```python\nds = load_dataset(\"mrbrobot/isic-2024\", split=\"train\")\n```\n\n### Define a transform that specifies an input column\n\n```python\ndef image_transform(batch):\n    batch[\"image\"] = batch[\"image\"]  # KeyError when batch doesn't contain \"image\"\n    return batch\n\n# apply transform only to image column\nds = ds.with_format(\"torch\")\nds = ds.with_transform(image_transform, columns=[\"image\"], output_all_columns=True)\n```\n\n### Iterate over non-specified column\n\n```python\n# iterate over a different column, triggers the transform on each row, but batch doesn't contain \"image\"\nfor t in ds[\"target\"]:  # KeyError: 'image'\n    print(t)\n```\n\n### Expected behavior\n\nIf a user iterates over `ds[\"target\"]` and the transform specifies `columns=[\"image\"]`, the transform should be skipped.\n\n### Environment info\n\n`datasets`: 4.2.0\nPython: 3.12.12\nLinux: Debian 11.11<end_of_turn>\n<start_of_turn>model\nTransform with columns parameter triggers on non-specified column access<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"animetimm/danbooru-wdtagger-v4-w640-ws-30k\")\n```\n\ngot\n\n```\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 626, in write_batch\n    arrays.append(pa.array(typed_sequence))\n  File \"pyarrow/array.pxi\", line 255, in pyarrow.lib.array\n  File \"pyarrow/array.pxi\", line 117, in pyarrow.lib._handle_arrow_array_protocol\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 258, in __arrow_array__\n    out = cast_array_to_feature(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2006, in cast_array_to_feature\n    arrays = [\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2007, in <listcomp>\n    _c(array.field(name) if name in array_fields else null_array, subfeature)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2066, in cast_array_to_feature\n    casted_array_values = _c(array.values, feature.feature)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 2103, in cast_array_to_feature\n    return array_cast(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1798, in wrapper\n    return func(array, *args, **kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/table.py\", line 1949, in array_cast\n    raise TypeError(f\"Couldn't cast array of type {_short_str(array.type)} to {_short_str(pa_type)}\")\nTypeError: Couldn't cast array of type string to null\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/load.py\", line 2084, in load_dataset\n    builder_instance.download_and_prepare(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 925, in download_and_prepare\n    self._download_and_prepare(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1649, in _download_and_prepare\n    super()._download_and_prepare(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1001, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1487, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n  File \"/home/ubuntu/miniconda3/lib/python3.10/site-packages/datasets/builder.py\", line 1644, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n```\n\n`datasets==3.5.1` whats wrong\n\nits inner json structure is like\n\n```yaml\n  features:\n    - name: \"image\"\n      dtype: \"image\"\n    - name: \"json.id\"\n      dtype: \"string\"\n    - name: \"json.width\"\n      dtype: \"int32\"\n    - name: \"json.height\"\n      dtype: \"int32\"\n    - name: \"json.rating\"\n      sequence:\n        dtype: \"string\"\n    - name: \"json.general_tags\"\n      sequence:\n        dtype: \"string\"\n    - name: \"json.character_tags\"\n      sequence:\n        dtype: \"string\"\n```\n\ni'm 100% sure all the jsons satisfies the abovementioned format.\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"animetimm/danbooru-wdtagger-v4-w640-ws-30k\")\n```\n\n### Expected behavior\n\nload the dataset successfully, with the abovementioned json format and webp images\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\n\n- `datasets` version: 3.5.1\n- Platform: Linux-6.8.0-52-generic-x86_64-with-glibc2.35\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 20.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0\n<end_of_turn>\n<start_of_turn>model\nTypeError: Couldn't cast array of type string to null on webdataset format dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf you `import` a module that runs `datasets.map(f, num_proc=N)` at the top-level, Python hangs.\n\n\n### Steps to reproduce the bug\n\n1. Create a file that runs datasets.map at the top-level:\n\n```bash\ncat <<EOF > import_me.py\nimport datasets\n\nthe_dataset = datasets.load_dataset(\"openai/openai_humaneval\")\nthe_dataset = the_dataset.map(lambda item: item, num_proc=2)\nEOF\n```\n\n2. Start Python REPL:\n\n```bash\nuv run --python 3.12.3 --with \"datasets==4.0.0\"  python3\nPython 3.12.3 (main, Aug 14 2025, 17:47:21) [GCC 13.3.0] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n```\n\n3. Import the file:\n\n```python\nimport import_me\n````\n\nObserve hang.\n\n### Expected behavior\n\nIdeally would not hang, or would fallback to num_proc=1 with a warning.\n\n### Environment info\n\n\n- `datasets` version: 4.0.0\n- Platform: Linux-6.14.0-29-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- `huggingface_hub` version: 0.34.4\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.2\n- `fsspec` version: 2025.3.0\n<end_of_turn>\n<start_of_turn>model\ndatasets.map(f, num_proc=N) hangs with N>1 when run on import<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen I tried to download opus_books using:\n\nfrom datasets import load_dataset\ndataset = load_dataset(\"Helsinki-NLP/opus_books\")\n\nI got the following errors:\nFileNotFoundError: Couldn't find any data file at /workspace/Helsinki-NLP/opus_books. Couldn't find 'Helsinki-NLP/opus_books' on the Hugging Face Hub either: LocalEntryNotFoundError: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.\n\nI also tried:\ndataset = load_dataset(\"opus_books\", \"en-zh\")\n\nand the errors remain the same. However, I can download \"mlabonne/FineTome-100k\" successfully. \nMy datasets is version 4.2.0\n\nAny clues? Big thanks.<end_of_turn>\n<start_of_turn>model\nCannot download opus dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI ran this code:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"common-canvas/commoncatalog-cc-by\", cache_dir=\"/datadrive/datasets/cc\", num_proc=1000)\r\n```\r\n\r\nAnd got this error.\r\nBefore it was some other file though (lie something...incomplete)\r\n\r\nrunnting\r\n```\r\nulimit -n 8192\r\n```\r\ndid not help at all.\n\n### Steps to reproduce the bug\n\nRun the code i sent\n\n### Expected behavior\n\nShould be no errors \n\n### Environment info\n\nlinux, jupyter lab.<end_of_turn>\n<start_of_turn>model\nToo many open files: '/root/.cache/huggingface/token'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReported in https://github.com/huggingface/datasets/issues/4760\r\n\r\nThe cache was not able to reload a dataset with a single config form the cache if the config name is not specificed\r\n\r\nFor example\r\n\r\n```python\r\nfrom datasets import load_dataset, config\r\n\r\nconfig.HF_DATASETS_OFFLINE = True\r\n\r\nload_dataset(\"openai_humaneval\")\r\n```\r\n\r\nThis was due to a regression in https://github.com/huggingface/datasets/pull/6632<end_of_turn>\n<start_of_turn>model\nFix offline mode with single config<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nbug introduced in #7230, it was preventing the Viewer limit writes to work<end_of_turn>\n<start_of_turn>model\nAlways set non-null writer batch size<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen calling `IterableDatasetDict.map()`, each split\u2019s `IterableDataset.map()` is invoked without a `features` argument. While omitting the argument isn\u2019t itself incorrect, the implementation then sets `info.features = features`, which destroys the original `features` content. Since `IterableDataset.column_names` relies on `info.features`, it ends up broken (`None`).\n\n**Reproduction**\n\n1.  Define an IterableDatasetDict with a non-None features schema.\n2.  my_iterable_dataset_dict contains \"text\" column.\n3. Call:\n```Python\nnew_dict = my_iterable_dataset_dict.map(\n    function=my_fn,\n    with_indices=False,\n    batched=True,\n    batch_size=16,\n)\n```\n4. Observe\n```Python\nnew_dict[\"train\"].info.features  # {'text': Value(dtype='string', id=None)}\nnew_dict[\"train\"].column_names   # ['text']\n```\n5. Call:\n```Python\nnew_dict = my_iterable_dataset_dict.map(\n    function=my_fn,\n    with_indices=False,\n    batched=True,\n    batch_size=16,\n    remove_columns=[\"foo\"]\n)\n```\n6. Observe:\n```Python\nnew_dict[\"train\"].info.features  # \u2192 None\nnew_dict[\"train\"].column_names   # \u2192 None\n```\n5. Internally, in dataset_dict.py this loop omits features ([code](https://github.com/huggingface/datasets/blob/b9efdc64c3bfb8f21f8a4a22b21bddd31ecd5a31/src/datasets/dataset_dict.py#L2047C5-L2056C14)):\n```Python\nfor split, dataset in self.items():\n    dataset_dict[split] = dataset.map(\n        function=function,\n        with_indices=with_indices,\n        input_columns=input_columns,\n        batched=batched,\n        batch_size=batch_size,\n        drop_last_batch=drop_last_batch,\n        remove_columns=remove_columns,\n        fn_kwargs=fn_kwargs,\n        # features omitted \u2192 defaults to None\n    )\n```\n7. Then inside IterableDataset.map() ([code](https://github.com/huggingface/datasets/blob/b9efdc64c3bfb8f21f8a4a22b21bddd31ecd5a31/src/datasets/iterable_dataset.py#L2619C1-L2622C37)) correct `info.features` is replaced by features which is None:\n```Python\ninfo = self.info.copy()\ninfo.features = features  # features is None here\nreturn IterableDataset(..., info=info, ...)\n```\n\n**Suggestion**\nIt looks like this replacement was added intentionally but maybe should be done only if `features` is `not None`.\n\n**Workarround:**\n`SFTTrainer` calls `dataset.map()` several times and then fails on `NoneType` when iterating `dataset.column_names`.\nI decided to write this patch - works form me.\n\n```python\ndef patch_iterable_dataset_map():\n    _orig_map = IterableDataset.map\n\n    def _patched_map(self, *args, **kwargs):\n        if \"features\" not in kwargs or kwargs[\"features\"] is None:\n            kwargs[\"features\"] = self.info.features\n        return _orig_map(self, *args, **kwargs)\n\n    IterableDataset.map = _patched_map\n```\n\n\n\n\n<end_of_turn>\n<start_of_turn>model\n`IterableDatasetDict.map()` call removes `column_names` (in fact info.features)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix https://github.com/huggingface/datasets/issues/7842<end_of_turn>\n<start_of_turn>model\nfix column with transform<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nMake `push_to_hub` atomic (#7600)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCI quality is broken: https://github.com/huggingface/datasets/actions/runs/9838873879/job/27159697027\r\n```\r\nerror: `ruff <path>` has been removed. Use `ruff check <path>` instead.\r\n```<end_of_turn>\n<start_of_turn>model\nCI quality is broken: use ruff check instead<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe transform fingerprinting logic relies on the `random` library for random bits when the function is not hashable (e.g. bound methods as used in `trl`: https://github.com/huggingface/trl/blob/main/trl/trainer/dpo_trainer.py#L356). This causes collisions when the training code sets a fixed random seed, which is common practice: https://github.com/huggingface/alignment-handbook/blob/main/recipes/zephyr-7b-beta/sft/config_full.yaml#L45.\r\n\r\nThis results in fingerprint collisions which leads to silently loading incorrect cache files corresponding to completely different datasets.\n\n### Steps to reproduce the bug\n\nn/a\n\n### Expected behavior\n\nUse `uuid` v4 instead of `random.getrandbits()`\n\n### Environment info\n\n`datasets` main branch<end_of_turn>\n<start_of_turn>model\nTransform fingerprint collisions due to setting fixed random seed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 4.1.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.15.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix docs phrasing.<end_of_turn>\n<start_of_turn>model\nFix docs phrasing about supported formats when sharing a dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse acronyms consistently across document_dataset docs.<end_of_turn>\n<start_of_turn>model\ndocs: document_dataset PDFs & OCR<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/6877<end_of_turn>\n<start_of_turn>model\nClose gzipped files properly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe link to \"the [WebDataset documentation](https://webdataset.github.io/webdataset).\" is broken. \nhttps://huggingface.co/docs/datasets/main/en/video_dataset#webdataset \n\n<img width=\"2048\" height=\"264\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/975dd10c-aad8-42fc-9fbc-de0e2747a326\" /><end_of_turn>\n<start_of_turn>model\nBroken link in documentation for \"Create a video dataset\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nMap has been taking extremely long to preprocess my data.\r\n\r\nIt seems to process 1000 examples (which it does really fast in about 10 seconds), then it hangs for a good 1-2 minutes, before it moves on to the next batch of 1000 examples.\r\n\r\nIt also keeps eating up my hard drive space for some reason by creating a file named tmp1335llua that is over 300GB.\r\n\r\nTrying to set num_proc to be >1 also gives me the following error: NameError: name 'processor' is not defined\r\n\r\nPlease advise on how I could optimise this?\n\n### Steps to reproduce the bug\n\nIn general, I have been using map as per normal. Here is a snippet of my code:\r\n````\r\n###########################        DATASET LOADING AND PREP        #########################\r\n\r\ndef load_custom_dataset(split):\r\n    ds = []\r\n    if split == 'train':\r\n        for dset in args.train_datasets:\r\n            ds.append(load_from_disk(dset))\r\n    if split == 'test':\r\n        for dset in args.test_datasets:\r\n            ds.append(load_from_disk(dset))\r\n\r\n    ds_to_return = concatenate_datasets(ds)\r\n    ds_to_return = ds_to_return.shuffle(seed=22)\r\n    return ds_to_return\r\n\r\n\r\n\r\ndef prepare_dataset(batch):\r\n    # load and (possibly) resample audio data to 16kHz\r\n    audio = batch[\"audio\"]\r\n\r\n    # compute log-Mel input features from input audio array\r\n    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\r\n    # compute input length of audio sample in seconds\r\n    batch[\"input_length\"] = len(audio[\"array\"]) / audio[\"sampling_rate\"]\r\n\r\n    # optional pre-processing steps\r\n    transcription = batch[\"sentence\"]\r\n    if do_lower_case:\r\n        transcription = transcription.lower()\r\n    if do_remove_punctuation:\r\n        transcription = normalizer(transcription).strip()\r\n\r\n    # encode target text to label ids\r\n    batch[\"labels\"] = processor.tokenizer(transcription).input_ids\r\n    return batch\r\n\r\nprint('DATASET PREPARATION IN PROGRESS...')\r\n\r\n# case 3: combine_and_shuffle is true, only train provided\r\n# load train datasets\r\ntrain_set = load_custom_dataset('train')\r\n\r\n # split dataset\r\nraw_dataset = DatasetDict()\r\nraw_dataset = train_set.train_test_split(test_size = args.test_size, shuffle=True, seed=42)\r\n\r\nraw_dataset = raw_dataset.cast_column(\"audio\", Audio(sampling_rate=args.sampling_rate))\r\n\r\nprint(\"Before Map:\")\r\nprint(raw_dataset)\r\n\r\nraw_dataset = raw_dataset.map(prepare_dataset, num_proc=1)\r\n\r\nprint(\"After Map:\")\r\nprint(raw_dataset)\r\n````\r\n\n\n### Expected behavior\n\nBased on the speed at which map is processing examples, I would expect a 5-6 hours completion for all mapping\r\n\r\nHowever, because it hangs every 1000 examples, I instead roughly estimate it would take about  40 hours!\r\n\r\nMoreover, i cant even finish the map because it keeps exponentially eating up my hard drive space\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Windows-10-10.0.22631-SP0\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nIssue with map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(basically the same content as the hfh upload docs, but adapted for datasets)<end_of_turn>\n<start_of_turn>model\nDocument usage of hfh cli instead of git<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI've been using the train_test_split method in the datasets module to split my HuggingFace Dataset into separate training, validation, and testing subsets. However, I've noticed an issue where the split datasets appear to maintain the same underlying PyArrow Table.\n\n### Steps to reproduce the bug\n\n1. Load any dataset  ```dataset = load_dataset(\"lhoestq/demo1\")``` \r\n2. Try the next code:\r\n```python\r\nfrom datasets import Dataset, DatasetDict\r\n\r\ntrain_size = 0.6\r\n\r\nsplit_train = dataset[\"train\"].train_test_split(\r\n    train_size=train_size,\r\n)\r\n\r\nseparate_dataset_dict = DatasetDict({\r\n    \"train\": split_train[\"train\"],\r\n    \"test\": split_train[\"test\"],\r\n})\r\n```\r\n3.  The next code ```print(separate_dataset_dict)``` when printing the dataset it gives the indication that they have 3 and 2 rows respectively.\r\n4. But the next code: \r\n ```python\r\nprint(len(separate_dataset_dict[\"train\"].data['id']))\r\nprint(len(separate_dataset_dict[\"test\"].data['id'])) \r\n```\r\n\r\n Indicates that both tables still have 5 rows.\n\n### Expected behavior\n\nHowever, I've noticed that train_test_split[\"train\"].data, test_val_split[\"train\"].data, and test_val_split[\"test\"].data are identical, suggesting that they all point to the same underlying PyArrow Table. This means that the split datasets are not independent, as I expected.\r\n\r\nI believe this is a bug in the train_test_split implementation, as I would expect this function to return datasets with separate underlying PyArrow Tables. Could you please help me understand if this is expected behavior, or if there's a workaround to create truly independent split datasets?\r\n\r\nI would appreciate any assistance with this issue. Thank you.\n\n### Environment info\n\nI tried in Colab:\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1\r\n\r\nand my PC:\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-5.15.107+-x86_64-with-glibc2.31\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nIssue with train_test_split maintaining the same underlying PyArrow Table<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf you download Squad, it will download the plain_text version, but the config still specifies \"default\", so if you set the offline mode the cache will try to look it up according to the config_id which is \"default\" and this will say;\r\nValueError: Couldn't find cache for squad for config 'default'\r\nAvailable configs in the cache: ['plain_text']\n\n### Steps to reproduce the bug\n\n1. export HF_DATASETS_OFFLINE=0\r\n2. load_dataset(\"squad\")\r\n3. export HF_DATASETS_OFFLINE=1\r\n4. load_dataset(\"squad\")\n\n### Expected behavior\n\nWe should change the config_name I guess? \n\n### Environment info\n\nlinux, latest version of datasets<end_of_turn>\n<start_of_turn>model\nDefault config for squad is incorrect<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen loading a large 2D data (1000 \u00d7 1152) with a large number of (2,000 data in this case) in `load_dataset`, the error message `OSError: Invalid flatbuffers message` is reported. \r\n\r\nWhen only 300 pieces of data of this size (1000 \u00d7 1152) are stored, they can be loaded correctly. \r\n\r\nWhen 2,000 2D arrays are stored in each file, about 100 files are generated, each with a file size of about 5-6GB. But when 300 2D arrays are stored in each file, **about 600 files are generated, which is too many files**.\r\n\r\n### Steps to reproduce the bug\r\n\r\nerror\uff1a\r\n```python\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\nCell In[2], line 4\r\n      1 from datasets import Dataset\r\n      2 from datasets import load_dataset\r\n----> 4 real_dataset = load_dataset(\"arrow\", data_files='tensorData/real_ResidueTensor/*', split=\"train\")#.with_format(\"torch\") # , split=\"train\"\r\n      5 # sim_dataset = load_dataset(\"arrow\", data_files='tensorData/sim_ResidueTensor/*', split=\"train\").with_format(\"torch\")\r\n      6 real_dataset\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/load.py:2151](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/load.py#line=2150), in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2148     return builder_instance.as_streaming_dataset(split=split)\r\n   2150 # Download and prepare data\r\n-> 2151 builder_instance.download_and_prepare(\r\n   2152     download_config=download_config,\r\n   2153     download_mode=download_mode,\r\n   2154     verification_mode=verification_mode,\r\n   2155     num_proc=num_proc,\r\n   2156     storage_options=storage_options,\r\n   2157 )\r\n   2159 # Build dataset for splits\r\n   2160 keep_in_memory = (\r\n   2161     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2162 )\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/builder.py:924](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/builder.py#line=923), in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    922 if num_proc is not None:\r\n    923     prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 924 self._download_and_prepare(\r\n    925     dl_manager=dl_manager,\r\n    926     verification_mode=verification_mode,\r\n    927     **prepare_split_kwargs,\r\n    928     **download_and_prepare_kwargs,\r\n    929 )\r\n    930 # Sync info\r\n    931 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/builder.py:978](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/builder.py#line=977), in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n    976 split_dict = SplitDict(dataset_name=self.dataset_name)\r\n    977 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 978 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    980 # Checksums verification\r\n    981 if verification_mode == VerificationMode.ALL_CHECKS and dl_manager.record_checksums:\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/packaged_modules/arrow/arrow.py:47](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/datasets/packaged_modules/arrow/arrow.py#line=46), in Arrow._split_generators(self, dl_manager)\r\n     45 with open(file, \"rb\") as f:\r\n     46     try:\r\n---> 47         reader = pa.ipc.open_stream(f)\r\n     48     except pa.lib.ArrowInvalid:\r\n     49         reader = pa.ipc.open_file(f)\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/ipc.py:190](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/ipc.py#line=189), in open_stream(source, options, memory_pool)\r\n    171 def open_stream(source, *, options=None, memory_pool=None):\r\n    172     \"\"\"\r\n    173     Create reader for Arrow streaming format.\r\n    174 \r\n   (...)\r\n    188         A reader for the given source\r\n    189     \"\"\"\r\n--> 190     return RecordBatchStreamReader(source, options=options,\r\n    191                                    memory_pool=memory_pool)\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/ipc.py:52](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/ipc.py#line=51), in RecordBatchStreamReader.__init__(self, source, options, memory_pool)\r\n     50 def __init__(self, source, *, options=None, memory_pool=None):\r\n     51     options = _ensure_default_ipc_read_options(options)\r\n---> 52     self._open(source, options=options, memory_pool=memory_pool)\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/ipc.pxi:1006](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/ipc.pxi#line=1005), in pyarrow.lib._RecordBatchStreamReader._open()\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/error.pxi:155](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/error.pxi#line=154), in pyarrow.lib.pyarrow_internal_check_status()\r\n\r\nFile [~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/error.pxi:92](http://localhost:8899/lab/tree/RTC%3Anew_world/esm3/~/miniforge3/envs/esmIne3/lib/python3.12/site-packages/pyarrow/error.pxi#line=91), in pyarrow.lib.check_status()\r\n\r\nOSError: Invalid flatbuffers message.\r\n```\r\n\r\nreproduce\uff1aHere is just an example result, the real 2D matrix is the output of the ESM large model, and the matrix size is approximate\r\n```python\r\nimport numpy as np\r\nimport pyarrow as pa\r\n\r\nrandom_arrays_list = [np.random.rand(1000, 1152) for _ in range(2000)]\r\ntable = pa.Table.from_pydict({\r\n    'tensor': [tensor.tolist() for tensor in random_arrays_list]\r\n})\r\n\r\nimport pyarrow.feather as feather\r\nfeather.write_feather(table, 'test.arrow')\r\n\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"arrow\", data_files='test.arrow', split=\"train\")\r\n```\r\n\r\n### Expected behavior\r\n\r\n`load_dataset` load the dataset as normal as `feather.read_feather`\r\n```python\r\nimport pyarrow.feather as feather\r\nfeather.read_feather('tensorData/real_ResidueTensor/real_tensor_1.arrow')\r\n```\r\n\r\nPlus `load_dataset(\"parquet\", data_files='test.arrow', split=\"train\")` works fine\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.2.0\r\n- Platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\r\n- Python version: 3.12.3\r\n- `huggingface_hub` version: 0.26.5\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0\r\n<end_of_turn>\n<start_of_turn>model\nOSError: Invalid flatbuffers message.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\"Requested\" in https://discuss.huggingface.co/t/utf-16-for-datasets/43828/3.\r\n\r\n`pd.read_json` also has these parameters, so it makes sense to be consistent.<end_of_turn>\n<start_of_turn>model\nAdd `encoding` and `errors` params to JSON loader<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAccording to the docs, there is a \"mono\" parameter to the Audio feature, which turns any stereo into mono. In practice the signal is not touched and the mono parameter, even though documented, does not exist.\n\n\nhttps://github.com/huggingface/datasets/blob/41c05299348a499807432ab476e1cdc4143c8772/src/datasets/features/audio.py#L52C1-L54C22<end_of_turn>\n<start_of_turn>model\nmono parameter to the Audio feature is missing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCommands such as `load_dataset` creates file locks with `filelock.FileLock`. It would be good if there was a way to disable this.\n\n### Motivation\n\nFile locking doesn't work on all file-systems (in my case NFS mounted Weka). If the `cache_dir` only had small files then it would be possible to point to local disk and the problem would be solved. However, as cache_dir is both where the small info files are written and the processed datasets are put this isn't a feasible solution.\r\n\r\nConsidering https://github.com/huggingface/datasets/issues/6395 I still do think this is something that belongs in HuggingFace. The possibility to control packages separately is valuable. It might be that a user has their dataset on a file-system that doesn't support file-locking while they are using file locking on local disk to control some other type of access.\n\n### Your contribution\n\nMy suggested solution:\r\n```\r\ndiff --git a/src/datasets/utils/_filelock.py b/src/datasets/utils/_filelock.py\r\nindex 19620e6e..58f41a02 100644\r\n--- a/src/datasets/utils/_filelock.py\r\n+++ b/src/datasets/utils/_filelock.py\r\n@@ -18,11 +18,15 @@\r\n import os\r\n \r\n from filelock import FileLock as FileLock_\r\n-from filelock import UnixFileLock\r\n+from filelock import SoftFileLock, UnixFileLock\r\n from filelock import __version__ as _filelock_version\r\n from packaging import version\r\n \r\n \r\n+if os.getenv('HF_USE_SOFTFILELOCK', 'false').lower() in ('true', '1'):\r\n+    FileLock_ = SoftFileLock\r\n+\r\n+\r\n class FileLock(FileLock_):\r\n     \"\"\"\r\n     A `filelock.FileLock` initializer that handles long paths.\r\n```\r\n\r\n<end_of_turn>\n<start_of_turn>model\nOption to disable file locking<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSome functions in `streaming_download_manager.py` are not closing the file they open which lead to `Unclosed file` warnings in our code. This fixes a few of them.<end_of_turn>\n<start_of_turn>model\nUse 'with' operator for some download functions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe \"Spawn\" method is preferred when multiprocessing on macOS or Windows systems, instead of the \"Fork\" method on linux systems.\r\n\r\nThis causes some methods of Iterable Datasets to break when using a dataloader with more than 0 workers.\r\n\r\nI fixed the issue by replacing lambda and local methods which are not pickle-able.\r\n\r\nSee the example below:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dataset = load_dataset(\"lhoestq/demo1\", split=\"train\")\r\n    dataset = dataset.to_iterable_dataset(num_shards=3)\r\n\r\n    dataset = dataset.remove_columns([\"package_name\"])\r\n    dataset = dataset.rename_columns({\r\n        \"review\": \"review1\"\r\n    })\r\n    dataset = dataset.rename_column(\"date\", \"date1\")\r\n    for sample in DataLoader(dataset, batch_size=None, num_workers=3):\r\n        print(sample)\r\n```\r\n\r\nTo notice the fix on a linux system, adding these lines should do the trick:\r\n\r\n```python\r\nimport multiprocessing\r\nmultiprocessing.set_start_method('spawn')\r\n```\r\n\r\nI also removed what looks like code duplication between rename_colums and rename_column\r\n<end_of_turn>\n<start_of_turn>model\nFix multiprocessing with spawn in iterable datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen update datasets library to version 2.16+ ( I test it on 2.16, 2.19.0 and 2.19.1), using the following code to load stas/c4-en-10k dataset \r\n\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\ndataset = load_dataset('stas/c4-en-10k')\r\n```\r\n\r\nand then it raise UnicodeDecodeError like\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/*/conda3/envs/watermark/lib/python3.10/site-packages/datasets/load.py\", line 2523, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/*/conda3/envs/watermark/lib/python3.10/site-packages/datasets/load.py\", line 2195, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/*/conda3/envs/watermark/lib/python3.10/site-packages/datasets/load.py\", line 1846, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/home/*/conda3/envs/watermark/lib/python3.10/site-packages/datasets/load.py\", line 1798, in dataset_module_factory\r\n    can_load_config_from_parquet_export = \"DEFAULT_CONFIG_NAME\" not in f.read()\r\n  File \"/home/*/conda3/envs/watermark/lib/python3.10/codecs.py\", line 322, in decode\r\n    (result, consumed) = self._buffer_decode(data, self.errors, final)\r\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte\r\n```\r\n\r\nI found that fs.open loads a gzip file and parses it like plain text using utf-8 encoder.\r\n\r\n```python\r\nfs = HfFileSystem('https://huggingface.co')\r\nfs.open(\"datasets/stas/c4-en-10k/c4-en-10k.py\", \"rb\")\r\ndata = fs.read()    # data is gzip bytes begin with b'\\x1f\\x8b\\x08\\x00\\x00\\tn\\x88\\x00...' \r\ndata2 = unzip_gzip_bytes(data)    #  data2 is what we want: '# coding=utf-8\\n# Copyright 2020 The HuggingFace Datasets...'\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Install datasets between version 2.16 and 2.19\r\n2. Use `datasets.load_dataset` method to load `stas/c4-en-10k` dataset.\r\n\r\n### Expected behavior\r\n\r\nLoad dataset normally.\r\n\r\n### Environment info\r\n\r\nPlatform = Linux-5.4.0-159-generic-x86_64-with-glibc2.35\r\nPython = 3.10.14\r\nDatasets = 2.19<end_of_turn>\n<start_of_turn>model\nFail to load \"stas/c4-en-10k\" dataset since 2.16 version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport `commit_description` parameter in `push_to_hub`.\r\n\r\nCC: @Wauplin <end_of_turn>\n<start_of_turn>model\nSupport commit_description parameter in push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis is no longer a pressing issue, but for completeness I am reporting that in August 26th, GET requests to `https://datasets-server.huggingface.co/info\\?dataset\\=livebench/math` were returning 502s when invoked from [github actions](https://github.com/UKGovernmentBEIS/inspect_evals/actions/runs/17241892475/job/48921123754) (that link will expire eventually, [here are the logs](https://github.com/user-attachments/files/22233578/logs_44225296943.zip)). \n\nWhen invoked from actions, it appeared to be consistently failing for ~6 hours. However, these 502s never occurred when the request was invoked from my local machine in that same time period.\n\nI suspect that this is related to how the requests are routed with github actions versus locally. \n\nIts not clear to me if the request even reached huggingface servers or if its the github proxy that stopped it from going through, but I wanted to report it nonetheless in case this is helpful information. I'm curious if huggingface can do anything on their end to confirm cause. \n\nAnd a feature request for if this happens in the future (assuming huggingface has visibilty on it):  A \"datasets status\" page highlighting if 502s occur for  specific individual datasets could be useful for people debugging on the other end of this!\n\n\n<end_of_turn>\n<start_of_turn>model\nComment/feature request: Huggingface 502s from GHA<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, we are pre-downloading a dataset using snapshot_download(). When loading this exact dataset with load_dataset() the cached snapshot is not used. In both calls, I provide the cache_dir parameter.\n\n### Steps to reproduce the bug\n\n```\nfrom datasets import load_dataset, concatenate_datasets\nfrom huggingface_hub import snapshot_download\n    \ndef download_ds(name: str):\n    snapshot_download(repo_id=name, repo_type=\"dataset\", cache_dir=\"G:/Datasets/cache\")\n\ndef prepare_ds():\n    audio_ds = load_dataset(\"openslr/librispeech_asr\", num_proc=4, cache_dir=\"G:/Datasets/cache\")\n    print(sfw_ds.features)\n\nif __name__ == '__main__':\n    download_ds(\"openslr/librispeech_asr\")\n    prepare_ds()\n```\n\n### Expected behavior\n\nI'd expect that the cached version of the dataset is used. Instead, the same dataset is downloaded again to the default cache directory.\n\n### Environment info\n\nWindows 11\ndatasets==4.0.0\nPython 3.12.11<end_of_turn>\n<start_of_turn>model\nCached dataset is not used when explicitly passing the cache_dir parameter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(wrong release number - I was continuing the 2.14 branch but 2.14.5 was released from `main`)<end_of_turn>\n<start_of_turn>model\nRelease 2.14.5<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCan anyone explain why looping over a dataset becomes slower after calling `to_iterable_dataset` to convert to `IterableDataset`\n\n### Steps to reproduce the bug\n\nAny dataset after converting to `IterableDataset`\n\n### Expected behavior\n\nMaybe it should be faster on big dataset? I only test on small dataset\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.15\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nDataset is slower after calling `to_iterable_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello everyone,\r\nI'm using **load_datasets** from **huggingface** to download the datasets and I'm facing an issue, the download starts but it reaches some state and then  fails with the following error:\r\nCouldn't reach https://huggingface.co/datasets/wikimedia/wikipedia/resolve/4cb9b0d719291f1a10f96f67d609c5d442980dc9/20231101.ext/train-00000-of-00001.parquet\r\n\r\nFailed to resolve \\'huggingface.co\\' ([Errno -3] Temporary failure in name resolution)\"))')))\r\n\r\n\r\n![image](https://github.com/huggingface/datasets/assets/143214684/8847d9cb-529e-4eda-9c76-282713dfa3af)\r\n\r\nso I was wondering is there a parameter to be passed to load_dataset() to skip files that can't be downloaded??\n\n### Steps to reproduce the bug\n\nParameter to be passed to load_dataset() of huggingface to skip files that can't be downloaded??\n\n### Expected behavior\n\nload_dataset() finishes without error\n\n### Environment info\n\nNone<end_of_turn>\n<start_of_turn>model\nSkip if a dataset has issues<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix largelist repr<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI was trying to load  the wiki dataset, but i got this error \r\n\r\n    traindata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='train')\r\n  File \"/home/aelkordy/.conda/envs/prune_llm/lib/python3.9/site-packages/datasets/load.py\", line 1804, in load_dataset\r\n    ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n  File \"/home/aelkordy/.conda/envs/prune_llm/lib/python3.9/site-packages/datasets/builder.py\", line 1108, in as_dataset\r\n    raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\r\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.<end_of_turn>\n<start_of_turn>model\nError loading wikitext data    raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen saving a dataset on disk and it has a single shard it is not loaded as when it is saved in multiple shards. I installed the latest version of datasets via pip. \r\n\r\n### Steps to reproduce the bug\r\n\r\nThe code below reproduces the behavior. All works well when the range of the loop is 10000 but it fails when it is 1000.\r\n\r\n```\r\nfrom PIL import Image\r\nimport numpy as np\r\nfrom datasets import Dataset, DatasetDict, load_dataset\r\n\r\n\r\ndef load_image():\r\n    # Generate random noise image\r\n    noise = np.random.randint(0, 256, (256, 256, 3), dtype=np.uint8)  \r\n    return Image.fromarray(noise)\r\n\r\ndef create_dataset():\r\n    input_images = []\r\n    output_images = []\r\n    text_prompts = []\r\n  \r\n    for _ in range(10000):  # this is the problematic parameter\r\n        input_images.append(load_image())\r\n        output_images.append(load_image())\r\n        text_prompts.append('test prompt')\r\n\r\n    data = {'input_image': input_images, 'output_image': output_images, 'text_prompt': text_prompts}\r\n    dataset = Dataset.from_dict(data)\r\n    \r\n    return DatasetDict({'train': dataset})\r\n\r\ndataset = create_dataset()\r\n\r\nprint('dataset before saving')\r\nprint(dataset)\r\nprint(dataset['train'].column_names)\r\ndataset.save_to_disk('test_ds')\r\nprint('dataset after loading')\r\n\r\n\r\ndataset_loaded = load_dataset('test_ds')\r\nprint(dataset_loaded)\r\nprint(dataset_loaded['train'].column_names)\r\n```\r\n\r\n\r\nThe output for 1000 iterations is: \r\n\r\n```\r\ndataset before saving\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input_image', 'output_image', 'text_prompt'],\r\n        num_rows: 1000\r\n    })\r\n})\r\n['input_image', 'output_image', 'text_prompt']\r\nSaving the dataset (1/1 shards): 100%|\u2588| 1000/1000 [00:00<00:00, 5156.00 example\r\ndataset after loading\r\nGenerating train split: 1 examples [00:00, 230.52 examples/s]\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['_data_files', '_fingerprint', '_format_columns', '_format_kwargs', '_format_type', '_output_all_columns', '_split'],\r\n        num_rows: 1\r\n    })\r\n})\r\n['_data_files', '_fingerprint', '_format_columns', '_format_kwargs', '_format_type', '_output_all_columns', '_split']\r\n```\r\n\r\nFor 10000 iteration (8 shards) it is correct:\r\n\r\n```\r\ndataset before saving\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input_image', 'output_image', 'text_prompt'],\r\n        num_rows: 10000\r\n    })\r\n})\r\n['input_image', 'output_image', 'text_prompt']\r\nSaving the dataset (8/8 shards): 100%|\u2588| 10000/10000 [00:01<00:00, 6237.68 examp\r\ndataset after loading\r\nGenerating train split: 10000 examples [00:00, 10773.16 examples/s]\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['input_image', 'output_image', 'text_prompt'],\r\n        num_rows: 10000\r\n    })\r\n})\r\n['input_image', 'output_image', 'text_prompt']\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe procedure should work for a dataset with one shrad the same as for one with multiple shards\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.18.0\r\n- Platform: macOS-14.1-arm64-arm-64bit\r\n- Python version: 3.11.8\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.2.0\r\n\r\nEdit: I looked in the source code of load.py in datasets. I should have used \"load_from_disk\" and it indeed works that way. But ideally load_dataset would have raisen an error the same way as if I call a path: \r\n\r\n```\r\n    if Path(path, config.DATASET_STATE_JSON_FILENAME).exists():\r\n        raise ValueError(\r\n            \"You are trying to load a dataset that was saved using `save_to_disk`. \"\r\n            \"Please use `load_from_disk` instead.\"\r\n        )\r\n```\r\nnevertheless I find it interesting that it works just well and without a warning if there are multiple shards. <end_of_turn>\n<start_of_turn>model\nLoading problems of Datasets with a single shard<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nInconsistent operation of data_files and data_dir in load_dataset method.\r\n\n\n### Steps to reproduce the bug\n\n# First\r\nI have three files, named 'train.json', 'val.json', 'test.json'.\r\nEach one has a simple dict `{text:'aaa'}`.\r\nTheir path are `/data/train.json`,  `/data/val.json`,  `/data/test.json`\r\nI load dataset with `data_files` argument:\r\n```py\r\nfiles = [os.path.join('./data',file) for file in os.listdir('./data')]\r\nds = load_dataset(\r\n            path='json',\r\n            data_files=files,)\r\n```\r\nAnd I get:\r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 3\r\n    })\r\n})\r\n```\r\nHowever, If I load dataset with `data_dir` argument:\r\n```py\r\nds = load_dataset(\r\n            path='json',\r\n            data_dir='./data',)\r\n```\r\nAnd I get:\r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text'],\r\n        num_rows: 1\r\n    })\r\n    validation: Dataset({\r\n        features: ['text'],\r\n        num_rows: 1\r\n    })\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 1\r\n    })\r\n})\r\n```\r\nTwo results are not the same. Their behaviors are not equal, even if the statement [here](https://github.com/huggingface/datasets/blob/d0c152a979d91cc34b605c0298aebc650ab7dd27/src/datasets/load.py#L1790) said that their behaviors are equal. \r\n\r\n\r\n# Second\r\nIf some filename include 'test' while others do not, `load_dataset` only return `test` dataset and others files are **abandoned**.\r\nGiven two files named `test.json` and `1.json`\r\nEach one has a simple dict `{text:'aaa'}`.\r\nI load the dataset using:\r\n```py\r\nds = load_dataset(\r\n            path='json',\r\n            data_dir='./data',)\r\n```\r\nOnly `test` is returned, `1.json` is missing:\r\n```py\r\nDatasetDict({\r\n    test: Dataset({\r\n        features: ['text'],\r\n        num_rows: 1\r\n    })\r\n})\r\n```\r\nThings do not change even I manually set `split='train'`\r\n\r\n \r\n\r\n\n\n### Expected behavior\n\n1. Fix the above bugs.\r\n2. Although the document says that load_dataset method will `Find which file goes into which split (e.g. train/test) based on file and directory names or on the YAML configuration`, I hope I can manually decide whether to do so. Sometimes users may accidentally put a `test` string in the filename but they just want a single `train` dataset. If the number of files in `data_dir` is huge, it's not easy to find out what cause the second situation metioned above.\n\n### Environment info\n\ndatasets==3.2.0\r\nUbuntu18.84<end_of_turn>\n<start_of_turn>model\n[Bug] Inconsistent behavior of data_files and data_dir in load_dataset method.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease 2.14.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport `data_dir` parameter in `push_to_hub`.\r\n\r\nThis allows users to organize the data files according to their specific needs. For example, \"wikimedia/wikipedia\" files could be organized by year and/or date, e.g. \"2024/20240101/20240101.en\".<end_of_turn>\n<start_of_turn>model\nSupport data_dir parameter in push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRemove unused argument in `_get_data_files_patterns`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen working on `dataset_info` yaml, I find my data column with format `list[dict[str, str]]` cannot be coded correctly.\r\n\r\nMy data looks like\r\n```\r\n{\"answers\":[{\"text\": \"ADDRESS\", \"label\": \"abc\"}]}\r\n```\r\n\r\nMy `dataset_info` in README.md is:\r\n```\r\ndataset_info:\r\n- config_name: default\r\n  features:\r\n  - name: answers\r\n    sequence:\r\n    - name: text\r\n      dtype: string\r\n    - name: label\r\n      dtype: string\r\n```\r\n\r\n**Error log**:\r\n```\r\npyarrow.lib.ArrowNotImplementedError: Unsupported cast from list<item: struct<text: string, label: string>> to struct using function cast_struct\r\n```\r\n\r\n## Potential Reason\r\nAfter some analysis, it turns out that my yaml config is requiring `dict[str, list[str]]` instead of `list[dict[str, str]]`. It would work if I change my data to\r\n```\r\n{\"answers\":{\"text\": [\"ADDRESS\"], \"label\": [\"abc\", \"def\"]}}\r\n```\r\n\r\nThese following 2 different `dataset_info` are actually equivalent.\r\n\r\n```\r\ndataset_info:\r\n- config_name: default\r\n  features:\r\n  - name: answers\r\n    dtype:\r\n    - name: text\r\n      sequence: string\r\n    - name: label\r\n      sequence: string\r\n\r\ndataset_info:\r\n- config_name: default\r\n  features:\r\n  - name: answers\r\n    sequence:\r\n    - name: text\r\n      dtype: string\r\n    - name: label\r\n      dtype: string\r\n```\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n\r\n```\r\n# README.md\r\n---\r\ndataset_info:\r\n- config_name: default\r\n  features:\r\n  - name: answers\r\n    sequence:\r\n    - name: text\r\n      dtype: string\r\n    - name: label\r\n      dtype: string\r\nconfigs:\r\n- config_name: default\r\n  default: true\r\n  data_files:\r\n  - split: train\r\n    path:\r\n    - \"test.jsonl\"\r\n---\r\n\r\n\r\n\r\n# test.jsonl\r\n\r\n# expected but not working\r\n{\"answers\":[{\"text\": \"ADDRESS\", \"label\": \"abc\"}]}\r\n\r\n# unexpected but working\r\n{\"answers\":{\"text\": [\"ADDRESS\"], \"label\": [\"abc\", \"def\"]}}\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n```\r\ndataset_info:\r\n- config_name: default\r\n  features:\r\n  - name: answers\r\n    sequence:\r\n    - name: text\r\n      dtype: string\r\n    - name: label\r\n      dtype: string\r\n```\r\nShould work on following data format:\r\n\r\n```\r\n{\"answers\":[{\"text\":\"ADDRESS\", \"label\": \"abc\"}]}\r\n```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.21.0\r\n- Platform: macOS-14.6.1-arm64-arm-64bit\r\n- Python version: 3.12.4\r\n- `huggingface_hub` version: 0.24.5\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\n[BUG] dataset_info sequence unexpected behavior in README.md YAML<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe [`DatasetBuilder._split_generators`](https://github.com/huggingface/datasets/blob/0f27d7b77c73412cfc50b24354bfd7a3e838202f/src/datasets/builder.py#L1449) function has currently the following signature:\r\n\r\n```python\r\nclass DatasetBuilder:\r\n    def _split_generators(self, dl_manager: DownloadManager):\r\n        ...\r\n```\r\n\r\nHowever, the `dl_manager` argument can also be of type [`StreamingDownloadManager`](https://github.com/huggingface/datasets/blob/0f27d7b77c73412cfc50b24354bfd7a3e838202f/src/datasets/download/streaming_download_manager.py#L962), which has different functionality. For example, the `download` function doesn't download, but rather just returns the given url(s). \r\n\r\nI suggest changing the function signature to:\r\n\r\n```python\r\nclass DatasetBuilder:\r\n    def _split_generators(self, dl_manager: Union[DownloadManager, StreamingDownloadManager]):\r\n        ...\r\n```\r\n\r\nand also adjust the docstring accordingly.\r\n\r\nI would like to create a Pull Request to fix this, and have the following questions:\r\n* Are there also other options than `DownloadManager`, and `StreamingDownloadManager`?\r\n* Should this also be changed in other functions?\n\n### Steps to reproduce the bug\n\nMinimal example to print the different class names:\r\n\r\n```python\r\nimport tempfile\r\nfrom datasets import load_dataset\r\n\r\nexample = b'''\r\nfrom datasets import GeneratorBasedBuilder, DatasetInfo, Features, Value, SplitGenerator\r\n\r\nclass Test(GeneratorBasedBuilder):\r\n    def _info(self):\r\n        return DatasetInfo(features=Features({\"x\": Value(\"int64\")}))\r\n    def _split_generators(self, dl_manager):\r\n        print(type(dl_manager))\r\n        return [SplitGenerator('test')]\r\n    def _generate_examples(self):\r\n        yield 0, {'x': 42}\r\n'''\r\n\r\nwith tempfile.NamedTemporaryFile(suffix='.py') as f:\r\n    f.write(example)\r\n    f.flush()\r\n    load_dataset(f.name, streaming=False)\r\n    load_dataset(f.name, streaming=True)\r\n```\n\n### Expected behavior\n\ncomplete type annotations\n\n### Environment info\n\n/<end_of_turn>\n<start_of_turn>model\n`DatasetBuilder._split_generators` incomplete type annotation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe `to_csv()` method does not output commas in lists.  So when the Dataset is loaded back in the data structure of the column with a list is not correct.\r\n\r\nHere's an example:\r\n\r\nObviously, it's not as trivial as inserting commas in the list, since its a comma-separated file.  But hopefully there's a way to export the list in a way that it'll be imported by `load_dataset()` correctly.\n\n### Steps to reproduce the bug\n\nHere's some code to reproduce the bug:\r\n\r\n\r\n```python\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict(\r\n    {\r\n        \"pokemon\": [\"bulbasaur\", \"squirtle\"],\r\n        \"type\": [\"grass\", \"water\"]\r\n    }\r\n)\r\n\r\ndef ascii_to_hex(text):\r\n    return [ord(c) for c in text]\r\n\r\nds = ds.map(lambda x: {\"int\": ascii_to_hex(x['pokemon'])})\r\n\r\nds.to_csv('../output/temp.csv')\r\n```\r\n\r\ntemp.csv then contains:\r\n```\n\n### Expected behavior\n\nACTUAL OUTPUT:\r\n```\r\npokemon,type,int\r\nbulbasaur,grass,[ 98 117 108  98  97 115  97 117 114]\r\nsquirtle,water,[115 113 117 105 114 116 108 101]\r\n```\r\n\r\nEXPECTED OUTPUT:\r\n```\r\npokemon,type,int\r\nbulbasaur,grass,[98, 117, 108,  98,  97, 115,  97, 117, 114]\r\nsquirtle,water,[115, 113, 117, 105, 114, 116, 108, 101]\r\n```\r\n\r\nor probably something more like this since it's a CSV file:\r\n```\r\npokemon,type,int\r\nbulbasaur,grass,\"[98, 117, 108,  98,  97, 115,  97, 117, 114]\"\r\nsquirtle,water,\"[115, 113, 117, 105, 114, 116, 108, 101]\"\r\n```\n\n### Environment info\n\n### Package Version\r\nName: datasets\r\nVersion: 2.16.1\r\n\r\n### Python\r\nversion: 3.10.12\r\n\r\n\r\n### OS Info\r\nPRETTY_NAME=\"Ubuntu 22.04.4 LTS\"\r\nNAME=\"Ubuntu\"\r\nVERSION_ID=\"22.04\"\r\nVERSION=\"22.04.4 LTS (Jammy Jellyfish)\"\r\nVERSION_CODENAME=jammy\r\nID=ubuntu\r\nID_LIKE=debian\r\n...\r\nUBUNTU_CODENAME=jammy<end_of_turn>\n<start_of_turn>model\nDataset.to_csv() missing commas in columns with lists<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDatasets allows users to create and register custom features.\r\n\r\nHowever if datasets are then pushed to the hub, this means that anyone calling load_dataset without registering the custom Features in the same way as the dataset creator will get an error message.\r\n\r\nIt would be nice to offer a fallback in this case.\n\n### Steps to reproduce the bug\n\n```python\r\nload_dataset(\"alex-hh/custom-features-example\")\r\n```\r\n(Dataset creation process - must be run in separate session so that NewFeature isn't registered in session in which download is attempted:)\r\n\r\n```python\r\nfrom dataclasses import dataclass, field\r\nimport pyarrow as pa\r\nfrom datasets.features.features import register_feature\r\n\r\nfrom datasets import Dataset, Features, Value, load_dataset\r\nfrom datasets import Feature\r\n\r\n@dataclass\r\nclass NewFeature(Feature):\r\n    _type: str = field(default=\"NewFeature\", init=False, repr=False)\r\n    def __call__(self):\r\n        return pa.int32()\r\n\r\ndef examples_generator():\r\n    for i in range(5):\r\n        yield {\"feature\": i}\r\n\r\nds = Dataset.from_generator(examples_generator, features=Features(feature=NewFeature()))\r\nds.push_to_hub(\"alex-hh/custom-features-example\")\r\nregister_feature(NewFeature, \"NewFeature\")\r\n```\r\n\r\n\n\n### Expected behavior\n\nIt would be nice, and offer greater extensibility, if there was some kind of graceful fallback mechanism in place for cases where user-defined features are stored in the dataset but not available locally.\n\n### Environment info\n\n3.0.2<end_of_turn>\n<start_of_turn>model\nFallback to arrow defaults when loading dataset with custom features that aren't registered locally<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`remove_columns` had no effect when running a function in `.map()` on dataset that is formatted\r\n\r\nThis aligns the logic of `map()` with the non formatted case and also with with https://github.com/huggingface/datasets/pull/7353<end_of_turn>\n<start_of_turn>model\nFix remove_columns in the formatted case<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## perf: reduce copies in TorchFormatter\r\n\r\nThis PR make changes the torch formatter to avoid unnecessary copies and casts when converting decoded batches to tensors.\r\n\r\nBecause many arrays are already in a torch-friendly memory layout and dtype, we can do zero\u2011copy conversions (`torch.from_numpy`) and only fall back to `as_tensor` when a dtype/device change is required. We also consolidate lists of same\u2011shape tensors with a cheap `stack` only when safe.\r\n\r\nWhy it helps\r\n- Avoids extra materialization and dtype churn during batched map and indexing.\r\n- Preserves API and outputs; only changes internal conversion logic.\r\n\r\n\r\nSmall benchmark script (based on https://github.com/huggingface/datasets/issues/6104)\r\n\r\n```python\r\nimport time\r\nfrom datasets import load_dataset\r\n\r\n\r\ndef main():\r\n    dataset = load_dataset(\"NightMachinery/hf_datasets_bug1\")\r\n    dataset = dataset[\"train\"] if \"train\" in dataset else dataset\r\n    t0 = time.time()\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # identity map with small batches\r\n    dataset = dataset.map(lambda x: x, batched=True, batch_size=20)\r\n\r\n    # force materialization\r\n    data = dataset[:300]\r\n    print(len(data.keys()))\r\n\r\n    t1 = time.time()\r\n    print(f\"Duration: {t1 - t0:.2f} s\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\nWithout changes\r\n\r\n```bash\r\nuv run bench.py\r\n```\r\n\r\n```bash\r\n# 303\r\n# Duration: 7.26 s\r\n```\r\n\r\nWith changes\r\n\r\n```bash\r\nuv run bench.py\r\n```\r\n\r\n```bash\r\n# 303\r\n# Duration: 4.43 s\r\n```\r\n\r\n\r\n# Updated reproduction scripts\r\n\r\nBelow are some simple test cases using `main` and this `refactor-torch-formatter` branch. I've included the two scripts and output when running on a local machine.\r\n\r\n```python\r\n# /// script\r\n# requires-python = \">=3.10\"\r\n# dependencies = [\r\n#     \"torch\",\r\n#     \"datasets\",\r\n#     \"pillow\",\r\n# ]\r\n#\r\n# [tool.uv.sources]\r\n# datasets = { git = \"https://github.com/huggingface/datasets.git\" }\r\n# ///\r\nimport time\r\nimport random\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom datasets import Dataset, load_dataset\r\nimport torch\r\n\r\n\r\ndef create_mock_images_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with PIL images.\"\"\"\r\n    random.seed(42)\r\n    np.random.seed(42)\r\n\r\n    images = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic RGB image\r\n        width, height = 64, 64\r\n        rgb_array = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\r\n        image = Image.fromarray(rgb_array)\r\n        images.append(image)\r\n        labels.append(i % 10)  # 10 classes\r\n\r\n    return Dataset.from_dict({\"image\": images, \"label\": labels})\r\n\r\n\r\ndef create_mock_text_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with text.\"\"\"\r\n    random.seed(42)\r\n\r\n    words = [\"apple\", \"banana\", \"cherry\", \"date\", \"elderberry\", \"fig\", \"grape\", \"honeydew\"]\r\n    texts = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic text\r\n        text_length = 5 + (i % 20)  # 5-24 words\r\n        text = \" \".join(random.choices(words, k=text_length))\r\n        texts.append(text)\r\n        labels.append(i % 3)  # 3 classes\r\n\r\n    return Dataset.from_dict({\"text\": texts, \"label\": labels})\r\n\r\n\r\ndef create_mock_ints_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with integers.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic integer arrays\r\n        arr = [random.randint(0, 1000) for _ in range(50)]  # 50 integers each\r\n        data.append(arr)\r\n        labels.append(i % 5)  # 5 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef create_mock_floats_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with floats.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic float arrays\r\n        arr = [random.uniform(0.0, 100.0) for _ in range(30)]  # 30 floats each\r\n        data.append(arr)\r\n        labels.append(i % 4)  # 4 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef benchmark_dataset(name, dataset, num_samples=1000):\r\n    \"\"\"Benchmark dataset access speed.\"\"\"\r\n    print(f\"\\n=== {name} Dataset Benchmark ===\")\r\n\r\n    t0 = time.time()\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # identity map with small batches\r\n    dataset = dataset.map(lambda x: x, batched=True, batch_size=20)\r\n\r\n    # force materialization\r\n    data = dataset[:num_samples]\r\n    print(f\"Keys: {list(data.keys())}\")\r\n    print(f\"Sample count: {len(data[list(data.keys())[0]])}\")\r\n\r\n    t1 = time.time()\r\n    print(f\"Duration: {t1 - t0:.2f} s\")\r\n    print(f\"Speed: {num_samples / (t1 - t0):.1f} samples/s\")\r\n\r\n\r\ndef main():\r\n    # PIL Images benchmark\r\n    images_dataset = create_mock_images_dataset()\r\n    benchmark_dataset(\"PIL Images\", images_dataset)\r\n\r\n    # Text benchmark\r\n    text_dataset = create_mock_text_dataset()\r\n    benchmark_dataset(\"Text\", text_dataset)\r\n\r\n    # Integers benchmark\r\n    ints_dataset = create_mock_ints_dataset()\r\n    benchmark_dataset(\"Integers\", ints_dataset)\r\n\r\n    # Floats benchmark\r\n    floats_dataset = create_mock_floats_dataset()\r\n    benchmark_dataset(\"Floats\", floats_dataset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\noutput\r\n\r\n```bash\r\nuv run --refresh example1.py\r\n```\r\n\r\n```text\r\n=== PIL Images Dataset Benchmark ===\r\nMap:   0%|                                                          | 0/5000 [00:00<?, ? examples/s]/Users/drbh/.cache/uv/environments-v2/example1-2aca1a30e84bdead/lib/python3.10/site-packages/datasets/features/image.py:352: UserWarning: Downcasting array dtype int64 to uint8 to be compatible with 'Pillow'\r\n  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:01<00:00, 3669.15 examples/s]\r\nKeys: ['image', 'label']\r\nSample count: 1000\r\nDuration: 2.14 s\r\nSpeed: 466.5 samples/s\r\n\r\n=== Text Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 141327.04 examples/s]\r\nKeys: ['text', 'label']\r\nSample count: 1000\r\nDuration: 0.04 s\r\nSpeed: 27004.3 samples/s\r\n\r\n=== Integers Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 112904.90 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.05 s\r\nSpeed: 21680.6 samples/s\r\n\r\n=== Floats Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 104084.25 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.05 s\r\nSpeed: 20215.1 samples/s\r\n```\r\n\r\nand this branch specifically\r\n\r\n```python\r\n# /// script\r\n# requires-python = \">=3.10\"\r\n# dependencies = [\r\n#     \"torch\",\r\n#     \"datasets\",\r\n#     \"pillow\",\r\n# ]\r\n#\r\n# [tool.uv.sources]\r\n# datasets = { git = \"https://github.com/huggingface/datasets.git\", rev = \"refactor-torch-formatter\" }\r\n# ///\r\nimport time\r\nimport random\r\nimport numpy as np\r\nfrom PIL import Image\r\nfrom datasets import Dataset, load_dataset\r\nimport torch\r\n\r\n\r\ndef create_mock_images_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with PIL images.\"\"\"\r\n    random.seed(42)\r\n    np.random.seed(42)\r\n\r\n    images = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic RGB image\r\n        width, height = 64, 64\r\n        rgb_array = np.random.randint(0, 256, (height, width, 3), dtype=np.uint8)\r\n        image = Image.fromarray(rgb_array)\r\n        images.append(image)\r\n        labels.append(i % 10)  # 10 classes\r\n\r\n    return Dataset.from_dict({\"image\": images, \"label\": labels})\r\n\r\n\r\ndef create_mock_text_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with text.\"\"\"\r\n    random.seed(42)\r\n\r\n    words = [\r\n        \"apple\",\r\n        \"banana\",\r\n        \"cherry\",\r\n        \"date\",\r\n        \"elderberry\",\r\n        \"fig\",\r\n        \"grape\",\r\n        \"honeydew\",\r\n    ]\r\n    texts = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic text\r\n        text_length = 5 + (i % 20)  # 5-24 words\r\n        text = \" \".join(random.choices(words, k=text_length))\r\n        texts.append(text)\r\n        labels.append(i % 3)  # 3 classes\r\n\r\n    return Dataset.from_dict({\"text\": texts, \"label\": labels})\r\n\r\n\r\ndef create_mock_ints_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with integers.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic integer arrays\r\n        arr = [random.randint(0, 1000) for _ in range(50)]  # 50 integers each\r\n        data.append(arr)\r\n        labels.append(i % 5)  # 5 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef create_mock_floats_dataset(num_samples=5000):\r\n    \"\"\"Create a deterministic mock dataset with floats.\"\"\"\r\n    random.seed(42)\r\n\r\n    data = []\r\n    labels = []\r\n\r\n    for i in range(num_samples):\r\n        # Create deterministic float arrays\r\n        arr = [random.uniform(0.0, 100.0) for _ in range(30)]  # 30 floats each\r\n        data.append(arr)\r\n        labels.append(i % 4)  # 4 classes\r\n\r\n    return Dataset.from_dict({\"data\": data, \"label\": labels})\r\n\r\n\r\ndef benchmark_dataset(name, dataset, num_samples=1000):\r\n    \"\"\"Benchmark dataset access speed.\"\"\"\r\n    print(f\"\\n=== {name} Dataset Benchmark ===\")\r\n\r\n    t0 = time.time()\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # identity map with small batches\r\n    dataset = dataset.map(lambda x: x, batched=True, batch_size=20)\r\n\r\n    # force materialization\r\n    data = dataset[:num_samples]\r\n    print(f\"Keys: {list(data.keys())}\")\r\n    print(f\"Sample count: {len(data[list(data.keys())[0]])}\")\r\n\r\n    t1 = time.time()\r\n    print(f\"Duration: {t1 - t0:.2f} s\")\r\n    print(f\"Speed: {num_samples / (t1 - t0):.1f} samples/s\")\r\n\r\n\r\ndef main():\r\n    # PIL Images benchmark\r\n    images_dataset = create_mock_images_dataset()\r\n    benchmark_dataset(\"PIL Images\", images_dataset)\r\n\r\n    # Text benchmark\r\n    text_dataset = create_mock_text_dataset()\r\n    benchmark_dataset(\"Text\", text_dataset)\r\n\r\n    # Integers benchmark\r\n    ints_dataset = create_mock_ints_dataset()\r\n    benchmark_dataset(\"Integers\", ints_dataset)\r\n\r\n    # Floats benchmark\r\n    floats_dataset = create_mock_floats_dataset()\r\n    benchmark_dataset(\"Floats\", floats_dataset)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\n```bash\r\nuv run --refresh example2.py\r\n```\r\n\r\n```text\r\n    Updated https://github.com/huggingface/datasets.git (2cb64d1b6503afb49d822b20979760efe4519d03)\r\n      Built datasets @ git+https://github.com/huggingface/datasets.git@2cb64d1b6503afb49d822b20979760efe\r\nUninstalled 1 package in 20ms\r\nInstalled 1 package in 5ms\r\n\r\n=== PIL Images Dataset Benchmark ===\r\nMap:   0%|                                                          | 0/5000 [00:00<?, ? examples/s]/Users/drbh/.cache/uv/environments-v2/example2-d4af608668b706ec/lib/python3.10/site-packages/datasets/features/image.py:352: UserWarning: Downcasting array dtype int64 to uint8 to be compatible with 'Pillow'\r\n  warnings.warn(f\"Downcasting array dtype {dtype} to {dest_dtype} to be compatible with 'Pillow'\")\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:01<00:00, 3645.14 examples/s]\r\nKeys: ['image', 'label']\r\nSample count: 1000\r\nDuration: 2.04 s\r\nSpeed: 491.2 samples/s\r\n\r\n=== Text Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 169877.28 examples/s]\r\nKeys: ['text', 'label']\r\nSample count: 1000\r\nDuration: 0.03 s\r\nSpeed: 32236.1 samples/s\r\n\r\n=== Integers Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 131940.33 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.04 s\r\nSpeed: 25493.3 samples/s\r\n\r\n=== Floats Dataset Benchmark ===\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5000/5000 [00:00<00:00, 120621.64 examples/s]\r\nKeys: ['data', 'label']\r\nSample count: 1000\r\nDuration: 0.04 s\r\nSpeed: 23370.6 samples/s\r\n```<end_of_turn>\n<start_of_turn>model\nfeat: avoid some copies in torch formatter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI drafted the logic to retrieve a no-script dataset in the cache.\r\nFor example it can reload datasets that were pushed to hub if they exist in the cache.\r\n\r\nexample:\r\n\r\n```python\r\n>>> Dataset.from_dict({\"a\": [1, 2]}).push_to_hub(\"lhoestq/tmp\")\r\n>>> load_dataset(\"lhoestq/tmp\")\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\nand later, without connection:\r\n\r\n```python\r\n>>> load_dataset(\"lhoestq/tmp\")\r\nUsing the latest cached version of the dataset from /Users/quentinlhoest/.cache/huggingface/datasets/lhoestq___tmp/*/*/0b3caccda1725efb(last modified on Wed Nov 29 16:50:27 2023) since it couldn't be found locally at lhoestq/tmp.\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['a'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\nfix https://github.com/huggingface/datasets/issues/3547\r\n\r\n## Implementation details (EDITED)\r\n\r\nI continued in https://github.com/huggingface/datasets/pull/6493, see the changes there\r\n\r\nTODO:\r\n- [x] tests\r\n- [ ] compatible with https://github.com/huggingface/datasets/pull/6458<end_of_turn>\n<start_of_turn>model\nRetrieve cached datasets that were pushed to hub when offline<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe new changes in 4.0.0 are breaking many datasets, including those from lm-evaluation-harness. \nI am trying to revert back to older versions, like 3.6.0 to make the eval work but I keep getting:\n``` Python\nFile /venv/main/lib/python3.12/site-packages/datasets/features/features.py:1474, in generate_from_dict(obj)\n   1471 class_type = _FEATURE_TYPES.get(_type, None) or globals().get(_type, None)\n   1473 if class_type is None:\n-> 1474     raise ValueError(f\"Feature type '{_type}' not found. Available feature types: {list(_FEATURE_TYPES.keys())}\")\n   1476 if class_type == LargeList:\n   1477     feature = obj.pop(\"feature\")\n\nValueError: Feature type 'List' not found. Available feature types: ['Value', 'ClassLabel', 'Translation', 'TranslationVariableLanguages', 'LargeList', 'Sequence', 'Array2D', 'Array3D', 'Array4D', 'Array5D', 'Audio', 'Image', 'Video', 'Pdf']\n\n```\n\n\n\n### Steps to reproduce the bug\n\n``` Python\nimport lm_eval\nmodel_eval = lm_eval.models.huggingface.HFLM(pretrained=model, tokenizer=tokenizer)\nlm_eval.evaluator.simple_evaluate(model_eval, tasks=[\"winogrande\"], num_fewshot=5, batch_size=1)\n```\n\n### Expected behavior\n\nOlder `datasets` versions should work just fine as before\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-6.8.0-60-generic-x86_64-with-glibc2.39\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.33.1\n- PyArrow version: 20.0.0\n- Pandas version: 2.3.1\n- `fsspec` version: 2025.3.0\n<end_of_turn>\n<start_of_turn>model\nMany things broken since the new 4.0.0 release<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRefactor the `dill` logic to make it easier to maintain (and fix some issues along the way)\r\n\r\nIt makes the following improvements to the serialization API:\r\n* consistent order of a `dict`'s keys\r\n* support for hashing `torch.compile`-ed modules and functions\r\n* deprecates `datasets.fingerprint.hashregister` as the `hashregister`-ed reducers are never invoked anyways (does not support nested data as `pickle`/`dill` do)\r\n\r\n\r\n~~TODO: optimize hashing of `pa.Table` and `datasets.table.Table`~~ The `pa_array.to_string` approach is faster for large arrays because it outputs the first 10 and last 10 elements (by default). The problem is that this can produce identical hashes for non-identical arrays if their differing elements get ellipsed...\r\n \r\nFix https://github.com/huggingface/datasets/issues/6440, fix https://github.com/huggingface/datasets/issues/5839<end_of_turn>\n<start_of_turn>model\nRefactor `dill` logic<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nChanges in the doc READMe:\r\n* adds two new sections (to be aligned with `transformers` and `hfh`): \"Previewing the documentation\" and \"Writing documentation examples\"\r\n* replaces the mentions of `transformers` with `datasets`\r\n* fixes some dead links<end_of_turn>\n<start_of_turn>model\nDoc readme improvements<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHello and thanks for developing this package!\r\n\r\nWhen I process a Dataset with the map function using multiple processors some set attributes of the DatasetInfo get lost and are None in the resulting Dataset.    \n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset, DatasetInfo\r\n\r\n\r\ndef run_map(num_proc):\r\n    dataset = Dataset.from_dict(\r\n        {\"col1\": [0, 1], \"col2\": [3, 4]},\r\n        info=DatasetInfo(\r\n            dataset_name=\"my_dataset\",\r\n        ),\r\n    )\r\n\r\n    ds = dataset.map(lambda x: x, num_proc=num_proc)\r\n    print(ds.info.dataset_name)\r\n\r\n\r\nrun_map(1)\r\nrun_map(2)\r\n```\r\n\r\nThis puts out:\r\n\r\n```bash\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 724.66 examples/s]\r\nmy_dataset\r\nMap (num_proc=2): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 18.25 examples/s]\r\nNone\r\n```\n\n### Expected behavior\n\nI expect the DatasetInfo to be kept as it was and there should be no difference in the output of running map with num_proc=1 and num_proc=2. \r\n\r\nExpected output:\r\n```bash\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 724.66 examples/s]\r\nmy_dataset\r\nMap (num_proc=2): 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 18.25 examples/s]\r\nmy_dataset\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-5.10.102.1-microsoft-standard-WSL2-x86_64-with-glibc2.17\r\n- Python version: 3.8.18\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.9.2<end_of_turn>\n<start_of_turn>model\nlosing DatasetInfo in Dataset.map when num_proc > 1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nminor fix for bfloat16<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nremove more script docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHey folks,\r\n\r\nI am trying to run this code -\r\n\r\n```python\r\nfrom datasets import load_dataset, get_dataset_config_names\r\nds = load_dataset(\"mteb/arguana\")\r\n```\r\nwith HF_HUB_OFFLINE=1\r\n\r\nBut I get the following error -\r\n\r\n\r\n```python\r\nUsing the latest cached version of the dataset since mteb/arguana couldn't be found on the Hugging Face Hub (offline mode is enabled).\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[2], line 1\r\n----> 1 ds = load_dataset(\"mteb/arguana\")\r\n\r\nFile ~/env/lib/python3.10/site-packages/datasets/load.py:2129, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2124 verification_mode = VerificationMode(\r\n   2125     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\r\n   2126 )\r\n   2128 # Create a dataset builder\r\n-> 2129 builder_instance = load_dataset_builder(\r\n   2130     path=path,\r\n   2131     name=name,\r\n   2132     data_dir=data_dir,\r\n   2133     data_files=data_files,\r\n   2134     cache_dir=cache_dir,\r\n   2135     features=features,\r\n   2136     download_config=download_config,\r\n   2137     download_mode=download_mode,\r\n   2138     revision=revision,\r\n   2139     token=token,\r\n   2140     storage_options=storage_options,\r\n   2141     trust_remote_code=trust_remote_code,\r\n   2142     _require_default_config_name=name is None,\r\n   2143     **config_kwargs,\r\n   2144 )\r\n   2146 # Return iterable dataset in case of streaming\r\n   2147 if streaming:\r\n\r\nFile ~/env/lib/python3.10/site-packages/datasets/load.py:1886, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\r\n   1884 builder_cls = get_dataset_builder_class(dataset_module, dataset_name=dataset_name)\r\n   1885 # Instantiate the dataset builder\r\n-> 1886 builder_instance: DatasetBuilder = builder_cls(\r\n   1887     cache_dir=cache_dir,\r\n   1888     dataset_name=dataset_name,\r\n   1889     config_name=config_name,\r\n   1890     data_dir=data_dir,\r\n   1891     data_files=data_files,\r\n   1892     hash=dataset_module.hash,\r\n   1893     info=info,\r\n   1894     features=features,\r\n   1895     token=token,\r\n   1896     storage_options=storage_options,\r\n   1897     **builder_kwargs,\r\n   1898     **config_kwargs,\r\n   1899 )\r\n   1900 builder_instance._use_legacy_cache_dir_if_possible(dataset_module)\r\n   1902 return builder_instance\r\n\r\nFile ~/env/lib/python3.10/site-packages/datasets/packaged_modules/cache/cache.py:124, in Cache.__init__(self, cache_dir, dataset_name, config_name, version, hash, base_path, info, features, token, repo_id, data_files, data_dir, storage_options, writer_batch_size, **config_kwargs)\r\n    122     config_kwargs[\"data_dir\"] = data_dir\r\n    123 if hash == \"auto\" and version == \"auto\":\r\n--> 124     config_name, version, hash = _find_hash_in_cache(\r\n    125         dataset_name=repo_id or dataset_name,\r\n    126         config_name=config_name,\r\n    127         cache_dir=cache_dir,\r\n    128         config_kwargs=config_kwargs,\r\n    129         custom_features=features,\r\n    130     )\r\n    131 elif hash == \"auto\" or version == \"auto\":\r\n    132     raise NotImplementedError(\"Pass both hash='auto' and version='auto' instead\")\r\n\r\nFile ~/env/lib/python3.10/site-packages/datasets/packaged_modules/cache/cache.py:84, in _find_hash_in_cache(dataset_name, config_name, cache_dir, config_kwargs, custom_features)\r\n     72 other_configs = [\r\n     73     Path(_cached_directory_path).parts[-3]\r\n     74     for _cached_directory_path in glob.glob(os.path.join(cached_datasets_directory_path_root, \"*\", version, hash))\r\n   (...)\r\n     81     )\r\n     82 ]\r\n     83 if not config_id and len(other_configs) > 1:\r\n---> 84     raise ValueError(\r\n     85         f\"There are multiple '{dataset_name}' configurations in the cache: {', '.join(other_configs)}\"\r\n     86         f\"\\nPlease specify which configuration to reload from the cache, e.g.\"\r\n     87         f\"\\n\\tload_dataset('{dataset_name}', '{other_configs[0]}')\"\r\n     88     )\r\n     89 config_name = cached_directory_path.parts[-3]\r\n     90 warning_msg = (\r\n     91     f\"Found the latest cached dataset configuration '{config_name}' at {cached_directory_path} \"\r\n     92     f\"(last modified on {time.ctime(_get_modification_time(cached_directory_path))}).\"\r\n     93 )\r\n\r\nValueError: There are multiple 'mteb/arguana' configurations in the cache: queries, corpus, default\r\nPlease specify which configuration to reload from the cache, e.g.\r\n        load_dataset('mteb/arguana', 'queries')\r\n```\r\n\r\nIt works when I run the same code with HF_HUB_OFFLINE=0, but after the data is downloaded, I turn off the HF hub cache with HF_HUB_OFFLINE=1, and then this error appears.\r\n\r\nAre there some files I am missing with hub disabled?\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset, get_dataset_config_names\r\nds = load_dataset(\"mteb/arguana\")\r\n\r\nwith HF_HUB_OFFLINE=1\r\n(after already running it with HF_HUB_OFFLINE=0 and populating the datasets cache)\n\n### Expected behavior\n\nDataset loaded successfully as it does with HF_HUB_OFFLINE=1\n\n### Environment info\n\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.15.148.2-2.cm2-x86_64-with-glibc2.35\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.27.0\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nThere are multiple 'mteb/arguana' configurations in the cache: default, corpus, queries with HF_HUB_OFFLINE=1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix https://github.com/huggingface/datasets/issues/7842<end_of_turn>\n<start_of_turn>model\nfix column with transform<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease 2.19.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen I load the dataset from local directory, it's cached copy is picked up after first time. However, for `map` operation, the operation is applied again and cached copy is not picked up. Is there any way to pick cached copy instead of processing it again? The only solution I could think of was to use `save_to_disk` after my last transform and then use that in my DataLoader pipeline. Are there any other solutions for the same?\r\n\r\nOne more thing, my dataset is occupying 6GB storage memory after I use `map`, is there any way I can reduce that memory usage?\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\n# make sure that dataset decodes audio with correct sampling rate\r\ndataset_sampling_rate = next(iter(self.raw_datasets.values())).features[\"audio\"].sampling_rate\r\nif dataset_sampling_rate != self.feature_extractor.sampling_rate:\r\n    self.raw_datasets = self.raw_datasets.cast_column(\r\n        \"audio\", datasets.features.Audio(sampling_rate=self.feature_extractor.sampling_rate)\r\n    )\r\n\r\nvectorized_datasets = self.raw_datasets.map(\r\n    self.prepare_dataset,\r\n    remove_columns=next(iter(self.raw_datasets.values())).column_names,\r\n    num_proc=self.num_workers,\r\n    desc=\"preprocess datasets\",\r\n)\r\n# filter data that is longer than max_input_length\r\nself.vectorized_datasets = vectorized_datasets.filter(\r\n    self.is_audio_in_length_range,\r\n    num_proc=self.num_workers,\r\n    input_columns=[\"input_length\"],\r\n        )\r\n\r\ndef prepare_dataset(self, batch):\r\n    # load audio\r\n    sample = batch[\"audio\"]\r\n    inputs = self.feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\r\n    batch[\"input_values\"] = inputs.input_values[0]\r\n    batch[\"input_length\"] = len(batch[\"input_values\"])\r\n\r\n    batch[\"labels\"] = self.tokenizer(batch[\"target_text\"]).input_ids\r\n    return batch\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\n`map` to use cached copy and if possible an alternative technique to reduce memory usage after using `map`\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-3.10.0-1160.71.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.8.16\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2\r\n<end_of_turn>\n<start_of_turn>model\n`datasets.map` not reusing cached copy by default<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix num_proc=1 ci test<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen doing a map of a dataset with complex types, sometimes `datasets` is unable to interpret the valid schema of a returned datasets.map() function. This often comes from conflicting types, like when both empty lists and filled lists are competing for the same field value.\r\n\r\nThis is prone to happen in batch mapping, when the mapper returns a sequence of null/empty values and other batches are non-null. A workaround is to manually cast the new batch to a pyarrow table (like implemented in this [workaround](https://github.com/piercefreeman/lassen/pull/3)) but it feels like this ideally should be solved at the core library level.\r\n\r\nNote that the reproduction case only throws this error if the first datapoint has the empty list. If it is processed later, datasets already detects its representation as list-type and therefore allows the empty list to be provided.\n\n### Steps to reproduce the bug\n\nA trivial reproduction case:\r\n\r\n```python\r\nfrom typing import Iterator, Any\r\nimport pandas as pd\r\nfrom datasets import Dataset\r\n\r\ndef batch_to_examples(batch: dict[str, list[Any]]) -> Iterator[dict[str, Any]]:\r\n    for i in range(next(iter(lengths))):\r\n        yield {feature: values[i] for feature, values in batch.items()}\r\n\r\ndef examples_to_batch(examples) -> dict[str, list[Any]]:\r\n    batch = {}\r\n\r\n    for example in examples:\r\n        for feature, value in example.items():\r\n            if feature not in batch:\r\n                batch[feature] = []\r\n            batch[feature].append(value)\r\n\r\n    return batch\r\n\r\ndef batch_process(examples, explicit_schema: bool):\r\n    new_examples = []\r\n    for example in batch_to_examples(examples):\r\n        new_examples.append(dict(texts=example[\"raw_text\"].split()))\r\n    return examples_to_batch(new_examples)\r\n\r\ndf = pd.DataFrame(\r\n    [\r\n        {\"raw_text\": \"\"},\r\n        {\"raw_text\": \"This is a test\"},\r\n        {\"raw_text\": \"This is another test\"},\r\n    ]\r\n)\r\n\r\ndataset = Dataset.from_pandas(df)\r\n\r\n# datasets won't be able to typehint a dataset that starts with an empty example.\r\nwith pytest.raises(TypeError, match=\"Couldn't cast array of type\"):\r\n    dataset = dataset.map(\r\n        batch_process,\r\n        batched=True,\r\n        batch_size=1,\r\n        num_proc=1,\r\n        remove_columns=dataset.column_names,\r\n    )\r\n```\r\n\r\nThis results in crashes like:\r\n\r\n```bash\r\n  File \"/Users/piercefreeman/Library/Caches/pypoetry/virtualenvs/example-9kBqeSPy-py3.11/lib/python3.11/site-packages/datasets/table.py\", line 1819, in wrapper\r\n    return func(array, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/piercefreeman/Library/Caches/pypoetry/virtualenvs/example-9kBqeSPy-py3.11/lib/python3.11/site-packages/datasets/table.py\", line 2109, in cast_array_to_feature\r\n    return array_cast(array, feature(), allow_number_to_str=allow_number_to_str)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/piercefreeman/Library/Caches/pypoetry/virtualenvs/example-9kBqeSPy-py3.11/lib/python3.11/site-packages/datasets/table.py\", line 1819, in wrapper\r\n    return func(array, *args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/piercefreeman/Library/Caches/pypoetry/virtualenvs/example-9kBqeSPy-py3.11/lib/python3.11/site-packages/datasets/table.py\", line 1998, in array_cast\r\n    raise TypeError(f\"Couldn't cast array of type {array.type} to {pa_type}\")\r\nTypeError: Couldn't cast array of type string to null\r\n```\n\n### Expected behavior\n\nThe code should successfully map and create a new dataset without error.\n\n### Environment info\n\nMac OSX, Linux<end_of_turn>\n<start_of_turn>model\n\"Couldn't cast array of type\" in complex datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\u2026builder (#4910 )\r\n\r\n### What does this PR do?\r\n\r\nFixes edge case in `load_dataset_builder` by raising a `TypeError` if the same key exists in both `builder_kwargs` and `config_kwargs`.\r\n\r\n### Implementation details\r\n\r\n- Added a guard clause in `load_dataset_builder` to detect duplicate keys between `builder_kwargs` and `config_kwargs`\r\n \r\n- Wrote a unit test in `tests/test_load_duplicate_keys.py` to verify the exception is raised correctly \r\n\r\n### Fixes\r\n\r\nCloses #4910 \r\n\r\n### Reviewers\r\n\r\n@zach-huggingface\r\n@SunMarc \r\n\r\nWould appreciate your review if you have time - thanks!<end_of_turn>\n<start_of_turn>model\nGuard against duplicate builder_kwargs/config_kwargs in load_dataset_\u2026<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRetry on the `huggingface_hub`'s `HfHubHTTPError` in the streaming mode.\r\n\r\nFix #6843 <end_of_turn>\n<start_of_turn>model\nRetry on HF Hub error when streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nMulti-GPU fine-tuning the stable diffusion X by following https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/README_sdxl.md hangs indefinitely.\r\n\r\n### Steps to reproduce the bug\r\n\r\naccelerate launch train_text_to_image_sdxl.py --pretrained_model_name_or_path=$MODEL_NAME --pretrained_vae_model_name_or_path=$VAE_NAME --dataset_name=$DATASET_NAME --enable_xformers_memory_efficient_attention --resolution=512 --center_crop --random_flip --proportion_empty_prompts=0.2 --train_batch_size=1 --gradient_accumulation_steps=4 --gradient_checkpointing --max_train_steps=10000 --use_8bit_adam --learning_rate=1e-06 --lr_scheduler=\"constant\" --lr_warmup_steps=0 --mixed_precision=\"fp16\" --report_to=\"wandb\" --validation_prompt=\"a cute Sundar Pichai creature\" --validation_epochs 5 --checkpointing_steps=5000 --output_dir=\"sdxl-pokemon-model\"\r\n\r\n### Expected behavior\r\n\r\nIt should start the training as it does for the single GPU training. I opened the issue in diffusers **https://github.com/huggingface/diffusers/issues/5534 but it does seem to be an issue with the Pokemon dataset.\r\n\r\nI added some debug prints\r\n\r\n  ```\r\n  print(\"==========HERE3=============\")\r\n    with accelerator.main_process_first():\r\n        print(accelerator.is_main_process)\r\n        print(\"===========Here3.1===========\")\r\n        if args.max_train_samples is not None:\r\n            dataset[\"train\"] = dataset[\"train\"].shuffle(seed=args.seed).select(range(args.max_train_samples))\r\n        print(\"===========Here3.2===========\")\r\n        # Set the training transforms\r\n        train_dataset = dataset[\"train\"].with_transform(preprocess_train)\r\n    print(\"==========HERE4=============\")\r\n\r\n\r\nCorresponding Output\r\n\r\nDetected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\r\n10/25/2023 21:18:04 - INFO - main - Distributed environment: MULTI_GPU Backend: nccl\r\nNum processes: 3\r\nProcess index: 1\r\nLocal process index: 1\r\nDevice: cuda:1\r\n\r\nMixed precision type: fp16\r\n\r\n10/25/2023 21:18:04 - INFO - main - Distributed environment: MULTI_GPU Backend: nccl\r\nNum processes: 3\r\nProcess index: 2\r\nLocal process index: 2\r\nDevice: cuda:2\r\n\r\nMixed precision type: fp16\r\n\r\n10/25/2023 21:18:04 - INFO - main - Distributed environment: MULTI_GPU Backend: nccl\r\nNum processes: 3\r\nProcess index: 0\r\nLocal process index: 0\r\nDevice: cuda:0\r\n\r\nMixed precision type: fp16\r\n\r\nYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\r\nYou are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\r\n{\u2018variance_type\u2019, \u2018clip_sample_range\u2019, \u2018thresholding\u2019, \u2018dynamic_thresholding_ratio\u2019} was not found in config. Values will be initialized to default values.\r\n{\u2018attention_type\u2019, \u2018reverse_transformer_layers_per_block\u2019, \u2018dropout\u2019} was not found in config. Values will be initialized to default values.\r\n==========HERE1=============\r\n==========HERE1=============\r\n==========HERE1=============\r\n==========HERE2=============\r\n==========HERE2=============\r\n==========HERE2=============\r\n==========HERE3=============\r\nTrue\r\n===========Here3.1===========\r\n===========Here3.2===========\r\n==========HERE3=============\r\n==========HERE3=========\r\n```\r\n\r\n### Environment info\r\n\r\n_libgcc_mutex 0.1 conda_forge conda-forge\r\n_openmp_mutex 4.5 2_kmp_llvm conda-forge\r\nabsl-py 2.0.0 pypi_0 pypi\r\naccelerate 0.24.0 pypi_0 pypi\r\naiohttp 3.8.6 pypi_0 pypi\r\naiosignal 1.3.1 pypi_0 pypi\r\nappdirs 1.4.4 pyh9f0ad1d_0 conda-forge\r\nasync-timeout 4.0.3 pypi_0 pypi\r\nattrs 23.1.0 pypi_0 pypi\r\nbitsandbytes 0.41.1 pypi_0 pypi\r\nblas 1.0 mkl\r\nblessings 1.7 py39h06a4308_1002\r\nbrotli-python 1.0.9 py39h6a678d5_7\r\nbzip2 1.0.8 h7b6447c_0\r\nca-certificates 2023.08.22 h06a4308_0\r\ncachetools 5.3.2 pypi_0 pypi\r\ncertifi 2023.7.22 py39h06a4308_0\r\ncffi 1.15.1 py39h5eee18b_3\r\ncharset-normalizer 2.0.4 pyhd3eb1b0_0\r\nclick 8.1.7 unix_pyh707e725_0 conda-forge\r\ncryptography 41.0.3 py39hdda0065_0\r\ncuda-cudart 11.7.99 0 nvidia\r\ncuda-cupti 11.7.101 0 nvidia\r\ncuda-libraries 11.7.1 0 nvidia\r\ncuda-nvrtc 11.7.99 0 nvidia\r\ncuda-nvtx 11.7.91 0 nvidia\r\ncuda-runtime 11.7.1 0 nvidia\r\ndatasets 2.14.6 pypi_0 pypi\r\ndiffusers 0.22.0.dev0 pypi_0 pypi\r\ndill 0.3.7 pypi_0 pypi\r\ndocker-pycreds 0.4.0 py_0 conda-forge\r\nffmpeg 4.3 hf484d3e_0 pytorch\r\nfilelock 3.12.4 pypi_0 pypi\r\nfreetype 2.12.1 h4a9f257_0\r\nfrozenlist 1.4.0 pypi_0 pypi\r\nfsspec 2023.10.0 pypi_0 pypi\r\nftfy 6.1.1 pypi_0 pypi\r\ngiflib 5.2.1 h5eee18b_3\r\ngitdb 4.0.11 pyhd8ed1ab_0 conda-forge\r\ngitpython 3.1.40 pyhd8ed1ab_0 conda-forge\r\ngmp 6.2.1 h295c915_3\r\ngnutls 3.6.15 he1e5248_0\r\ngoogle-auth 2.23.3 pypi_0 pypi\r\ngoogle-auth-oauthlib 1.1.0 pypi_0 pypi\r\ngpustat 0.6.0 pyhd3eb1b0_1\r\ngrpcio 1.59.0 pypi_0 pypi\r\nhuggingface-hub 0.17.3 pypi_0 pypi\r\nidna 3.4 py39h06a4308_0\r\nimportlib-metadata 6.8.0 pypi_0 pypi\r\nintel-openmp 2023.1.0 hdb19cb5_46305\r\njinja2 3.1.2 pypi_0 pypi\r\njpeg 9e h5eee18b_1\r\nlame 3.100 h7b6447c_0\r\nlcms2 2.12 h3be6417_0\r\nld_impl_linux-64 2.38 h1181459_1\r\nlerc 3.0 h295c915_0\r\nlibcublas 11.10.3.66 0 nvidia\r\nlibcufft 10.7.2.124 h4fbf590_0 nvidia\r\nlibcufile 1.8.0.34 0 nvidia\r\nlibcurand 10.3.4.52 0 nvidia\r\nlibcusolver 11.4.0.1 0 nvidia\r\nlibcusparse 11.7.4.91 0 nvidia\r\nlibdeflate 1.17 h5eee18b_1\r\nlibffi 3.4.4 h6a678d5_0\r\nlibgcc-ng 13.2.0 h807b86a_2 conda-forge\r\nlibgfortran-ng 13.2.0 h69a702a_2 conda-forge\r\nlibgfortran5 13.2.0 ha4646dd_2 conda-forge\r\nlibiconv 1.16 h7f8727e_2\r\nlibidn2 2.3.4 h5eee18b_0\r\nlibnpp 11.7.4.75 0 nvidia\r\nlibnvjpeg 11.8.0.2 0 nvidia\r\nlibpng 1.6.39 h5eee18b_0\r\nlibprotobuf 3.20.3 he621ea3_0\r\nlibstdcxx-ng 13.2.0 h7e041cc_2 conda-forge\r\nlibtasn1 4.19.0 h5eee18b_0\r\nlibtiff 4.5.1 h6a678d5_0\r\nlibunistring 0.9.10 h27cfd23_0\r\nlibwebp 1.3.2 h11a3e52_0\r\nlibwebp-base 1.3.2 h5eee18b_0\r\nllvm-openmp 14.0.6 h9e868ea_0\r\nlz4-c 1.9.4 h6a678d5_0\r\nmarkdown 3.5 pypi_0 pypi\r\nmarkupsafe 2.1.3 pypi_0 pypi\r\nmkl 2023.1.0 h213fc3f_46343\r\nmkl-service 2.4.0 py39h5eee18b_1\r\nmkl_fft 1.3.8 py39h5eee18b_0\r\nmkl_random 1.2.4 py39hdb19cb5_0\r\nmultidict 6.0.4 pypi_0 pypi\r\nmultiprocess 0.70.15 pypi_0 pypi\r\nncurses 6.4 h6a678d5_0\r\nnettle 3.7.3 hbbd107a_1\r\nnumpy 1.26.0 py39h5f9d8c6_0\r\nnumpy-base 1.26.0 py39hb5e798b_0\r\nnvidia-ml 7.352.0 pyhd3eb1b0_0\r\noauthlib 3.2.2 pypi_0 pypi\r\nopenh264 2.1.1 h4ff587b_0\r\nopenjpeg 2.4.0 h3ad879b_0\r\nopenssl 3.0.11 h7f8727e_2\r\npackaging 23.2 pypi_0 pypi\r\npandas 2.1.1 pypi_0 pypi\r\npathtools 0.1.2 py_1 conda-forge\r\npillow 10.0.1 py39ha6cbd5a_0\r\npip 23.3 py39h06a4308_0\r\nprotobuf 4.23.4 pypi_0 pypi\r\npsutil 5.9.6 pypi_0 pypi\r\npyarrow 13.0.0 pypi_0 pypi\r\npyasn1 0.5.0 pypi_0 pypi\r\npyasn1-modules 0.3.0 pypi_0 pypi\r\npycparser 2.21 pyhd3eb1b0_0\r\npyopenssl 23.2.0 py39h06a4308_0\r\npysocks 1.7.1 py39h06a4308_0\r\npython 3.9.18 h955ad1f_0\r\npython-dateutil 2.8.2 pypi_0 pypi\r\npython_abi 3.9 2_cp39 conda-forge\r\npytorch 1.13.1 py3.9_cuda11.7_cudnn8.5.0_0 pytorch\r\npytorch-cuda 11.7 h778d358_5 pytorch\r\npytorch-mutex 1.0 cuda pytorch\r\npytz 2023.3.post1 pypi_0 pypi\r\npyyaml 6.0.1 pypi_0 pypi\r\nreadline 8.2 h5eee18b_0\r\nregex 2023.10.3 pypi_0 pypi\r\nrequests 2.31.0 py39h06a4308_0\r\nrequests-oauthlib 1.3.1 pypi_0 pypi\r\nrsa 4.9 pypi_0 pypi\r\nsafetensors 0.4.0 pypi_0 pypi\r\nscipy 1.11.3 py39h5f9d8c6_0\r\nsentry-sdk 1.32.0 pyhd8ed1ab_0 conda-forge\r\nsetproctitle 1.1.10 py39h3811e60_1004 conda-forge\r\nsetuptools 68.0.0 py39h06a4308_0\r\nsix 1.16.0 pyh6c4a22f_0 conda-forge\r\nsmmap 5.0.0 pyhd8ed1ab_0 conda-forge\r\nsqlite 3.41.2 h5eee18b_0\r\ntbb 2021.8.0 hdb19cb5_0\r\ntensorboard 2.15.0 pypi_0 pypi\r\ntensorboard-data-server 0.7.2 pypi_0 pypi\r\ntk 8.6.12 h1ccaba5_0\r\ntokenizers 0.14.1 pypi_0 pypi\r\ntorchaudio 0.13.1 py39_cu117 pytorch\r\ntorchtriton 2.1.0 py39 pytorch\r\ntorchvision 0.14.1 py39_cu117 pytorch\r\ntqdm 4.66.1 pypi_0 pypi\r\ntransformers 4.34.1 pypi_0 pypi\r\ntyping_extensions 4.7.1 py39h06a4308_0\r\ntzdata 2023.3 pypi_0 pypi\r\nurllib3 1.26.18 py39h06a4308_0\r\nwandb 0.15.12 pyhd8ed1ab_0 conda-forge\r\nwcwidth 0.2.8 pypi_0 pypi\r\nwerkzeug 3.0.1 pypi_0 pypi\r\nwheel 0.41.2 py39h06a4308_0\r\nxformers 0.0.22.post7 py39_cu11.7.1_pyt1.13.1 xformers\r\nxxhash 3.4.1 pypi_0 pypi\r\nxz 5.4.2 h5eee18b_0\r\nyaml 0.2.5 h7f98852_2 conda-forge\r\nyarl 1.9.2 pypi_0 pypi\r\nzipp 3.17.0 pypi_0 pypi\r\nzlib 1.2.13 h5eee18b_0\r\nzstd 1.5.5 hc292b87_0<end_of_turn>\n<start_of_turn>model\ndataset.transform() hangs indefinitely while finetuning the stable diffusion XL<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease 2.19.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nsolve  #6798:\r\n\r\nadd missing `StreamingDownloadManager` type annotation to the `dl_manager` argument of the `DatasetBuilder._split_generators` function<end_of_turn>\n<start_of_turn>model\nfix `DatasetBuilder._split_generators` incomplete type annotation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI used a custom builder to ``download_and_prepare`` a dataset. The first (very minor) issue is that the doc seems to suggest ``download_and_prepare`` will return the dataset, while it does not ([builder.py](https://github.com/huggingface/datasets/blob/main/src/datasets/builder.py#L718-L738)).\r\n```\r\n        >>> from datasets import load_dataset_builder\r\n        >>> builder = load_dataset_builder(\"rotten_tomatoes\")\r\n        >>> ds = builder.download_and_prepare(\"./output_dir\", file_format=\"parquet\")\r\n```\r\n\r\nThe main issue I am facing is loading the dataset from those parquet files. I used the `as_dataset` method suggested by the doc, however it returns:\r\n`\r\nFileNotFoundError: [Errno 2] Failed to open local file 'output_dir/__main__-train-00000-of-00245.arrow'. Detail:\r\n[errno 2] No such file or directory.\r\n` \n\n### Steps to reproduce the bug\n\n1. Create a custom builder of some sort: `builder = CustomBuilder()`.\r\n2. Run `download_and_prepare` with the parquet format: `builder.download_and_prepare(\"./output_dir\", file_format=\"parquet\")`.\r\n3. Run `dataset = builder.as_dataset()`.  \n\n### Expected behavior\n\nI guess I'd expect `as_dataset` to generate the dataset in arrow format if it has to, or to suggest an alternative way to load the dataset (I've also tried other methods with `load_dataset` to no avail, probably due to misunderstandings on my part). \n\n### Environment info\n\n```\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.15.0-1027-gcp-x86_64-with-glibc2.31\r\n- Python version: 3.10.0\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 8.0.0\r\n- Pandas version: 1.5.3\r\n```<end_of_turn>\n<start_of_turn>model\nUsing as_dataset on a \"parquet\" builder  <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nremove unused code<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRename `LargeList.dtype` to `LargeList.feature`.\r\n\r\nNote that `dtype` is usually used for NumPy data types (\"int64\", \"float32\",...): see `Value.dtype`.\r\n\r\nHowever, `LargeList` attribute (like `Sequence.feature`) expects a `FeatureType` instead.\r\n\r\nWith this renaming:\r\n- we avoid confusion about the expected type and\r\n- we also align `LargeList` with `Sequence`.<end_of_turn>\n<start_of_turn>model\nRename LargeList.dtype to LargeList.feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to load the Flores data set\r\n\r\nthe code which is given is\r\n```\r\n\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"facebook/flores\")\r\n\r\n```\r\nThis gives the error of config name \r\n\r\n\"\"ValueError: Config name is missing\"\r\n\r\nNow if I add some config it gives me the some error\r\n\r\n\"HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'facebook/flores, 'ace_Arab''.\r\n\"\r\n\r\nHow I can load the data of the specific language ?\r\n\r\nCouldn't find any tutorial\r\n\r\nany one can help me out?\n\n### Steps to reproduce the bug\n\nstep one load the data set \r\n\r\n`from datasets import load_dataset\r\n\r\ndataset = load_dataset(\"facebook/flores\")`\r\n\r\nit gives the error of config\r\n\r\nonce config is given \r\n\r\nit gives the error of\r\n\r\n\"HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: 'facebook/flores, 'ace_Arab''.\r\n\"\n\n### Expected behavior\n\nData set should be loaded but  I am receiving error\n\n### Environment info\n\nDatasets , python , <end_of_turn>\n<start_of_turn>model\nLoading The flores data set for specific language<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix test using latest commit SHA in hf-internal-testing/dataset_with_script dataset: https://huggingface.co/datasets/hf-internal-testing/dataset_with_script/commits/refs%2Fconvert%2Fparquet\r\n\r\nFix #6796.<end_of_turn>\n<start_of_turn>model\nFix hf-internal-testing/dataset_with_script commit SHA in CI test<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI tried to load a music file using `datasets.load_dataset()` from the repository - https://huggingface.co/datasets/susnato/pop2piano_real_music_test\r\n\r\nI got the following error - \r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2803, in __getitem__\r\n    return self._getitem(key)\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/arrow_dataset.py\", line 2788, in _getitem\r\n    formatted_output = format_table(\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/formatting/formatting.py\", line 629, in format_table\r\n    return formatter(pa_table, query_type=query_type)\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/formatting/formatting.py\", line 398, in __call__\r\n    return self.format_column(pa_table)\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/formatting/formatting.py\", line 442, in format_column\r\n    column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/formatting/formatting.py\", line 218, in decode_column\r\n    return self.features.decode_column(column, column_name) if self.features else column\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/features/features.py\", line 1924, in decode_column\r\n    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/features/features.py\", line 1924, in <listcomp>\r\n    [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/features/features.py\", line 1325, in decode_nested_example\r\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/datasets/features/audio.py\", line 184, in decode_example\r\n    array, sampling_rate = sf.read(f)\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/soundfile.py\", line 372, in read\r\n    with SoundFile(file, 'r', samplerate, channels,\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/soundfile.py\", line 740, in __init__\r\n    self._file = self._open(file, mode_int, closefd)\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/soundfile.py\", line 1264, in _open\r\n    _error_check(_snd.sf_error(file_ptr),\r\n  File \"/home/susnato/anaconda3/envs/p2p/lib/python3.9/site-packages/soundfile.py\", line 1455, in _error_check\r\n    raise RuntimeError(prefix + _ffi.string(err_str).decode('utf-8', 'replace'))\r\nRuntimeError: Error opening <_io.BufferedReader name='/home/susnato/.cache/huggingface/datasets/downloads/d2b09cb974b967b13f91553297c40c0f02f3c0d4c8356350743598ff48d6f29e'>: Format not recognised.\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nCode to reproduce the error - \r\n\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"susnato/pop2piano_real_music_test\", split=\"test\")\r\nprint(ds[0])\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nI should be able to read the music file without any error.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.14.0\r\n- Platform: Linux-5.19.0-50-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n<end_of_turn>\n<start_of_turn>model\nError loading music files using `load_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMake convert_to_parquet CLI command create a \"script\" branch and keep the script file on it.\r\n\r\nThis PR proposes the simplest UX approach: whenever `--revision` is not explicitly passed (i.e., when the script is in the main branch), try to create a \"script\" branch from the \"main\" branch; if the \"script\" branch exists already, then do nothing.\r\n\r\nFollow-up of:\r\n- #6795\r\n\r\nClose #6808.\r\n\r\nCC: @severo <end_of_turn>\n<start_of_turn>model\nMake convert_to_parquet CLI command create script branch<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nReturn the audio filename when the audio decoding fails. Although currently there are some checks for mp3 and opus formats with the library version there are still cases when the audio decoding could fail, eg. Corrupt file. \r\n\r\n### Motivation\r\n\r\nWhen you try to load an object file dataset and the decoding fails you can't know which file is corrupt\r\n```\r\n\r\nraise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\r\nsoundfile.LibsndfileError: Error opening <_io.BytesIO object at 0x7f5ab7e38290>: Format not recognised.\r\n```\r\n\r\n### Your contribution\r\n\r\nMake a PR to Add exceptions for LIbsndfileError to return the audio filename or path when soundfile decoding fails.<end_of_turn>\n<start_of_turn>model\nReturn the audio filename when decoding fails due to corrupt files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI can't apply the filter method on my dataset. \n\n### Steps to reproduce the bug\n\nThe following snippet generates a bug: \r\n\r\n```python\r\nfrom datasets import load_dataset \r\n\r\nami = load_dataset('kamilakesbi/ami', 'ihm')\r\n\r\nami['train'].filter(\r\n    lambda example: example[\"file_name\"] == 'EN2001a'\r\n)\r\n```\r\n\r\nI get the following error: \r\n\r\n`ImportError: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).`\n\n### Expected behavior\n\nIt should work properly! \n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-5.15.0-67-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\nImportError: numpy.core.multiarray when using `filter`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\npandas columns: song_id, song_name\r\nds = Dataset.from_pandas(df)\r\n\r\ndef has_cover(song_name):\r\n    if song_name is None or pd.isna(song_name):\r\n        return False\r\n    return 'cover' in song_name.lower()\r\n\r\ndf['has_cover'] = df.song_name.progress_apply(has_cover)\r\nds = ds.map(lambda x: {'has_cover': has_cover(x['song_name'])}, num_proc=10)\r\n\r\ntime cost: \r\n1. df.apply: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 12500592/12500592 [00:13<00:00, 959825.47it/s]\r\n2. ds.map: Map\u2007(num_proc=10):\u2007\u200731%\r\n\u20073899028/12500592\u2007[00:28<00:38,\u2007222532.89\u2007examples/s]\n\n### Steps to reproduce the bug\n\npandas columns: song_id, song_name\r\nds = Dataset.from_pandas(df)\r\n\r\ndef has_cover(song_name):\r\n    if song_name is None or pd.isna(song_name):\r\n        return False\r\n    return 'cover' in song_name.lower()\r\n\r\ndf['has_cover'] = df.song_name.progress_apply(has_cover)\r\nds = ds.map(lambda x: {'has_cover': has_cover(x['song_name'])}, num_proc=10)\n\n### Expected behavior\n\nds.map is ~num_proc faster than df.apply\n\n### Environment info\n\npandas: 2.2.2\r\ndatasets: 2.19.1<end_of_turn>\n<start_of_turn>model\nds.map(f, num_proc=10) is slower than df.apply<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently adding a new column with `map` will cause all the data in the dataset to be duplicated and stored/cached on the disk again. It should reuse unchanged columns. \n\n### Motivation\n\nThis allows having datasets with different columns but sharing some basic columns. Currently, these datasets would become too expensive to store and one would need some kind of on-the-fly join; which also doesn't seem implemented.\n\n### Your contribution\n\n_<end_of_turn>\n<start_of_turn>model\n[FR] `map` should reuse unchanged columns from the previous dataset to avoid disk usage<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nValidate `name` parameter in `make_file_instructions`.\r\n\r\nThis way users get more informative error messages, instead of:\r\n```stacktrace\r\n.../huggingface/datasets/src/datasets/arrow_reader.py in make_file_instructions(name, split_infos, instruction, filetype_suffix, prefix_path)\r\n    110     name2len = {info.name: info.num_examples for info in split_infos}\r\n    111     name2shard_lengths = {info.name: info.shard_lengths for info in split_infos}\r\n--> 112     name2filenames = {\r\n    113         info.name: filenames_for_dataset_split(\r\n    114             path=prefix_path,\r\n\r\n.../huggingface/datasets/src/datasets/arrow_reader.py in <dictcomp>(.0)\r\n    111     name2shard_lengths = {info.name: info.shard_lengths for info in split_infos}\r\n    112     name2filenames = {\r\n--> 113         info.name: filenames_for_dataset_split(\r\n    114             path=prefix_path,\r\n    115             dataset_name=name,\r\n\r\n.../huggingface/datasets/src/datasets/naming.py in filenames_for_dataset_split(path, dataset_name, split, filetype_suffix, shard_lengths)\r\n     68 \r\n     69 def filenames_for_dataset_split(path, dataset_name, split, filetype_suffix=None, shard_lengths=None):\r\n---> 70     prefix = filename_prefix_for_split(dataset_name, split)\r\n     71     prefix = os.path.join(path, prefix)\r\n     72 \r\n\r\n.../huggingface/datasets/src/datasets/naming.py in filename_prefix_for_split(name, split)\r\n     52 \r\n     53 def filename_prefix_for_split(name, split):\r\n---> 54     if os.path.basename(name) != name:\r\n     55         raise ValueError(f\"Should be a dataset name, not a path: {name}\")\r\n     56     if not re.match(_split_re, split):\r\n\r\n.../lib/python3.9/posixpath.py in basename(p)\r\n    140 def basename(p):\r\n    141     \"\"\"Returns the final component of a pathname\"\"\"\r\n--> 142     p = os.fspath(p)\r\n    143     sep = _get_sep(p)\r\n    144     i = p.rfind(sep) + 1\r\n\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n```\r\n\r\nRelated to #5895.<end_of_turn>\n<start_of_turn>model\nValidate name parameter in make_file_instructions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n\u8fd9\u91cc\u6211\u8fdb\u884c\u5927\u6a21\u578b\u5fae\u8c03\uff0c\u5f53\u6570\u636e\u96c6\u6570\u91cf\u4e3a718\u65f6\uff0c\u6a21\u578b\u53ef\u4ee5\u6b63\u5e38\u5fae\u8c03\uff0c\u4f46\u662f\u5f53\u6211\u6dfb\u52a0\u4e00\u4e2a\u5728\u524d718\u4e2a\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u6216\u8005\u65b0\u589e\u4e00\u4e2a\u6570\u636e\u5c31\u4f1a\u62a5\u9519\n\n### Steps to reproduce the bug\n\n1.\r\n\u8fd9\u91cc\u6211\u7684\u6570\u636e\u96c6\u53ef\u4ee5\u5fae\u8c03\u7684\u6700\u540e\u4e24\u4e2a\u6570\u636e\u96c6\u662f\uff1a\r\n{\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"\u5b8c\u6210\u6821\u6b63\u88c5\u7f6e\u8bbe\u8ba1\u540e\u9700\u8981\u8fdb\u884c\u54ea\u4e9b\u5de5\u4f5c\uff1f\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"\u4e00\u65e6\u5b8c\u6210\u6821\u6b63\u88c5\u7f6e\u8bbe\u8ba1\u540e\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9645\u8c03\u6821\u5de5\u4f5c\uff0c\u6216\u8005\u8fdb\u884cMATLAB\u4eff\u771f\u4ee5\u68c0\u67e5\u7cfb\u7edf\u7684\u65f6\u95f4\u54cd\u5e94\u7279\u6027\u3002\u8fd9\u65f6\uff0c\u9700\u5c06\u7cfb\u7edf\u5efa\u6a21\u65f6\u7701\u7565\u7684\u90e8\u5206\u5c3d\u53ef\u80fd\u52a0\u5165\u7cfb\u7edf\uff0c\u4ee5\u4fdd\u8bc1\u4eff\u771f\u7ed3\u679c\u7684\u903c\u771f\u5ea6\u3002\u5982\u679c\u7531\u4e8e\u7cfb\u7edf\u5404\u79cd\u56fa\u6709\u975e\u7ebf\u6027\u56e0\u7d20\u5f71\u54cd\uff0c\u6216\u8005\u7531\u4e8e\u7cfb\u7edf\u566a\u58f0\u548c\u8d1f\u8f7d\u6548\u5e94\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u4f7f\u5df2\u6821\u6b63\u7cfb\u7edf\u4e0d\u80fd\u6ee1\u8db3\u5168\u90e8\u6027\u80fd\u6307\u6807\u8981\u6c42\uff0c\u5219\u9700\u8981\u9002\u5f53\u8c03\u6574\u6821\u6b63\u88c5\u7f6e\u7684\u5f62\u5f0f\u6216\u53c2\u6570\uff0c\u76f4\u5230\u5df2\u6821\u6b63\u7cfb\u7edf\u6ee1\u8db3\u5168\u90e8\u6027\u80fd\u6307\u6807\u4e3a\u6b62\u3002\"\r\n    }\r\n  ]\r\n}\r\n\r\n{\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"\u4e3a\u4ec0\u4e48\u95ed\u73af\u5e26\u5bbd\u8981\u6c42\u4f1a\u5f71\u54cd\u4e32\u8054\u8d85\u524d\u6821\u6b63\u7684\u6548\u679c\uff1f\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"\u5982\u679c\u5f85\u6821\u6b63\u7cfb\u7edf\u4e0d\u7a33\u5b9a\uff0c\u4e3a\u4e86\u6ee1\u8db3\u76f8\u89d2\u88d5\u5ea6\u7684\u8981\u6c42\uff0c\u8d85\u524d\u7f51\u7edc\u9700\u8981\u63d0\u4f9b\u5f88\u5927\u7684\u76f8\u89d2\u8d85\u524d\u91cf\u3002\u8fd9\u4f1a\u5bfc\u81f4\u8d85\u524d\u7f51\u7edc\u7684a\u503c\u5fc5\u987b\u9009\u5f97\u5f88\u5927\uff0c\u4ece\u800c\u4f7f\u5df2\u6821\u6b63\u7cfb\u7edf\u7684\u5e26\u5bbd\u8fc7\u5927\u3002\u5e26\u5bbd\u8fc7\u5927\u4f1a\u4f7f\u901a\u8fc7\u7cfb\u7edf\u7684\u9ad8\u9891\u566a\u58f0\u7535\u5e73\u53d8\u9ad8\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u5931\u63a7\u3002\u56e0\u6b64\uff0c\u95ed\u73af\u5e26\u5bbd\u7684\u8981\u6c42\u4f1a\u5f71\u54cd\u4e32\u8054\u8d85\u524d\u6821\u6b63\u7684\u6548\u679c\u3002\"\r\n    }\r\n  ]\r\n}\r\n2.\r\n\u5f53\u6211\u628a\u5012\u6570\u7b2c\u4e8c\u4e2a\u6570\u636e\u96c6\u518d\u6b21\u6dfb\u52a0\u5230\u6570\u636e\u96c6\u4e2d\u65f6\uff0c\u6b64\u65f6\u6211\u7684\u6700\u540e\u4e09\u4e2a\u6570\u636e\u96c6\u662f\r\n{\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"\u5b8c\u6210\u6821\u6b63\u88c5\u7f6e\u8bbe\u8ba1\u540e\u9700\u8981\u8fdb\u884c\u54ea\u4e9b\u5de5\u4f5c\uff1f\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"\u4e00\u65e6\u5b8c\u6210\u6821\u6b63\u88c5\u7f6e\u8bbe\u8ba1\u540e\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9645\u8c03\u6821\u5de5\u4f5c\uff0c\u6216\u8005\u8fdb\u884cMATLAB\u4eff\u771f\u4ee5\u68c0\u67e5\u7cfb\u7edf\u7684\u65f6\u95f4\u54cd\u5e94\u7279\u6027\u3002\u8fd9\u65f6\uff0c\u9700\u5c06\u7cfb\u7edf\u5efa\u6a21\u65f6\u7701\u7565\u7684\u90e8\u5206\u5c3d\u53ef\u80fd\u52a0\u5165\u7cfb\u7edf\uff0c\u4ee5\u4fdd\u8bc1\u4eff\u771f\u7ed3\u679c\u7684\u903c\u771f\u5ea6\u3002\u5982\u679c\u7531\u4e8e\u7cfb\u7edf\u5404\u79cd\u56fa\u6709\u975e\u7ebf\u6027\u56e0\u7d20\u5f71\u54cd\uff0c\u6216\u8005\u7531\u4e8e\u7cfb\u7edf\u566a\u58f0\u548c\u8d1f\u8f7d\u6548\u5e94\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u4f7f\u5df2\u6821\u6b63\u7cfb\u7edf\u4e0d\u80fd\u6ee1\u8db3\u5168\u90e8\u6027\u80fd\u6307\u6807\u8981\u6c42\uff0c\u5219\u9700\u8981\u9002\u5f53\u8c03\u6574\u6821\u6b63\u88c5\u7f6e\u7684\u5f62\u5f0f\u6216\u53c2\u6570\uff0c\u76f4\u5230\u5df2\u6821\u6b63\u7cfb\u7edf\u6ee1\u8db3\u5168\u90e8\u6027\u80fd\u6307\u6807\u4e3a\u6b62\u3002\"\r\n    }\r\n  ]\r\n}\r\n\r\n{\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"\u4e3a\u4ec0\u4e48\u95ed\u73af\u5e26\u5bbd\u8981\u6c42\u4f1a\u5f71\u54cd\u4e32\u8054\u8d85\u524d\u6821\u6b63\u7684\u6548\u679c\uff1f\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"\u5982\u679c\u5f85\u6821\u6b63\u7cfb\u7edf\u4e0d\u7a33\u5b9a\uff0c\u4e3a\u4e86\u6ee1\u8db3\u76f8\u89d2\u88d5\u5ea6\u7684\u8981\u6c42\uff0c\u8d85\u524d\u7f51\u7edc\u9700\u8981\u63d0\u4f9b\u5f88\u5927\u7684\u76f8\u89d2\u8d85\u524d\u91cf\u3002\u8fd9\u4f1a\u5bfc\u81f4\u8d85\u524d\u7f51\u7edc\u7684a\u503c\u5fc5\u987b\u9009\u5f97\u5f88\u5927\uff0c\u4ece\u800c\u4f7f\u5df2\u6821\u6b63\u7cfb\u7edf\u7684\u5e26\u5bbd\u8fc7\u5927\u3002\u5e26\u5bbd\u8fc7\u5927\u4f1a\u4f7f\u901a\u8fc7\u7cfb\u7edf\u7684\u9ad8\u9891\u566a\u58f0\u7535\u5e73\u53d8\u9ad8\uff0c\u53ef\u80fd\u5bfc\u81f4\u7cfb\u7edf\u5931\u63a7\u3002\u56e0\u6b64\uff0c\u95ed\u73af\u5e26\u5bbd\u7684\u8981\u6c42\u4f1a\u5f71\u54cd\u4e32\u8054\u8d85\u524d\u6821\u6b63\u7684\u6548\u679c\u3002\"\r\n    }\r\n  ]\r\n}\r\n\r\n{\r\n  \"messages\": [\r\n    {\r\n      \"role\": \"user\",\r\n      \"content\": \"\u5b8c\u6210\u6821\u6b63\u88c5\u7f6e\u8bbe\u8ba1\u540e\u9700\u8981\u8fdb\u884c\u54ea\u4e9b\u5de5\u4f5c\uff1f\"\r\n    },\r\n    {\r\n      \"role\": \"assistant\",\r\n      \"content\": \"\u4e00\u65e6\u5b8c\u6210\u6821\u6b63\u88c5\u7f6e\u8bbe\u8ba1\u540e\uff0c\u9700\u8981\u8fdb\u884c\u7cfb\u7edf\u5b9e\u9645\u8c03\u6821\u5de5\u4f5c\uff0c\u6216\u8005\u8fdb\u884cMATLAB\u4eff\u771f\u4ee5\u68c0\u67e5\u7cfb\u7edf\u7684\u65f6\u95f4\u54cd\u5e94\u7279\u6027\u3002\u8fd9\u65f6\uff0c\u9700\u5c06\u7cfb\u7edf\u5efa\u6a21\u65f6\u7701\u7565\u7684\u90e8\u5206\u5c3d\u53ef\u80fd\u52a0\u5165\u7cfb\u7edf\uff0c\u4ee5\u4fdd\u8bc1\u4eff\u771f\u7ed3\u679c\u7684\u903c\u771f\u5ea6\u3002\u5982\u679c\u7531\u4e8e\u7cfb\u7edf\u5404\u79cd\u56fa\u6709\u975e\u7ebf\u6027\u56e0\u7d20\u5f71\u54cd\uff0c\u6216\u8005\u7531\u4e8e\u7cfb\u7edf\u566a\u58f0\u548c\u8d1f\u8f7d\u6548\u5e94\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u4f7f\u5df2\u6821\u6b63\u7cfb\u7edf\u4e0d\u80fd\u6ee1\u8db3\u5168\u90e8\u6027\u80fd\u6307\u6807\u8981\u6c42\uff0c\u5219\u9700\u8981\u9002\u5f53\u8c03\u6574\u6821\u6b63\u88c5\u7f6e\u7684\u5f62\u5f0f\u6216\u53c2\u6570\uff0c\u76f4\u5230\u5df2\u6821\u6b63\u7cfb\u7edf\u6ee1\u8db3\u5168\u90e8\u6027\u80fd\u6307\u6807\u4e3a\u6b62\u3002\"\r\n    }\r\n  ]\r\n}\r\n\u8fd9\u65f6\u7cfb\u7edf\u4f1a\u663e\u793abug\uff1a\r\nroot@autodl-container-027f4cad3d-6baf4e64:~/autodl-tmp# python GLM-4/finetune_demo/finetune.py  datasets/  ZhipuAI/glm-4-9b-chat  GLM-4/finetune_demo/configs/lora.yaml\r\nLoading checkpoint shards: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:02<00:00,  4.04it/s]\r\nThe installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\r\ntrainable params: 2,785,280 || all params: 9,402,736,640 || trainable%: 0.0296\r\nGenerating train split: 0 examples [00:00, ? examples/s]Failed to load JSON from file '/root/autodl-tmp/datasets/train.jsonl' with error <class 'pyarrow.lib.ArrowInvalid'>: JSON parse error: Missing a name for object member. in row 718\r\nGenerating train split: 0 examples [00:00, ? examples/s]\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/packaged_modules/json/json.py:153 in _generate_tables                                                                                                                                       \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   150 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   with open(                                                                                                                                                                                                   \u2502\r\n\u2502   151 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   file, encoding=self.config.encoding, errors=self.con                                                                                                                                                     \u2502\r\n\u2502   152 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   ) as f:                                                                                                                                                                                                      \u2502\r\n\u2502 \u2771 153 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   df = pd.read_json(f, dtype_backend=\"pyarrow\")                                                                                                                                                            \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   except ValueError:                                                                                                                                                                                               \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   logger.error(f\"Failed to load JSON from file '{file}' wi                                                                                                                                                     \u2502\r\n\u2502   156 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   raise e                                                                                                                                                                                                      \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:815 in read_json                                                                                                                                                             \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502    812 \u2502   if chunksize:                                                                                                                                                                                                                           \u2502\r\n\u2502    813 \u2502   \u2502   return json_reader                                                                                                                                                                                                                  \u2502\r\n\u2502    814 \u2502   else:                                                                                                                                                                                                                                   \u2502\r\n\u2502 \u2771  815 \u2502   \u2502   return json_reader.read()                                                                                                                                                                                                           \u2502\r\n\u2502    816                                                                                                                                                                                                                                             \u2502\r\n\u2502    817                                                                                                                                                                                                                                             \u2502\r\n\u2502    818 class JsonReader(abc.Iterator, Generic[FrameSeriesStrT]):                                                                                                                                                                                   \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:1025 in read                                                                                                                                                                 \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1022 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   data_lines = data.split(\"\\n\")                                                                                                                                                                                       \u2502\r\n\u2502   1023 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   obj = self._get_object_parser(self._combine_lines(data_lines))                                                                                                                                                      \u2502\r\n\u2502   1024 \u2502   \u2502   \u2502   \u2502   else:                                                                                                                                                                                                                       \u2502\r\n\u2502 \u2771 1025 \u2502   \u2502   \u2502   \u2502   \u2502   obj = self._get_object_parser(self.data)                                                                                                                                                                                \u2502\r\n\u2502   1026 \u2502   \u2502   \u2502   \u2502   if self.dtype_backend is not lib.no_default:                                                                                                                                                                                \u2502\r\n\u2502   1027 \u2502   \u2502   \u2502   \u2502   \u2502   return obj.convert_dtypes(                                                                                                                                                                                              \u2502\r\n\u2502   1028 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   infer_objects=False, dtype_backend=self.dtype_backend                                                                                                                                                               \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:1051 in _get_object_parser                                                                                                                                                   \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1048 \u2502   \u2502   }                                                                                                                                                                                                                                   \u2502\r\n\u2502   1049 \u2502   \u2502   obj = None                                                                                                                                                                                                                          \u2502\r\n\u2502   1050 \u2502   \u2502   if typ == \"frame\":                                                                                                                                                                                                                  \u2502\r\n\u2502 \u2771 1051 \u2502   \u2502   \u2502   obj = FrameParser(json, **kwargs).parse()                                                                                                                                                                                       \u2502\r\n\u2502   1052 \u2502   \u2502                                                                                                                                                                                                                                       \u2502\r\n\u2502   1053 \u2502   \u2502   if typ == \"series\" or obj is None:                                                                                                                                                                                                  \u2502\r\n\u2502   1054 \u2502   \u2502   \u2502   if not isinstance(dtype, bool):                                                                                                                                                                                                 \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:1187 in parse                                                                                                                                                                \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1184 \u2502                                                                                                                                                                                                                                           \u2502\r\n\u2502   1185 \u2502   @final                                                                                                                                                                                                                                  \u2502\r\n\u2502   1186 \u2502   def parse(self):                                                                                                                                                                                                                        \u2502\r\n\u2502 \u2771 1187 \u2502   \u2502   self._parse()                                                                                                                                                                                                                       \u2502\r\n\u2502   1188 \u2502   \u2502                                                                                                                                                                                                                                       \u2502\r\n\u2502   1189 \u2502   \u2502   if self.obj is None:                                                                                                                                                                                                                \u2502\r\n\u2502   1190 \u2502   \u2502   \u2502   return None                                                                                                                                                                                                                     \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/pandas/io/json/_json.py:1403 in _parse                                                                                                                                                               \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1400 \u2502   \u2502                                                                                                                                                                                                                                       \u2502\r\n\u2502   1401 \u2502   \u2502   if orient == \"columns\":                                                                                                                                                                                                             \u2502\r\n\u2502   1402 \u2502   \u2502   \u2502   self.obj = DataFrame(                                                                                                                                                                                                           \u2502\r\n\u2502 \u2771 1403 \u2502   \u2502   \u2502   \u2502   ujson_loads(json, precise_float=self.precise_float), dtype=None                                                                                                                                                             \u2502\r\n\u2502   1404 \u2502   \u2502   \u2502   )                                                                                                                                                                                                                               \u2502\r\n\u2502   1405 \u2502   \u2502   elif orient == \"split\":                                                                                                                                                                                                             \u2502\r\n\u2502   1406 \u2502   \u2502   \u2502   decoded = {                                                                                                                                                                                                                     \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nValueError: Trailing data\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1997 in _prepare_split_single                                                                                                                                                    \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1994 \u2502   \u2502   \u2502   )                                                                                                                                                                                                                               \u2502\r\n\u2502   1995 \u2502   \u2502   \u2502   try:                                                                                                                                                                                                                            \u2502\r\n\u2502   1996 \u2502   \u2502   \u2502   \u2502   _time = time.time()                                                                                                                                                                                                         \u2502\r\n\u2502 \u2771 1997 \u2502   \u2502   \u2502   \u2502   for _, table in generator:                                                                                                                                                                                                  \u2502\r\n\u2502   1998 \u2502   \u2502   \u2502   \u2502   \u2502   if max_shard_size is not None and writer._num_bytes > max_shard_size                                                                                                                                                    \u2502\r\n\u2502   1999 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   num_examples, num_bytes = writer.finalize()                                                                                                                                                                         \u2502\r\n\u2502   2000 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   writer.close()                                                                                                                                                                                                      \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/packaged_modules/json/json.py:156 in _generate_tables                                                                                                                                       \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   153 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   df = pd.read_json(f, dtype_backend=\"pyarrow\")                                                                                                                                                            \u2502\r\n\u2502   154 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   except ValueError:                                                                                                                                                                                               \u2502\r\n\u2502   155 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   logger.error(f\"Failed to load JSON from file '{file}' wi                                                                                                                                                     \u2502\r\n\u2502 \u2771 156 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   raise e                                                                                                                                                                                                      \u2502\r\n\u2502   157 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   if df.columns.tolist() == [0]:                                                                                                                                                                                   \u2502\r\n\u2502   158 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   df.columns = list(self.config.features) if self.config.f                                                                                                                                                     \u2502\r\n\u2502   159 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   try:                                                                                                                                                                                                             \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/packaged_modules/json/json.py:130 in _generate_tables                                                                                                                                       \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   127 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   try:                                                                                                                                                                                                                 \u2502\r\n\u2502   128 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   while True:                                                                                                                                                                                                      \u2502\r\n\u2502   129 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   try:                                                                                                                                                                                                         \u2502\r\n\u2502 \u2771 130 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   pa_table = paj.read_json(                                                                                                                                                                                \u2502\r\n\u2502   131 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   io.BytesIO(batch), read_options=paj.ReadOptions(                                                                                                                                                     \u2502\r\n\u2502   132 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   )                                                                                                                                                                                                        \u2502\r\n\u2502   133 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   break                                                                                                                                                                                                    \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 in pyarrow._json.read_json:308                                                                                                                                                                                                                     \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 in pyarrow.lib.pyarrow_internal_check_status:154                                                                                                                                                                                                   \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 in pyarrow.lib.check_status:91                                                                                                                                                                                                                     \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nArrowInvalid: JSON parse error: Missing a name for object member. in row 718\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\n\u256d\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Traceback (most recent call last) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\r\n\u2502 /root/autodl-tmp/GLM-4/finetune_demo/finetune.py:406 in main                                                                                                                                                                                       \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   403 ):                                                                                                                                                                                                                                           \u2502\r\n\u2502   404 \u2502   ft_config = FinetuningConfig.from_file(config_file)                                                                                                                                                                                      \u2502\r\n\u2502   405 \u2502   tokenizer, model = load_tokenizer_and_model(model_dir, peft_config=ft_config.peft_co                                                                                                                                                     \u2502\r\n\u2502 \u2771 406 \u2502   data_manager = DataManager(data_dir, ft_config.data_config)                                                                                                                                                                              \u2502\r\n\u2502   407 \u2502                                                                                                                                                                                                                                            \u2502\r\n\u2502   408 \u2502   train_dataset = data_manager.get_dataset(                                                                                                                                                                                                \u2502\r\n\u2502   409 \u2502   \u2502   Split.TRAIN,                                                                                                                                                                                                                         \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/autodl-tmp/GLM-4/finetune_demo/finetune.py:204 in __init__                                                                                                                                                                                   \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   201 \u2502   def __init__(self, data_dir: str, data_config: DataConfig):                                                                                                                                                                              \u2502\r\n\u2502   202 \u2502   \u2502   self._num_proc = data_config.num_proc                                                                                                                                                                                                \u2502\r\n\u2502   203 \u2502   \u2502                                                                                                                                                                                                                                        \u2502\r\n\u2502 \u2771 204 \u2502   \u2502   self._dataset_dct = _load_datasets(                                                                                                                                                                                                  \u2502\r\n\u2502   205 \u2502   \u2502   \u2502   data_dir,                                                                                                                                                                                                                        \u2502\r\n\u2502   206 \u2502   \u2502   \u2502   data_config.data_format,                                                                                                                                                                                                         \u2502\r\n\u2502   207 \u2502   \u2502   \u2502   data_config.data_files,                                                                                                                                                                                                          \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/autodl-tmp/GLM-4/finetune_demo/finetune.py:189 in _load_datasets                                                                                                                                                                             \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   186 \u2502   \u2502   num_proc: Optional[int],                                                                                                                                                                                                             \u2502\r\n\u2502   187 ) -> DatasetDict:                                                                                                                                                                                                                            \u2502\r\n\u2502   188 \u2502   if data_format == '.jsonl':                                                                                                                                                                                                              \u2502\r\n\u2502 \u2771 189 \u2502   \u2502   dataset_dct = load_dataset(                                                                                                                                                                                                          \u2502\r\n\u2502   190 \u2502   \u2502   \u2502   data_dir,                                                                                                                                                                                                                        \u2502\r\n\u2502   191 \u2502   \u2502   \u2502   data_files=data_files,                                                                                                                                                                                                           \u2502\r\n\u2502   192 \u2502   \u2502   \u2502   split=None,                                                                                                                                                                                                                      \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/load.py:2616 in load_dataset                                                                                                                                                                \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   2613 \u2502   \u2502   return builder_instance.as_streaming_dataset(split=split)                                                                                                                                                                           \u2502\r\n\u2502   2614 \u2502                                                                                                                                                                                                                                           \u2502\r\n\u2502   2615 \u2502   # Download and prepare data                                                                                                                                                                                                             \u2502\r\n\u2502 \u2771 2616 \u2502   builder_instance.download_and_prepare(                                                                                                                                                                                                  \u2502\r\n\u2502   2617 \u2502   \u2502   download_config=download_config,                                                                                                                                                                                                    \u2502\r\n\u2502   2618 \u2502   \u2502   download_mode=download_mode,                                                                                                                                                                                                        \u2502\r\n\u2502   2619 \u2502   \u2502   verification_mode=verification_mode,                                                                                                                                                                                                \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1029 in download_and_prepare                                                                                                                                                     \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1026 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   prepare_split_kwargs[\"max_shard_size\"] = max_shard_size                                                                                                                                                         \u2502\r\n\u2502   1027 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   if num_proc is not None:                                                                                                                                                                                            \u2502\r\n\u2502   1028 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   prepare_split_kwargs[\"num_proc\"] = num_proc                                                                                                                                                                     \u2502\r\n\u2502 \u2771 1029 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   self._download_and_prepare(                                                                                                                                                                                         \u2502\r\n\u2502   1030 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   dl_manager=dl_manager,                                                                                                                                                                                          \u2502\r\n\u2502   1031 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   verification_mode=verification_mode,                                                                                                                                                                            \u2502\r\n\u2502   1032 \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   \u2502   **prepare_split_kwargs,                                                                                                                                                                                         \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1124 in _download_and_prepare                                                                                                                                                    \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1121 \u2502   \u2502   \u2502                                                                                                                                                                                                                                   \u2502\r\n\u2502   1122 \u2502   \u2502   \u2502   try:                                                                                                                                                                                                                            \u2502\r\n\u2502   1123 \u2502   \u2502   \u2502   \u2502   # Prepare split will record examples associated to the split                                                                                                                                                                \u2502\r\n\u2502 \u2771 1124 \u2502   \u2502   \u2502   \u2502   self._prepare_split(split_generator, **prepare_split_kwargs)                                                                                                                                                                \u2502\r\n\u2502   1125 \u2502   \u2502   \u2502   except OSError as e:                                                                                                                                                                                                            \u2502\r\n\u2502   1126 \u2502   \u2502   \u2502   \u2502   raise OSError(                                                                                                                                                                                                              \u2502\r\n\u2502   1127 \u2502   \u2502   \u2502   \u2502   \u2502   \"Cannot find data file. \"                                                                                                                                                                                               \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/builder.py:1884 in _prepare_split                                                                                                                                                           \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   1881 \u2502   \u2502   \u2502   gen_kwargs = split_generator.gen_kwargs                                                                                                                                                                                         \u2502\r\n\u2502   1882 \u2502   \u2502   \u2502   job_id = 0                                                                                                                                                                                                                      \u2502\r\n\u2502   1883 \u2502   \u2502   \u2502   with pbar:                                                                                                                                                                                                                      \u2502\r\n\u2502 \u2771 1884 \u2502   \u2502   \u2502   \u2502   for job_id, done, content in self._prepare_split_single(                                                                                                                                                                    \u2502\r\n\u2502   1885 \u2502   \u2502   \u2502   \u2502   \u2502   gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args                                                                                                                                                             \u2502\r\n\u2502   1886 \u2502   \u2502   \u2502   \u2502   ):                                                                                                                                                                                                                          \u2502\r\n\u2502   1887 \u2502   \u2502   \u2502   \u2502   \u2502   if done:                                                                                                                                                                                                                \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502 /root/miniconda3/lib/python3.10/site-packages/datasets/builder.py:2040 in _prepare_split_single                                                                                                                                                    \u2502\r\n\u2502                                                                                                                                                                                                                                                    \u2502\r\n\u2502   2037 \u2502   \u2502   \u2502   \u2502   e = e.__context__                                                                                                                                                                                                           \u2502\r\n\u2502   2038 \u2502   \u2502   \u2502   if isinstance(e, DatasetGenerationError):                                                                                                                                                                                       \u2502\r\n\u2502   2039 \u2502   \u2502   \u2502   \u2502   raise                                                                                                                                                                                                                       \u2502\r\n\u2502 \u2771 2040 \u2502   \u2502   \u2502   raise DatasetGenerationError(\"An error occurred while generating the dataset                                                                                                                                                    \u2502\r\n\u2502   2041 \u2502   \u2502                                                                                                                                                                                                                                       \u2502\r\n\u2502   2042 \u2502   \u2502   yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_                                                                                                                                                    \u2502\r\n\u2502   2043                                                                                                                                                                                                                                             \u2502\r\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n3.\u8bf7\u95ee\u662f\u5426\u53ef\u4ee5\u5e2e\u6211\u89e3\u51b3\r\n\n\n### Expected behavior\n\n\u5e0c\u671b\u95ee\u9898\u53ef\u4ee5\u5f97\u5230\u89e3\u51b3\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.20.0\r\n- Platform: Linux-4.19.90-2107.6.0.0192.8.oe1.bclinux.x86_64-x86_64-with-glibc2.35\r\n- Python version: 3.10.8\r\n- `huggingface_hub` version: 0.24.6\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2023.12.2<end_of_turn>\n<start_of_turn>model\n\u6570\u636e\u96c6\u6570\u91cf\u95ee\u9898<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nA dataset repository with one image and one metadata file was considered a JSON dataset instead of an ImageFolder dataset. This is because we pick the dataset type with the most compatible data file extensions present in the repository and it results in a tie in this case.\r\n\r\ne.g. for https://huggingface.co/datasets/multimodalart/repro_1_image\r\n\r\nI fixed this by deprioritizing metadata files in the count.\r\n\r\nfix https://github.com/huggingface/datasets/issues/6545<end_of_turn>\n<start_of_turn>model\nFix imagefolder with one image<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nImplements #7210 using method suggested in https://github.com/huggingface/datasets/pull/7207#issuecomment-2411789307\r\n\r\n\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset, Features, Array3D\r\nfeatures=Features(**{\"array0\": Array3D((None, 10, 10), dtype=\"float32\"), \"array1\": Array3D((None,10,10), dtype=\"float32\")})\r\ndataset = Dataset.from_dict({f\"array{i}\": [np.zeros((x,10,10), dtype=np.float32) for x in [2000,1000]*25] for i in range(2)}, features=features)\r\n```\r\n~0.02 s vs 0.9s on main\r\n\r\n```python\r\nds = dataset.to_iterable_dataset()\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\n```\r\n< 0.01 s vs 1.3 s on main\r\n\r\n@lhoestq I can see this breaks a bunch of array-related tests but can update the test cases if you would support making this change?\r\n\r\nI also added an Array1D feature which will always be decoded into a numpy array and likewise improves extraction performance:\r\n\r\n```python\r\nfrom datasets import Dataset, Features, Array1D, Sequence, Value\r\narray_features=Features(**{\"array0\": Array1D((None,), dtype=\"float32\"), \"array1\": Array1D((None,), dtype=\"float32\")})\r\nsequence_features=Features(**{\"array0\": Sequence(feature=Value(\"float32\"), length=-1), \"array1\": Sequence(feature=Value(\"float32\"), length=-1)})\r\narray_dataset = Dataset.from_dict({f\"array{i}\": [np.zeros((x,), dtype=np.float32) for x in [20000,10000]*25] for i in range(2)}, features=array_features)\r\nsequence_dataset = Dataset.from_dict({f\"array{i}\": [np.zeros((x,), dtype=np.float32) for x in [20000,10000]*25] for i in range(2)}, features=sequence_features)\r\n\r\n\r\n```python\r\nt0 = time.time()\r\nfor ex in array_dataset.to_iterable_dataset():\r\n    pass\r\nt1 = time.time()\r\n```\r\n< 0.01 s\r\n\r\n```python\r\nt0 = time.time()\r\nfor ex in sequence_dataset.to_iterable_dataset():\r\n    pass\r\nt1 = time.time()\r\n```\r\n~1.1s\r\n\r\nAnd also added support for extracting structs of arrays as dicts of numpy arrays:\r\n\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset, Features, Array3D, Sequence\r\nfeatures=Features(struct={\"array0\": Array3D((None,10,10), dtype=\"float32\"), \"array1\": Array3D((None,10,10), dtype=\"float32\")}, _list=Sequence(feature=Array3D((None,10,10), dtype=\"float32\")))\r\ndataset = Dataset.from_dict({\"struct\": [{f\"array{i}\": np.zeros((x,10,10), dtype=np.float32) for i in range(2)} for x in [2000,1000]*25], \"_list\": [[np.zeros((x,10,10), dtype=np.float32) for i in range(2)] for x in [2000,1000]*25]}, features=features)\r\n```\r\n\r\n```python\r\nt0 = time.time()\r\nfor ex in dataset.to_iterable_dataset():\r\n    pass\r\nt1 = time.time()\r\nassert isinstance(ex[\"struct\"][\"array0\"], np.ndarray) and ex[\"struct\"][\"array0\"].ndim == 3\r\n```\r\n~0.02 s and no exception vs ~7s with an exception on main<end_of_turn>\n<start_of_turn>model\nfast array extraction<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\r\n### Related Issues/PRs\r\n\r\n<!-- Uncomment 'Resolve' if this PR can close the linked items. -->\r\n<!-- Resolve --> #6152\r\n\r\n---\r\n\r\n### What changes are proposed in this pull request?\r\n\r\nThis PR adds an early validation step for folder-based datasets (like `audiofolder`) to prevent silent fallback behavior.\r\n\r\n**Before this fix**:\r\n- When `data_dir` or `data_files` were not provided, the loader defaulted to the current working directory.\r\n- This caused unexpected behavior like:\r\n  - Long loading times\r\n  - Scanning unintended local files\r\n\r\n**Now**:\r\n- If both `data_dir` and `data_files` are missing, a `ValueError` is raised early with a helpful message.\r\n\r\n---\r\n\r\n### How is this PR tested?\r\n\r\n- [x] Manual test via `load_dataset(\"audiofolder\")` with missing `data_dir`\r\n- [ ] Existing unit tests (should not break any)\r\n- [ ] New tests (if needed, maintainers can guide)\r\n\r\n---\r\n\r\n### Does this PR require documentation update?\r\n\r\n- [x] No. You can skip the rest of this section.\r\n\r\n---\r\n\r\n### Release Notes\r\n\r\n#### Is this a user-facing change?\r\n\r\n- [x] Yes. Give a description of this change to be included in the release notes for users.\r\n\r\n> Adds early error handling for folder-based datasets when neither `data_dir` nor `data_files` is specified, avoiding unintended resolution to the current directory.\r\n\r\n#### What component(s), interfaces, languages, and integrations does this PR affect?\r\n\r\nComponents:\r\n- [x] `area/datasets`\r\n- [x] `area/load`\r\n\r\n---\r\n\r\n<a name=\"release-note-category\"></a>\r\n\r\n#### How should the PR be classified in the release notes? Choose one:\r\n\r\n- [x] `rn/bug-fix` - A user-facing bug fix worth mentioning in the release notes\r\n\r\n---\r\n\r\n#### Should this PR be included in the next patch release?\r\n\r\n- [x] Yes (this PR will be cherry-picked and included in the next patch release)\r\n<end_of_turn>\n<start_of_turn>model\nfix: raise error when folder-based datasets are loaded without data_dir or data_files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI am using PyTorch DDP (Distributed Data Parallel) to train my model. Since the data is too large to load into memory at once, I am using load_dataset to read the data as an iterable dataset. I have used datasets.distributed.split_dataset_by_node to distribute the dataset. However, I have noticed that this distribution results in different processes having different amounts of data to train on. As a result, when the earliest process finishes training and starts predicting on the test set, other processes are still training, causing the overall training speed to be very slow.\r\n\r\n### Steps to reproduce the bug\r\n```\r\ndef train(args, model, device, train_loader, optimizer, criterion, epoch, length):\r\n    model.train()\r\n    idx_length = 0\r\n    for batch_idx, data in enumerate(train_loader):\r\n        s_time = time.time()\r\n        X = data['X']\r\n        target = data['y'].reshape(-1, 28)\r\n        X, target = X.to(device), target.to(device)\r\n        optimizer.zero_grad()\r\n        output = model(X)\r\n        loss = criterion(output, target)\r\n        loss.backward()\r\n        optimizer.step()\r\n        idx_length += 1\r\n        if batch_idx % args.log_interval == 0:\r\n            # print('Train Epoch: {} Batch_idx: {} Process: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\r\n            #     epoch, batch_idx, torch.distributed.get_rank(), batch_idx * len(X), length / torch.distributed.get_world_size(),\r\n            #                                          100. * batch_idx * len(\r\n            #                                              X) * torch.distributed.get_world_size() / length, loss.item()))\r\n            print('Train Epoch: {} Batch_idx: {} Process: {} [{}/{} ({:.0f}%)]\\t'.format(\r\n                epoch, batch_idx, torch.distributed.get_rank(), batch_idx * len(X), length / torch.distributed.get_world_size(),\r\n                                                                100. * batch_idx * len(\r\n                                                                    X) * torch.distributed.get_world_size() / length))\r\n            if args.dry_run:\r\n                break\r\n    print('Process %s length: %s time: %s' % (torch.distributed.get_rank(), idx_length, datetime.datetime.now()))\r\n\r\ntrain_iterable_dataset = load_dataset(\"parquet\", data_files=data_files, split=\"train\", streaming=True)\r\ntest_iterable_dataset = load_dataset(\"parquet\", data_files=data_files, split=\"test\", streaming=True)\r\ntrain_iterable_dataset = train_iterable_dataset.map(process_fn)\r\ntest_iterable_dataset = test_iterable_dataset.map(process_fn)\r\ntrain_iterable_dataset = train_iterable_dataset.map(scale)\r\ntest_iterable_dataset = test_iterable_dataset.map(scale)\r\n\r\ntrain_iterable_dataset = datasets.distributed.split_dataset_by_node(train_iterable_dataset,\r\n                                                                    world_size=world_size, rank=local_rank).shuffle(seed=1234)\r\ntest_iterable_dataset = datasets.distributed.split_dataset_by_node(test_iterable_dataset,\r\n                                                                   world_size=world_size, rank=local_rank).shuffle(seed=1234)\r\nprint(torch.distributed.get_rank(), train_iterable_dataset.n_shards, test_iterable_dataset.n_shards)\r\n\r\ntrain_kwargs = {'batch_size': args.batch_size}\r\ntest_kwargs = {'batch_size': args.test_batch_size}\r\nif use_cuda:\r\n    cuda_kwargs = {'num_workers': 3,#ngpus_per_node,\r\n                   'pin_memory': True,\r\n                   'shuffle': False}\r\n    train_kwargs.update(cuda_kwargs)\r\n    test_kwargs.update(cuda_kwargs)\r\ntrain_loader = torch.utils.data.DataLoader(train_iterable_dataset, **train_kwargs,\r\n                                           # sampler=torch.utils.data.distributed.DistributedSampler(\r\n                                           #     train_iterable_dataset,\r\n                                           #     num_replicas=ngpus_per_node,\r\n                                           #     rank=0)\r\n                                           )\r\ntest_loader = torch.utils.data.DataLoader(test_iterable_dataset, **test_kwargs,\r\n                                          # sampler=torch.utils.data.distributed.DistributedSampler(\r\n                                          #     test_iterable_dataset,\r\n                                          #     num_replicas=ngpus_per_node,\r\n                                          #     rank=0)\r\n                                          )\r\nfor epoch in range(1, args.epochs + 1):\r\n    start_time = time.time()\r\n    train_iterable_dataset.set_epoch(epoch)\r\n    test_iterable_dataset.set_epoch(epoch)\r\n    train(args, model, device, train_loader, optimizer, criterion, epoch, train_len)\r\n    test(args, model, device, criterion2, test_loader)\r\n\r\n```\r\n\r\n\r\n\r\nAnd here\u2019s the part of output:\r\n```\r\nTrain Epoch: 1 Batch_idx: 5000 Process: 0 [320000/4710975.0 (7%)]\t\r\nTrain Epoch: 1 Batch_idx: 5000 Process: 1 [320000/4710975.0 (7%)]\t\r\nTrain Epoch: 1 Batch_idx: 5000 Process: 2 [320000/4710975.0 (7%)]\t\r\nTrain Epoch: 1 Batch_idx: 5862 Process: 3 Data_length: 12 coststime: 0.04095172882080078\r\nTrain Epoch: 1 Batch_idx: 5862 Process: 0 Data_length: 3 coststime: 0.0751960277557373\r\nTrain Epoch: 1 Batch_idx: 5867 Process: 3 Data_length: 49 coststime: 0.0032558441162109375\r\nTrain Epoch: 1 Batch_idx: 5872 Process: 1 Data_length: 2 coststime: 0.022842884063720703\r\nTrain Epoch: 1 Batch_idx: 5876 Process: 3 Data_length: 63 coststime: 0.002694845199584961\r\nProcess 3 length: 5877 time: 2023-11-17 17:03:26.582317\r\nTrain epoch 1 costTime: 241.72063446044922s . Process 3 Start to test.\r\n3 0 tensor(45508.8516, device='cuda:3')\r\n3 100 tensor(45309.0469, device='cuda:3')\r\n3 200 tensor(45675.3047, device='cuda:3')\r\n3 300 tensor(45263.0273, device='cuda:3')\r\nProcess 3 Reduce metrics.\r\nTrain Epoch: 2 Batch_idx: 0 Process: 3 [0/4710975.0 (0%)]\t\r\nTrain Epoch: 1 Batch_idx: 5882 Process: 1 Data_length: 63 coststime: 0.05185818672180176\r\nTrain Epoch: 1 Batch_idx: 5887 Process: 1 Data_length: 12 coststime: 0.006895303726196289\r\nProcess 1 length: 5888 time: 2023-11-17 17:20:48.578204\r\nTrain epoch 1 costTime: 1285.7279663085938s . Process 1 Start to test.\r\n1 0 tensor(45265.9141, device='cuda:1')\r\n```\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\nI'd like to know how to fix this problem.\r\n\r\n### Environment info\r\n```\r\ntorch==2.0\r\ndatasets==2.14.0\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nProblem in training iterable dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWe should not used Parquet exports if `revision` is passed.\r\n\r\nI think this is a regression.<end_of_turn>\n<start_of_turn>model\nParquet exports are used even if revision is passed<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI wrote the following script:\r\n\r\n```\r\nimport datasets\r\n\r\ndataset = datasets.load.load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:10000]\")\r\n```\r\n\r\nI ran it and spent 90 minutes downloading a 20GB file. Then I saw:\r\n\r\n```\r\nDownloading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20.3G/20.3G [1:30:29<00:00, 3.73MB/s]\r\nTraceback (most recent call last):\r\n  File \"/home/jack/Code/Projects/Transformers/Codebase/main.py\", line 5, in <module>\r\n    dataset = datasets.load.load_dataset(\"wikipedia\", \"20220301.en\", split=\"train[:10000]\")\r\n  File \"/home/jack/.local/lib/python3.10/site-packages/datasets/load.py\", line 1782, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/jack/.local/lib/python3.10/site-packages/datasets/builder.py\", line 883, in download_and_prepare\r\n    self._save_info()\r\n  File \"/home/jack/.local/lib/python3.10/site-packages/datasets/builder.py\", line 2037, in _save_info\r\n    import apache_beam as beam\r\nModuleNotFoundError: No module named 'apache_beam'\r\n```\r\n\r\nAnd the 20GB of data was seemingly instantly gone forever, because when I ran the script again, it had to do the download again.\n\n### Steps to reproduce the bug\n\nSee above\n\n### Expected behavior\n\nSee above\n\n### Environment info\n\ndatasets 2.10.1\r\nPython 3.10<end_of_turn>\n<start_of_turn>model\nHuggingFace does not cache downloaded files aggressively/early enough<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```python\r\ndef get_dataset(data_path, train_folder=\"train\", val_folder=\"val\"):\r\n    traindir = os.path.join(data_path, train_folder)\r\n    valdir = os.path.join(data_path, val_folder)\r\n\r\n    def transform_val_examples(examples):\r\n        transform = Compose([\r\n            Resize(256),\r\n            CenterCrop(224),\r\n            ToTensor(),\r\n        ])\r\n        examples[\"image\"] = [transform(image.convert(\"RGB\")) for image in examples[\"image\"]]\r\n        return examples\r\n\r\n    def transform_train_examples(examples):\r\n        transform = Compose([\r\n            RandomResizedCrop(224),\r\n            RandomHorizontalFlip(),\r\n            ToTensor(),\r\n        ])\r\n        examples[\"image\"] = [transform(image.convert(\"RGB\")) for image in examples[\"image\"]]\r\n        return examples\r\n\r\n    # @fengsicheng: This way is very slow for big dataset like ImageNet-1K (but can pass the network problem using local dataset)\r\n    # train_set = load_dataset(\"imagefolder\", data_dir=traindir, num_proc=4)\r\n    # test_set = load_dataset(\"imagefolder\", data_dir=valdir, num_proc=4)\r\n\r\n    train_set = load_dataset(\"imagenet-1K\", split=\"train\", trust_remote_code=True)                                                                                                                                                                                                            \r\n    test_set = load_dataset(\"imagenet-1K\", split=\"test\", trust_remote_code=True)\r\n\r\n    print(train_set[\"label\"])\r\n\r\n    train_set.set_transform(transform_train_examples)\r\n    test_set.set_transform(transform_val_examples)\r\n\r\n    return train_set, test_set\r\n```\r\n    above the code, but output of the print is a list of None:\r\n    \r\n<img width=\"952\" alt=\"image\" src=\"https://github.com/user-attachments/assets/c4e2fdd8-3b8f-481e-8f86-9bbeb49d79fb\">\r\n\n\n### Steps to reproduce the bug\n\n1. just ran the code \r\n2. see the print\r\n\n\n### Expected behavior\n\nI do not know how to fix this, can anyone provide help or something? It is hurry for me\n\n### Environment info\n\n- `datasets` version: 2.21.0\r\n- Platform: Linux-5.4.0-190-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.24.6\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nUse load_dataset to load imagenet-1K But find a empty dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nSay I have the following code:\r\n\r\n```\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nnew_data = {\r\n    \"column_1\": [\"value1\", \"value2\"],\r\n    \"column_2\": [\"value3\", \"value4\"],\r\n}\r\ndf_new = pd.DataFrame(new_data)\r\ndataset_new = Dataset.from_pandas(df_new)\r\n# add these samples to a remote dataset\r\n```\r\nIt would be great to have a way to push dataset_new to a remote dataset that respects the same schema. This way one would not have to do the following:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('username/dataset_name', use_auth_token='your_hf_token_here')\r\nupdated_dataset = dataset['train'].concatenate(dataset_new)\r\nupdated_dataset.push_to_hub('username/dataset_name', use_auth_token='your_hf_token_here')\r\n```\r\n\r\n\r\n\r\n\n\n### Motivation\n\nNo need to download the dataset.\n\n### Your contribution\n\nMaybe this feature already exists, didnt see it though. I do not have the expertise to do this.<end_of_turn>\n<start_of_turn>model\nPush samples to dataset on hub without having the dataset locally<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## What's in this PR?\r\n\r\nThis PR addresses some minor fixes and general improvements in the `to_tf_dataset` method of `datasets.Dataset`, to convert a \ud83e\udd17HuggingFace Dataset as a TensorFlow Dataset.\r\n\r\nThe main bug solved in this PR comes with the string-encoding, since for safety purposes the internal conversion of `numpy.arrays` when `dtype` is unicode/string, is to convert it into `numpy.bytes`, more information in the docstring of https://github.com/tensorflow/tensorflow/blob/388d952114e59a1aeda440ed4737b29f8b7c6e8a/tensorflow/python/ops/script_ops.py#L210. That's triggered when using `tensorflow.numpy_function` as it's applying another type cast besides the one that `datasets` does, so the casting is applied at least twice per entry/batch. So this means that the definition of the `numpy.unicode_` dtype when the data in the batch is a string, is ignored, and replaced by `numpy.bytes_`.\r\n\r\nBesides that, some other minor things have been fixed:\r\n\r\n* Made `batch_size` an optional parameter in `to_tf_dataset`\r\n* Map the `tensorflow` output dtypes just once, and not in every `tf.function` call during `map`\r\n* Keep `numpy` formatting in the `datasets.Dataset` if already formatted like it, no need to format it again as `numpy`\r\n* Docstring indentation in `dataset_to_tf` and `multiprocess_dataset_to_tf`\r\n\r\n## What's missing in this PR?\r\n\r\nI can include some integration tests if needed, to validate that `batch_size` is optional, and that the tensors in the TF-Dataset can be looped over with no issues as before.<end_of_turn>\n<start_of_turn>model\nFix string-encoding, make `batch_size` optional, and minor improvements in `Dataset.to_tf_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\nMap:   6%|\u2588\u2588\u2588\u2588\u2588\u2588                                                                                                   | 8000/138120 [19:27<5:16:36,  6.85 examples/s]\r\nTraceback (most recent call last):\r\n  File \"/workspace/diffusers/examples/controlnet/train_controlnet_sd3.py\", line 1416, in <module>\r\n    main(args)\r\n  File \"/workspace/diffusers/examples/controlnet/train_controlnet_sd3.py\", line 1132, in main\r\n    train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 560, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 3035, in map\r\n    for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 3461, in _map_single\r\n    writer.write_batch(batch)\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\", line 567, in write_batch\r\n    self.write_table(pa_table, writer_batch_size)\r\n  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_writer.py\", line 579, in write_table\r\n    pa_table = pa_table.combine_chunks()\r\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"pyarrow/table.pxi\", line 4387, in pyarrow.lib.Table.combine_chunks\r\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: offset overflow while concatenating arrays\r\nTraceback (most recent call last):\r\n  File \"/usr/local/bin/accelerate\", line 8, in <module>\r\n    sys.exit(main())\r\n             ^^^^^^\r\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 48, in main\r\n    args.func(args)\r\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1174, in launch_command\r\n    simple_launcher(args)\r\n  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 769, in simple_launcher\r\n  ```\n\n### Steps to reproduce the bug\n\nThe dataset has no problem training on sd1.5 controlnet train script\r\n\r\n\n\n### Expected behavior\n\nScript not randomly erroing with error above\n\n### Environment info\n\n- `datasets` version: 3.0.0\r\n- Platform: Linux-6.5.0-44-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.9\r\n- `huggingface_hub` version: 0.25.1\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.6.1\r\n\r\n\r\ntraining on A100<end_of_turn>\n<start_of_turn>model\nError Mapping on sd3, sdxl and upcoming flux controlnet training scripts in diffusers<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI wanted to be able to save a HF dataset for translations and load it again in another script, but I'm a bit confused with the documentation and the result I've got so I'm opening this issue to ask if this behavior is as expected.\n\n### Steps to reproduce the bug\n\nThe documentation I've mainly consulted is https://huggingface.co/docs/datasets/v2.16.1/en/package_reference/loading_methods#datasets.load_dataset and https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset (where I've found `.to_csv()`)\r\n\r\n```python\r\n# Load a dataset of translations\r\ntest_dataset = load_dataset(\"opus100\", name=\"en-fr\", split=\"test\")\r\n\r\n# Save with .to_csv()\r\ntest_csv_path = \"try_testset_save.csv\"\r\ntest_dataset.to_csv(test_csv_path)\r\n\r\n# Load dataset from the CSV\r\nloaded_dataset = load_dataset(\"csv\", data_files=test_csv_path)\r\nprint(test_dataset_fromfile[0][\"translation\"])\r\nprint(test_dataset_fromfile[0][\"translation\"][\"en\"])\r\n```\r\n```\r\nCreating CSV from Arrow format: 100%\r\n2/2 [00:00<00:00, 47.99ba/s]\r\nDownloading data files: 100%\r\n1/1 [00:00<00:00, 65.33it/s]\r\nExtracting data files: 100%\r\n1/1 [00:00<00:00, 42.10it/s]\r\nGenerating train split:\r\n2000/0 [00:00<00:00, 47486.09 examples/s]\r\n\r\n{'en': \"She wasn't going to vaccinate her kid against polio,  no way.\", 'fr': 'Elle ne vaccinerait pas son enfant contre la polio. Pas question.'}\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[29], line 11\r\n      9 loaded_dataset = load_dataset(\"csv\", data_files=test_csv_path)\r\n     10 print(test_dataset_fromfile[0][\"translation\"])\r\n---> 11 print(test_dataset_fromfile[0][\"translation\"][\"en\"])\r\n\r\nTypeError: string indices must be integers, not 'str'\r\n```\r\n\n\n### Expected behavior\n\nEach translation was saved as a stringified dict like `\"{'en': \"\"She wasn't going to vaccinate her kid against polio,  no way.\"\", 'fr': 'Elle ne vaccinerait pas son enfant contre la polio. Pas question.'}\"` where I would have expected 2 columns (1st with English segments, and 2nd with French segments), and I was expecting `load_dataset` to infer the type of feature automatically as I haven't seen anything about it in the documentation.\r\n\r\nDo you have an example of how to effectively save and load datasets of translations ?\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-3.10.0-1160.36.2.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.16.4\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nLoading CSV exported dataset has unexpected format<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nMake dataset optionaly generic to datasets usage with type annotations like it was done in `torch.Dataloader` https://github.com/pytorch/pytorch/blob/134179474539648ba7dee1317959529fbd0e7f89/torch/utils/data/dataloader.py#L131\n\n### Motivation\n\nIn MTEB we're using a lot of datasets objects, but they're a bit poor in typehints. E.g. we can specify this for dataloder\n\n```python\nfrom typing import TypedDict\nfrom torch.utils.data import DataLoader\n\nclass CorpusInput(TypedDict):\n    title: list[str]\n    body: list[str]\n\n\nclass QueryInput(TypedDict):\n    query: list[str]\n    instruction: list[str]\n\ndef queries_loader() -> DataLoader[QueryInput]:\n    ...\n\ndef corpus_loader() -> DataLoader[CorpusInput]:\n   ...\n```\nBut for datasets we can only specify columns in type in comments \n```python\nfrom datasets import Dataset\n\nQueryDataset = Dataset\n\"\"\"Query dataset should have `query` and `instructions` columns as `str` \"\"\"\n```\n\n### Your contribution\n\nI can create draft implementation<end_of_turn>\n<start_of_turn>model\nSpecify dataset columns types in typehint<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAs already posted in https://github.com/microsoft/pylance-release/issues/6534, the correct type hinting fails when building a dataset using the `from_parquet` constructor.\r\nTheir suggestion is to comprehensively annotate the method's return type to better align with the docstring information.\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset\r\n\r\ndataset = Dataset.from_parquet(path_or_paths=\"file\")\r\ndataset.map(lambda x: {\"new\": x[\"old\"]}, batched=True)\r\n```\n\n### Expected behavior\n\nmap is a [valid](https://huggingface.co/docs/datasets/v3.0.1/en/package_reference/main_classes#datasets.Dataset.map), no error should be thrown.\n\n### Environment info\n\n- `datasets` version: 3.0.1\r\n- Platform: macOS-15.0.1-arm64-arm-64bit\r\n- Python version: 3.12.6\r\n- `huggingface_hub` version: 0.25.1\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\n`from_parquet` return type annotation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nArrowExamplesIterable.shard_data_sources was outdated\r\n\r\nI also fixed a warning message by not using format_type= in with_format()<end_of_turn>\n<start_of_turn>model\nFix ArrowExamplesIterable.shard_data_sources<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n              > @Frankie123421 what was the resolution to this?\r\n\r\nuse glue_metric.py instead of glue.py in load_metric\r\n\r\n_Originally posted by @Frankie123421 in https://github.com/huggingface/datasets/issues/2117#issuecomment-905093763_\r\n            <end_of_turn>\n<start_of_turn>model\nwhere is glue_metric.py> @Frankie123421 what was the resolution to this?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReplace the `shape` tuple with a list in the `ArrayXD` YAML conversion. \r\n\r\nFix #6112 <end_of_turn>\n<start_of_turn>model\nFix ArrayXD YAML conversion<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading https://huggingface.co/datasets/laion/filtered-wit in streaming mode fails after update to `datasets==3.4.0`. The dataset loads fine on v3.3.2.\n\n### Steps to reproduce the bug\n\nSteps to reproduce:\n```\npip install datastes==3.4.0\npython -c \"from datasets import load_dataset; load_dataset('laion/filtered-wit', split='train', streaming=True)\"\n```\n\nResults in:\n```\n$ python -c \"from datasets import load_dataset; load_dataset('laion/filtered-wit', split='train', streaming=True)\"\nRepo card metadata block was not found. Setting CardData to empty.\nResolving data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 560/560 [00:00<00:00, 2280.24it/s]\nTraceback (most recent call last):\n  File \"<string>\", line 1, in <module>\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/load.py\", line 2080, in load_dataset\n    return builder_instance.as_streaming_dataset(split=split)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/builder.py\", line 1265, in as_streaming_dataset\n    splits_generators = {sg.name: sg for sg in self._split_generators(dl_manager)}\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 49, in _split_generators\n    data_files = dl_manager.download_and_extract(self.config.data_files)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 169, in download_and_extract\n    return self.extract(self.download(url_or_urls))\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 121, in extract\n    urlpaths = map_nested(self._extract, url_or_urls, map_tuple=True)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 496, in map_nested\n    mapped = [\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 497, in <listcomp>\n    map_nested(\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 513, in map_nested\n    mapped = [\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 514, in <listcomp>\n    _single_map_nested((function, obj, batched, batch_size, types, None, True, None))\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 375, in _single_map_nested\n    return function(data_struct)\n  File \"/home/nsavel/venvs/tmp/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 131, in _extract\n    raise NotImplementedError(\nNotImplementedError: Extraction protocol for TAR archives like 'hf://datasets/laion/filtered-wit@c38ca7464e9934d9a49f88b3f60f5ad63b245465/data/00000.tar' is not implemented in streaming mode. Please use `dl_manager.iter_archive` instead.\n\nExample usage:\n\n        url = dl_manager.download(url)\n        tar_archive_iterator = dl_manager.iter_archive(url)\n\n        for filename, file in tar_archive_iterator:\n                ...\n```\n\n### Expected behavior\n\nDataset loads successfully.\n\n### Environment info\n\nUbuntu 20.04.6. Python 3.9. Datasets 3.4.0.\n\npip freeze:\n```\naiohappyeyeballs==2.6.1\naiohttp==3.11.14\naiosignal==1.3.2\nasync-timeout==5.0.1\nattrs==25.3.0\ncertifi==2025.1.31\ncharset-normalizer==3.4.1\ndatasets==3.4.0\ndill==0.3.8\nfilelock==3.18.0\nfrozenlist==1.5.0\nfsspec==2024.12.0\nhuggingface-hub==0.29.3\nidna==3.10\nmultidict==6.1.0\nmultiprocess==0.70.16\nnumpy==2.0.2\npackaging==24.2\npandas==2.2.3\npropcache==0.3.0\npyarrow==19.0.1\npython-dateutil==2.9.0.post0\npytz==2025.1\nPyYAML==6.0.2\nrequests==2.32.3\nsix==1.17.0\ntqdm==4.67.1\ntyping_extensions==4.12.2\ntzdata==2025.1\nurllib3==2.3.0\nxxhash==3.5.0\nyarl==1.18.3\n```<end_of_turn>\n<start_of_turn>model\nLoading the `laion/filtered-wit` dataset in streaming mode fails on v3.4.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nJSON lines with missing columns raise CastError:\r\n> CastError: Couldn't cast ... to ... because column names don't match\r\n\r\nRelated to:\r\n- #7159\r\n- #7161<end_of_turn>\n<start_of_turn>model\nJSON lines with missing columns raise CastError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen creating a dataset from a list of datapoints, information is lost of the individual items.\r\n\r\nSpecifically, when creating a dataset from a list of datapoints (from another dataset). Either the datatype is lost or the values are lost. See examples below. \r\n\r\n-> What is the best way to create a dataset from a list of datapoints?\r\n\r\n---\r\ne.g.:\r\n**When running this code:**\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\ncommonvoice_data = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"it\", split=\"test\", streaming=True)\r\ndatapoint = next(iter(commonvoice_data))\r\nout = [datapoint]\r\nnew_data = Dataset.from_list(out) #this loses datatype information\r\nnew_data2= Dataset.from_list(out,features=commonvoice_data.features) #this loses value information\r\n```\r\n\r\n**We get the following**:\r\n---\r\n1. `datapoint`: (the original datapoint)\r\n```\r\n'audio': {'path': 'it_test_0/common_voice_it_23606167.mp3', 'array': array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\r\n       2.21619011e-05, 2.72628222e-05, 0.00000000e+00]), 'sampling_rate': 48000}\r\n ```\r\nOriginal Dataset Features:\r\n```\r\n>>> commonvoice_data.features\r\n'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None)\r\n```\r\n - Here we see column \"audio\", has the proper values (both `path` & and `array`) and has the correct datatype (Audio).\r\n\r\n \r\n ----\r\n 2. new_data[0]:\r\n```\r\n# Cannot be printed (as it prints the entire array).\r\n```\r\nNew Dataset 1 Features:\r\n```\r\n>>> new_data.features\r\n'audio': {'array': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None), 'path': Value(dtype='string', id=None), 'sampling_rate': Value(dtype='int64', id=None)}\r\n```\r\n - Here we see that the column \"audio\", has the correct values, but is not the Audio datatype anymore.\r\n\r\n---\r\n3. new_data2[0]:\r\n```\r\n'audio': {'path': None, 'array': array([0., 0., 0., ..., 0., 0., 0.]), 'sampling_rate': 48000},\r\n```\r\nNew Dataset 2 Features:\r\n```\r\n>>> new_data2.features\r\n'audio': Audio(sampling_rate=48000, mono=True, decode=True, id=None),\r\n```\r\n - Here we see that the column \"audio\", has the correct datatype, but all the array & path values were lost!\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n## Run:\r\n```python\r\nfrom datasets import load_dataset, Dataset\r\ncommonvoice_data = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"it\", split=\"test\", streaming=True)\r\ndatapoint = next(iter(commonvoice_data))\r\nout = [datapoint]\r\nnew_data = Dataset.from_list(out) #this loses datatype information\r\nnew_data2= Dataset.from_list(out,features=commonvoice_data.features) #this loses value information\r\n```\r\n\r\n\r\n\r\n\r\n\r\n### Expected behavior\r\n\r\n## Expected:\r\n```datapoint == new_data[0]```\r\n\r\nAND\r\n\r\n```datapoint == new_data2[0]```\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.1.0\r\n- Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.26.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nCreating new dataset from list loses information. (Audio Information Lost - either Datatype or Values).<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease 2.14.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nOn [this](https://huggingface.co/docs/datasets/create_dataset) page, the example code for loading in images and audio is incorrect.\r\n\r\nCurrently, examples are:\r\n\r\n``` python\r\nfrom datasets import ImageFolder\r\ndataset = load_dataset(\"imagefolder\", data_dir=\"/path/to/pokemon\")\r\n```\r\nand \r\n``` python\r\nfrom datasets import AudioFolder\r\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\r\n```\r\n\r\nI'm pretty sure the imports are wrong and should be:\r\n\r\n``` python \r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"audiofolder\", data_dir=\"/path/to/folder\")\r\n```\r\n\r\nI am happy to update this if this is right but just wanted to check before making any changes.\n\n### Steps to reproduce the bug\n\nGo to https://huggingface.co/docs/datasets/create_dataset\n\n### Expected behavior\n\nN/A\n\n### Environment info\n\nN/A<end_of_turn>\n<start_of_turn>model\nIncorrect example code in 'Create a dataset' docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://huggingface.co/datasets/Omega02gdfdd/bioclip-demo-zero-shot-mistakes for example. The error is:\r\n\r\n```\r\nFileNotFoundError: Couldn't find a dataset script at /src/services/worker/Omega02gdfdd/bioclip-demo-zero-shot-mistakes/bioclip-demo-zero-shot-mistakes.py or any data file in the same directory. Couldn't find 'Omega02gdfdd/bioclip-demo-zero-shot-mistakes' on the Hugging Face Hub either: FileNotFoundError: Unable to find 'hf://datasets/Omega02gdfdd/bioclip-demo-zero-shot-mistakes@12b0313ba4c3189ee5a24cb76200959e9bf7492e/data.csv' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.geoparquet', '.gpq', '.arrow', '.txt', '.tar', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']\r\n```\r\n\r\nThe issue there is that a `configs` parameter is set in the README, while the mentioned data file (`data.csv`) does not exist.<end_of_turn>\n<start_of_turn>model\ndon't mention the script if trust_remote_code=False<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nxlistdir return name is empty string \r\nOverloaded os.listdir\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets.download.streaming_download_manager import xjoin\r\nfrom datasets.download.streaming_download_manager import xlistdir\r\nconfig = DownloadConfig(storage_options=options)\r\nmanger = StreamingDownloadManager(\"ILSVRC2012\",download_config=config)\r\ninput_path = \"lakefs://datalab/main/imagenet/ILSVRC2012.zip\"\r\n\r\ndownload_files = manger.download_and_extract(input_path)\r\n\r\ncurrent_dir = xjoin(download_files,\"ILSVRC2012/Images/ILSVRC2012_img_train\")\r\nfolder_list = xlistdir(current_dir)\r\n```\r\n\r\nin xlistdir function\r\n\r\nObj [\"name\"] ends with \"/\"\r\n\r\nlast return \"\"\n\n### Expected behavior\n\nObj [\"name\"] ends with \"/\" \r\nreturn folder name\n\n### Environment info\n\nno<end_of_turn>\n<start_of_turn>model\nfix os.listdir return name is empty string<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nTypical torchvision / torch Datasets in image applications apply color conversion in the Dataset portion of the code as part of image decode, separately from the image transform stack. This is true for PIL.Image where convert is usually called in dataset, for native torchvision https://pytorch.org/vision/main/generated/torchvision.io.decode_jpeg.html, and similarly in tensorflow.data pipelines decode_jpeg or https://www.tensorflow.org/api_docs/python/tf/io/decode_and_crop_jpeg have a channels arg that allows controlling the image mode in the decode step.\r\n\r\ndatasets currently requires this pattern (from [examples](https://huggingface.co/docs/datasets/main/en/image_process)):\r\n```\r\nfrom torchvision.transforms import Compose, ColorJitter, ToTensor\r\n\r\njitter = Compose(\r\n    [\r\n         ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.7),\r\n         ToTensor(),\r\n    ]\r\n)\r\n\r\ndef transforms(examples):\r\n    examples[\"pixel_values\"] = [jitter(image.convert(\"RGB\")) for image in examples[\"image\"]]\r\n    return examples\r\n```\r\n\r\n\r\n\r\n\r\n### Motivation\r\n\r\nIt would be nice to be able to handle `image.convert(\"RGB\")` (or other modes) in the decode step, before applying torchvision transforms, this would reduce differences in code when handling pipelines that can handle torchvision, webdatset, or hf datasets with fewer code differences and without needing to handle image mode argument passing in two different stages of the pipelines...\r\n\r\n### Your contribution\r\n\r\nCan do a PR with guidance on how mode should be passed / set on the dataset.<end_of_turn>\n<start_of_turn>model\nAllow image model (color conversion) to be specified as part of datasets Image() decode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis line prints user token on command line! Is it safe?\r\n\r\nhttps://github.com/huggingface/datasets/blob/12ebe695b4748c5a26e08b44ed51955f74f5801d/src/datasets/load.py#L2091<end_of_turn>\n<start_of_turn>model\nUser token is printed out!<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix style with ruff 0.0.278<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nds = load_dataset(\r\n    \"./xxx.py\",\r\n    name=\"default\",\r\n    split=\"train\",\r\n)\r\n\r\nThe datasets does not support debugging locally anymore...\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\r\n    \"./repo.py\",\r\n    name=\"default\",\r\n    split=\"train\",\r\n)\r\n\r\nfor item in ds:\r\n    print(item)\r\n```\r\n\r\nIt works fine for \"username/repo\", but it does not work for \"./repo.py\" when debugging locally...\r\nRunning above code template will report TypeError: Value.__init__() missing 1 required positional argument: 'dtype'\n\n### Expected behavior\n\nfix this bug\n\n### Environment info\n\npython 3.10 datasets==2.21<end_of_turn>\n<start_of_turn>model\nTypeError: Value.__init__() missing 1 required positional argument: 'dtype'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport jax 0.4.27 in CI tests by using jax Array `devices` method instead of `device` (which no longer exists).\r\n\r\nFix #6884.<end_of_turn>\n<start_of_turn>model\nSupport jax 0.4.27 in CI tests<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Description\n\nI encountered an inconsistency in behavior between `load_dataset` and `load_from_disk` when loading sharded datasets. Here is a minimal example to reproduce the issue:\n\n#### Code 1: Using `load_dataset`\n```python\nfrom datasets import Dataset, load_dataset\n\n# First save with max_shard_size=10\nDataset.from_dict({\"id\": range(1000)}).train_test_split(test_size=0.1).save_to_disk(\"my_sharded_datasetdict\", max_shard_size=10)\n\n# Second save with max_shard_size=10\nDataset.from_dict({\"id\": range(500)}).train_test_split(test_size=0.1).save_to_disk(\"my_sharded_datasetdict\", max_shard_size=10)\n\n# Load the DatasetDict\nloaded_datasetdict = load_dataset(\"my_sharded_datasetdict\")\nprint(loaded_datasetdict)\n```\n**Output**:\n- `train` has 1350 samples.\n- `test` has 150 samples.\n\n#### Code 2: Using `load_from_disk`\n```python\nfrom datasets import Dataset, load_from_disk\n\n# First save with max_shard_size=10\nDataset.from_dict({\"id\": range(1000)}).train_test_split(test_size=0.1).save_to_disk(\"my_sharded_datasetdict\", max_shard_size=10)\n\n# Second save with max_shard_size=10\nDataset.from_dict({\"id\": range(500)}).train_test_split(test_size=0.1).save_to_disk(\"my_sharded_datasetdict\", max_shard_size=10)\n\n# Load the DatasetDict\nloaded_datasetdict = load_from_disk(\"my_sharded_datasetdict\")\nprint(loaded_datasetdict)\n```\n**Output**:\n- `train` has 450 samples.\n- `test` has 50 samples.\n\n### Expected Behavior\nI expected both `load_dataset` and `load_from_disk` to load the same dataset, as they are pointing to the same directory. However, the results differ significantly:\n- `load_dataset` seems to merge all shards, resulting in a combined dataset.\n- `load_from_disk` only loads the last saved dataset, ignoring previous shards.\n\n### Questions\n1. Is this behavior intentional? If so, could you clarify the difference between `load_dataset` and `load_from_disk` in the documentation?\n2. If this is not intentional, could this be considered a bug?\n3. What is the recommended way to handle cases where multiple datasets are saved to the same directory?\n\n\nThank you for your time and effort in maintaining this great library! I look forward to your feedback.<end_of_turn>\n<start_of_turn>model\nInconsistent Behavior Between `load_dataset` and `load_from_disk` When Loading Sharded Datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix #6299\r\nfix #6202<end_of_turn>\n<start_of_turn>model\nUnpin `jax` maximum version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I create a dataset, it gets stuck while generating cached data. \r\nThe image format is PNG, and it will not get stuck when the image format is jpeg.\r\n\r\n![image](https://github.com/huggingface/datasets/assets/22740819/3b888fd8-e6d6-488f-b828-95a8f206a152)\r\n\r\nAfter debugging, I know that it is because of the `pa.array` operation in [arrow_writer](https://github.com/huggingface/datasets/blob/2.13.0/src/datasets/arrow_writer.py#L553), but i don't why.\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import Dataset\r\n\r\ndef generator(lines):\r\n    for line in lines:\r\n        img = Image.open(open(line[\"url\"], \"rb\"))\r\n        # print(img.format)  # \"PNG\"\r\n        yield {\r\n            \"image\": img,\r\n        }\r\n\r\nlines = open(dataset_path, \"r\")\r\ndataset = Dataset.from_generator(\r\n    generator,\r\n    gen_kwargs={\"lines\": lines}\r\n)\r\n```\n\n### Expected behavior\n\nGenerating split done.\n\n### Environment info\n\ndatasets    2.13.0<end_of_turn>\n<start_of_turn>model\n Generating split is very slow when Image format is PNG<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRefers to [this issue](https://github.com/huggingface/datasets/issues/7381).<end_of_turn>\n<start_of_turn>model\nImplementation of iteration over values of a column in an IterableDataset object<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI did `from datasets import load_dataset, load_metric` and then `ds = load_dataset(\"imdb\")` and it gave me the error:\r\nExpectedMoreDownloadedFiles: {'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'}\r\n\r\nI tried doing `ds = load_dataset(\"imdb\",download_mode=\"force_redownload\")` as well as reinstalling dataset. I still face this problem.\n\n### Steps to reproduce the bug\n\n1. from datasets import load_dataset, load_metric\r\n2. ds = load_dataset(\"imdb\")\n\n### Expected behavior\n\nIt should load and give me this when I run `ds`\r\n\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 25000\r\n    })\r\n    test: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 25000\r\n    })\r\n    unsupervised: Dataset({\r\n        features: ['text', 'label'],\r\n        num_rows: 50000\r\n    })\r\n})\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.4.0-164-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.18\r\n- Huggingface_hub version: 0.16.2\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.2<end_of_turn>\n<start_of_turn>model\nCan't load ds = load_dataset(\"imdb\")<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDataset https://huggingface.co/datasets/blog_authorship_corpus has an issue with its hosting platform, since https://drive.google.com/u/0/uc?id=1cGy4RNDV87ZHEXbiozABr9gsSrZpPaPz&export=download returns 404 error.\r\n\r\nBut when trying to generate the split names, we get an exception which is now correctly caught.\r\n\r\nSeen originally in https://github.com/huggingface/datasets-server/blob/adbdcd6710ffed4e2eb2e4cd905b5e0dff530a15/services/worker/src/worker/job_runners/config/parquet_and_info.py#L435\n\n### Steps to reproduce the bug\n\n```python\r\n>>> from datasets import StreamingDownloadManager, load_dataset_builder\r\n>>> builder = load_dataset_builder(path=\"blog_authorship_corpus\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5.60k/5.60k [00:00<00:00, 23.1MB/s]\r\nDownloading metadata: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2.81k/2.81k [00:00<00:00, 14.7MB/s]\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.30k/7.30k [00:00<00:00, 30.8MB/s]\r\n>>> dl_manager = StreamingDownloadManager(base_path=builder.base_path)\r\n>>> builder._split_generators(dl_manager)\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/slesage/.cache/huggingface/modules/datasets_modules/datasets/blog_authorship_corpus/6f5d78241afd8313111956f877a57db7a0e9fc6718255dc85df0928197feb683/blog_authorship_corpus.py\", line 79, in _split_generators\r\n    data = dl_manager.download_and_extract(_DATA_URL)\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 1087, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 1039, in extract\r\n    urlpaths = map_nested(self._extract, url_or_urls, map_tuple=True)\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 435, in map_nested\r\n    return function(data_struct)\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 1044, in _extract\r\n    protocol = _get_extraction_protocol(urlpath, use_auth_token=self.download_config.use_auth_token)\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/datasets/download/streaming_download_manager.py\", line 433, in _get_extraction_protocol\r\n    with fsspec.open(urlpath, **kwargs) as f:\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/fsspec/core.py\", line 439, in open\r\n    return open_files(\r\n  File \"/home/slesage/hf/datasets-server/services/worker/.venv/lib/python3.9/site-packages/fsspec/core.py\", line 194, in __getitem__\r\n    out = super().__getitem__(item)\r\nIndexError: list index out of range\r\n```\n\n### Expected behavior\n\nWe should have an Exception raised by the datasets library.\n\n### Environment info\n\n\r\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.19.0-1026-aws-x86_64-with-glibc2.35\r\n- Python version: 3.9.15\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.2<end_of_turn>\n<start_of_turn>model\nUncaught exception when generating the splits from a dataset that miss data<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nBasically https://github.com/huggingface/datasets/pull/7718 just for binary view instead of string view<end_of_turn>\n<start_of_turn>model\nAdd pyarrow's binary view to features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.13.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm struggling to get things to respect HF_DATASETS_CACHE.\n\nRationale: I'm on a system that uses NFS for homedir, so downloading to NFS is expensive, slow, and wastes valuable quota compared to local disk. Instead, it seems to rely mostly on HF_HUB_CACHE.\n\nCurrent version: 3.2.1dev. In the process of testing 3.4.0\n\n### Steps to reproduce the bug\n\n[Currently writing using datasets 3.2.1dev. Will follow up with 3.4.0 results]\n\ndump.py:\n```python\nfrom datasets import load_dataset\ndataset = load_dataset(\"HuggingFaceFW/fineweb\", name=\"sample-100BT\", split=\"train\")\n```\n\nRepro steps\n```bash\n# ensure no cache\n$ mv ~/.cache/huggingface ~/.cache/huggingface.bak\n\n$ export HF_DATASETS_CACHE=/tmp/roller/datasets\n$ rm -rf ${HF_DATASETS_CACHE}\n$ env | grep HF | grep -v TOKEN\nHF_DATASETS_CACHE=/tmp/roller/datasets\n\n$ python dump.py\n# (omitted for brevity)\n\n# (while downloading) \n$ du -hcs ~/.cache/huggingface/hub\n18G     hub\n18G     total\n\n# (after downloading)\n$ du -hcs ~/.cache/huggingface/hub\n```\n\nIt's a shame because datasets supports s3 (which I could really use right now) but hub does not.\n\n### Expected behavior\n\n* ~/.cache/huggingface/hub stays empty\n* /tmp/roller/datasets becomes full of stuff\n\n### Environment info\n\n[Currently writing using datasets 3.2.1dev. Will follow up with 3.4.0 results]<end_of_turn>\n<start_of_turn>model\nHF_DATASETS_CACHE ignored?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nCalling `add_faiss_index` on a `Dataset` with a column argument raises a ValueError. The following is the trace\r\n\r\n```python\r\n    214 def replacement_add(self, x):\r\n    215     \"\"\"Adds vectors to the index.\r\n    216     The index must be trained before vectors can be added to it.\r\n    217     The vectors are implicitly numbered in sequence. When `n` vectors are\r\n   (...)\r\n    224         `dtype` must be float32.\r\n    225     \"\"\"\r\n--> 227     n, d = x.shape\r\n    228     assert d == self.d\r\n    229     x = np.ascontiguousarray(x, dtype='float32')\r\n\r\nValueError: not enough values to unpack (expected 2, got 1)\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Load any dataset like `ds = datasets.load_dataset(\"wikimedia/wikipedia\", \"20231101.en\")[\"train\"]`\r\n2. Add an FAISS index on any column `ds.add_faiss_index('title')`\r\n\r\n### Expected behavior\r\n\r\nThe index should be created\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.18.0\r\n- Platform: Linux-6.5.0-26-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.19\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.2.0\r\n- `faiss-cpu` version: 1.8.0<end_of_turn>\n<start_of_turn>model\n`add_faiss_index` raises ValueError: not enough values to unpack (expected 2, got 1)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI would like to be able to concatenate multiple `IterableDataset` with possibly different features. I would like to then be able to stream the results in parallel (both using DDP and multiple workers in the pytorch DataLoader). I want the merge of datasets to be well balanced between the different processes.\n\n### Motivation\n\nI want to train a model on a combination of datasets, which I can convert to a single representation. This applies to converting different datasets items to the same Python class, as using a tokenizer on multiple modalities.\n\nAssuming that my original datasets are not necessarily well balanced as they may have different size and thus different number of shards, I would like the merged dataset to be distributed evenly over the multiple processes. I don't mind if it's not perfectly balanced, and as result, some workers of the torch DataLoader do nothing, as long as the DDP is properly handled causing no deadlock.\n\n### What I've tried\n\nI've tried the two functions already provided in datasets, namely `interleave_datasets` and `concatenate_datasets`. \n\n- Interleave seems to be the best approach of what I'm trying to do. However, it doesn't suit my purpose because as I understand it, it stops as soon as one of the dataset source is exhausted, or repeat the smallest source items until the largest is exhausted. I would like something in-between, similarly to what [roundrobin does](https://more-itertools.readthedocs.io/en/stable/api.html#more_itertools.roundrobin).\n- Concatenate does not mix the data enough and one dataset may be overrepresented in some early batches.\n\nLet's consider we have 3 datasets composed of different number of shards as follow [[s0_0, s0_1], [s1_0], [s2_0, s2_1, s2_3]], where s denotes the underlying shard, the first index the dataset and the second the shard number.\nIf we request 3 shards in the `shard_data_source` we should obtain the following:\nindex 0 gets s0_0 s2_0\nindex 1 gets s0_1 s2_1\nindex 2 gets s1_0 s2_3\n\nI started implementing the following, but I'm afraid my sharding logic is incorrect.\n\n```python\nfrom copy import deepcopy\nfrom itertools import chain, islice\n\nimport datasets\nimport numpy as np\nfrom datasets import IterableDataset\nfrom datasets.iterable_dataset import _BaseExamplesIterable\nfrom more_itertools import roundrobin\n\n\nclass MixMultiSourcesExampleIterable(_BaseExamplesIterable):\n    def __init__(self, ex_iterables: list[_BaseExamplesIterable]):\n        super().__init__()\n        self.ex_iterables = ex_iterables\n\n    def _init_state_dict(self) -> dict:\n        self._state_dict = {\n            \"ex_iterables\": [ex_iterable._init_state_dict() for ex_iterable in self.ex_iterables],\n            \"type\": self.__class__.__name__,\n        }\n        return self._state_dict\n\n    @property\n    def num_shards(self) -> int:\n        return sum(ex_iterable.num_shards for ex_iterable in self.ex_iterables)\n\n    def __iter__(self):\n        yield from roundrobin(*self.ex_iterables)\n\n    def shuffle_data_sources(self, generator: np.random.Generator) -> \"MixMultiSourcesExampleIterable\":\n        \"\"\"Shuffle the list of examples iterable, as well as each underlying examples iterable.\"\"\"\n        rng = deepcopy(generator)\n        ex_iterables = list(self.ex_iterables)\n        rng.shuffle(ex_iterables)\n        ex_iterables = [ex_iterable.shuffle_data_sources(generator) for ex_iterable in ex_iterables]\n        return MixMultiSourcesExampleIterable(ex_iterables)\n\n    def shard_data_sources(self, num_shards: int, index: int, contiguous=True) -> \"MixMultiSourceExampleIterable\":\n        \"\"\"Shard the underlying iterables in a roundrobin manner.\n\n        Let's consider we have our iterables as [[s0_0, s0_1], [s1_0], [s2_0, s2_1, s2_3]],\n        and we request 3 shards.\n        index 0 gets s0_0 s2_0\n        index 1 gets s0_1 s2_1\n        index 2 gets s1_0 s2_3\n        \"\"\"\n        return MixMultiSourcesExampleIterable(\n            list(\n                islice(\n                    # flatten all underlying iterables\n                    chain.from_iterable([ex_iterable.shard_data_sources(1, 0) for ex_iterable in self.ex_iterables]),\n                    # offset the starting point by the index\n                    index,\n                    # take over the full list, so exhaust the iterators\n                    None,\n                    # step by the number of shards requested\n                    num_shards,\n                )\n            )\n        )\n\n\ndef mix_dataset(iterable_datasets: list[datasets.IterableDataset]) -> IterableDataset:\n    ex_iterable = MixMultiSourcesExampleIterable([ds._ex_iterable for ds in iterable_datasets])\n    return IterableDataset(\n        ex_iterable, distributed=iterable_datasets[0]._distributed, formatting=iterable_datasets[0]._formatting\n    )\n\n```\n\n### Questions\n\n- Am I missing something? Is there a way to use `interleave_datasets` or `concatenate_datasets` to fit my purpose?\n- Would it be the right approach to spread the maximum number of underlying shards across my different processes?\n\n### Your contribution\n\nAs much as I can.<end_of_turn>\n<start_of_turn>model\nConcatenate IterableDataset instances and distribute underlying shards in a RoundRobin manner<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nto solve the issue in #5880 \r\n\r\n1.  add s3 support in the link check step, previous we only check `http` and `https`,\r\n\r\n2. change the parameter of `use_auth_token` to `download_config` to support both `storage_options` and `use_auth_token` parameter when trying to handle(list, open, read, etc,.) the remote files.\r\n\r\n3.  integrate the check part's duplicate code to make adding or deleting other sources easier.<end_of_turn>\n<start_of_turn>model\nadd support for storage_options for load_dataset API<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport pyarrow 14.0.0 by fixing the root cause of:\r\n- #6374\r\n\r\nand revert:\r\n- #6375<end_of_turn>\n<start_of_turn>model\nSupport pyarrow 14.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix save_infos<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to access your database through python using \"datasets.load_dataset(\"irc_disentangle\")\" and I am getting this error message:\r\n\r\nValueError: Instruction \"train\" corresponds to no data!\n\n### Steps to reproduce the bug\n\nimport datasets\r\nds = datasets.load_dataset('irc_disentangle')\r\nds\n\n### Expected behavior\n\nThe data is supposed to load into ds and be accessable as such:\r\nds['train'][1050], ds['train'][1055]\n\n### Environment info\n\nI tired Python 3.12 and 3.10<end_of_turn>\n<start_of_turn>model\nirc_disentangle - Issue with splitting data<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nAdd metadata ui screenshot in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi,\n\nI'm trying to load a dataset on Kaggle TPU image. There is some known compat issue with progress bar on Kaggle, so I'm trying to disable the progress bar globally. This does not work as you can see in [here](https://www.kaggle.com/code/windmaple/hf-datasets-issue).\n\nIn contract, disabling progress bar for snapshot_download() works as expected as in [here](https://www.kaggle.com/code/windmaple/snapshot-download-error).\n\n### Steps to reproduce the bug\n\nSee this [notebook](https://www.kaggle.com/code/windmaple/hf-datasets-issue). \n\nThere is sth. wrong with `shell_paraent`.\n\n### Expected behavior\n\nThe downloader should disable progress bar and move forward w/ no error.\n\n### Environment info\n\nThe latest version as I did:\n\n!pip install -U datasets ipywidgets ipykernel<end_of_turn>\n<start_of_turn>model\ndisable_progress_bar() not working as expected<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI need to process some data using the set_transform method but I also need the data to be formatted for pytorch before processing it.\r\n\r\nI don't see anywhere in the documentation something that says that both methods cannot be used at the same time.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"mnist\", split=\"train\")\r\nds.set_format(type=\"torch\")\r\ndef transform(entry):\r\n    return entry[\"image\"].double()\r\nds.set_transform(transform)\r\n\r\nprint(ds[0])\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt should print the pytorch tensor image as a double, but it errors because \"entry\" in the transform function doesn't receive a pytorch tensor to begin with, it receives a PIL Image -> entry.double() errors because entry isn't a pytorch tensor.\r\n\r\n### Environment info\r\nLatest versions.\r\n\r\n\r\n### Note:\r\nIt would be at least handy to have access to a function that can do the dataset.set_format in the set_transform function.\r\n\r\nSomething like:\r\n```\r\nfrom datasets import load_dataset, do_format\r\nds = load_dataset(\"mnist\", split=\"train\")\r\ndef transform(entry):\r\n    entry = do_format(entry, type=\"torch\")\r\n    return entry[\"image\"].double()\r\nds.set_transform(transform)\r\n\r\nprint(ds[0])\r\n```<end_of_turn>\n<start_of_turn>model\nCannot use both set_format and set_transform<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLike to #6172, `push_to_hub` will crash if Hub resets the connection and raise the following error:\r\n```\r\nPushing dataset shards to the dataset hub:  32%|\u2588\u2588\u2588\u258f      | 54/171 [06:38<14:23,  7.38s/it]\r\nTraceback (most recent call last):\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 715, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 467, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 462, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"/usr/lib/python3.8/http/client.py\", line 1348, in getresponse\r\n    response.begin()\r\n  File \"/usr/lib/python3.8/http/client.py\", line 316, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"/usr/lib/python3.8/http/client.py\", line 285, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nhttp.client.RemoteDisconnected: Remote end closed connection without response\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/requests/adapters.py\", line 486, in send\r\n    resp = conn.urlopen(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 799, in urlopen\r\n    retries = retries.increment(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/util/retry.py\", line 550, in increment\r\n    raise six.reraise(type(error), error, _stacktrace)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/packages/six.py\", line 769, in reraise\r\n    raise value.with_traceback(tb)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 715, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 467, in _make_request\r\n    six.raise_from(e, None)\r\n  File \"<string>\", line 3, in raise_from\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 462, in _make_request\r\n    httplib_response = conn.getresponse()\r\n  File \"/usr/lib/python3.8/http/client.py\", line 1348, in getresponse\r\n    response.begin()\r\n  File \"/usr/lib/python3.8/http/client.py\", line 316, in begin\r\n    version, status, reason = self._read_status()\r\n  File \"/usr/lib/python3.8/http/client.py\", line 285, in _read_status\r\n    raise RemoteDisconnected(\"Remote end closed connection without\"\r\nurllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/_commit_api.py\", line 383, in _wrapped_lfs_upload\r\n    lfs_upload(operation=operation, lfs_batch_action=batch_action, token=token)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/lfs.py\", line 223, in lfs_upload\r\n    _upload_multi_part(operation=operation, header=header, chunk_size=chunk_size, upload_url=upload_action[\"href\"])\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/lfs.py\", line 319, in _upload_multi_part\r\n    else _upload_parts_iteratively(operation=operation, sorted_parts_urls=sorted_parts_urls, chunk_size=chunk_size)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/lfs.py\", line 375, in _upload_parts_iteratively\r\n    part_upload_res = http_backoff(\"PUT\", part_upload_url, data=fileobj_slice)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/utils/_http.py\", line 258, in http_backoff\r\n    response = session.request(method=method, url=url, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/utils/_http.py\", line 63, in send\r\n    return super().send(request, *args, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/requests/adapters.py\", line 501, in send\r\n    raise ConnectionError(err, request=request)\r\nrequests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 2bab8c06-b701-4266-aead-fe2e0dc0e3ed)')\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"convert_to_hf.py\", line 116, in <module>\r\n    main()\r\n  File \"convert_to_hf.py\", line 108, in main\r\n    audio_dataset.push_to_hub(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 1641, in push_to_hub\r\n    repo_id, split, uploaded_size, dataset_nbytes, _, _ = self[split]._push_parquet_shards_to_hub(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 5308, in _push_parquet_shards_to_hub\r\n    _retry(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/datasets/utils/file_utils.py\", line 290, in _retry\r\n    return func(*func_args, **func_kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 3221, in upload_file\r\n    commit_info = self.create_commit(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 828, in _inner\r\n    return fn(self, *args, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 2695, in create_commit\r\n    upload_lfs_files(\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/_commit_api.py\", line 393, in upload_lfs_files\r\n    _wrapped_lfs_upload(filtered_actions[0])\r\n  File \"/admin/home-piraka9011/.virtualenvs/w2v2/lib/python3.8/site-packages/huggingface_hub/_commit_api.py\", line 385, in _wrapped_lfs_upload\r\n    raise RuntimeError(f\"Error while uploading '{operation.path_in_repo}' to the Hub.\") from exc\r\nRuntimeError: Error while uploading 'batch_19/train-00054-of-00171-932beb4082c034bf.parquet' to the Hub.\r\n```\r\n\r\nThe function should retry if the operations fails, or at least offer a way to recover after such a failure. \r\nRight now, calling the function again will start sending all the parquets files leading to duplicates in the repository, with no guarantee that it will actually  be pushed.\r\n\r\nPreviously, it would crash with an error 400 #4677 .\n\n### Steps to reproduce the bug\n\nAny large dataset pushed the hub:\r\n\r\n```py\r\naudio_dataset.push_to_hub(\r\n        repo_id=\"org/dataset\",\r\n    )\r\n```\n\n### Expected behavior\n\n`push_to_hub` should have an option for max retries or resume.\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.15.0-1044-aws-x86_64-with-glibc2.29\r\n- Python version: 3.8.10\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\n`push_to_hub` is not robust to hub closing connection<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix https://github.com/huggingface/datasets/issues/7085 as noted by @VeryLazyBoy and reported by @AjayP13<end_of_turn>\n<start_of_turn>model\nFix typed examples iterable state dict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am constructing a large time series dataset and observed that first constructing a list of entries and then using `Dataset.from_list` led to a crash as the number of items became large. However, this is not a problem when using `Dataset.from_generator`.\n\n### Steps to reproduce the bug\n\n#### Snippet A (crashes)\n\n```py\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport datasets\n\n\ndef data_generator():\n    for i in tqdm(range(10_000_000)):\n        length = np.random.randint(2048)\n        series = np.random.rand(length)\n        yield {\"target\": series, \"item_id\": str(i), \"start\": np.datetime64(\"2000\", \"ms\")}\n\ndata_list = list(data_generator())\nds = datasets.Dataset.from_list(data_list)\n```\nThe last line crashes with\n```\nArrowInvalid: Value 2147483761 too large to fit in C integer type\n```\n\n#### Snippet B (works)\n\n```py\nfrom tqdm.auto import tqdm\nimport numpy as np\nimport datasets\n\n\ndef data_generator():\n    for i in tqdm(range(10_000_000)):\n        length = np.random.randint(2048)\n        series = np.random.rand(length)\n        yield {\"target\": series, \"item_id\": str(i), \"start\": np.datetime64(\"2000\", \"ms\")}\n\nds = datasets.Dataset.from_generator(data_generator)\n```\n\n### Expected behavior\n\nI expected both the approaches to work or to fail similarly. \n\n### Environment info\n\n```\n- `datasets` version: 3.6.0\n- Platform: Linux-6.8.0-1029-aws-x86_64-with-glibc2.35\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.32.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0\n```<end_of_turn>\n<start_of_turn>model\n`from_list` fails while `from_generator` works for large datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRevert temporary pin huggingface-hub < 0.23.0 introduced by\r\n- #6861\r\n\r\nonce the following issue is fixed and released:\r\n- huggingface/transformers#30618<end_of_turn>\n<start_of_turn>model\nRevert temporary pin huggingface-hub < 0.23.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis fixes cases where keys have upper case identifiers<end_of_turn>\n<start_of_turn>model\nfix(webdataset): don't .lower() field_name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a dataset that consists of a bunch of text files, each representing an example. There is an undocumented `sample_by` argument for the `TextConfig` class that is used by `Text` to decide whether to split files into lines, paragraphs or take them whole. Passing `sample_by=\u201cdocument\u201d` to `load_dataset` results in files getting split into lines regardless. I have edited `src/datasets/packaged_modules/text/text.py` for myself to switch the default and it works fine.\r\n\r\nAs a side note, the `if-else` for `sample_by` will silently load an empty dataset if someone makes a typo in the argument, which is not ideal.\n\n### Steps to reproduce the bug\n\n1. Prepare data as a bunch of files in a directory.\r\n2. Load that data via `load_dataset(\u201ctext\u201d, data_files=<data_dir>/<files_glob>, \u2026, sample_by=\u201cdocument\u201d)`.\r\n3. Inspect the resultant dataset \u2014 every item should have the form of `{\u201ctext\u201d: <a line from a file>}`.\n\n### Expected behavior\n\n`load_dataset(\u201ctext\u201d, data_files=<data_dir>/<files_glob>, \u2026, sample_by=\u201cdocument\u201d)` should result in a dataset with items of the form `{\u201ctext\u201d: <one document>}`.\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.15.0-1046-nvidia-x86_64-with-glibc2.35\r\n- Python version: 3.11.8\r\n- `huggingface_hub` version: 0.21.4\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nPassing `sample_by` to `load_dataset` when loading text data does not work<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I use load_dataset to load the nyanko7/danbooru2023 data set, the cache is read in the form of a symlink. There may be a problem with the arrow_dataset initialization process and I get FileNotFoundError: [Errno 2] No such file or directory: '2945000.jpg'\n\n### Steps to reproduce the bug\n\n#code show as below\r\nfrom datasets import load_dataset\r\ndata = load_dataset(\"nyanko7/danbooru2023\",cache_dir=<symlink>)\r\ndata[\"train\"][0]\n\n### Expected behavior\n\nI should get this result:\r\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=365x256 at 0x7FB730CB4070>, 'label': 0}\n\n### Environment info\n\ndatasets==2.12.0\r\npython==3.10.14\r\n<end_of_turn>\n<start_of_turn>model\ngot fileNotFound<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHello,\r\n\r\nI'm new here and I need to concatenate the squad dataset with my own dataset i created. I find the following error when i try to do it: Traceback (most recent call last):\r\n\r\n  Cell In[9], line 1\r\n    concatenated_dataset = concatenate_datasets([train_dataset, dataset1])\r\n\r\n  File ~\\anaconda3\\Lib\\site-packages\\datasets\\combine.py:213 in concatenate_datasets\r\n    return _concatenate_map_style_datasets(dsets, info=info, split=split, axis=axis)\r\n\r\n  File ~\\anaconda3\\Lib\\site-packages\\datasets\\arrow_dataset.py:6002 in _concatenate_map_style_datasets\r\n    _check_if_features_can_be_aligned([dset.features for dset in dsets])\r\n\r\n  File ~\\anaconda3\\Lib\\site-packages\\datasets\\features\\features.py:2122 in _check_if_features_can_be_aligned\r\n    raise ValueError(\r\n\r\nValueError: The features can't be aligned because the key answers of features {'id': Value(dtype='string', id=None), 'title': Value(dtype='string', id=None), 'context': Value(dtype='string', id=None), 'question': Value(dtype='string', id=None), 'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None)} has unexpected type - Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None) (expected either {'answer_start': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'text': Value(dtype='string', id=None)} or Value(\"null\").\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\n\r\nfrom huggingface_hub import notebook_login\r\nfrom datasets import load_dataset\r\n\r\n\r\n\r\nnotebook_login(\"mymailadresse\", \"mypassword\")\r\nsquad = load_dataset(\"squad\", split=\"train[:5000]\")\r\nsquad = squad.train_test_split(test_size=0.2)\r\ndataset1 = squad[\"train\"]\r\n\r\n\r\n\r\n\r\nimport json\r\n\r\nmybase = [\r\n    {\r\n        \"id\": \"1\",\r\n        \"context\": \"She lives in Nantes\",\r\n        \"question\": \"Where does she live?\",\r\n        \"answers\": {\r\n            \"text\": \"Nantes\",\r\n            \"answer_start\": [13],\r\n        }\r\n    }\r\n]\r\n\r\n\r\n\r\n\r\n# Save the data to a JSON file\r\njson_file_path = r\"C:\\Users\\mypath\\thefile.json\"\r\nwith open(json_file_path, \"w\", encoding= \"utf-8\") as json_file:\r\n    json.dump(mybase, json_file, indent=4)\r\n\r\n\r\n\r\n\r\n# Load the JSON file as a dataset\r\ncustom_dataset = load_dataset(\"json\", data_files=json_file_path)\r\n\r\n\r\n# Access the train split\r\ntrain_dataset = custom_dataset[\"train\"]\r\n\r\n\r\nfrom datasets import concatenate_datasets\r\n\r\n\r\n# Concatenate the datasets\r\nconcatenated_dataset = concatenate_datasets([train_dataset, dataset1])\r\n```\r\n\r\n### Expected behavior\r\n\r\nI would expect the two datasets to be concatenated without error. The len(dataset1) is equal to 4000 and the len(train_dataset) is equal to 1 so I would exepect concatenated_dataset to be created and having lenght 4001.\r\n\r\n### Environment info\r\n\r\nPython 3.11.4 and using windows\r\n\r\nThank you for your help<end_of_turn>\n<start_of_turn>model\nGet an error when i try to concatenate the squad dataset with my own dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(also added the missing numpy docs and fixed a small bug in pyarrow formatting)<end_of_turn>\n<start_of_turn>model\nAdd Pandas, PyArrow and Polars docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nOnce root issue is fixed, remove temporary pin of fsspec < 2023.9.0 introduced by:\r\n- #6210\r\n\r\nRelated to issue:\r\n- #6209\r\n\r\nAfter investigation, I think the root issue is related to the new glob behavior with double asterisk `**` they have introduced in:\r\n- https://github.com/fsspec/filesystem_spec/pull/1329<end_of_turn>\n<start_of_turn>model\nUnpin fsspec < 2023.9.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n...now that `squad` and `paws` don't have a script anymore<end_of_turn>\n<start_of_turn>model\nFix tests based on datasets that used to have scripts<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI added scan options, useful to control buffering and caching when streaming and docs, including how to select a subset of columns and apply filters<end_of_turn>\n<start_of_turn>model\nAdd parquet scan options and docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhile trying to load the dataset `https://huggingface.co/datasets/pysentimiento/spanish-tweets-small`, I get this error:\r\n\r\n```python\r\n---------------------------------------------------------------------------\r\nNonMatchingSplitsSizesError               Traceback (most recent call last)\r\n[<ipython-input-1-d6a3c721d3b8>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 ds = load_dataset(\"pysentimiento/spanish-tweets-small\")\r\n\r\n3 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2150 \r\n   2151     # Download and prepare data\r\n-> 2152     builder_instance.download_and_prepare(\r\n   2153         download_config=download_config,\r\n   2154         download_mode=download_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    946                         if num_proc is not None:\r\n    947                             prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 948                         self._download_and_prepare(\r\n    949                             dl_manager=dl_manager,\r\n    950                             verification_mode=verification_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1059 \r\n   1060         if verification_mode == VerificationMode.BASIC_CHECKS or verification_mode == VerificationMode.ALL_CHECKS:\r\n-> 1061             verify_splits(self.info.splits, split_dict)\r\n   1062 \r\n   1063         # Update the info object with the splits.\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/utils/info_utils.py](https://localhost:8080/#) in verify_splits(expected_splits, recorded_splits)\r\n     98     ]\r\n     99     if len(bad_splits) > 0:\r\n--> 100         raise NonMatchingSplitsSizesError(str(bad_splits))\r\n    101     logger.info(\"All the splits matched successfully.\")\r\n    102 \r\n\r\nNonMatchingSplitsSizesError: [{'expected': SplitInfo(name='train', num_bytes=82649695458, num_examples=597433111, shard_lengths=None, dataset_name=None), 'recorded': SplitInfo(name='train', num_bytes=3358310095, num_examples=24898932, shard_lengths=[3626991, 3716991, 4036990, 3506990, 3676990, 3716990, 2616990], dataset_name='spanish-tweets-small')}]\r\n```\r\n\r\nI think I had this dataset updated, might be related to #6271 \r\n\r\nIt is working fine as late in `2.10.0` , but not in `2.13.0` onwards.\r\n\r\n### Steps to reproduce the bug\r\n\r\n\r\n```python\r\n\r\nfrom datasets import load_dataset\r\n\r\nds = load_dataset(\"pysentimiento/spanish-tweets-small\")\r\n```\r\n\r\nYou can run it in [this notebook](https://colab.research.google.com/drive/1FdhqLiVimHIlkn7B54DbhizeQ4U3vGVl#scrollTo=YgA50cBSibUg)\r\n\r\n### Expected behavior\r\n\r\nLoad the dataset without any error\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.20.3\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nRegression bug: `NonMatchingSplitsSizesError` for (possibly) overwritten dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt was reloading from the wrong cache dir because of a bug in `_check_legacy_cache2`. This function should not trigger if there are config_kwars like `sample_by=`\r\n\r\nfix https://github.com/huggingface/datasets/issues/6758<end_of_turn>\n<start_of_turn>model\nFix cache conflict in `_check_legacy_cache2`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nBy default, the standard Image Dataset maps out `file_name` to `image` when loading an Image Dataset. \r\n\r\nHowever, if the dataset contains only 1 image, this does not take place \r\n\r\n### Steps to reproduce the bug\r\n\r\nInput\r\n(dataset with one image `multimodalart/repro_1_image`) \r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"multimodalart/repro_1_image\")\r\ndataset\r\n```\r\n\r\nOutput: \r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['file_name', 'prompt'],\r\n        num_rows: 1\r\n    })\r\n})\r\n```\r\n\r\nInput \r\n(dataset with 2+ images `multimodalart/repro_2_image`) \r\n```py\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"multimodalart/repro_2_image\")\r\ndataset\r\n```\r\n\r\nOutput:\r\n```py\r\nDatasetDict({\r\n    train: Dataset({\r\n        features: ['image', 'prompt'],\r\n        num_rows: 2\r\n    })\r\n})\r\n```\r\n\r\n### Expected behavior\r\n\r\nExpected to map `file_name`  \u2192 `image` for all dataset sizes, including 1. \r\n\r\n### Environment info\r\n\r\nBoth latest main and 2.16.0<end_of_turn>\n<start_of_turn>model\n`image` column not automatically inferred if image dataset only contains 1 image<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRight now we consider all the files of a dataset to be the same data, e.g.\r\n```\r\nsingle_subset_dataset/\r\n\u251c\u2500\u2500 train0.jsonl\r\n\u251c\u2500\u2500 train1.jsonl\r\n\u2514\u2500\u2500 train2.jsonl\r\n```\r\nbut in cases like this, each file is actually a different subset of the dataset and should be loaded separately\r\n```\r\nmany_subsets_dataset/\r\n\u251c\u2500\u2500 animals.jsonl\r\n\u251c\u2500\u2500 trees.jsonl\r\n\u2514\u2500\u2500 metadata.jsonl\r\n```\r\n\r\nIt would be nice to detect those subsets automatically using a simple heuristic. For example we can group files together if their paths names are the same except some digits ?<end_of_turn>\n<start_of_turn>model\nOne subset per file in repo ?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #7767<end_of_turn>\n<start_of_turn>model\nCustom `dl_manager` in `load_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen adding \"cache_dir\" to datasets.load_dataset, the cache_dir gets lost in the function calls, changing the cache dir to the default path. This fixes a few of these instances.<end_of_turn>\n<start_of_turn>model\nAdded cache dirs to load and file_utils<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nExperiencing persistent 401 Unauthorized errors when attempting to upload datasets to Hugging Face Hub using the `datasets` library. The error occurs specifically with the CAS (Content Addressable Storage) service during the upload process. Tried using HF_HUB_DISABLE_XET=1. It seems to work for smaller files. \n\nExact error message : \n```\nProcessing Files (0 / 0)                : |                                                                                                                                 |  0.00B /  0.00B             2025-09-10T09:44:35.657565Z ERROR  Fatal Error: \"cas::upload_xorb\" api call failed (request id 01b[...]XXX): HTTP status client error (401 Unauthorized) for url (https://cas-server.xethub.hf.co/xorb/default/7f3abdc[...]XXX)\n    at /home/runner/work/xet-core/xet-core/cas_client/src/retry_wrapper.rs:113\n\nProcessing Files (0 / 0)                :   0%|                                                                                                                             |  0.00B /  184kB,  0.00B/s  \nNew Data Upload                         :   0%|                                                                                                                             |  0.00B /  184kB,  0.00B/s  \n\u274c Failed to push some_dataset: Data processing error: CAS service error : Reqwest Error: HTTP status client error (401 Unauthorized), domain: https://cas-server.xethub.hf.co/xorb/default/7f3abdc[...]XXX\n```\n\nWorkaround Attempts\n\n1. **Disabled XET**: Set `HF_HUB_DISABLE_XET=1` environment variable\n2. **Updated hf-xet**: Use `hf-xet==1.1.9` rather than latest\n3. **Verified Authentication**: Confirmed HF token is valid and has write permissions\n4. **Tested with Smaller Datasets**: \n   - 100 samples: \u2705 **SUCCESS** (uploaded successfully)\n   - 10,000 samples: \u274c **FAILS** (401 Unauthorized)\n\n### Steps to reproduce the bug\n\n   ```python\n   from datasets import Dataset, DatasetDict\n   \n   # Create dataset (example with 10,000 samples)\n   dataset = Dataset.from_dict({\n       \"question\": questions,\n       \"answer\": answers,\n       # ... other fields\n   })\n   \n   # Split into train/test\n   dataset_dict = dataset.train_test_split(test_size=0.1)\n   \n   # Upload to Hub\n   dataset_dict.push_to_hub(\"Org/some-dataset\")\n   ```\n\n### Expected behavior\n\n## Expected Behavior\n- Dataset should upload successfully to Hugging Face Hub\n- Progress bars should complete without authentication errors\n- Dataset should be accessible at the specified repository URL\n\n## Actual Behavior\n- Upload fails consistently with 401 Unauthorized error\n- Error occurs specifically during CAS service interaction\n- No progress is made on the upload (0% completion)\n- Dataset is created on Hugging Face Hub with no data folder\n\n### Environment info\n\n\n- **Platform**: SageMaker (AWS)\n- **Python Version**: 3.12\n- **Libraries**: \n  - `datasets` library (latest version)\n  - `hf-xet==1.1.9` (attempted fix)\n- **Authentication**: Hugging Face token configured\n- **Dataset Size**: ~10,000 samples, works for smaller sizes (e.g. 100)<end_of_turn>\n<start_of_turn>model\nHugging Face Hub Dataset Upload CAS Error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe `extract_compressed_file` and `force_extract` properties of DownloadConfig are always being set to True in the function `dataset_module_factory` in the `load.py` file. This behavior is very annoying because data extracted will just be ignored the next time the dataset is loaded.\r\n\r\nSee this image below:\r\n![image](https://github.com/user-attachments/assets/9e76ebb7-09b1-4c95-adc8-a959b536f93c)\r\n\n\n### Steps to reproduce the bug\n\n1. Have a local dataset that contains archived files (zip, tar.gz, etc)\r\n2. Build a dataset loading script to download and extract these files\r\n3. Run the load_dataset function with a DownloadConfig that specifically set `force_extract` to False\r\n4. The extraction process will start no matter if the archives was extracted previously\n\n### Expected behavior\n\nThe extraction process should not run when the archives were previously extracted and `force_extract` is set to False.\n\n### Environment info\n\ndatasets==2.20.0\r\npython3.9<end_of_turn>\n<start_of_turn>model\nSome of DownloadConfig's properties are always being overridden in load.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe links at the bottom of [add_dataset](https://huggingface.co/docs/datasets/v1.2.1/add_dataset.html) to examples of specific datasets are all broken, for example\r\n\r\n- text classification: [ag_news](https://github.com/huggingface/datasets/blob/master/datasets/ag_news/ag_news.py) (original data are in csv files)\r\n\r\n### Steps to reproduce the bug\r\n\r\nClick on links to examples from latest documentation\r\n\r\n### Expected behavior\r\n\r\nLinks should be up to date - it might be more stable to link to https://huggingface.co/datasets/ag_news/blob/main/ag_news.py\r\n\r\n### Environment info\r\n\r\ndataset v1.2.1<end_of_turn>\n<start_of_turn>model\nDocumentation links to examples are broken<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI\u2019d like to request native support in the Hugging Face datasets library for reading .conll files (CoNLL format). This format is widely used in NLP tasks, especially for Named Entity Recognition (NER), POS tagging, and other token classification problems.\n\nRight now `.conll` datasets need to be manually parsed or preprocessed before being loaded into datasets. Having built in support would save time and make workflows smoother for researchers and practitioners.\n\nI propose - \nAdd a conll dataset builder or file parser to datasets that can:\n- Read `.conll` files with customizable delimiters (space, tab).\n- Handle sentence/document boundaries (typically indicated by empty lines).\n- Support common CoNLL variants (e.g., CoNLL-2000 chunking, CoNLL-2003 NER).\n- Output a dataset where each example contains:\n- tokens: list of strings\n- tags (or similar): list of labels aligned with tokens\n\nGiven a .conll snippet like:\n```\nEU NNP B-ORG\nrejects VBZ O\nGerman JJ B-MISC\ncall NN O\n. . O\n```\n\n\nThe dataset should load as:\n```\n{\n  \"tokens\": [\"EU\", \"rejects\", \"German\", \"call\", \".\"],\n  \"tags\": [\"B-ORG\", \"O\", \"B-MISC\", \"O\", \"O\"]\n}\n```\n\n\n\n\n### Motivation\n\n- CoNLL files are a standard benchmark format in NLP (e.g., CoNLL-2003, CoNLL-2000).\n- Many users train NER or sequence labeling models (like BERT for token classification) directly on `.conll` \n- Right now you have to write your own parsing scripts. Built in support would unify this process and would be much more convenient \n\n### Your contribution\n\nI\u2019d be happy to contribute by implementing this feature. My plan is to- \n- Add a new dataset script (conll.py) to handle .conll files.\n- Implement parsing logic that supports sentence/document boundaries and token-label alignment.\n- Write unit tests with small `.conll` examples to ensure correctness.\n- Add documentation and usage examples so new users can easily load `.conll` datasets.\n\nThis would be my first open source contribution, so I\u2019ll follow the `CONTRIBUTING.md` guidelines closely and adjust based on feedback from the maintainers.<end_of_turn>\n<start_of_turn>model\nAdd support for `.conll` file format in datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds the EXR feature to store depth images (or can be normals, etc) in float32.\r\nIt relies on [openexr_numpy](https://github.com/martinResearch/openexr_numpy/tree/main) to manipulate EXR images. \r\n\r\n<end_of_turn>\n<start_of_turn>model\nAdds EXR format to store depth images in float32<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### What\r\nFix the \"WebDataset documentation\" link on the Create a video dataset page to point\r\nto the correct section on the video load guide.\r\n\r\n### Why\r\nThe link currently points to an external repo, but the Hugging Face docs\r\nhave an internal \"WebDataset\" section under video_load.\r\n\r\n### How\r\n- docs/source/video_dataset.mdx: updated link to\r\n  `https://huggingface.co/docs/datasets/main/en/video_load#webdataset`\r\n\r\n### Issue\r\nFixes #7699\r\n<end_of_turn>\n<start_of_turn>model\n[docs] Fix broken WebDataset link on \u201cCreate a video dataset\u201d page<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n**Task Done:** \r\n\r\n- Resolve issue #7788 : Add the missing argument mapping in Dataset.to_sql (`src/datasets/arrow_dataset.py`)\r\n\r\n\r\n<end_of_turn>\n<start_of_turn>model\nfix: add `num_proc` argument to `Dataset.to_sql`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n[Click here for Dataset link](https://huggingface.co/datasets/discofuse/viewer/discofuse-wikipedia/train?row=6)\r\n**Below is the following way, as per my understanding , Is it correct :question: :question:**\r\n\r\nThe **columns/features from `DiscoFuse dataset`** that will be the **input to the `encoder` and `decoder`** are:\r\n\r\n[Click here for Dataset link](https://huggingface.co/datasets/discofuse/viewer/discofuse-wikipedia/train?row=6)\r\n\r\n1. **coherent_first_sentence**\r\n\r\n2. **coherent_second_sentence**\r\n\r\n3. **incoherent_first_sentence**\r\n\r\n4. **incoherent_second_sentence**\r\n\r\n[Click here for Dataset link](https://huggingface.co/datasets/discofuse/viewer/discofuse-wikipedia/train?row=6)\r\n\r\nThe **`encoder` will take these four columns as input and encode them into a sequence of hidden states. The `decoder` will then take these hidden states as input and decode them into a new sentence that fuses the two original sentences together.**\r\n\r\nThe **discourse type, connective_string, has_coref_type_pronoun, and has_coref_type_nominal columns will not be used as input to the encoder or decoder.** These columns are used to provide additional information about the dataset, but they are not necessary for the task of sentence fusion.\r\n\r\nPlease correct me if I am wrong; otherwise, if this understanding is right, how shall I implement this task practically?<end_of_turn>\n<start_of_turn>model\nWhat is the Right way to use discofuse dataset??<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix download for a dict of dicts of URLs when batched (default), introduced by:\r\n- #6794\r\n\r\nThis PR also implements regression tests.\r\n\r\nFix #6869, fix #6850.<end_of_turn>\n<start_of_turn>model\nFix download for dict of dicts of URLs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSame as https://github.com/huggingface/datasets/pull/7564 but for `Dataset`, cc @TopCoder2K FYI\r\n\r\ne.g. `ds[col]` now returns a lazy Column instead of a list\r\n\r\nThis way calling `ds[col][idx]` only loads the required data in memory\r\n\r\n(bonus: also supports subfields access with `ds[col][subcol][idx]`)\r\n\r\nthe breaking change will be for the next major release, which also includes removal of dataset scripts support\r\n\r\nclose https://github.com/huggingface/datasets/issues/4180<end_of_turn>\n<start_of_turn>model\nLazy column<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes https://github.com/huggingface/datasets/issues/6980<end_of_turn>\n<start_of_turn>model\nUnblock NumPy 2.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6738 <end_of_turn>\n<start_of_turn>model\nAllow null values in dict columns<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`IterableDataset.shard()` has the wrong typing for its return as `\"Dataset\"`. It should be `\"IterableDataset\"`. Makes my IDE unhappy.\n\n### Steps to reproduce the bug\n\nlook at [the source code](https://github.com/huggingface/datasets/blob/main/src/datasets/iterable_dataset.py#L2668)?\n\n### Expected behavior\n\nCorrect return type as `\"IterableDataset\"`\n\n### Environment info\n\ndatasets==3.1.0<end_of_turn>\n<start_of_turn>model\nwrong return type for `IterableDataset.shard()`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI'm trying to save a `Dataset` using the `save_to_disk()` function with:\r\n- `num_proc > 1`\r\n- `dataset_path` being a s3 bucket path e.g. \"s3://{bucket_name}/{dataset_folder}/\"\r\n\r\nThe hf progress bar shows up but the saving does not seem to start. \r\nWhen using one processor only (`num_proc=1`), everything works fine.\r\nWhen saving the dataset on local disk (as opposed to s3 bucket) with `num_proc > 1`, everything works fine.\r\n\r\nThank you for your help! :)\r\n\r\n### Steps to reproduce the bug\r\n\r\nI tried without any storage options:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\nsandbox_ds = load_dataset(\"openai_humaneval\")\r\nsandbox_ds[\"test\"].save_to_disk(\r\n    \"s3://bucket-name/test_multiprocessing_saving/\",\r\n    num_proc=4,\r\n)\r\n```\r\n\r\n\r\nand with the specific s3fs storage options: \r\n```\r\nfrom datasets import load_dataset\r\nfrom s3fs import S3FileSystem\r\n\r\ndef get_s3fs():\r\n    return S3FileSystem()\r\n\r\nsandbox_ds = load_dataset(\"openai_humaneval\")\r\nsandbox_ds[\"test\"].save_to_disk(\r\n    \"s3://bucket-name/test_multiprocessing_saving/\",\r\n    num_proc=4,\r\n    storage_options=get_s3fs().storage_options, # also tried: storage_options=S3FileSystem().storage_options\r\n)\r\n```\r\n\r\nI'm guessing I might use `storage_options` parameter wrongly, but I didn't find anything online that made it work.\r\n\r\n**NB**: Behavior is the same when trying to save the whole `DatasetDict`.\r\n\r\n### Expected behavior\r\n\r\nProgress bar fills in and saving is carried out.\r\n\r\n### Environment info\r\n\r\n`datasets==2.18.0`<end_of_turn>\n<start_of_turn>model\nsave_to_disk() freezes when saving on s3 bucket with multiprocessing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using the `filter` or `map` function to preprocess a dataset, a ValueError is encountered with the error message \"array is too big; arr.size * arr.dtype.itemsize is larger than the maximum possible size.\" \r\n\r\nDetailed error message:\r\nTraceback (most recent call last):\r\n  File \"data_processing.py\", line 26, in <module>\r\n    processed_dataset[split] = samromur_children[split].map(prepare_dataset, cache_file_name=cache_dict[split],writer_batch_size = 50)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2405, in map\r\n    desc=desc,\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 557, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 524, in wrapper\r\n    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/fingerprint.py\", line 480, in wrapper\r\n    out = func(self, *args, **kwargs)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2756, in _map_single\r\n    example = apply_function_on_filtered_inputs(example, i, offset=offset)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2655, in apply_function_on_filtered_inputs\r\n    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 2347, in decorated\r\n    result = f(decorated_item, *args, **kwargs)\r\n  File \"data_processing.py\", line 11, in prepare_dataset\r\n    audio = batch[\"audio\"]\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/arrow_dataset.py\", line 123, in __getitem__\r\n    value = decode_nested_example(self.features[key], value) if value is not None else None\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/features/features.py\", line 1260, in decode_nested_example\r\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/features/audio.py\", line 156, in decode_example\r\n    array, sampling_rate = self._decode_non_mp3_path_like(path, token_per_repo_id=token_per_repo_id)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/datasets/features/audio.py\", line 257, in _decode_non_mp3_path_like\r\n    array, sampling_rate = librosa.load(f, sr=self.sampling_rate, mono=self.mono)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/librosa/core/audio.py\", line 176, in load\r\n    y, sr_native = __soundfile_load(path, offset, duration, dtype)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/librosa/core/audio.py\", line 222, in __soundfile_load\r\n    y = sf_desc.read(frames=frame_duration, dtype=dtype, always_2d=False).T\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/soundfile.py\", line 891, in read\r\n    out = self._create_empty_array(frames, always_2d, dtype)\r\n  File \"/projects/zhwa3087/software/anaconda/envs/mycustomenv/lib/python3.7/site-packages/soundfile.py\", line 1323, in _create_empty_array\r\n    return np.empty(shape, dtype, order='C')\r\nValueError: array is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size.\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset, DatasetDict\r\nfrom transformers import WhisperFeatureExtractor\r\nfrom transformers import WhisperTokenizer\r\n\r\nsamromur_children= load_dataset(\"language-and-voice-lab/samromur_children\")\r\nfeature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")\r\ntokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"icelandic\", task=\"transcribe\")\r\n\r\ndef prepare_dataset(batch):\r\n    # load and resample audio data from 48 to 16kHz\r\n    audio = batch[\"audio\"]\r\n\r\n    # compute log-Mel input features from input audio array \r\n    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=16000).input_features[0]\r\n\r\n    # encode target text to label ids \r\n    batch[\"labels\"] = tokenizer(batch[\"normalized_text\"]).input_ids\r\n    return batch\r\n\r\ncache_dict = {\"train\": \"./cache/audio_train.cache\", \\\r\n              \"validation\": \"./cache/audio_validation.cache\", \\\r\n              \"test\": \"./cache/audio_test.cache\"}\r\nfilter_cache_dict = {\"train\": \"./cache/filter_train.arrow\", \\\r\n                \"validation\": \"./cache/filter_validation.arrow\", \\\r\n                \"test\": \"./cache/filter_test.arrow\"}\r\n\r\nprint(\"before filtering\")\r\nprint(samromur_children)\r\n#filter the dataset to only include examples with more than 2 seconds of audio\r\nsamromur_children = samromur_children.filter(lambda example: example[\"audio\"][\"array\"].shape[0] > 16000*2, cache_file_names=filter_cache_dict)   \r\nprint(\"after filtering\")\r\nprint(samromur_children)\r\nprocessed_dataset = DatasetDict()\r\n# processed_dataset = samromur_children.map(prepare_dataset, cache_file_names=cache_dict, num_proc=10,)\r\nfor split in [\"train\", \"validation\", \"test\"]:\r\n    processed_dataset[split] = samromur_children[split].map(prepare_dataset, cache_file_name=cache_dict[split])\r\n```\n\n### Expected behavior\n\nThe dataset is successfully processed and ready to train the model.\n\n### Environment info\n\nPython version:  3.7.13\r\ndatasets package version: 2.4.0\r\nlibrosa package version: 0.10.0.post2<end_of_turn>\n<start_of_turn>model\narray is too big; `arr.size * arr.dtype.itemsize` is larger than the maximum possible size in Datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR builds on #6832 by @mariosasko.\r\n\r\nMay close - #4101, #2538\r\n\r\nDiscussion - https://github.com/huggingface/datasets/pull/7648#issuecomment-3084050130\r\n\r\n---\r\n### Note - This PR is under work and frequent changes will be pushed.<end_of_turn>\n<start_of_turn>model\nSupport downloading specific splits in load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\ni'm using datasets.download.download_manager.DownloadManager to download files like \"file:///a/b/c.txt\", and i disable_progress_bar() to disable bar. there will be an exception as follows:\r\n\r\n`AttributeError: 'function' object has no attribute 'close'\r\nException ignored in: <function TqdmCallback.__del__ at 0x7fa8683d84c0>\r\nTraceback (most recent call last):\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/fsspec/callbacks.py\", line 233, in __del__\r\n    self.tqdm.close()`\r\n\r\ni check your source code in datasets/utils/file_utils.py:348 you define TqdmCallback derive from fsspec.callbacks.TqdmCallback\r\n\r\nbut in the newest fsspec code [https://github.com/fsspec/filesystem_spec/blob/master/fsspec/callbacks.py](url) , line 146, in this case, _DEFAULT_CALLBACK will take effect, but in line 234, it calls \"close()\" function which _DEFAULT_CALLBACK don't have such thing.\r\n\r\nso i think the class \"TqdmCallback\" in datasets/utils/file_utils.py may override \"__del__\" function or report this bug to fsspec.\r\n\r\n\n\n### Steps to reproduce the bug\n\nas i said\n\n### Expected behavior\n\nno exception\n\n### Environment info\n\ndatasets: 2.14.4\r\npython: 3.9\r\nplatform: x86_64<end_of_turn>\n<start_of_turn>model\nusing DownloadManager to download from local filesystem and disable_progress_bar, there will be an exception <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n I'm using the Hugging Face datasets library to load images in batch and would like to apply a torchvision transform to solve the inconsistent image sizes in the dataset and apply some on the fly image augmentation. I can just think about using the collate_fn, but seems quite inefficient. \r\n \r\n I'm new to the Hugging Face datasets library, I didn't find nothing in the documentation or the issues here on github.\r\n\r\nIs there an existing way to add image transformations directly to the dataset loading pipeline? \r\n\r\n### Steps to reproduce the bug\r\n\r\nfrom datasets import load_dataset\r\nfrom torch.utils.data import DataLoader\r\n\r\n```python\r\ndef collate_fn(batch):\r\n    images = [item['image'] for item in batch]\r\n    texts = [item['text'] for item in batch]\r\n    return {\r\n        'images': images,\r\n        'texts': texts\r\n    }\r\n\r\ndataset = load_dataset(\"Yuki20/pokemon_caption\", split=\"train\")\r\ndataloader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)\r\n\r\n# Output shows varying image sizes:\r\n# [(1280, 1280), (431, 431), (789, 789), (769, 769)]\r\n```\r\n\r\n### Expected behavior\r\n\r\nI'm looking for a way to resize images on-the-fly when loading the dataset, similar to PyTorch's Dataset.__getitem__ functionality. This would be more efficient than handling resizing in the collate_fn.\r\n\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.1.0\r\n- Platform: Linux-6.5.0-41-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.10\r\n- `huggingface_hub` version: 0.26.2\r\n- PyArrow version: 18.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.9.0\r\n<end_of_turn>\n<start_of_turn>model\nEfficient Image Augmentation in Hugging Face Datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported at https://github.com/huggingface/datasets/issues/3444<end_of_turn>\n<start_of_turn>model\nwith_format docstring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCan we have a utility function that will use `load_from_disk` when given the local path and `load_dataset` if given an HF dataset?\n\nIt can be something as simple as this one:\n\n```\ndef load_hf_dataset(path_or_name):\n    if os.path.exists(path_or_name):\n        return load_from_disk(path_or_name)\n    else:\n        return load_dataset(path_or_name)\n```\n\n### Motivation\n\nThis can be done inside the user codebase, too, but in my experience, it becomes repetitive code.\n\n### Your contribution\n\nI can open a pull request.<end_of_turn>\n<start_of_turn>model\nFlexible Loader<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupporting this should be fairly easy.\r\n\r\nRequested on the forum [here](https://discuss.huggingface.co/t/how-can-i-convert-a-loaded-dataset-in-to-a-parquet-file-and-save-it-to-the-s3/48353).<end_of_turn>\n<start_of_turn>model\nSupport `fsspec` in `Dataset.to_<format>` methods<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWe don't need to pass `use_auth_token=True` anymore to download gated datasets or models, so the following should work if correctly logged in.\r\n\r\n```py\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"mozilla-foundation/common_voice_6_1\", \"tr\", split=\"train+validation\")\r\n```\r\n\r\nHowever it throws an error - probably because something weird is hardcoded into the dataset loading script.\n\n### Steps to reproduce the bug\n\n1.) \r\n```\r\nhuggingface-cli login\r\n```\r\n\r\n2.) Make sure that you have accepted the license here:\r\nhttps://huggingface.co/datasets/mozilla-foundation/common_voice_6_1\r\n\r\n3.) Run:\r\n```py\r\nfrom datasets import load_dataset\r\n\r\nload_dataset(\"mozilla-foundation/common_voice_6_1\", \"tr\", split=\"train+validation\")\r\n```\r\n\r\n4.)  You'll get:\r\n\r\n```\r\nFile ~/hf/lib/python3.10/site-packages/datasets/builder.py:963, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n    961 split_dict = SplitDict(dataset_name=self.name)\r\n    962 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n--> 963 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n    965 # Checksums verification\r\n    966 if verification_mode == VerificationMode.ALL_CHECKS and dl_manager.record_checksums:\r\n\r\nFile ~/.cache/huggingface/modules/datasets_modules/datasets/mozilla-foundation--common_voice_6_1/f4d7854c466f5bd4908988dbd39044ec4fc634d89e0515ab0c51715c0127ffe3/common_voice_6_1.py:150, in CommonVoice._split_generators(self, dl_manager)\r\n    148 hf_auth_token = dl_manager.download_config.use_auth_token\r\n    149 if hf_auth_token is None:\r\n--> 150     raise ConnectionError(\r\n    151         \"Please set use_auth_token=True or use_auth_token='<TOKEN>' to download this dataset\"\r\n    152     )\r\n    154 bundle_url_template = STATS[\"bundleURLTemplate\"]\r\n    155 bundle_version = bundle_url_template.split(\"/\")[0]\r\n\r\nConnectionError: Please set use_auth_token=True or use_auth_token='<TOKEN>' to download this dataset\r\n```\n\n### Expected behavior\n\nOne should not have to pass `use_auth_token=True`. Also see discussion here: https://github.com/huggingface/blog/pull/1243#discussion_r1235131150\n\n### Environment info\n\n```\r\n- `datasets` version: 2.13.0\r\n- Platform: Linux-6.2.0-76060200-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.16.0.dev0\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 1.5.3\r\n```<end_of_turn>\n<start_of_turn>model\nCommon Voice datasets still need `use_auth_token=True`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIt would be useful for PDF.\r\n\r\nhttps://github.com/huggingface/dataset-viewer/issues/2991#issuecomment-2242656069<end_of_turn>\n<start_of_turn>model\nNew feature type: Document<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI would like to bring to your attention an issue related to the accuracy of bounding boxes within the \"wildreceipt\" dataset, which is made available through the Hugging Face API. Specifically, I have identified a discrepancy between the bounding boxes generated by the dataset loading commands, namely `load_dataset(\"Theivaprakasham/wildreceipt\")` and `load_dataset(\"jinhybr/WildReceipt\")`, and the actual labels and corresponding bounding boxes present in the dataset.\r\n\r\nTo illustrate this divergence, I've provided two examples in the form of screenshots. These screenshots highlight the contrasting outcomes between my personal implementation of the dataloader and the implementation offered by Hugging Face:\r\n\r\n**Example 1:**\r\n\r\n![image](https://github.com/huggingface/datasets/assets/50714796/7a6604d2-899d-4102-a008-1a28c90698f1)\r\n![image](https://github.com/huggingface/datasets/assets/50714796/eba458c7-d3af-4868-a520-8b683aa96f66)\r\n![image](https://github.com/huggingface/datasets/assets/50714796/9f394891-5f5b-46f7-8e52-071b724aedab)\r\n\r\n**Example 2:**\r\n\r\n![image](https://github.com/huggingface/datasets/assets/50714796/a2b2a8d3-124e-4990-b64a-5133cf4be2fe)\r\n![image](https://github.com/huggingface/datasets/assets/50714796/6ee25642-35aa-40ad-ac1e-899d33be90df)\r\n![image](https://github.com/huggingface/datasets/assets/50714796/5e42ff91-9fc4-4520-8803-0e225656f96c)\r\n\r\nIt's important to note that my dataloader implementation is based on the same dataset files as utilized in the Hugging Face implementation. For your reference, you can access the dataset files through this link: [wildreceipt dataset files](https://download.openmmlab.com/mmocr/data/wildreceipt.tar).\r\n\r\nThis inconsistency in bounding box accuracy warrants investigation and rectification for maintaining the integrity of the \"wildreceipt\" dataset. Your attention and assistance in addressing this matter would be greatly appreciated.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport matplotlib.pyplot as plt\r\nfrom datasets import load_dataset\r\n\r\n\r\n# Define functions to convert bounding box formats\r\ndef convert_format1(box):\r\n    x, y, w, h = box\r\n    x2, y2 = x + w, y + h\r\n    return [x, y, x2, y2]\r\n\r\n\r\ndef convert_format2(box):\r\n    x1, y1, x2, y2 = box\r\n    return [x1, y1, x2, y2]\r\n\r\n\r\ndef plot_cropped_image(image, box, title):\r\n    cropped_image = image.crop(box)\r\n    plt.imshow(cropped_image)\r\n    plt.title(title)\r\n    plt.axis('off')\r\n    plt.savefig(title+'.png')\r\n    plt.show()\r\n\r\n\r\ndoc_index = 1\r\nword_index = 3\r\ndataset = load_dataset(\"Theivaprakasham/wildreceipt\")['train']\r\n\r\nbbox_hugging_face = dataset[doc_index]['bboxes'][word_index]\r\ntext_unit_face = dataset[doc_index]['words'][word_index]\r\n\r\ncommon_box_hugface_1 = convert_format1(bbox_hugging_face)\r\ncommon_box_hugface_2 = convert_format2(bbox_hugging_face)\r\n\r\nplot_cropped_image(image_hugging, common_box_hugface_1,\r\n                   f'Hugging Face Bouding boxes (x,y,w,h format) \\n its associated text unit: {text_unit_face}')\r\nplot_cropped_image(image_hugging, common_box_hugface_2,\r\n                   f'Hugging Face Bouding boxes (x1,y1,x2, y2 format) \\n its associated text unit: {text_unit_face}')\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nThe bounding boxes generated by the \"wildreceipt\" dataset in HuggingFace implementation loading commands should accurately match the actual labels and bounding boxes of the dataset.\r\n\r\n\r\n### Environment info\r\n\r\n- Python version: 3.8\r\n- Hugging Face datasets version: 2.14.2\r\n- Dataset file taken from this link: https://download.openmmlab.com/mmocr/data/wildreceipt.tar<end_of_turn>\n<start_of_turn>model\nInaccurate Bounding Boxes in \"wildreceipt\" Dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix string_to_dict usage for windows<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI've refined several type annotations throughout the codebase to align with current best practices and enhance overall clarity. Given the complexity of the code, there may still be areas that need further attention. I welcome any feedback or suggestions to make these improvements even better.\r\n\r\n - Fixes #7202 <end_of_turn>\n<start_of_turn>model\nImproved type annotation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nDon't alter input in Features.from_dict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds support for specifying image, video, audio, and pdf features using `pathlib.Path`.<end_of_turn>\n<start_of_turn>model\nSupport pathlib.Path for feature input<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```python\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/site-packages/datasets/load.py\", line 1034, in get_module\r\n    data_files = DataFilesDict.from_patterns(\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/site-packages/datasets/data_files.py\", line 671, in from_patterns\r\n    DataFilesList.from_patterns(\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/site-packages/datasets/data_files.py\", line 586, in from_patterns\r\n    origin_metadata = _get_origin_metadata(data_files, download_config=download_config)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/site-packages/datasets/data_files.py\", line 502, in _get_origin_metadata\r\n    return thread_map(\r\n           ^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/contrib/concurrent.py\", line 70, in thread_map\r\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/contrib/concurrent.py\", line 48, in _executor_map\r\n    with ensure_lock(tqdm_class, lock_name=lock_name) as lk:\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/contextlib.py\", line 144, in __exit__\r\n    next(self.gen)\r\n  File \"/Users/codingl2k1/.pyenv/versions/3.11.4/lib/python3.11/site-packages/tqdm/contrib/concurrent.py\", line 25, in ensure_lock\r\n    del tqdm_class._lock\r\n        ^^^^^^^^^^^^^^^^\r\nAttributeError: '_tqdm_cls' object has no attribute '_lock'\r\n```\n\n### Steps to reproduce the bug\n\nHappens ocasionally.\n\n### Expected behavior\n\nI added a print in tqdm `ensure_lock()`, got a `ensure_lock <datasets.utils.logging._tqdm_cls object at 0x16dddead0> ` print.\r\n\r\nAccording to the code in https://github.com/tqdm/tqdm/blob/master/tqdm/contrib/concurrent.py#L24\r\n\r\n```python\r\n@contextmanager\r\ndef ensure_lock(tqdm_class, lock_name=\"\"):\r\n    \"\"\"get (create if necessary) and then restore `tqdm_class`'s lock\"\"\"\r\n    print(\"ensure_lock\", tqdm_class, lock_name)\r\n    old_lock = getattr(tqdm_class, '_lock', None)  # don't create a new lock\r\n    lock = old_lock or tqdm_class.get_lock()  # maybe create a new lock\r\n    lock = getattr(lock, lock_name, lock)  # maybe subtype\r\n    tqdm_class.set_lock(lock)\r\n    yield lock\r\n    if old_lock is None:\r\n        del tqdm_class._lock  # <-- It tries to del the `_lock` attribute from tqdm_class.\r\n    else:\r\n        tqdm_class.set_lock(old_lock)\r\n```\r\n\r\nBut, huggingface datasets `datasets.utils.logging._tqdm_cls` does not have the field `_lock`: https://github.com/huggingface/datasets/blob/main/src/datasets/utils/logging.py#L205\r\n\r\n```python\r\nclass _tqdm_cls:\r\n    def __call__(self, *args, disable=False, **kwargs):\r\n        if _tqdm_active and not disable:\r\n            return tqdm_lib.tqdm(*args, **kwargs)\r\n        else:\r\n            return EmptyTqdm(*args, **kwargs)\r\n\r\n    def set_lock(self, *args, **kwargs):\r\n        self._lock = None\r\n        if _tqdm_active:\r\n            return tqdm_lib.tqdm.set_lock(*args, **kwargs)\r\n\r\n    def get_lock(self):\r\n        if _tqdm_active:\r\n            return tqdm_lib.tqdm.get_lock()\r\n```\r\n\r\n\n\n### Environment info\n\nPython 3.11.4\r\ntqdm '4.65.0'\r\ndatasets master<end_of_turn>\n<start_of_turn>model\nAttributeError: '_tqdm_cls' object has no attribute '_lock'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, I tried to merge in total 22M sequences of data, where each sequence is of maximum length 2000. I found that merging these datasets and then `save_to_disk` is extremely slow because of flattening the indices. Wondering if you have any suggestions or guidance on this. Thank you very much! \n\n### Steps to reproduce the bug\n\nThe source data is too big to demonstrate\n\n### Expected behavior\n\nThe source data is too big to demonstrate\n\n### Environment info\n\npython                     3.9.0\r\ndatasets                  2.7.0\r\npytorch                   2.0.0 \r\ntokenizers                0.13.1\r\ntransformers              4.31.0<end_of_turn>\n<start_of_turn>model\nExtreme inefficiency for `save_to_disk` when merging datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi, folks, I'm newbie in huggingface dataset api.\nAs title, i'm facing the issue that the dataset.load api can not connect to the installed dataset.\n\ncode snippet :\n<img width=\"572\" height=\"253\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/10f48aaf-d6ca-4239-b1cf-145d74f125d1\" />\n\ndata path : \n\"/xxx/joseph/llava_ds/vlm_ds\"\nit contains all video clips i want!\n<img width=\"1398\" height=\"261\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/bf213b66-e344-4311-97e7-bc209677ae77\" />\n\ni run the py script by \n<img width=\"1042\" height=\"38\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/8b3fcee4-e1a6-41b8-bee1-91567b00d9d2\" />\n\nBut bad happended, even i provide dataset path by \"HF_HUB_CACHE\", it still attempt to download data from remote side : \n\n<img width=\"1697\" height=\"813\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/baa6cff1-a724-4710-a8c4-4805459deffb\" />\n\nAny suggestion will be appreciated!!<end_of_turn>\n<start_of_turn>model\nCan Not read installed dataset in dataset.load(.)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIterableDataset lost but not keep columns when map function adding columns with names in remove_columns,\r\nDataset not.\r\n\r\nMay be related to the code below:\r\nhttps://github.com/huggingface/datasets/blob/06c3ffb8d068b6307b247164b10f7c7311cefed4/src/datasets/iterable_dataset.py#L750-L756\n\n### Steps to reproduce the bug\n\n```python\r\ndataset: IterableDataset = load_dataset(\"Anthropic/hh-rlhf\", streaming=True, split=\"train\")\r\ncolumn_names = list(next(iter(dataset)).keys())  #  ['chosen', 'rejected']\r\n# map_fn will return dict {\"chosen\": xxx, \"rejected\": xxx, \"prompt\": xxx, \"history\": xxxx}\r\ndataset = dataset.map(map_fn, batched=True, remove_columns=column_names)\r\nnext(iter(dataset))\r\n# output\r\n# {'prompt': 'xxx, 'history': xxx}   \r\n```\r\n\r\n```python\r\n# when load_dataset with streaming=False, the column_names are kept: \r\ndataset: Dataset = load_dataset(\"Anthropic/hh-rlhf\", streaming=False, split=\"train\")\r\ncolumn_names = list(next(iter(dataset)).keys())  #  ['chosen', 'rejected']\r\n# map_fn will return dict {\"chosen\": xxx, \"rejected\": xxx, \"prompt\": xxx, \"history\": xxxx}\r\ndataset = dataset.map(map_fn, batched=True, remove_columns=column_names)\r\nnext(iter(dataset))\r\n# output\r\n# {'prompt': 'xxx, 'history': xxx, \"chosen\": xxx, \"rejected\": xxx}   \r\n```\r\n\n\n### Expected behavior\n\nIterableDataset keep columns when map function adding columns with names in remove_columns\n\n### Environment info\n\ndatasets==2.14.6<end_of_turn>\n<start_of_turn>model\n`IterableDataset` lost but not keep columns when map function adding columns with names in `remove_columns`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6497.<end_of_turn>\n<start_of_turn>model\nEnable setting config as default when push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nArrow issues when running the example Notebook laptop locally on Mac with M1. Works on Google Collab.\r\n\r\n**Notebook**: https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLMv3/Fine_tune_LayoutLMv3_on_FUNSD_(HuggingFace_Trainer).ipynb\r\n\r\n**Error**: `ValueError: Arrow type extension<arrow.py_extension_type<pyarrow.lib.UnknownExtensionType>> does not have a datasets dtype equivalent.`\r\n\r\n**Caused by**:\r\n\r\n```\r\n# we need to define custom features for `set_format` (used later on) to work properly\r\nfeatures = Features({\r\n    'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\r\n    'input_ids': Sequence(feature=Value(dtype='int64')),\r\n    'attention_mask': Sequence(Value(dtype='int64')),\r\n    'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\r\n    'labels': Sequence(feature=Value(dtype='int64')),\r\n})\r\n```\n\n### Steps to reproduce the bug\n\nRun the notebook provided, locally. If possible also on M1.\n\n### Expected behavior\n\nThe cell where features are mapped to Array2D and Array3D should work without any issues.\n\n### Environment info\n\nTried with Python 3.9 and 3.10 conda envs. Running Mac M1.\r\n\r\n`pip show datasets`\r\n\r\n> Name: datasets\r\nVersion: 2.14.6\r\nSummary: HuggingFace community-driven open-source library of datasets\r\n\r\n`pip list`\r\n\r\n> Package                   Version\r\n> ------------------------- ------------\r\n> accelerate                0.24.1\r\n> aiohttp                   3.8.6\r\n> aiosignal                 1.3.1\r\n> anyio                     3.5.0\r\n> appnope                   0.1.2\r\n> argon2-cffi               21.3.0\r\n> argon2-cffi-bindings      21.2.0\r\n> asttokens                 2.0.5\r\n> async-timeout             4.0.3\r\n> attrs                     23.1.0\r\n> backcall                  0.2.0\r\n> beautifulsoup4            4.12.2\r\n> bleach                    4.1.0\r\n> certifi                   2023.7.22\r\n> cffi                      1.15.1\r\n> charset-normalizer        3.3.2\r\n> comm                      0.1.2\r\n> datasets                  2.14.6\r\n> debugpy                   1.6.7\r\n> decorator                 5.1.1\r\n> defusedxml                0.7.1\r\n> dill                      0.3.7\r\n> entrypoints               0.4\r\n> exceptiongroup            1.0.4\r\n> executing                 0.8.3\r\n> fastjsonschema            2.16.2\r\n> filelock                  3.13.1\r\n> frozenlist                1.4.0\r\n> fsspec                    2023.10.0\r\n> huggingface-hub           0.17.3\r\n> idna                      3.4\r\n> importlib-metadata        6.0.0\r\n> IProgress                 0.4\r\n> ipykernel                 6.25.0\r\n> ipython                   8.15.0\r\n> ipython-genutils          0.2.0\r\n> jedi                      0.18.1\r\n> Jinja2                    3.1.2\r\n> joblib                    1.3.2\r\n> jsonschema                4.19.2\r\n> jsonschema-specifications 2023.7.1\r\n> jupyter_client            7.4.9\r\n> jupyter_core              5.5.0\r\n> jupyter-server            1.23.4\r\n> jupyterlab-pygments       0.1.2\r\n> MarkupSafe                2.1.1\r\n> matplotlib-inline         0.1.6\r\n> mistune                   2.0.4\r\n> mpmath                    1.3.0\r\n> multidict                 6.0.4\r\n> multiprocess              0.70.15\r\n> nbclassic                 1.0.0\r\n> nbclient                  0.8.0\r\n> nbconvert                 7.10.0\r\n> nbformat                  5.9.2\r\n> nest-asyncio              1.5.6\r\n> networkx                  3.2.1\r\n> notebook                  6.5.4\r\n> notebook_shim             0.2.3\r\n> numpy                     1.26.1\r\n> packaging                 23.1\r\n> pandas                    2.1.3\r\n> pandocfilters             1.5.0\r\n> parso                     0.8.3\r\n> pexpect                   4.8.0\r\n> pickleshare               0.7.5\r\n> Pillow                    10.1.0\r\n> pip                       23.3\r\n> platformdirs              3.10.0\r\n> prometheus-client         0.14.1\r\n> prompt-toolkit            3.0.36\r\n> psutil                    5.9.0\r\n> ptyprocess                0.7.0\r\n> pure-eval                 0.2.2\r\n> pyarrow                   14.0.1\r\n> pycparser                 2.21\r\n> Pygments                  2.15.1\r\n> python-dateutil           2.8.2\r\n> pytz                      2023.3.post1\r\n> PyYAML                    6.0.1\r\n> pyzmq                     23.2.0\r\n> referencing               0.30.2\r\n> regex                     2023.10.3\r\n> requests                  2.31.0\r\n> rpds-py                   0.10.6\r\n> safetensors               0.4.0\r\n> scikit-learn              1.3.2\r\n> scipy                     1.11.3\r\n> Send2Trash                1.8.2\r\n> seqeval                   1.2.2\r\n> setuptools                68.0.0\r\n> six                       1.16.0\r\n> sniffio                   1.2.0\r\n> soupsieve                 2.5\r\n> stack-data                0.2.0\r\n> sympy                     1.12\r\n> terminado                 0.17.1\r\n> threadpoolctl             3.2.0\r\n> tinycss2                  1.2.1\r\n> tokenizers                0.14.1\r\n> torch                     2.1.0\r\n> tornado                   6.3.3\r\n> tqdm                      4.66.1\r\n> traitlets                 5.7.1\r\n> transformers              4.36.0.dev0\r\n> typing_extensions         4.7.1\r\n> tzdata                    2023.3\r\n> urllib3                   2.0.7\r\n> wcwidth                   0.2.5\r\n> webencodings              0.5.1\r\n> websocket-client          0.58.0\r\n> wheel                     0.41.2\r\n> xxhash                    3.4.1\r\n> yarl                      1.9.2\r\n> zipp                      3.11.0<end_of_turn>\n<start_of_turn>model\n Bug: LayoutLMv3 finetuning on FUNSD Notebook; Arrow Error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe dataset loading was throwing some safety errors for this popular dataset `wmt14`. \r\n\r\n[in]:\r\n\r\n```\r\nimport datasets\r\n\r\n# train_data = datasets.load_dataset(\"wmt14\", \"de-en\", split=\"train\")\r\ntrain_data = datasets.load_dataset(\"wmt14\", \"de-en\", split=\"train\")\r\nval_data = datasets.load_dataset(\"wmt14\", \"de-en\", split=\"validation[:10%]\")\r\n```\r\n\r\n\r\n[out]:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-9-445f0ecc4817>](https://localhost:8080/#) in <cell line: 4>()\r\n      2 \r\n      3 # train_data = datasets.load_dataset(\"wmt14\", \"de-en\", split=\"train\")\r\n----> 4 train_data = datasets.load_dataset(\"wmt14\", \"de-en\", split=\"train\")\r\n      5 val_data = datasets.load_dataset(\"wmt14\", \"de-en\", split=\"validation[:10%]\")\r\n\r\n12 frames\r\n[/usr/local/lib/python3.10/dist-packages/huggingface_hub/hf_api.py](https://localhost:8080/#) in __init__(self, **kwargs)\r\n    636         if security is not None:\r\n    637             security = BlobSecurityInfo(\r\n--> 638                 safe=security[\"safe\"], av_scan=security[\"avScan\"], pickle_import_scan=security[\"pickleImportScan\"]\r\n    639             )\r\n    640         self.security = security\r\n\r\nKeyError: 'safe'\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nSee above.\r\n\r\n### Expected behavior\r\n\r\nDataset properly loaded.\r\n\r\n### Environment info\r\n\r\nversion: 2.21.0<end_of_turn>\n<start_of_turn>model\nOlder datasets throwing safety errors with 2.21.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI got to this by hacking around a bit but it seems to solve #7206 \r\n\r\nI have no idea if this approach makes sense or would break something else? \r\n\r\nCould maybe work on a full pr if this looks reasonable @lhoestq ? I imagine the same issue might affect other iterable dataset methods?<end_of_turn>\n<start_of_turn>model\napply formatting after iter_arrow to speed up format -> map, filter for iterable datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7793 \r\n\r\n## Problem\r\nLoading datasets with deeply nested structures (like `metr-evals/malt-public`) fails with:\r\n\r\nArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\r\nThis occurs when parquet files contain nested data (lists, structs, maps) that exceed PyArrow's 16MB chunk limit.\r\n\r\n## Root Cause\r\nPyArrow's C++ implementation explicitly rejects nested data conversions when data is split across multiple chunks. The limitation exists in the `WrapIntoListArray` function where repetition levels cannot be reconstructed across chunk boundaries.\r\n\r\n## Solution\r\n- **Fallback mechanism**: Catches the specific PyArrow error and switches to non-chunked reading\r\n- **Selective optimization**: Only combines chunks for problematic nested columns to minimize memory impact  \r\n- **Manual batching**: Maintains batching behavior even in fallback mode\r\n- **Backward compatibility**: Zero impact on existing datasets\r\n\r\n## Implementation Details\r\n- Added `_is_nested_type()` helper to detect nested PyArrow types\r\n- Added `_handle_nested_chunked_conversion()` for selective chunk combining\r\n- Modified `_generate_tables()` to catch and handle the specific error\r\n- Preserves all existing error handling and logging\r\n\r\n## Testing\r\n- [x] No regressions: Normal parquet datasets continue working\r\n- [x] Code follows existing patterns in the datasets codebase\r\n- [x]  tested by original reporter (gated dataset access needed)\r\n\r\n**Note**: This fix is based on thorough research of PyArrow limitations and similar issues in the ecosystem. While we cannot test with the original dataset due to access restrictions, the implementation follows established patterns for handling this PyArrow limitation.\r\n\r\n## Request for Testing\r\n@neevparikh Could you please test this fix with your original failing dataset? The implementation should resolve the nested data conversion error you encountered.<end_of_turn>\n<start_of_turn>model\nFix nested data conversions error in parquet loading (fixes #7793)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPython 3.10 CI is broken for `test_py310`.\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/6322990957/job/17169678812?pr=6262\r\n\r\n```\r\nFAILED tests/test_py_utils.py::TempSeedTest::test_tensorflow - ImportError: cannot import name 'context' from 'tensorflow.python' (/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/tensorflow/python/__init__.py)\r\n```\r\n\r\n```\r\n_________________________ TempSeedTest.test_tensorflow _________________________\r\n[gw1] linux -- Python 3.10.13 /opt/hostedtoolcache/Python/3.10.13/x64/bin/python\r\n\r\nself = <tests.test_py_utils.TempSeedTest testMethod=test_tensorflow>\r\n\r\n    @require_tf\r\n    def test_tensorflow(self):\r\n        import tensorflow as tf\r\n        from tensorflow.keras import layers\r\n    \r\n        model = layers.Dense(2)\r\n    \r\n        def gen_random_output():\r\n            x = tf.random.uniform((1, 3))\r\n            return model(x).numpy()\r\n    \r\n>       with temp_seed(42, set_tensorflow=True):\r\n\r\ntests/test_py_utils.py:155: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/contextlib.py:135: in __enter__\r\n    return next(self.gen)\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nseed = 42, set_pytorch = False, set_tensorflow = True\r\n\r\n    @contextmanager\r\n    def temp_seed(seed: int, set_pytorch=False, set_tensorflow=False):\r\n        \"\"\"Temporarily set the random seed. This works for python numpy, pytorch and tensorflow.\"\"\"\r\n        np_state = np.random.get_state()\r\n        np.random.seed(seed)\r\n    \r\n        if set_pytorch and config.TORCH_AVAILABLE:\r\n            import torch\r\n    \r\n            torch_state = torch.random.get_rng_state()\r\n            torch.random.manual_seed(seed)\r\n    \r\n            if torch.cuda.is_available():\r\n                torch_cuda_states = torch.cuda.get_rng_state_all()\r\n                torch.cuda.manual_seed_all(seed)\r\n    \r\n        if set_tensorflow and config.TF_AVAILABLE:\r\n            import tensorflow as tf\r\n>           from tensorflow.python import context as tfpycontext\r\nE           ImportError: cannot import name 'context' from 'tensorflow.python' (/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/tensorflow/python/__init__.py)\r\n\r\n/opt/hostedtoolcache/Python/3.10.13/x64/lib/python3.10/site-packages/datasets/utils/py_utils.py:257: ImportError\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken: ImportError: cannot import name 'context' from 'tensorflow.python'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis allows to stream datasets like [Major-TOM/Core-S2L2A](https://huggingface.co/datasets/Major-TOM/Core-S2L2A) which have row groups with few rows (one row is ~10MB). Previously the cold start would take a lot of time and OOM because it would download many row groups before yielding the first example.\r\n\r\nI tried on OpenOrca and imagenet-hard and it does't affect overall throughput.\r\n\r\nEven if the overall throughput doesn't change for datasets like imagenet-hard with big rows, note that it does create shorter and more frequent pauses to download the next row group. Though I find it fine because previously the pauses were less frequent but very long (downloading multiple row groups at a time)<end_of_turn>\n<start_of_turn>model\nBase parquet batch_size on parquet row group size<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix `test_convert_to_parquet` by patching `HfApi.preupload_lfs_files` and revert temporary fix:\r\n- #7074<end_of_turn>\n<start_of_turn>model\nFix CI test_convert_to_parquet<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTo make it easier to understand for users.\r\n\r\nThey can use \"path\" to specify a single path, <s>or \"paths\" to use a list of paths.</s>\r\n\r\nGlob patterns are still supported though\r\n<end_of_turn>\n<start_of_turn>model\nRename \"pattern\" to \"path\" in YAML data_files configs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nA simple implementation of a mechanism to resume an IterableDataset.\r\nIt works by restarting at the latest shard and skip samples. It provides fast resuming (though not instantaneous).\r\n\r\nExample:\r\n\r\n```python\r\nfrom datasets import Dataset, concatenate_datasets\r\n\r\n\r\nds = Dataset.from_dict({\"a\": range(5)}).to_iterable_dataset(num_shards=3)\r\nds = concatenate_datasets([ds] * 2)\r\n\r\nprint(f\"{ds.state_dict()=}\")\r\nfor i, example in enumerate(ds):\r\n    print(example)\r\n    if i == 6:\r\n        state_dict = ds.state_dict()\r\n        print(\"checkpoint\")\r\nds.load_state_dict(state_dict)\r\nprint(f\"resuming from checkpoint {ds.state_dict()=}\")\r\nfor example in ds:\r\n    print(example)\r\n```\r\n\r\nreturns\r\n\r\n```\r\nds.state_dict()={'ex_iterable_idx': 0, 'ex_iterables': [{'shard_idx': 0, 'shard_example_idx': 0}, {'shard_idx': 0, 'shard_example_idx': 0}]}\r\n{'a': 0}\r\n{'a': 1}\r\n{'a': 2}\r\n{'a': 3}\r\n{'a': 4}\r\n{'a': 0}\r\n{'a': 1}\r\ncheckpoint\r\n{'a': 2}\r\n{'a': 3}\r\n{'a': 4}\r\nresuming from checkpoint ds.state_dict()={'ex_iterable_idx': 1, 'ex_iterables': [{'shard_idx': 3, 'shard_example_idx': 0}, {'shard_idx': 0, 'shard_example_idx': 2}]}\r\n{'a': 2}\r\n{'a': 3}\r\n{'a': 4}\r\n```\r\n\r\nusing torchdata:\r\n```python\r\nfrom datasets import load_dataset\r\nfrom torchdata.stateful_dataloader import StatefulDataLoader\r\nmy_iterable_dataset = load_dataset(\"deepmind/code_contests\", streaming=True, split=\"train\")\r\ndataloader = StatefulDataLoader(my_iterable_dataset, batch_size=32, num_workers=4)\r\n# save in the middle of training\r\nstate_dict = dataloader.state_dict()\r\n# and resume later\r\ndataloader.load_state_dict(state_dict)\r\n```\r\n\r\ndocs: https://huggingface.co/docs/datasets/main/en/use_with_pytorch#checkpoint-and-resume<end_of_turn>\n<start_of_turn>model\n[Resumable IterableDataset] Add IterableDataset state_dict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nInferring the type seems to be unnecessary given that the pyarrow array has already been created.\r\nBecause pyarrow array creation is sometimes extremely slow this doubles the time write_batch takes.<end_of_turn>\n<start_of_turn>model\nRemove get_inferred_type from ArrowWriter write_batch<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6225 <end_of_turn>\n<start_of_turn>model\nRemove RGB -> BGR image conversion in Object Detection tutorial<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using `IterableDataset.from_spark()` with a Spark DataFrame containing image data, the `Image` feature class fails to properly process this data type, causing an `AttributeError: 'bytearray' object has no attribute 'get'`\n\n### Steps to reproduce the bug\n\n1. Create a Spark DataFrame with a column containing image data as bytearray objects\n2. Define a Feature schema with an Image feature\n3. Create an IterableDataset using `IterableDataset.from_spark()`\n4. Attempt to iterate through the dataset\n\n```\nfrom pyspark.sql import SparkSession\nfrom datasets import Dataset, IterableDataset, Features, Image, Value\n\n# initialize spark\nspark = SparkSession.builder.appName(\"MinimalRepro\").getOrCreate()\n\n# create spark dataframe\ndata = [(0, open(\"image.png\", \"rb\").read())]\ndf = spark.createDataFrame(data, \"idx: int, image: binary\")\n\n# convert to dataset\nfeatures = Features({\"idx\": Value(\"int64\"), \"image\": Image()})\nds = Dataset.from_spark(df, features=features)\nds_iter = IterableDataset.from_spark(df, features=features)\n\n# iterate\nprint(next(iter(ds)))\nprint(next(iter(ds_iter)))\n```\n\n### Expected behavior\n\nThe features should work on `IterableDataset` the same way they work on `Dataset`\n\n### Environment info\n\n- `datasets` version: 3.5.0\n- Platform: macOS-15.3.2-arm64-arm-64bit\n- Python version: 3.12.7\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n<end_of_turn>\n<start_of_turn>model\nImage Feature in Datasets Library Fails to Handle bytearray Objects from Spark DataFrames<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen working with large arrays, setting with_format to e.g. numpy then applying map causes a significant slowdown for iterable datasets.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport numpy as np\r\nimport time\r\nfrom datasets import Dataset, Features, Array3D\r\n\r\nfeatures=Features(**{\"array0\": Array3D((None, 10, 10), dtype=\"float32\"), \"array1\": Array3D((None,10,10), dtype=\"float32\")})\r\ndataset = Dataset.from_dict({f\"array{i}\": [np.zeros((x,10,10), dtype=np.float32) for x in [2000,1000]*25] for i in range(2)}, features=features)\r\n```\r\n\r\nThen\r\n```python\r\nds = dataset.to_iterable_dataset()\r\nds = ds.with_format(\"numpy\").map(lambda x: x)\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\nprint(t1-t0)\r\n```\r\n\r\ntakes 27 s, whereas\r\n\r\n```python\r\nds = dataset.to_iterable_dataset()\r\nds = ds.with_format(\"numpy\")\r\nds = dataset.to_iterable_dataset()\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\nprint(t1 - t0)\r\n```\r\n\r\ntakes ~1s\r\n\r\n### Expected behavior\r\n\r\nMap should not introduce a slowdown when formatting is enabled.\r\n\r\n### Environment info\r\n\r\n3.0.2<end_of_turn>\n<start_of_turn>model\nSlow iteration for iterable dataset with numpy formatting for array data<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nrelease: 4.2.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\nhttps://github.com/huggingface/datasets/blob/4.0.0/src/datasets/load.py#L1411-L1418\n\n```\n\ndef load_dataset(\n    ...\n    dl_manager: Optional[DownloadManager] = None,  # add this new argument\n    **config_kwargs,\n) -> Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset]:\n\n    ...\n\n    # Create a dataset builder\n    builder_instance = load_dataset_builder(\n        path=path,\n        name=name,\n        data_dir=data_dir,\n        data_files=data_files,\n        cache_dir=cache_dir,\n        features=features,\n        download_config=download_config,\n        download_mode=download_mode,\n        revision=revision,\n        token=token,\n        storage_options=storage_options,\n        **config_kwargs,\n    )\n\n    # Return iterable dataset in case of streaming\n    if streaming:\n        return builder_instance.as_streaming_dataset(split=split)\n\n    # Note: This is the revised part\n    if dl_manager is None:\n        if download_config is None:\n            download_config = DownloadConfig(\n                cache_dir=builder_instance._cache_downloaded_dir,\n                force_download=download_mode == DownloadMode.FORCE_REDOWNLOAD,\n                force_extract=download_mode == DownloadMode.FORCE_REDOWNLOAD,\n                use_etag=False,\n                num_proc=num_proc,\n                token=builder_instance.token,\n                storage_options=builder_instance.storage_options,\n            )  # We don't use etag for data files to speed up the process\n\n        dl_manager = DownloadManager(\n            dataset_name=builder_instance.dataset_name,\n            download_config=download_config,\n            data_dir=builder_instance.config.data_dir,\n            record_checksums=(\n                builder_instance._record_infos or verification_mode == VerificationMode.ALL_CHECKS\n            ),\n        )\n\n    # Download and prepare data\n    builder_instance.download_and_prepare(\n        download_config=download_config,\n        download_mode=download_mode,\n        verification_mode=verification_mode,\n        dl_manager=dl_manager,  # pass the new argument\n        num_proc=num_proc,\n        storage_options=storage_options,\n    )\n\n    ...\n\n```\n\n### Motivation\n\nIn my case, I'm hoping to deal with the cache files downloading manually (not using hash filenames and save to another location, or using potential existing local files).\n\n### Your contribution\n\nIt's already implemented above. If maintainers think this should be considered, I'll open a PR.<end_of_turn>\n<start_of_turn>model\nCustom `dl_manager` in `load_dataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\ndefaulting it to False is nice balance. we need manully setting it to True in certain scenarios!\n\nAdd `trust_remote_code` arg back please!\n\n### Motivation\n\ndefaulting it to False is nice balance. we need manully setting it to True in certain scenarios!\n\n\n### Your contribution\n\ndefaulting it to False is nice balance. we need manully setting it to True in certain scenarios!\n<end_of_turn>\n<start_of_turn>model\nDon't remove `trust_remote_code` arg!!!<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\ncan't use or download the nih exporter pile data. \r\n```\r\n15     experiment_compute_diveristy_coeff_single_dataset_then_combined_datasets_with_domain_weights()\r\n16   File \"/lfs/ampere1/0/brando9/beyond-scale-language-data-diversity/src/diversity/div_coeff.py\", line 474, in experiment_compute_diveristy_coeff_single_dataset_then_combined_datasets_with_domain_weights\r\n17     column_names = next(iter(dataset)).keys()\r\n18   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 1353, in __iter__\r\n19     for key, example in ex_iterable:\r\n20   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 207, in __iter__\r\n21     yield from self.generate_examples_fn(**self.kwargs)\r\n22   File \"/lfs/ampere1/0/brando9/.cache/huggingface/modules/datasets_modules/datasets/EleutherAI--pile/ebea56d358e91cf4d37b0fde361d563bed1472fbd8221a21b38fc8bb4ba554fb/pile.py\", line 236, in _generate_examples\r\n23     with zstd.open(open(files[subset], \"rb\"), \"rt\", encoding=\"utf-8\") as f:\r\n24   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/streaming.py\", line 74, in wrapper\r\n25     return function(*args, download_config=download_config, **kwargs)\r\n26   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py\", line 496, in xopen\r\n27     file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n28   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/core.py\", line 134, in open\r\n29     return self.__enter__()\r\n30   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/core.py\", line 102, in __enter__\r\n31     f = self.fs.open(self.path, mode=mode)\r\n32   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/spec.py\", line 1241, in open\r\n33     f = self._open(\r\n34   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/implementations/http.py\", line 356, in _open\r\n35     size = size or self.info(path, **kwargs)[\"size\"]\r\n36   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/asyn.py\", line 121, in wrapper\r\n37     return sync(self.loop, func, *args, **kwargs)\r\n38   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/asyn.py\", line 106, in sync\r\n39     raise return_result\r\n40   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/asyn.py\", line 61, in _runner\r\n41     result[0] = await coro\r\n42   File \"/lfs/ampere1/0/brando9/miniconda/envs/beyond_scale/lib/python3.10/site-packages/fsspec/implementations/http.py\", line 430, in _info\r\n43     raise FileNotFoundError(url) from exc\r\n44 FileNotFoundError: https://the-eye.eu/public/AI/pile_preliminary_components/NIH_ExPORTER_awarded_grant_text.jsonl.zst\r\n```\n\n### Steps to reproduce the bug\n\nrun this:\r\n```\r\nfrom datasets import load_dataset\r\npath, name = 'EleutherAI/pile', 'nih_exporter'\r\n\r\n# -- Get data set\r\ndataset = load_dataset(path, name, streaming=True, split=\"train\").with_format(\"torch\")\r\nbatch = dataset.take(512)\r\nprint(f'{batch=}')\r\n```\n\n### Expected behavior\n\nprint the batch\n\n### Environment info\n\n```\r\n(beyond_scale) brando9@ampere1:~/beyond-scale-language-data-diversity$ datasets-cli env\r\n\r\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.4.0-122-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n```<end_of_turn>\n<start_of_turn>model\nNIH exporter file not found<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nDifferently sized object is saved than it is loaded.\n\n### Steps to reproduce the bug\n\nHi, I save dataset in a following way:\r\n```\r\n        dataset = load_dataset(\"json\",\r\n                               data_files={\r\n                                   \"train\": os.path.join(input_folder, f\"{task_meta_type}_{task_type}_train.jsonl\"),\r\n                                   \"test\": os.path.join(input_folder, f\"{task_meta_type}_{task_type}_test.jsonl\")})\r\n        print(os.path.join(output_folder, f\"{task_meta_type}_{task_type}\"))\r\n        print(f\"Length of train dataset: {len(dataset['train'])}\")\r\n        print(f\"Length of test dataset: {len(dataset['test'])}\")\r\n        dataset.save_to_disk(os.path.join(output_folder, f\"{task_meta_type}_{task_type}\"))\r\n```\r\n\r\nthis yields output\r\n```\r\n.data/hf_dataset/propaganda_zanr\r\nLength of train dataset: 7642\r\nLength of test dataset: 1000\r\n```\r\n\r\nEverything looks fine.\r\n\r\nThen I load the dataset\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset_path = \".data/hf_dataset/propaganda_zanr\"\r\ndataset = load_dataset(dataset_path)\r\n\r\nprint(f\"Length of train dataset: {len(dataset['train'])}\")\r\nprint(f\"Length of test dataset: {len(dataset['test'])}\")\r\n\r\n```\r\n\r\nthis prints\r\n```\r\nGenerating train split: 1 examples [00:00, 72.10 examples/s]\r\nGenerating test split: 1 examples [00:00, 100.69 examples/s]\r\nLength of train dataset: 1\r\nLength of test dataset: 1\r\n```\r\n\r\nI dont' understand :(\r\n\n\n### Expected behavior\n\nsame object is loaded\n\n### Environment info\n\ndatasets==2.16.1<end_of_turn>\n<start_of_turn>model\nDifferently dataset object saved than it is loaded.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe issue shall be open: https://github.com/huggingface/datasets/issues/6435\r\n\r\nWhen i try to pass `with_rank` to `Dataset.filter()`, i get this:\r\n\r\n`Dataset.filter() got an unexpected keyword argument 'with_rank'`\n\n### Steps to reproduce the bug\n\nRun notebook:\r\n\r\nhttps://colab.research.google.com/drive/1WUNKph8BdP0on5ve3gQnh_PE0cFLQqTn?usp=sharing\n\n### Expected behavior\n\nShould work?\n\n### Environment info\n\nNVIDIA RTX 4090<end_of_turn>\n<start_of_turn>model\n`Dataset.filter` missing `with_rank` parameter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNot ElasticSearch :)<end_of_turn>\n<start_of_turn>model\n[DOCS] Fix typo: Elasticsearch<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses https://github.com/huggingface/datasets/issues/6604, closes https://github.com/huggingface/datasets/issues/2775<end_of_turn>\n<start_of_turn>model\nDedicated RNG object for fingerprinting<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ndon't import soundfile in tests<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nMissing video docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nSee https://huggingface.co/datasets/datasets-examples/doc-unsupported-1\r\n\r\nI would like to use CSV loader for the \"csv\" config, JSONL loader for the \"jsonl\" config, etc.\n\n### Motivation\n\nIt would be more flexible for the users\n\n### Your contribution\n\nNo specific contribution<end_of_turn>\n<start_of_turn>model\nSupport one dataset loader per config when using YAML<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nmove it to after the `trust_remote_code` check\r\n\r\nNote that it only affects local datasets that already exist on disk, not datasets loaded from HF directly<end_of_turn>\n<start_of_turn>model\nFix check_library_imports<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`datasets==4.0.0`\n```\ncolumns_to_return = ['input_ids','attention_mask', 'start_positions', 'end_positions']\ntrain_ds.set_format(type='tf', columns=columns_to_return)\n```\n`train_ds`:\n```\ntrain_ds type: <class 'datasets.arrow_dataset.Dataset'>, shape: (1000, 9)\ncolumns: ['question', 'sentences', 'answer', 'str_idx', 'end_idx', 'input_ids', 'attention_mask', 'start_positions', 'end_positions']\nfeatures:{'question': Value('string'), 'sentences': Value('string'), 'answer': Value('string'), 'str_idx': Value('int64'), 'end_idx': Value('int64'), 'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'start_positions': Value('int64'), 'end_positions': Value('int64')}\n```\n`train_ds_tensor = train_ds['start_positions'].to_tensor(shape=(-1,1))` hits the following error:\n```\nAttributeError: 'Column' object has no attribute 'to_tensor'\n```\n`tf.reshape(train_ds['start_positions'], shape=[-1,1])` hits the following error:\n```\nTypeError: Scalar tensor has no `len()`\n```<end_of_turn>\n<start_of_turn>model\nError processing scalar columns using tensorflow.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRefetch metadata files in case they were dropped by `filter_extensions` in the previous step. \r\n\r\nFix #6442 \r\n<end_of_turn>\n<start_of_turn>model\nFix metadata file resolution when inferred pattern is `**`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI wanted to select only some columns of data.\r\nAnd I thought that's why the argument `input_columns` exists.\r\nWhat I expected is like this:\r\nIf there are [\"a\", \"b\", \"c\", \"d\"] columns, and if I set `input_columns=[\"a\", \"d\"]`, the data will have only [\"a\", \"d\"] columns.\r\n\r\nBut it doesn't select columns.\r\nIt preserves existing columns.\r\nThe main cause is `update` function of `dictionary` type `transformed_batch`.\r\n\r\nhttps://github.com/huggingface/datasets/blob/682d21e94ab1e64c11b583de39dc4c93f0101c5a/src/datasets/iterable_dataset.py#L687-L691\r\n\r\n`transformed_batch` gets all the columns by `transformed_batch = dict(batch)`.\r\nEven `function_args` selects `input_columns`, `update` preserves columns other than `input_columns`.\r\nI think it should take a new dictionary with columns in `input_columns` like this:\r\n```\r\n# transformed_batch = dict(batch)\r\n# transformed_batch.update(self.function(*function_args, **self.fn_kwargs)\r\n\r\n# This is what I think correct.\r\ntransformed_batch = self.function(*function_args, **self.fn_kwargs)\r\n```\r\n\r\nLet me know how to use `input_columns`.\r\n\r\n### Steps to reproduce the bug\r\n\r\nDescribed all above.\r\n\r\n### Expected behavior\r\n\r\nDescribed all above.\r\n\r\n### Environment info\r\n\r\ndatasets: 2.12\r\npython: 3.8<end_of_turn>\n<start_of_turn>model\n`map` function doesn't fully utilize `input_columns`.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently our CI usually raises 404 errors when trying to delete temporary repositories. See, e.g.: https://github.com/huggingface/datasets/actions/runs/6314980985/job/17146507884\r\n```\r\nFAILED tests/test_upstream_hub.py::TestPushToHub::test_push_dataset_dict_to_hub_multiple_files_with_max_shard_size - huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6512fb99-4a52c561752ece3d77eb6d57;2b61cae4-613d-4a73-bbb1-2faf9e32b02d)\r\nRepository Not Found for url: https://hub-ci.huggingface.co/api/repos/delete.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\n\r\nFAILED tests/test_upstream_hub.py::TestPushToHub::test_push_dataset_to_hub_custom_features_audio - huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6512fbb2-0333dd666d42f0e173c2bb68;dfdc4271-b49b-4008-8c49-f05cf7c1d53d)\r\nRepository Not Found for url: https://hub-ci.huggingface.co/api/repos/delete.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\n\r\nFAILED tests/test_upstream_hub.py::TestPushToHub::test_push_dataset_dict_to_hub_custom_splits - huggingface_hub.utils._errors.RepositoryNotFoundError: 404 Client Error. (Request ID: Root=1-6512fbca-167690694f39770a5b3a444e;baeaa905-0a57-4585-ac97-9aaae12dd47d)\r\nRepository Not Found for url: https://hub-ci.huggingface.co/api/repos/delete.\r\nPlease make sure you specified the correct `repo_id` and `repo_type`.\r\nIf you are trying to access a private or gated repo, make sure you are authenticated.\r\n```\r\n\r\nI think this can be caused by collisions in temporary repository IDs because we create them in multiprocessing:\r\n```python\r\nwith temporary_repo(f\"{CI_HUB_USER}/test-{int(time.time() * 10e3)}\") as ds_name:\r\n```\r\nThis can also be caused when there is another issue that does not allow the creation of the repository, thus making it impossible to delete it.\r\n\r\nThis PR tries to fix this issue by increasing the precision of the number on the repository ID: `10e6` instead of `10e3`.\r\nAdditionally, this PR catches RepositoryNotFoundError.\r\n<end_of_turn>\n<start_of_turn>model\nFix CI 404 errors<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Problem\r\nThe current datasets.Dataset.map() example in the documentation demonstrates batched processing using a tokenizer object without defining or importing it. This causes a NameError when users copy and run the example as-is, breaking the expected seamless experience.\r\n\r\n## Correction\r\nThis PR fixes the issue by explicitly importing and initializing the tokenizer using the Transformers library (AutoTokenizer.from_pretrained(\"bert-base-uncased\")), making the example self-contained and runnable without errors.\r\nThis will help new users understand the workflow and apply the method correctly.\r\n\r\nCloses #7703 \r\n\r\n<end_of_turn>\n<start_of_turn>model\nFix map() example in datasets documentation: define tokenizer before use<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### System Info\n\nkaggle notebook\r\n\r\ni transformed dataset:\r\n```\r\ndataset = load_dataset(\"Dahoas/first-instruct-human-assistant-prompt\")\r\n```\r\nto \r\nformatted_dataset: \r\n```\r\nDataset({\r\n    features: ['message_tree_id', 'message_tree_text'],\r\n    num_rows: 33143\r\n})\r\n```\r\nbut would like to know how to upload to hub\n\n### Who can help?\n\n@ArthurZucker @younesbelkada\n\n### Information\n\n- [ ] The official example scripts\n- [ ] My own modified scripts\n\n### Tasks\n\n- [ ] An officially supported task in the `examples` folder (such as GLUE/SQuAD, ...)\n- [ ] My own task or dataset (give details below)\n\n### Reproduction\n\nshared above\n\n### Expected behavior\n\nload dataset to hub<end_of_turn>\n<start_of_turn>model\ncustom load dataset to hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n# Fix IterableDataset's state_dict shard_example_idx reporting\r\n\r\n## Description\r\n\r\nThis PR fixes issue #7475 where the `shard_example_idx` value in `IterableDataset`'s `state_dict()` always equals the number of samples in a shard, even if only a few examples have been consumed.\r\n\r\nThe issue is in the `_iter_arrow` method of the `ArrowExamplesIterable` class where it updates the `shard_example_idx` state by the full length of the batch (`len(pa_table)`) even when we're only partway through processing the examples. \r\n\r\n## Changes\r\n\r\nModified the `_iter_arrow` method of `ArrowExamplesIterable` to:\r\n1. Track the actual number of examples processed\r\n2. Only increment the `shard_example_idx` by the number of examples actually yielded\r\n3. Handle partial batches correctly\r\n\r\n## How to Test\r\n\r\nI've included a simple test case that demonstrates the fix:\r\n\r\n```python\r\nfrom datasets import Dataset\r\n\r\n# Create a test dataset\r\nds = Dataset.from_dict({\"a\": range(6)}).to_iterable_dataset(num_shards=1)\r\n\r\n# Iterate through part of the dataset\r\nfor idx, example in enumerate(ds):\r\n    print(example)\r\n    if idx == 2:  # Stop after 3 examples (0, 1, 2)\r\n        state_dict = ds.state_dict()\r\n        print(\"Checkpoint state_dict:\", state_dict)\r\n        break\r\n\r\n# Before the fix, the output would show shard_example_idx: 6\r\n# After the fix, it shows shard_example_idx: 3, correctly reflecting the 3 processed examples\r\n```\r\n\r\n## Implementation Details\r\n\r\n1. Added logic to track the number of examples actually seen in the current shard\r\n2. Modified the state update to only count examples actually yielded\r\n3. Improved handling of partial batches and skipped examples\r\n\r\nThis fix ensures that checkpointing and resuming works correctly with exactly the expected number of examples, rather than skipping ahead to the end of the batch.<end_of_turn>\n<start_of_turn>model\nFix IterableDataset state_dict shard_example_idx counting<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to run some trainings on Google's TPUs using Huggingface's DataLoader on [SlimPajama-627B](https://huggingface.co/datasets/cerebras/SlimPajama-627B) and [c4](https://huggingface.co/datasets/allenai/c4), but I end up running into `429 Client Error: Too Many Requests for URL` error when I call `load_dataset`. The even odder part is that I am able to sucessfully run trainings with the [wikitext dataset](https://huggingface.co/datasets/Salesforce/wikitext). Is there something I need to setup to specifically train with SlimPajama or C4 with TPUs because I am not clear why I am getting these errors.\r\n\r\n\n\n### Steps to reproduce the bug\n\nThese are the commands you could run to produce the error below but you will require a ClearML account (you can create one [here](https://app.clear.ml/login?redirect=%2Fdashboard)) with a queue setup to run on Google TPUs\r\n```bash\r\ngit clone https://github.com/clankur/muGPT.git\r\ncd muGPT\r\npython -m train --config-name=slim_v4-32_84m.yaml +training.queue={NAME_OF_CLEARML_QUEUE}\r\n```\r\n\r\nThe error I see:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/clearml/binding/hydra_bind.py\", line 230, in _patched_task_function\r\n    return task_function(a_config, *a_args, **a_kwargs)\r\n  File \"/home/clankur/.clearml/venvs-builds/3.10/task_repository/muGPT.git/train.py\", line 1037, in main\r\n    main_contained(config, logger)\r\n  File \"/home/clankur/.clearml/venvs-builds/3.10/task_repository/muGPT.git/train.py\", line 840, in main_contained\r\n    loader = get_loader(\"train\", config.training_data, config.training.tokens)\r\n  File \"/home/clankur/.clearml/venvs-builds/3.10/task_repository/muGPT.git/input_loader.py\", line 549, in get_loader\r\n    return HuggingFaceDataLoader(split, config, token_batch_params)\r\n  File \"/home/clankur/.clearml/venvs-builds/3.10/task_repository/muGPT.git/input_loader.py\", line 395, in __init__\r\n    self.dataset = load_dataset(\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/load.py\", line 2112, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/load.py\", line 1798, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/load.py\", line 1495, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/load.py\", line 1479, in dataset_module_factory\r\n    ).get_module()\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/load.py\", line 1034, in get_module\r\n    else get_data_patterns(base_path, download_config=self.download_config)\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/data_files.py\", line 457, in get_data_patterns\r\n    return _get_data_files_patterns(resolver)\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/data_files.py\", line 248, in _get_data_files_patterns\r\n    data_files = pattern_resolver(pattern)\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/datasets/data_files.py\", line 340, in resolve_pattern\r\n    for filepath, info in fs.glob(pattern, detail=True).items()\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 409, in glob\r\n    return super().glob(path, **kwargs)\r\n  File \"/home/clankur/.clearml/venvs-builds/3.10/lib/python3.10/site-packages/fsspec/spec.py\", line 602, in glob\r\n    allpaths = self.find(root, maxdepth=depth, withdirs=True, detail=True, **kwargs)\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 429, in find\r\n    out = self._ls_tree(path, recursive=True, refresh=refresh, revision=resolved_path.revision, **kwargs)\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 358, in _ls_tree\r\n    self._ls_tree(\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/huggingface_hub/hf_file_system.py\", line 375, in _ls_tree\r\n    for path_info in tree:\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 3080, in list_repo_tree\r\n    for path_info in paginate(path=tree_url, headers=headers, params={\"recursive\": recursive, \"expand\": expand}):\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/huggingface_hub/utils/_pagination.py\", line 46, in paginate\r\n    hf_raise_for_status(r)\r\n  File \"/home/clankur/conda/envs/jax/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 477, in hf_raise_for_status\r\n    raise _format(HfHubHTTPError, str(e), response) from e\r\nhuggingface_hub.errors.HfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/cerebras/SlimPajama-627B/tree/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543?recursive=True&expand=True&cursor=ZXlKbWFXeGxYMjVoYldVaU9pSjBaWE4wTDJOb2RXNXJNUzlsZUdGdGNHeGxYMmh2YkdSdmRYUmZPVFEzTG1wemIyNXNMbnB6ZENKOTo2MjUw (Request ID: Root=1-67673de9-1413900606ede7712b08ef2c;1304c09c-3e69-4222-be14-f10ee709d49c)\r\nmaximum queue size reached\r\nSet the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\r\n```\n\n### Expected behavior\n\nI'd expect the DataLoader to load from the SlimPajama-627B and c4 dataset without issue.\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-5.8.0-1035-gcp-x86_64-with-glibc2.31\r\n- Python version: 3.10.16\r\n- Huggingface_hub version: 0.26.5\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3<end_of_turn>\n<start_of_turn>model\nHfHubHTTPError: 429 Client Error: Too Many Requests for URL when trying to access SlimPajama-627B or c4 on TPUs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSetting _format_type to None, should return plain python object but as of 4.0.0 returns Column. This fails in libs such as sentencetransformers (such as in generation of hard negatives) where plain python is expected.<end_of_turn>\n<start_of_turn>model\nFixing __getitem__ of datasets which behaves inconsistent to documentation when setting _format_type to None<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to fine-tune llama2-7b model in GCP. The notebook I am using for this can be found [here](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_peft_finetuning.ipynb).  \r\n\r\nWhen I use the dataset given in the example, the training gets successfully completed (example dataset can be found [here](https://huggingface.co/datasets/timdettmers/openassistant-guanaco)).  \r\nHowever when I use my own dataset which is in the same format as the example dataset, I get the below error (my dataset can be found [here](https://huggingface.co/datasets/kk2491/finetune_dataset_002)).   \r\n\r\n![image](https://github.com/huggingface/datasets/assets/38481564/47fa2de3-95e0-478b-a35f-58cbaf90427a)  \r\n\r\nI see the files are being read correctly from the logs:  \r\n\r\n![image](https://github.com/huggingface/datasets/assets/38481564/b0b6316c-2cc7-476c-9674-ca2222c8f4e3)\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. Clone the [vertex-ai-samples](https://github.com/GoogleCloudPlatform/vertex-ai-samples) repository. \r\n2. Run the [llama2-7b peft fine-tuning](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_pytorch_llama2_peft_finetuning.ipynb).\r\n3. Change the dataset `kk2491/finetune_dataset_002`   \n\n### Expected behavior\n\nThe training should complete successfully, and model gets deployed to an endpoint. \r\n\r\n\n\n### Environment info\n\nPython version : Python 3.10.12\r\nDataset : https://huggingface.co/datasets/kk2491/finetune_dataset_002  \r\n\r\n<end_of_turn>\n<start_of_turn>model\nIndexError: Invalid key: 0 is out of bounds for size 0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7837\r\n\r\nWe currently have the [mono attribute for Audio documented](https://github.com/huggingface/datasets/blob/41c05299348a499807432ab476e1cdc4143c8772/src/datasets/features/audio.py#L52C1-L54C22) but not used anywhere resulting in confusion for users. Since torchcodec does not know this attribute I suggest using `num_channels` (currently supported `None` (leave unchanged), mono: `1`, stereo: `2`).\r\n\r\nI could also add a mono attribute but found that to be more confusing for developers and would restrict us if at any point in the future more than 2 channels are supported.<end_of_turn>\n<start_of_turn>model\nAdd num channels to audio<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee: https://github.com/huggingface/datasets/actions/runs/10095313567/job/27915185756\r\n```\r\nFAILED tests/test_hub.py::test_convert_to_parquet - huggingface_hub.utils._errors.RevisionNotFoundError: 404 Client Error. (Request ID: Root=1-66a25839-31ce7b475e70e7db1e4d44c2;b0c8870f-d5ef-4bf2-a6ff-0191f3df0f64)\r\n\r\nRevision Not Found for url: https://hub-ci.huggingface.co/api/datasets/__DUMMY_TRANSFORMERS_USER__/test-dataset-5188a8-17219154347516/preupload/refs%2Fpr%2F1.\r\nInvalid rev id: refs/pr/1\r\n```\r\n```\r\n /opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/datasets/hub.py:86: in convert_to_parquet\r\n    dataset.push_to_hub(\r\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/datasets/dataset_dict.py:1722: in push_to_hub\r\n    split_additions, uploaded_size, dataset_nbytes = self[split]._push_parquet_shards_to_hub(\r\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/datasets/arrow_dataset.py:5511: in _push_parquet_shards_to_hub\r\n    api.preupload_lfs_files(\r\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/huggingface_hub/hf_api.py:4231: in preupload_lfs_files\r\n    _fetch_upload_modes(\r\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118: in _inner_fn\r\n    return fn(*args, **kwargs)\r\n/opt/hostedtoolcache/Python/3.8.18/x64/lib/python3.8/site-packages/huggingface_hub/_commit_api.py:507: in _fetch_upload_modes\r\n    hf_raise_for_status(resp)\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken for convert_to_parquet: Invalid rev id: refs/pr/1 404 error causes RevisionNotFoundError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfix #7129 \r\n\r\n@stevhliu<end_of_turn>\n<start_of_turn>model\nUpdated inconsistent output in documentation examples for `ClassLabel`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nConcurrent processes modifying and pushing a dataset can overwrite each others' dataset card, leaving the dataset unusable.\n\nConsider this scenario:\n- we have an Arrow dataset\n- there are `N` configs of the dataset\n- there are `N` independent processes operating on each of the individual configs (e.g. adding a column, `new_col`)\n- each process calls `push_to_hub` on their particular config when they're done processing\n- all calls to `push_to_hub` succeed\n- the `README.md` now has some configs with `new_col` added and some with `new_col` missing\n\nAny attempt to load a config (using `load_dataset`) where `new_col` is missing will fail because of a schema mismatch between `README.md` and the Arrow files. Fixing the dataset requires updating `README.md` by hand with the correct schema for the affected config. In effect, `push_to_hub` is doing a `git push --force` (I found this behavior quite surprising).\n\nWe have hit this issue every time we run processing jobs over our datasets and have to fix corrupted schemas by hand.\n\nReading through the code, it seems that specifying a [`parent_commit`](https://github.com/huggingface/huggingface_hub/blob/v0.32.4/src/huggingface_hub/hf_api.py#L4587) hash around here https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_dataset.py#L5794 would get us to a normal, non-forced git push, and avoid schema corruption. I'm not familiar enough with the code to know how to determine the commit hash from which the in-memory dataset card was loaded.\n\n### Steps to reproduce the bug\n\nSee above.\n\n### Expected behavior\n\nConcurrent edits to disjoint configs of a dataset should never corrupt the dataset schema.\n\n### Environment info\n\n- `datasets` version: 2.20.0\n- Platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\n- Python version: 3.10.14\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.2\n- `fsspec` version: 2023.9.0<end_of_turn>\n<start_of_turn>model\n`push_to_hub` is not concurrency safe (dataset schema corruption)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nbefore the release, to make sure everything works fine<end_of_turn>\n<start_of_turn>model\nupdate torchcodec in ci<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHi!\n\n#Dataset\n\nI\u2019m currently uploading a dataset that includes an `\"image\"` column (PNG files), along with some metadata columns. The dataset is loaded from a .jsonl file. My goal is to have the \"image\" column appear as the first column in the dataset card preview UI on the :hugs: Hub.\n\nHowever, at the moment, the `\"image\"` column is not the first\u2014in fact, it appears last, which is not ideal for the presentation I\u2019d like to achieve.\n\nI have a couple of questions:\n\nIs there a way to force the dataset card to display the `\"image\"` column first?\nIs there currently any way to control or influence the column order in the dataset preview UI?\nDoes the order of keys in the .jsonl file or the features argument affect the display order?\nThanks again for your time and help! :blush:<end_of_turn>\n<start_of_turn>model\n#Dataset Make \"image\" column appear first in dataset preview UI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSet temporary numpy upper version < 2.0.0 to fix CI. See: https://github.com/huggingface/datasets/actions/runs/9546031216/job/26308072017\r\n```\r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.0.0 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.\r\n```<end_of_turn>\n<start_of_turn>model\nSet temporary numpy upper version < 2.0.0 to fix CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSo I am getting this bug when I try to run cell 4 of the Chapter 6 notebook code:\r\n`dataset = load_dataset(\"ccdv/cnn_dailymail\", version=\"3.0.0\")`\r\n\r\nError Message:\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\nCell In[4], line 4\r\n      1 #hide_output\r\n      2 from datasets import load_dataset\r\n----> 4 dataset = load_dataset(\"ccdv/cnn_dailymail\", version=\"3.0.0\")\r\n      7 # dataset = load_dataset(\"ccdv/cnn_dailymail\", version=\"3.0.0\", trust_remote_code=True)\r\n      8 print(f\"Features: {dataset['train'].column_names}\")\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\load.py:2587, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2583 # Build dataset for splits\r\n   2584 keep_in_memory = (\r\n   2585     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2586 )\r\n-> 2587 ds = builder_instance.as_dataset(split=split, verification_mode=verification_mode, in_memory=keep_in_memory)\r\n   2588 # Rename and cast features to match task schema\r\n   2589 if task is not None:\r\n   2590     # To avoid issuing the same warning twice\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\builder.py:1244, in DatasetBuilder.as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\r\n   1241 verification_mode = VerificationMode(verification_mode or VerificationMode.BASIC_CHECKS)\r\n   1243 # Create a dataset for each of the given splits\r\n-> 1244 datasets = map_nested(\r\n   1245     partial(\r\n   1246         self._build_single_dataset,\r\n   1247         run_post_process=run_post_process,\r\n   1248         verification_mode=verification_mode,\r\n   1249         in_memory=in_memory,\r\n   1250     ),\r\n   1251     split,\r\n   1252     map_tuple=True,\r\n   1253     disable_tqdm=True,\r\n   1254 )\r\n   1255 if isinstance(datasets, dict):\r\n   1256     datasets = DatasetDict(datasets)\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\utils\\py_utils.py:477, in map_nested(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, types, disable_tqdm, desc)\r\n    466     mapped = [\r\n    467         map_nested(\r\n    468             function=function,\r\n   (...)\r\n    474         for obj in iterable\r\n    475     ]\r\n    476 elif num_proc != -1 and num_proc <= 1 or len(iterable) < parallel_min_length:\r\n--> 477     mapped = [\r\n    478         _single_map_nested((function, obj, types, None, True, None))\r\n    479         for obj in hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    480     ]\r\n    481 else:\r\n    482     with warnings.catch_warnings():\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\utils\\py_utils.py:478, in <listcomp>(.0)\r\n    466     mapped = [\r\n    467         map_nested(\r\n    468             function=function,\r\n   (...)\r\n    474         for obj in iterable\r\n    475     ]\r\n    476 elif num_proc != -1 and num_proc <= 1 or len(iterable) < parallel_min_length:\r\n    477     mapped = [\r\n--> 478         _single_map_nested((function, obj, types, None, True, None))\r\n    479         for obj in hf_tqdm(iterable, disable=disable_tqdm, desc=desc)\r\n    480     ]\r\n    481 else:\r\n    482     with warnings.catch_warnings():\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\utils\\py_utils.py:370, in _single_map_nested(args)\r\n    368 # Singleton first to spare some computation\r\n    369 if not isinstance(data_struct, dict) and not isinstance(data_struct, types):\r\n--> 370     return function(data_struct)\r\n    372 # Reduce logging to keep things readable in multiprocessing with tqdm\r\n    373 if rank is not None and logging.get_verbosity() < logging.WARNING:\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\builder.py:1274, in DatasetBuilder._build_single_dataset(self, split, run_post_process, verification_mode, in_memory)\r\n   1271     split = Split(split)\r\n   1273 # Build base dataset\r\n-> 1274 ds = self._as_dataset(\r\n   1275     split=split,\r\n   1276     in_memory=in_memory,\r\n   1277 )\r\n   1278 if run_post_process:\r\n   1279     for resource_file_name in self._post_processing_resources(split).values():\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\builder.py:1348, in DatasetBuilder._as_dataset(self, split, in_memory)\r\n   1346 if self._check_legacy_cache():\r\n   1347     dataset_name = self.name\r\n-> 1348 dataset_kwargs = ArrowReader(cache_dir, self.info).read(\r\n   1349     name=dataset_name,\r\n   1350     instructions=split,\r\n   1351     split_infos=self.info.splits.values(),\r\n   1352     in_memory=in_memory,\r\n   1353 )\r\n   1354 fingerprint = self._get_dataset_fingerprint(split)\r\n   1355 return Dataset(fingerprint=fingerprint, **dataset_kwargs)\r\n\r\nFile ~\\anaconda3\\envs\\nlp-transformers\\lib\\site-packages\\datasets\\arrow_reader.py:254, in BaseReader.read(self, name, instructions, split_infos, in_memory)\r\n    252 if not files:\r\n    253     msg = f'Instruction \"{instructions}\" corresponds to no data!'\r\n--> 254     raise ValueError(msg)\r\n    255 return self.read_files(files=files, original_instructions=instructions, in_memory=in_memory)\r\n\r\n**ValueError: Instruction \"validation\" corresponds to no data!**\r\n````\r\n\r\nLooks like the data is not being loaded. Any advice would be appreciated. Thanks!\r\n\n\n### Steps to reproduce the bug\n\nRun all cells of Chapter 6 notebook.\n\n### Expected behavior\n\nData should load correctly without any errors.\n\n### Environment info\n\n- `datasets` version: 2.17.0\r\n- Platform: Windows-10-10.0.19045-SP0\r\n- Python version: 3.9.18\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nChapter 6 - Issue Loading `cnn_dailymail` dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSeems `datasets.load_dataset` raises FileNotFoundError for some hub datasets with the latest `datasets==2.16.0`\n\n### Steps to reproduce the bug\n\nFor example `pip install datasets==2.16.0`\r\nthen\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"wentingzhao/anthropic-hh-first-prompt\", cache_dir='cache1')[\"train\"]\r\n\r\n```\r\n\r\nThis will raise:\r\n\r\n```bash\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/load.py\", line 2545, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/builder.py\", line 1003, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/builder.py\", line 1076, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 43, in _split_generators\r\n    data_files = dl_manager.download_and_extract(self.config.data_files)\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/download/download_manager.py\", line 566, in download_and_extract\r\n    return self.extract(self.download(url_or_urls))\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/download/download_manager.py\", line 539, in extract\r\n    extracted_paths = map_nested(\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 466, in map_nested\r\n    mapped = [\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 467, in <listcomp>\r\n    _single_map_nested((function, obj, types, None, True, None))\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 387, in _single_map_nested\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 387, in <listcomp>\r\n    mapped = [_single_map_nested((function, v, types, None, True, None)) for v in pbar]\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/utils/py_utils.py\", line 370, in _single_map_nested\r\n    return function(data_struct)\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/download/download_manager.py\", line 451, in _download\r\n    out = cached_path(url_or_filename, download_config=download_config)\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/utils/file_utils.py\", line 188, in cached_path\r\n    output_path = get_from_cache(\r\n  File \"/Users/xxx/miniconda3/envs/env/lib/python3.9/site-packages/datasets/utils/file_utils.py\", line 570, in get_from_cache\r\n    raise FileNotFoundError(f\"Couldn't find file at {url}\")\r\nFileNotFoundError: Couldn't find file at https://huggingface.co/datasets/wentingzhao/anthropic-hh-first-prompt/resolve/11b393a5545f706a357ebcd4a5285d93db176715/cache1/downloads/87d66c365626feca116cba323c4856c9aae056e4503f09f23e34aa085eb9de15\r\n```\r\n\r\nHowever, seems it works fine for some datasets, for example, if works fine for `datasets.load_dataset(\"ag_news\", cache_dir='cache2')[\"test\"]`\r\n\r\nBut the dataset works fine for datasets==2.15.0, for example `pip install datasets==2.15.0`,\r\nthen\r\n\r\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset(\"wentingzhao/anthropic-hh-first-prompt\", cache_dir='cache3')[\"train\"]\r\n\r\nDataset({\r\n    features: ['user', 'system', 'source'],\r\n    num_rows: 8552\r\n})\r\n```\r\n\r\n\r\n\n\n### Expected behavior\n\n2.16.0 should work as same as 2.15.0 for all datasets\n\n### Environment info\n\npython3.9\r\nconda env\r\ntested on MacOS and Linux<end_of_turn>\n<start_of_turn>model\ndatasets.load_dataset raises FileNotFoundError for datasets==2.16.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove deprecated `verbose` parameter from CSV builder.\r\n\r\nNote that the `verbose` parameter is deprecated since pandas 2.2.0. See:\r\n- https://github.com/pandas-dev/pandas/pull/56556\r\n- https://github.com/pandas-dev/pandas/pull/57450 \r\n\r\nFix #6671.<end_of_turn>\n<start_of_turn>model\nRemove deprecated verbose parameter from CSV builder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAttributeError                            Traceback (most recent call last)\r\nCell In[39], line 2\r\n      1 # Start the training process\r\n----> 2 trainer.train()\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1539, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\r\n   1537         hf_hub_utils.enable_progress_bars()\r\n   1538 else:\r\n-> 1539     return inner_training_loop(\r\n   1540         args=args,\r\n   1541         resume_from_checkpoint=resume_from_checkpoint,\r\n   1542         trial=trial,\r\n   1543         ignore_keys_for_eval=ignore_keys_for_eval,\r\n   1544     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1836, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\r\n   1833     rng_to_sync = True\r\n   1835 step = -1\r\n-> 1836 for step, inputs in enumerate(epoch_iterator):\r\n   1837     total_batched_samples += 1\r\n   1839     if self.args.include_num_input_tokens_seen:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:451, in DataLoaderShard.__iter__(self)\r\n    449 # We iterate one batch ahead to check when we are at the end\r\n    450 try:\r\n--> 451     current_batch = next(dataloader_iter)\r\n    452 except StopIteration:\r\n    453     yield\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630, in _BaseDataLoaderIter.__next__(self)\r\n    627 if self._sampler_iter is None:\r\n    628     # TODO([https://github.com/pytorch/pytorch/issues/76750)](https://github.com/pytorch/pytorch/issues/76750)%3C/span%3E)\r\n    629     self._reset()  # type: ignore[call-arg]\r\n--> 630 data = self._next_data()\r\n    631 self._num_yielded += 1\r\n    632 if self._dataset_kind == _DatasetKind.Iterable and \\\r\n    633         self._IterableDataset_len_called is not None and \\\r\n    634         self._num_yielded > self._IterableDataset_len_called:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674, in _SingleProcessDataLoaderIter._next_data(self)\r\n    672 def _next_data(self):\r\n    673     index = self._next_index()  # may raise StopIteration\r\n--> 674     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n    675     if self._pin_memory:\r\n    676         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\r\n     49         data = self.dataset.__getitems__(possibly_batched_index)\r\n     50     else:\r\n---> 51         data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     52 else:\r\n     53     data = self.dataset[possibly_batched_index]\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51, in <listcomp>(.0)\r\n     49         data = self.dataset.__getitems__(possibly_batched_index)\r\n     50     else:\r\n---> 51         data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     52 else:\r\n     53     data = self.dataset[possibly_batched_index]\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1764, in Dataset.__getitem__(self, key)\r\n   1762 def __getitem__(self, key):  # noqa: F811\r\n   1763     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 1764     return self._getitem(\r\n   1765         key,\r\n   1766     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1749, in Dataset._getitem(self, key, decoded, **kwargs)\r\n   1747 formatter = get_formatter(format_type, features=self.features, decoded=decoded, **format_kwargs)\r\n   1748 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 1749 formatted_output = format_table(\r\n   1750     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   1751 )\r\n   1752 return formatted_output\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:540, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    538 else:\r\n    539     pa_table_to_format = pa_table.drop(col for col in pa_table.column_names if col not in format_columns)\r\n--> 540     formatted_output = formatter(pa_table_to_format, query_type=query_type)\r\n    541     if output_all_columns:\r\n    542         if isinstance(formatted_output, MutableMapping):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:281, in Formatter.__call__(self, pa_table, query_type)\r\n    279 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    280     if query_type == \"row\":\r\n--> 281         return self.format_row(pa_table)\r\n    282     elif query_type == \"column\":\r\n    283         return self.format_column(pa_table)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/torch_formatter.py:57, in TorchFormatter.format_row(self, pa_table)\r\n     56 def format_row(self, pa_table: pa.Table) -> dict:\r\n---> 57     row = self.numpy_arrow_extractor().extract_row(pa_table)\r\n     58     return self.recursive_tensorize(row)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:154, in NumpyArrowExtractor.extract_row(self, pa_table)\r\n    153 def extract_row(self, pa_table: pa.Table) -> dict:\r\n--> 154     return _unnest(self.extract_batch(pa_table))\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:160, in NumpyArrowExtractor.extract_batch(self, pa_table)\r\n    159 def extract_batch(self, pa_table: pa.Table) -> dict:\r\n--> 160     return {col: self._arrow_array_to_numpy(pa_table[col]) for col in pa_table.column_names}\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:160, in <dictcomp>(.0)\r\n    159 def extract_batch(self, pa_table: pa.Table) -> dict:\r\n--> 160     return {col: self._arrow_array_to_numpy(pa_table[col]) for col in pa_table.column_names}\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:196, in NumpyArrowExtractor._arrow_array_to_numpy(self, pa_array)\r\n    194         array: List = pa_array.to_numpy(zero_copy_only=zero_copy_only).tolist()\r\n    195 if len(array) > 0:\r\n--> 196     if any(\r\n    197         (isinstance(x, np.ndarray) and (x.dtype == np.object or x.shape != array[0].shape))\r\n    198         or (isinstance(x, float) and np.isnan(x))\r\n    199         for x in array\r\n    200     ):\r\n    201         return np.array(array, copy=False, **{**self.np_array_kwargs, \"dtype\": np.object})\r\n    202 return np.array(array, copy=False, **self.np_array_kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:197, in <genexpr>(.0)\r\n    194         array: List = pa_array.to_numpy(zero_copy_only=zero_copy_only).tolist()\r\n    195 if len(array) > 0:\r\n    196     if any(\r\n--> 197         (isinstance(x, np.ndarray) and (x.dtype == np.object or x.shape != array[0].shape))\r\n    198         or (isinstance(x, float) and np.isnan(x))\r\n    199         for x in array\r\n    200     ):\r\n    201         return np.array(array, copy=False, **{**self.np_array_kwargs, \"dtype\": np.object})\r\n    202 return np.array(array, copy=False, **self.np_array_kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/numpy/__init__.py:324, in __getattr__(attr)\r\n    319     warnings.warn(\r\n    320         f\"In the future `np.{attr}` will be defined as the \"\r\n    321         \"corresponding NumPy scalar.\", FutureWarning, stacklevel=2)\r\n    323 if attr in __former_attrs__:\r\n--> 324     raise AttributeError(__former_attrs__[attr])\r\n    326 if attr == 'testing':\r\n    327     import numpy.testing as testing\r\n\r\nAttributeError: module 'numpy' has no attribute 'object'.\r\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\r\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecationsAttributeError                            Traceback (most recent call last)\r\nCell In[39], line 2\r\n      1 # Start the training process\r\n----> 2 trainer.train()\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1539, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\r\n   1537         hf_hub_utils.enable_progress_bars()\r\n   1538 else:\r\n-> 1539     return inner_training_loop(\r\n   1540         args=args,\r\n   1541         resume_from_checkpoint=resume_from_checkpoint,\r\n   1542         trial=trial,\r\n   1543         ignore_keys_for_eval=ignore_keys_for_eval,\r\n   1544     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1836, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\r\n   1833     rng_to_sync = True\r\n   1835 step = -1\r\n-> 1836 for step, inputs in enumerate(epoch_iterator):\r\n   1837     total_batched_samples += 1\r\n   1839     if self.args.include_num_input_tokens_seen:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:451, in DataLoaderShard.__iter__(self)\r\n    449 # We iterate one batch ahead to check when we are at the end\r\n    450 try:\r\n--> 451     current_batch = next(dataloader_iter)\r\n    452 except StopIteration:\r\n    453     yield\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630, in _BaseDataLoaderIter.__next__(self)\r\n    627 if self._sampler_iter is None:\r\n    628     # TODO([https://github.com/pytorch/pytorch/issues/76750)](https://github.com/pytorch/pytorch/issues/76750)%3C/span%3E)\r\n    629     self._reset()  # type: ignore[call-arg]\r\n--> 630 data = self._next_data()\r\n    631 self._num_yielded += 1\r\n    632 if self._dataset_kind == _DatasetKind.Iterable and \\\r\n    633         self._IterableDataset_len_called is not None and \\\r\n    634         self._num_yielded > self._IterableDataset_len_called:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:674, in _SingleProcessDataLoaderIter._next_data(self)\r\n    672 def _next_data(self):\r\n    673     index = self._next_index()  # may raise StopIteration\r\n--> 674     data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\r\n    675     if self._pin_memory:\r\n    676         data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51, in _MapDatasetFetcher.fetch(self, possibly_batched_index)\r\n     49         data = self.dataset.__getitems__(possibly_batched_index)\r\n     50     else:\r\n---> 51         data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     52 else:\r\n     53     data = self.dataset[possibly_batched_index]\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51, in <listcomp>(.0)\r\n     49         data = self.dataset.__getitems__(possibly_batched_index)\r\n     50     else:\r\n---> 51         data = [self.dataset[idx] for idx in possibly_batched_index]\r\n     52 else:\r\n     53     data = self.dataset[possibly_batched_index]\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1764, in Dataset.__getitem__(self, key)\r\n   1762 def __getitem__(self, key):  # noqa: F811\r\n   1763     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 1764     return self._getitem(\r\n   1765         key,\r\n   1766     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/arrow_dataset.py:1749, in Dataset._getitem(self, key, decoded, **kwargs)\r\n   1747 formatter = get_formatter(format_type, features=self.features, decoded=decoded, **format_kwargs)\r\n   1748 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 1749 formatted_output = format_table(\r\n   1750     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   1751 )\r\n   1752 return formatted_output\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:540, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    538 else:\r\n    539     pa_table_to_format = pa_table.drop(col for col in pa_table.column_names if col not in format_columns)\r\n--> 540     formatted_output = formatter(pa_table_to_format, query_type=query_type)\r\n    541     if output_all_columns:\r\n    542         if isinstance(formatted_output, MutableMapping):\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:281, in Formatter.__call__(self, pa_table, query_type)\r\n    279 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    280     if query_type == \"row\":\r\n--> 281         return self.format_row(pa_table)\r\n    282     elif query_type == \"column\":\r\n    283         return self.format_column(pa_table)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/torch_formatter.py:57, in TorchFormatter.format_row(self, pa_table)\r\n     56 def format_row(self, pa_table: pa.Table) -> dict:\r\n---> 57     row = self.numpy_arrow_extractor().extract_row(pa_table)\r\n     58     return self.recursive_tensorize(row)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:154, in NumpyArrowExtractor.extract_row(self, pa_table)\r\n    153 def extract_row(self, pa_table: pa.Table) -> dict:\r\n--> 154     return _unnest(self.extract_batch(pa_table))\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:160, in NumpyArrowExtractor.extract_batch(self, pa_table)\r\n    159 def extract_batch(self, pa_table: pa.Table) -> dict:\r\n--> 160     return {col: self._arrow_array_to_numpy(pa_table[col]) for col in pa_table.column_names}\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:160, in <dictcomp>(.0)\r\n    159 def extract_batch(self, pa_table: pa.Table) -> dict:\r\n--> 160     return {col: self._arrow_array_to_numpy(pa_table[col]) for col in pa_table.column_names}\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:196, in NumpyArrowExtractor._arrow_array_to_numpy(self, pa_array)\r\n    194         array: List = pa_array.to_numpy(zero_copy_only=zero_copy_only).tolist()\r\n    195 if len(array) > 0:\r\n--> 196     if any(\r\n    197         (isinstance(x, np.ndarray) and (x.dtype == np.object or x.shape != array[0].shape))\r\n    198         or (isinstance(x, float) and np.isnan(x))\r\n    199         for x in array\r\n    200     ):\r\n    201         return np.array(array, copy=False, **{**self.np_array_kwargs, \"dtype\": np.object})\r\n    202 return np.array(array, copy=False, **self.np_array_kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/formatting/formatting.py:197, in <genexpr>(.0)\r\n    194         array: List = pa_array.to_numpy(zero_copy_only=zero_copy_only).tolist()\r\n    195 if len(array) > 0:\r\n    196     if any(\r\n--> 197         (isinstance(x, np.ndarray) and (x.dtype == np.object or x.shape != array[0].shape))\r\n    198         or (isinstance(x, float) and np.isnan(x))\r\n    199         for x in array\r\n    200     ):\r\n    201         return np.array(array, copy=False, **{**self.np_array_kwargs, \"dtype\": np.object})\r\n    202 return np.array(array, copy=False, **self.np_array_kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/numpy/__init__.py:324, in __getattr__(attr)\r\n    319     warnings.warn(\r\n    320         f\"In the future `np.{attr}` will be defined as the \"\r\n    321         \"corresponding NumPy scalar.\", FutureWarning, stacklevel=2)\r\n    323 if attr in __former_attrs__:\r\n--> 324     raise AttributeError(__former_attrs__[attr])\r\n    326 if attr == 'testing':\r\n    327     import numpy.testing as testing\r\n\r\nAttributeError: module 'numpy' has no attribute 'object'.\r\n`np.object` was a deprecated alias for the builtin `object`. To avoid this error in existing code, use `object` by itself. Doing this will not modify any behavior and is safe. \r\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\r\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\r\n\r\nPlease help me to resolve the above error\n\n### Steps to reproduce the bug\n\nPlease resolve the issue of deprecated function np.object to object in the numpy\n\n### Expected behavior\n\nnp.object should be written as object only\n\n### Environment info\n\nkaggle notebook<end_of_turn>\n<start_of_turn>model\nattribute error when writing trainer.train()<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWill be useful to distribute a dataset across workers (other than pytorch) like spark\r\n\r\nI also renamed `.n_shards` -> `.num_shards` for consistency and kept the old name for backward compatibility. And a few changes in internal functions for consistency as well (rank, world_size -> num_shards, index)\r\n\r\nBreaking change: the new default for `contiguous` in `Dataset.shard()` is `True`, but imo not a big deal since I couldn't find any usage of `contiguous=False` internally (we always do contiguous=True for map-style datasets since its more optimized) or in the wild<end_of_turn>\n<start_of_turn>model\nAdd IterableDataset.shard()<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfollows https://github.com/huggingface/huggingface_hub/pull/3035\r\n\r\nrelated to https://github.com/huggingface/datasets/issues/7526<end_of_turn>\n<start_of_turn>model\nEnable xet in push to hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSilence ruff deprecation messages<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfixes #5871 \r\n\r\nThe second commit is optional but improves readability.<end_of_turn>\n<start_of_turn>model\ncanonicalize data dir in config ID hash<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI have a HUGE dataset about 14TB, I unable to download all parquet all. I just take about 100 from it.\r\n\r\ndataset = load_dataset(\"xx/\", data_files=\"data/train-001*-of-00314.parquet\")\r\n\r\nHow can I just using 000 - 100 from a 00314 from all partially?\r\n\r\nI search whole net didn't found a solution, **this is stupid if they didn't support it, and I swear I wont using stupid parquet any more**\r\n<end_of_turn>\n<start_of_turn>model\nHow can I load partial parquet files only?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee https://huggingface.co/datasets/Nguyendo1999/mmath/discussions/1\r\n\r\n```\r\nError code:   ConfigNamesError\r\nException:    TypeError\r\nMessage:      __init__() missing 1 required positional argument: 'dtype'\r\nTraceback:    Traceback (most recent call last):\r\n                File \"/src/services/worker/src/worker/job_runners/dataset/config_names.py\", line 65, in compute_config_names_response\r\n                  for config in sorted(get_dataset_config_names(path=dataset, token=hf_token))\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 351, in get_dataset_config_names\r\n                  dataset_module = dataset_module_factory(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1512, in dataset_module_factory\r\n                  raise e1 from None\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1489, in dataset_module_factory\r\n                  return HubDatasetModuleFactoryWithoutScript(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1039, in get_module\r\n                  dataset_infos = DatasetInfosDict.from_dataset_card_data(dataset_card_data)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/info.py\", line 468, in from_dataset_card_data\r\n                  dataset_info = DatasetInfo._from_yaml_dict(dataset_card_data[\"dataset_info\"])\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/info.py\", line 399, in _from_yaml_dict\r\n                  yaml_data[\"features\"] = Features._from_yaml_list(yaml_data[\"features\"])\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1838, in _from_yaml_list\r\n                  return cls.from_dict(from_yaml_inner(yaml_data))\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1690, in from_dict\r\n                  obj = generate_from_dict(dic)\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1345, in generate_from_dict\r\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1345, in <dictcomp>\r\n                  return {key: generate_from_dict(value) for key, value in obj.items()}\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/features/features.py\", line 1353, in generate_from_dict\r\n                  return class_type(**{k: v for k, v in obj.items() if k in field_names})\r\n              TypeError: __init__() missing 1 required positional argument: 'dtype'\r\n```\r\n\r\nThis is the CSV file: https://huggingface.co/datasets/Nguyendo1999/mmath/blob/dbcdd7c2c6fc447f852ec136a7532292802bb46f/math_train.csv<end_of_turn>\n<start_of_turn>model\nConfigNamesError on a simple CSV file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis caught me out - thought it might be better to explicitly handle None?<end_of_turn>\n<start_of_turn>model\nhandle config_name=None in push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nminor docs data aug<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCI to test the support of `huggingface-hub` 0.20.0: https://github.com/huggingface/datasets/compare/main...ci-test-huggingface-hub-v0.20.0.rc1\r\n\r\nWe need to merge:\r\n- #6510 \r\n- #6512\r\n- #6516<end_of_turn>\n<start_of_turn>model\nSupport huggingface-hub 0.20.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nMove `info_utils` errors to `exceptions` module.\r\n\r\nAdditionally rename some of them, deprecate the former ones, and make the deprecation backward compatible (by making the new errors inherit from the former ones).<end_of_turn>\n<start_of_turn>model\nMove info_utils errors to exceptions module<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7404<end_of_turn>\n<start_of_turn>model\nFix filter speed regression<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #7142.\r\n\r\n**Before (Add + Cast)**:\r\n\r\n```\r\nfrom datasets import load_dataset, Value\r\nds = load_dataset(\"rotten_tomatoes\", split=\"test\")\r\nlst = [i for i in range(len(ds))]\r\n\r\nds = ds.add_column(\"new_col\", lst)\r\n# Assigns int64 to new_col by default\r\nprint(ds.features)\r\n\r\nds = ds.cast_column(\"new_col\", Value(dtype=\"uint16\", id=None))\r\nprint(ds.features)\r\n```\r\n\r\n\r\n**Before (Numpy Workaround)**:\r\n\r\n```\r\nfrom datasets import load_dataset\r\nimport numpy as np\r\nds = load_dataset(\"rotten_tomatoes\", split=\"test\")\r\nlst = [i for i in range(len(ds))]\r\n\r\nds = ds.add_column(\"new_col\", np.array(lst, dtype=np.uint16))\r\nprint(ds.features)\r\n```\r\n\r\n**After**:\r\n```\r\nfrom datasets import load_dataset, Value\r\nds = load_dataset(\"rotten_tomatoes\", split=\"test\")\r\nlst = [i for i in range(len(ds))]\r\nval = Value(dtype=\"uint16\", id=None))\r\nds = ds.add_column(\"new_col\", lst, feature=val)\r\nprint(ds.features)\r\n```<end_of_turn>\n<start_of_turn>model\nModify add_column() to optionally accept a FeatureType as param<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm loading a HuggingFace Dataset for images. \r\n\r\nI'm running a preprocessing (map operation) step that runs a few operations, one of them being conversion to float16.  The Dataset features also say that the 'img' is of type float16.  Whenever I take an image from that HuggingFace Dataset instance, the type turns out to be float32.  \r\n\r\n\n\n### Steps to reproduce the bug\n\n```python\r\nimport torchvision.transforms.v2 as transforms\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('cifar10', split='test')\r\ndataset = dataset.with_format(\"torch\")\r\n\r\ndata_transform = transforms.Compose([transforms.Resize((32, 32)), \r\n                                     transforms.ToDtype(torch.float16, scale=True),\r\n                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\r\n                                     ])\r\ndef _preprocess(examples):\r\n    # Permutes from (BS x H x W x C) to (BS x C x H x W)\r\n    images = torch.permute(examples['img'], (0, 3, 2, 1))\r\n    examples['img'] = data_transform(images)\r\n    return examples\r\n\r\ndataset = dataset.map(_preprocess, batched=True, batch_size=8)\r\n```\r\n\r\nNow at this point the dataset.features are showing float16 which is great because that's what I want. \r\n\r\n```python\r\nprint(data_loader.features['img'])\r\n\r\nSequence(feature=Sequence(feature=Sequence(feature=Value(dtype='float16', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None)\r\n```\r\n\r\nBut when I try to sample an image from this dataloader; I'm getting a float32 image, when I'm expecting float16:\r\n```python\r\nprint(next(iter(data_loader))['img'].dtype)\r\n\r\ntorch.float32\r\n```\n\n### Expected behavior\n\nI'm expecting the images loaded after the transformation to stay in float16.\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-5.15.146.1-microsoft-standard-WSL2-x86_64-with-glibc2.31\r\n- Python version: 3.10.9\r\n- `huggingface_hub` version: 0.21.4\r\n- PyArrow version: 14.0.2\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nPrecision being changed from float16 to float32 unexpectedly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAfter loading `wiki_dpr` using:\r\n```py\r\nds = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')\r\nprint(ds.get_index(\"embeddings\").metric_type) # prints nothing because the value is None\r\n```\r\nthe index does not have a defined `metric_type`. This is an issue because I do not know how the `scores` are being computed for `get_nearest_examples()`.\n\n### Steps to reproduce the bug\n\nSystem: Python 3.9.16, Transformers 4.30.2, WSL\r\n\r\nAfter loading `wiki_dpr` using:\r\n```py\r\nds = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')\r\nprint(ds.get_index(\"embeddings\").metric_type) # prints nothing because the value is None\r\n```\r\nthe index does not have a defined `metric_type`. This is an issue because I do not know how the `scores` are being computed for `get_nearest_examples()`.\r\n\r\n```py\r\nfrom transformers import DPRQuestionEncoder, DPRContextEncoder, DPRQuestionEncoderTokenizer, DPRContextEncoderTokenizer\r\n\r\ntokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\r\nencoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-multiset-base\")\r\n\r\ndef encode_question(query, tokenizer=tokenizer, encoder=encoder):\r\n    inputs = tokenizer(query, return_tensors='pt')\r\n    question_embedding = encoder(**inputs)[0].detach().numpy()\r\n    return question_embedding\r\n\r\ndef get_knn(query, k=5, tokenizer=tokenizer, encoder=encoder, verbose=False):\r\n    enc_question = encode_question(query, tokenizer, encoder)\r\n    topk_results = ds.get_nearest_examples(index_name='embeddings',\r\n                                           query=enc_question,\r\n                                          k=k)\r\n    \r\n    \r\n    a = torch.tensor(enc_question[0]).reshape(768)\r\n    b = torch.tensor(topk_results.examples['embeddings'][0])\r\n    print(a.shape, b.shape)\r\n    print(torch.dot(a, b))\r\n    print((a-b).pow(2).sum())\r\n\r\n    return topk_results\r\n```\r\n\r\nThe [FAISS documentation](https://github.com/facebookresearch/faiss/wiki/MetricType-and-distances) suggests the metric is usually L2 distance (without the square root) or the inner product. I compute both for the sample query:\r\n```py\r\nquery = \"\"\" it catapulted into popular culture along with a line of action figures and other toys by Bandai.[2] By 2001, the media franchise had generated over $6 billion in toy sales.\r\nDespite initial criticism that its action violence targeted child audiences, the franchise has been commercially successful.\"\"\"\r\nget_knn(query,k=5)\r\n```\r\n\r\nHere, I get dot product of 80.6020 and L2 distance of 77.6616 and \r\n```py\r\nNearestExamplesResults(scores=array([76.20431 , 75.312416, 74.945404, 74.866394, 74.68506 ],\r\n      dtype=float32), examples={'id': ['3081096', '2004811', '8908258', '9594124', '286575'], 'text': ['actors, resulting in the \"Power Rangers\" franchise which has continued since then into sequel TV series (with \"Power Rangers Beast Morphers\" set to premiere in 2019), comic books, video games, and three feature films, with a further cinematic universe planned. Following from the success of \"Power Rangers\", Saban acquired the rights to more of Toei\\'s library, creating \"VR Troopers\" and \"Big Bad Beetleborgs\" from several Metal Hero Series shows and \"Masked Rider\" from Kamen Rider Series footage. DIC Entertainment joined this boom by acquiring the rights to \"Gridman the Hyper Agent\" and turning it into \"Superhuman Samurai Syber-Squad\". In 2002,', \r\n```\r\n\r\nDoing `k=1` indicates the higher the outputted number, the better the match, so the metric should not be L2 distance. However, my manually computed inner product (80.6) has a discrepancy with the reported (76.2). Perhaps, this has to do with me using the `compressed` embeddings?\n\n### Expected behavior\n\n```py\r\nds = load_dataset(path='wiki_dpr', name='psgs_w100.multiset.compressed', split='train')\r\nprint(ds.get_index(\"embeddings\").metric_type) # METRIC_INNER_PRODUCT\r\n```\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-4.18.0-477.13.1.el8_8.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.9.16\r\n- Huggingface_hub version: 0.14.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.1<end_of_turn>\n<start_of_turn>model\nDocumentation: wiki_dpr Dataset has no metric_type for Faiss Index<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAs stated in the title, since Numpy changed in version >2.0 with copy, the stratify parameters break.\n\ne.g. `all_dataset.train_test_split(test_size=0.2,stratify_by_column=\"label\")` returns a Numpy error.\n\nIt works if you downgrade Numpy to a version lower than 2.0.\n\n### Steps to reproduce the bug\n\n1. Numpy > 2.0\n2. `all_dataset.train_test_split(test_size=0.2,stratify_by_column=\"label\")`\n\n### Expected behavior\n\nIt returns a stratified split as per the results of Numpy < 2.0\n\n### Environment info\n\n- `datasets` version: 2.14.4\n- Platform: Linux-6.8.0-85-generic-x86_64-with-glibc2.35\n- Python version: 3.13.7\n- Huggingface_hub version: 0.34.4\n- PyArrow version: 19.0.0\n- Pandas version: 2.3.2\n<end_of_turn>\n<start_of_turn>model\ntrain_test_split and stratify breaks with Numpy 2.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee: https://github.com/huggingface/datasets/actions/runs/11106149906/job/30853879890\r\n\r\n```\r\n=========================== short test summary info ============================\r\nFAILED tests/test_arrow_dataset.py::BaseDatasetTest::test_filter_caching_on_disk - AssertionError: Lists differ: [{'fi[44 chars] {'filename': '/tmp/tmp6xcyyjs4/cache-9533fe2601cd3e48.arrow'}] != [{'fi[44 chars] {'filename': '/tmp/tmp6xcyyjs4/cache-e6e0a8b830976289.arrow'}]\r\n\r\nFirst differing element 1:\r\n{'filename': '/tmp/tmp6xcyyjs4/cache-9533fe2601cd3e48.arrow'}\r\n{'filename': '/tmp/tmp6xcyyjs4/cache-e6e0a8b830976289.arrow'}\r\n\r\n  [{'filename': '/tmp/tmp6xcyyjs4/dataset0.arrow'},\r\n-  {'filename': '/tmp/tmp6xcyyjs4/cache-9533fe2601cd3e48.arrow'}]\r\n?                                        ^^^^^ --------\r\n\r\n+  {'filename': '/tmp/tmp6xcyyjs4/cache-e6e0a8b830976289.arrow'}]\r\n?                                       ++++++++++ ^^  +\r\nFAILED tests/test_arrow_dataset.py::BaseDatasetTest::test_map_caching_on_disk - AssertionError: Lists differ: [{'filename': '/tmp/tmp5gxrti_n/cache-e58d327daec8626f.arrow'}] != [{'filename': '/tmp/tmp5gxrti_n/cache-d87234c5763e54a3.arrow'}]\r\n\r\nFirst differing element 0:\r\n{'filename': '/tmp/tmp5gxrti_n/cache-e58d327daec8626f.arrow'}\r\n{'filename': '/tmp/tmp5gxrti_n/cache-d87234c5763e54a3.arrow'}\r\n\r\n- [{'filename': '/tmp/tmp5gxrti_n/cache-e58d327daec8626f.arrow'}]\r\n?                                         ^^ -----------\r\n\r\n+ [{'filename': '/tmp/tmp5gxrti_n/cache-d87234c5763e54a3.arrow'}]\r\n?                                       +++++++++++  ^^\r\nFAILED tests/test_fingerprint.py::TokenizersHashTest::test_hash_regex - NameError: name 'log' is not defined\r\nFAILED tests/test_fingerprint.py::RecurseHashTest::test_hash_ignores_line_definition_of_function - AssertionError: '52e56ee04ad92499' != '0a4f75cec280f634'\r\n- 52e56ee04ad92499\r\n+ 0a4f75cec280f634\r\nFAILED tests/test_fingerprint.py::RecurseHashTest::test_hash_ipython_function - AssertionError: 'a6bd2041ca63d6c0' != '517bf36b7eecdef5'\r\n- a6bd2041ca63d6c0\r\n+ 517bf36b7eecdef5\r\nFAILED tests/test_fingerprint.py::HashingTest::test_hash_tiktoken_encoding - NameError: name 'log' is not defined\r\nFAILED tests/test_fingerprint.py::HashingTest::test_hash_torch_compiled_module - NameError: name 'log' is not defined\r\nFAILED tests/test_fingerprint.py::HashingTest::test_hash_torch_generator - NameError: name 'log' is not defined\r\nFAILED tests/test_fingerprint.py::HashingTest::test_hash_torch_tensor - NameError: name 'log' is not defined\r\nFAILED tests/test_fingerprint.py::HashingTest::test_set_doesnt_depend_on_order - NameError: name 'log' is not defined\r\nFAILED tests/test_fingerprint.py::HashingTest::test_set_stable - NameError: name 'log' is not defined\r\nERROR tests/test_iterable_dataset.py::test_iterable_dataset_from_file - NameError: name 'log' is not defined\r\n= 11 failed, 2850 passed, 3 skipped, 23 warnings, 1 error in 191.06s (0:03:11) =\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken for deps-latest<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7765 \r\n\r\nThe problem here is that polars uses pyarrow large_string for images, while pandas and others just use the string type. This PR solves that and adds a test.\r\n\r\n```python\r\nimport polars as pl\r\nfrom datasets import Dataset\r\nimport pandas as pd\r\nimport pyarrow as pa\r\nfrom pathlib import Path\r\n\r\nshared_datadir = Path(\"tests/features/data\")\r\nimage_path = str(shared_datadir / \"test_image_rgb.jpg\")\r\n\r\n# Load via polars\r\ndf_polars = pl.DataFrame({\"image_path\": [image_path]})\r\ndataset_polars = Dataset.from_polars(df_polars)\r\nprint(\"Polars DF is large string:\", pa.types.is_large_string(df_polars.to_arrow().schema[0].type))\r\nprint(\"Polars DF is string:\", pa.types.is_string(df_polars.to_arrow().schema[0].type))\r\n\r\n# Load via pandas\r\ndf_pandas = pd.DataFrame({\"image_path\": [image_path]})\r\ndataset_pandas = Dataset.from_pandas(df_pandas)\r\narrow_table_pd = pa.Table.from_pandas(df_pandas)\r\nprint(\"Pandas DF is large string\", pa.types.is_large_string(arrow_table_pd.schema[0].type))\r\nprint(\"Pandas DF is string\", pa.types.is_string(arrow_table_pd.schema[0].type))\r\n```\r\nOutputs:\r\n```bash\r\nPolars DF is large string: True\r\nPolars DF is string: False\r\nPandas DF is large string False\r\nPandas DF is string True\r\n```\r\n<end_of_turn>\n<start_of_turn>model\nFix polars cast column image<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate process.mdx: Code Listings Fixes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\nimport datasets\r\nimport torch.utils.data\r\n\r\n\r\ndef gen(shards):\r\n    yield {\"shards\": shards}\r\n\r\n\r\ndef main():\r\n    dataset = datasets.IterableDataset.from_generator(\r\n        gen,\r\n        gen_kwargs={'shards': list(range(25))}\r\n    )\r\n    dataset = dataset.shuffle(buffer_size=1)\r\n    dataset = datasets.interleave_datasets(\r\n        [dataset, dataset], probabilities=[1, 0], stopping_strategy=\"all_exhausted\"\r\n    )\r\n\r\n    dataloader = torch.utils.data.DataLoader(\r\n        dataset,\r\n        batch_size=8,\r\n        num_workers=8,\r\n    )\r\n\r\n    for i, batch in enumerate(dataloader):\r\n        print(batch)\r\n        if i >= 10:\r\n            break\r\n\r\n\r\nif __name__ == \"__main__\":\r\n     main()\r\n```\n\n### Steps to reproduce the bug\n\nRun the script, it will output\r\n\r\n```\r\n{'shards': [tensor([ 0,  8, 16, 24,  0,  8, 16, 24])]}\r\n{'shards': [tensor([ 1,  9, 17,  1,  9, 17,  1,  9])]}\r\n{'shards': [tensor([ 2, 10, 18,  2, 10, 18,  2, 10])]}\r\n{'shards': [tensor([ 3, 11, 19,  3, 11, 19,  3, 11])]}\r\n{'shards': [tensor([ 4, 12, 20,  4, 12, 20,  4, 12])]}\r\n{'shards': [tensor([ 5, 13, 21,  5, 13, 21,  5, 13])]}\r\n{'shards': [tensor([ 6, 14, 22,  6, 14, 22,  6, 14])]}\r\n{'shards': [tensor([ 7, 15, 23,  7, 15, 23,  7, 15])]}\r\n{'shards': [tensor([ 0,  8, 16, 24,  0,  8, 16, 24])]}\r\n{'shards': [tensor([17,  1,  9, 17,  1,  9, 17,  1])]}\r\n{'shards': [tensor([18,  2, 10, 18,  2, 10, 18,  2])]}\r\n```\n\n### Expected behavior\n\nThe shards should be shuffled.\n\n### Environment info\n\n- `datasets` version: 3.0.0\r\n- Platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.25.0\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.6.0<end_of_turn>\n<start_of_turn>model\ninterleave_datasets resets shuffle state<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAn example from [1], does not work when limiting shards with `max_shard_size`.\r\n\r\nTry the following example with low `max_shard_size`, such as:\r\n\r\n```python\r\nbuilder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\", max_shard_size=\"10MB\")\r\n```\r\n\r\nThe reason for this is that, in line [2] `writer._num_bytes > max_shard_size` is never true, because the `write` method of `ArrowWriter` [3] does not increase `self._num_bytes`.\r\n\r\nSuch that respective Arrow/Parquet shards are only written to file based on the `writer_batch_size` or `config.DEFAULT_MAX_BATCH_SIZE`, but not based on `max_shard_size`.\r\n\r\n\r\n[1] https://huggingface.co/docs/datasets/filesystems#download-and-prepare-a-dataset-into-a-cloud-storage\r\n\r\n[2] https://github.com/huggingface/datasets/blob/3e8d420808718c9a1453a2e7ee3484ca12c9c70d/src/datasets/builder.py#L1677\r\n\r\n[3] https://github.com/huggingface/datasets/blob/3e8d420808718c9a1453a2e7ee3484ca12c9c70d/src/datasets/arrow_writer.py#L459\n\n### Steps to reproduce the bug\n\nGet example from: https://huggingface.co/docs/datasets/filesystems#download-and-prepare-a-dataset-into-a-cloud-storage\r\n\r\nCall `builder.download_and_prepare` with low `max_shard_size` such as `10MB`, e.g.:\r\n```python\r\nbuilder.download_and_prepare(output_dir, storage_options=storage_options, file_format=\"parquet\", max_shard_size=\"10MB\")\r\n```\n\n### Expected behavior\n\nShards should be written based on `max_shard_size` instead of batch size.\n\n### Environment info\n\n```\r\n>>> import datasets\r\n>>> datasets.__version__\r\n'2.14.6.dev0\r\n```<end_of_turn>\n<start_of_turn>model\nArrowWriter/ParquetWriter `write` method does not increase `_num_bytes` and hence datasets not sharding at `max_shard_size`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI work with bioinformatics data and often these tables have thousands and even tens of thousands of features. These tables are also accompanied by metadata that I do not want to pass in the model. When I perform `set_format('pt', columns=large_column_list)` , it can take several minutes before it finishes. The culprit is when the following check is performed: `any(col not in self._data.column_names for col in columns)`. Replacing this by  `set(columns) - (self._data.column_names)` is more efficient. <end_of_turn>\n<start_of_turn>model\nFaster column validation and reordering<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix imagefolder dataset url<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n```python\r\nfrom datasets import load_dataset\r\n\r\non_bad_file = \"error\"  # default\r\n# on_bad_file = \"warn\"  # warn and skip bad file\r\n# on_bad_file = \"skip\"  # skip bad file\r\nds = load_dataset(parquet_dataset_id, on_bad_file=on_bad_file)\r\n```<end_of_turn>\n<start_of_turn>model\nParquet: add `on_bad_file` argument to error/warn/skip bad files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis bumps `dill` to 0.3.9 and closes #7510 \r\n\r\nIt turns out the only thing required to make the tests pass was to extend the version checks to include 0.3.9.<end_of_turn>\n<start_of_turn>model\nBump dill to 0.4.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\ninterleave_dataset and [RandomlyCyclingMultiSourcesExamplesIterable](https://github.com/huggingface/datasets/blob/3813ce846e52824b38e53895810682f0a496a2e3/src/datasets/iterable_dataset.py#L816) enable us to sample data examples from different sources. But can we also sample batches in a similar manner (each batch only contains data from a single source)?\r\n\n\n### Motivation\n\nSome recent research [[1](https://blog.salesforceairesearch.com/sfr-embedded-mistral/), [2](https://arxiv.org/pdf/2310.07554)] shows that source homogenous batching can be helpful for contrastive learning. Can we add a function called `RandomlyCyclingMultiSourcesBatchesIterable` to support this functionality?\n\n### Your contribution\n\nI can contribute a PR. But I wonder what the best way is to test its correctness and robustness.<end_of_turn>\n<start_of_turn>model\n[interleave_dataset] sample batches from a single source at a time<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(minor) just to make it more prominent in the docs page for the soon-to-be-released new torchdata<end_of_turn>\n<start_of_turn>model\nadd checkpoint and resume title in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport JSON lines with empty struct.\r\n\r\nFix #7161.\r\n\r\nRelated to:\r\n- #7160<end_of_turn>\n<start_of_turn>model\nSupport JSON lines with empty struct<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\npicklable batch_fn<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nOSError                                   Traceback (most recent call last)\r\nCell In[24], line 28\r\n     23     return example\r\n     25 # Filter the dataset\r\n     26 # filtered_dataset = dataset.filter(contains_number)\r\n     27 # Add the 'label' field in the dataset\r\n---> 28 labeled_dataset = dataset.filter(contains_number).map(add_label)\r\n     29 # View the structure of the updated dataset\r\n     30 print(labeled_dataset)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py:975, in DatasetDict.filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_names, writer_batch_size, fn_kwargs, num_proc, desc)\r\n    972 if cache_file_names is None:\r\n    973     cache_file_names = {k: None for k in self}\r\n    974 return DatasetDict(\r\n--> 975     {\r\n    976         k: dataset.filter(\r\n    977             function=function,\r\n    978             with_indices=with_indices,\r\n    979             input_columns=input_columns,\r\n    980             batched=batched,\r\n    981             batch_size=batch_size,\r\n    982             keep_in_memory=keep_in_memory,\r\n    983             load_from_cache_file=load_from_cache_file,\r\n    984             cache_file_name=cache_file_names[k],\r\n    985             writer_batch_size=writer_batch_size,\r\n    986             fn_kwargs=fn_kwargs,\r\n    987             num_proc=num_proc,\r\n    988             desc=desc,\r\n    989         )\r\n    990         for k, dataset in self.items()\r\n    991     }\r\n    992 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py:976, in <dictcomp>(.0)\r\n    972 if cache_file_names is None:\r\n    973     cache_file_names = {k: None for k in self}\r\n    974 return DatasetDict(\r\n    975     {\r\n--> 976         k: dataset.filter(\r\n    977             function=function,\r\n    978             with_indices=with_indices,\r\n    979             input_columns=input_columns,\r\n    980             batched=batched,\r\n    981             batch_size=batch_size,\r\n    982             keep_in_memory=keep_in_memory,\r\n    983             load_from_cache_file=load_from_cache_file,\r\n    984             cache_file_name=cache_file_names[k],\r\n    985             writer_batch_size=writer_batch_size,\r\n    986             fn_kwargs=fn_kwargs,\r\n    987             num_proc=num_proc,\r\n    988             desc=desc,\r\n    989         )\r\n    990         for k, dataset in self.items()\r\n    991     }\r\n    992 )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    550 self_format = {\r\n    551     \"type\": self._format_type,\r\n    552     \"format_kwargs\": self._format_kwargs,\r\n    553     \"columns\": self._format_columns,\r\n    554     \"output_all_columns\": self._output_all_columns,\r\n    555 }\r\n    556 # apply actual function\r\n--> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    559 # re-apply format to the output\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/fingerprint.py:481, in fingerprint_transform.<locals>._fingerprint.<locals>.wrapper(*args, **kwargs)\r\n    477             validate_fingerprint(kwargs[fingerprint_name])\r\n    479 # Call actual function\r\n--> 481 out = func(dataset, *args, **kwargs)\r\n    483 # Update fingerprint of in-place transforms + update in-place history of transforms\r\n    485 if inplace:  # update after calling func so that the fingerprint doesn't change if the function fails\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3623, in Dataset.filter(self, function, with_indices, input_columns, batched, batch_size, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   3620 if len(self) == 0:\r\n   3621     return self\r\n-> 3623 indices = self.map(\r\n   3624     function=partial(\r\n   3625         get_indices_from_mask_function, function, batched, with_indices, input_columns, self._indices\r\n   3626     ),\r\n   3627     with_indices=True,\r\n   3628     features=Features({\"indices\": Value(\"uint64\")}),\r\n   3629     batched=True,\r\n   3630     batch_size=batch_size,\r\n   3631     remove_columns=self.column_names,\r\n   3632     keep_in_memory=keep_in_memory,\r\n   3633     load_from_cache_file=load_from_cache_file,\r\n   3634     cache_file_name=cache_file_name,\r\n   3635     writer_batch_size=writer_batch_size,\r\n   3636     fn_kwargs=fn_kwargs,\r\n   3637     num_proc=num_proc,\r\n   3638     suffix_template=suffix_template,\r\n   3639     new_fingerprint=new_fingerprint,\r\n   3640     input_columns=input_columns,\r\n   3641     desc=desc or \"Filter\",\r\n   3642 )\r\n   3643 new_dataset = copy.deepcopy(self)\r\n   3644 new_dataset._indices = indices.data\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:592, in transmit_tasks.<locals>.wrapper(*args, **kwargs)\r\n    590     self: \"Dataset\" = kwargs.pop(\"self\")\r\n    591 # apply actual function\r\n--> 592 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    593 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    594 for dataset in datasets:\r\n    595     # Remove task templates if a column mapping of the template is no longer valid\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:557, in transmit_format.<locals>.wrapper(*args, **kwargs)\r\n    550 self_format = {\r\n    551     \"type\": self._format_type,\r\n    552     \"format_kwargs\": self._format_kwargs,\r\n    553     \"columns\": self._format_columns,\r\n    554     \"output_all_columns\": self._output_all_columns,\r\n    555 }\r\n    556 # apply actual function\r\n--> 557 out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\r\n    558 datasets: List[\"Dataset\"] = list(out.values()) if isinstance(out, dict) else [out]\r\n    559 # re-apply format to the output\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3093, in Dataset.map(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\r\n   3087 if transformed_dataset is None:\r\n   3088     with hf_tqdm(\r\n   3089         unit=\" examples\",\r\n   3090         total=pbar_total,\r\n   3091         desc=desc or \"Map\",\r\n   3092     ) as pbar:\r\n-> 3093         for rank, done, content in Dataset._map_single(**dataset_kwargs):\r\n   3094             if done:\r\n   3095                 shards_done += 1\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3470, in Dataset._map_single(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\r\n   3466 indices = list(\r\n   3467     range(*(slice(i, i + batch_size).indices(shard.num_rows)))\r\n   3468 )  # Something simpler?\r\n   3469 try:\r\n-> 3470     batch = apply_function_on_filtered_inputs(\r\n   3471         batch,\r\n   3472         indices,\r\n   3473         check_same_num_examples=len(shard.list_indexes()) > 0,\r\n   3474         offset=offset,\r\n   3475     )\r\n   3476 except NumExamplesMismatchError:\r\n   3477     raise DatasetTransformationNotAllowedError(\r\n   3478         \"Using `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn't create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\"\r\n   3479     ) from None\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3349, in Dataset._map_single.<locals>.apply_function_on_filtered_inputs(pa_inputs, indices, check_same_num_examples, offset)\r\n   3347 if with_rank:\r\n   3348     additional_args += (rank,)\r\n-> 3349 processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\r\n   3350 if isinstance(processed_inputs, LazyDict):\r\n   3351     processed_inputs = {\r\n   3352         k: v for k, v in processed_inputs.data.items() if k not in processed_inputs.keys_to_format\r\n   3353     }\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:6212, in get_indices_from_mask_function(function, batched, with_indices, input_columns, indices_mapping, *args, **fn_kwargs)\r\n   6209 if input_columns is None:\r\n   6210     # inputs only contains a batch of examples\r\n   6211     batch: dict = inputs[0]\r\n-> 6212     num_examples = len(batch[next(iter(batch.keys()))])\r\n   6213     for i in range(num_examples):\r\n   6214         example = {key: batch[key][i] for key in batch}\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:272, in LazyDict.__getitem__(self, key)\r\n    270 value = self.data[key]\r\n    271 if key in self.keys_to_format:\r\n--> 272     value = self.format(key)\r\n    273     self.data[key] = value\r\n    274     self.keys_to_format.remove(key)\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:375, in LazyBatch.format(self, key)\r\n    374 def format(self, key):\r\n--> 375     return self.formatter.format_column(self.pa_table.select([key]))\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:442, in PythonFormatter.format_column(self, pa_table)\r\n    440 def format_column(self, pa_table: pa.Table) -> list:\r\n    441     column = self.python_arrow_extractor().extract_column(pa_table)\r\n--> 442     column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\r\n    443     return column\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py:218, in PythonFeaturesDecoder.decode_column(self, column, column_name)\r\n    217 def decode_column(self, column: list, column_name: str) -> list:\r\n--> 218     return self.features.decode_column(column, column_name) if self.features else column\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/features/features.py:1951, in Features.decode_column(self, column, column_name)\r\n   1938 def decode_column(self, column: list, column_name: str):\r\n   1939     \"\"\"Decode column with custom feature decoding.\r\n   1940 \r\n   1941     Args:\r\n   (...)\r\n   1948         `list[Any]`\r\n   1949     \"\"\"\r\n   1950     return (\r\n-> 1951         [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n   1952         if self._column_requires_decoding[column_name]\r\n   1953         else column\r\n   1954     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/features/features.py:1951, in <listcomp>(.0)\r\n   1938 def decode_column(self, column: list, column_name: str):\r\n   1939     \"\"\"Decode column with custom feature decoding.\r\n   1940 \r\n   1941     Args:\r\n   (...)\r\n   1948         `list[Any]`\r\n   1949     \"\"\"\r\n   1950     return (\r\n-> 1951         [decode_nested_example(self[column_name], value) if value is not None else None for value in column]\r\n   1952         if self._column_requires_decoding[column_name]\r\n   1953         else column\r\n   1954     )\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/features/features.py:1339, in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1336 elif isinstance(schema, (Audio, Image)):\r\n   1337     # we pass the token to read and decode files from private repositories in streaming mode\r\n   1338     if obj is not None and schema.decode:\r\n-> 1339         return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1340 return obj\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/datasets/features/image.py:185, in Image.decode_example(self, value, token_per_repo_id)\r\n    183 else:\r\n    184     image = PIL.Image.open(BytesIO(bytes_))\r\n--> 185 image.load()  # to avoid \"Too many open files\" errors\r\n    186 return image\r\n\r\nFile /usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py:254, in ImageFile.load(self)\r\n    252         break\r\n    253     else:\r\n--> 254         raise OSError(\r\n    255             \"image file is truncated \"\r\n    256             f\"({len(b)} bytes not processed)\"\r\n    257         )\r\n    259 b = b + s\r\n    260 n, err_code = decoder.decode(b)\r\n\r\nOSError: image file is truncated (1 bytes not processed)\r\n```\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset(\"mehul7/captioned_military_aircraft\")\r\n\r\nfrom transformers import AutoImageProcessor\r\n\r\ncheckpoint = \"microsoft/resnet-50\"\r\nimage_processor = AutoImageProcessor.from_pretrained(checkpoint)\r\n\r\nimport re\r\nfrom PIL import Image\r\nimport io\r\n\r\ndef contains_number(example):\r\n    try:\r\n        image = Image.open(io.BytesIO(example[\"image\"]['bytes']))\r\n        t = image_processor(images=image, return_tensors=\"pt\")['pixel_values']\r\n    except Exception as e:\r\n        print(f\"Error processing image\uff1a{example['text']}\")\r\n        return False\r\n    return bool(re.search(r'\\d', example['text']))\r\n\r\n# Define a function to add the 'label' field\r\ndef add_label(example):\r\n    lab = example['text'].split()\r\n    temp = 'NOT'\r\n    for item in lab:\r\n        if str(item[-1]).isdigit():\r\n            temp = item\r\n            break\r\n    example['label'] = temp\r\n    return example\r\n\r\n# Filter the dataset\r\n# filtered_dataset = dataset.filter(contains_number)\r\n# Add the 'label' field in the dataset\r\nlabeled_dataset = dataset.filter(contains_number).map(add_label)\r\n# View the structure of the updated dataset\r\nprint(labeled_dataset)\r\n```\r\n\r\n\n\n### Expected behavior\n\nneeds to form labels\r\nsame as : https://www.kaggle.com/code/jiabaowangts/dataset-air/notebook\n\n### Environment info\n\nKaggle notebook P100 <end_of_turn>\n<start_of_turn>model\nOSError: image file is truncated (1 bytes not processed) #28323<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR improves the robustness and user experience by:\r\n\r\n1. **Audio Module**:\r\n   - Added clear error messages when required fields ('path' or 'bytes') are missing in audio encoding\r\n\r\n2. **DatasetDict**:\r\n   - Enhanced key access error messages to show available splits when an invalid key is accessed\r\n\r\n3. **NonMutableDict**:\r\n   - Added input validation for the update() method to ensure proper mapping types\r\n\r\n4. **Arrow Reader**:\r\n   - Improved error messages for small dataset percentage splits with suggestions for alternatives\r\n\r\n5. **FaissIndex**:\r\n   - Strengthened input validation with descriptive error messages\r\n   - Added proper type checking and shape validation for search queries\r\n\r\nThese changes make the code more maintainable and user-friendly by providing actionable feedback when issues arise.<end_of_turn>\n<start_of_turn>model\nEnhance error handling and input validation across multiple modules<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nResolve  #6256. As far as I tested, `HF_DATASETS_CACHE` was ignored and I could not specify the cache directory at all except for the default one by this environment variable. `HF_HOME` has worked. Perhaps the recent change on file downloading by `huggingface_hub` could affect this bug.\r\n\r\nIn my testing, I could not specify the cache directory even by `load_dataset(\"dataset_name\" cache_dir=\"...\")`. It might be another issue. I also welcome any advice to solve this issue.<end_of_turn>\n<start_of_turn>model\nFix the environment variable for huggingface cache<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi all - I see that in the past a network dependency has been mistakenly introduced into `load_dataset` even for local loads. Is it possible this has happened again?\r\n\n\n### Steps to reproduce the bug\n\n```\r\n>>> import datasets\r\n>>> datasets.load_dataset(\"hh-rlhf\")\r\nRepo card metadata block was not found. Setting CardData to empty.\r\n*hangs bc i'm firewalled*\r\n````\r\nstack trace from ctrl-c:\r\n```\r\n^CTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/jobuser/.local/lib/python3.10/site-packages/datasets/load.py\", line 2582, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n    output_path = get_from_cache(                                                                                                                              [0/122]\r\n  File \"/home/jobuser/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 532, in get_from_cache\r\n    response = http_head(\r\n  File \"/home/jobuser/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 419, in http_head\r\n    response = _request_with_retry(\r\n  File \"/home/jobuser/.local/lib/python3.10/site-packages/datasets/utils/file_utils.py\", line 304, in _request_with_retry\r\n    response = requests.request(method=method.upper(), url=url, timeout=timeout, **params)\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/requests/api.py\", line 59, in request\r\n    return session.request(method=method, url=url, **kwargs)\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/requests/sessions.py\", line 587, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/requests/sessions.py\", line 701, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/requests/adapters.py\", line 487, in send\r\n    resp = conn.urlopen(\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 703, in urlopen\r\n    httplib_response = self._make_request(\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 386, in _make_request\r\n    self._validate_conn(conn)\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 1042, in _validate_conn\r\n    conn.connect()\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/urllib3/connection.py\", line 363, in connect\r\n    self.sock = conn = self._new_conn()\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/urllib3/connection.py\", line 174, in _new_conn\r\n    conn = connection.create_connection(\r\n  File \"/home/jobuser/build/lipy-flytekit-image/environments/satellites/python/lib/python3.10/site-packages/urllib3/util/connection.py\", line 85, in create_connection\r\n    sock.connect(sa)\r\nKeyboardInterrupt\r\n```\n\n### Expected behavior\n\nloads the dataset\n\n### Environment info\n\n```\r\n> pip show datasets\r\nName: datasets\r\nVersion: 2.18.0\r\n```\r\n\r\nPython 3.10.2<end_of_turn>\n<start_of_turn>model\n`load_dataset` requires a network connection for local download?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nDoing a simple `some_dataset[:10]` can take more than a minute.\r\n\r\nProfiling it:\r\n<img width=\"1280\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/36224762/e641fb95-ff02-4072-9016-5416a65f75ab\">\r\n\r\n`some_dataset` is completely in memory with no disk cache.\r\n\r\nThis is proving fatal to my usage of HF Datasets. Is there a way I can forgo the arrow format and store the dataset as PyTorch tensors so that `_tensorize` is not needed? And is `_consolidate` supposed to take this long?\r\n\r\nIt's faster to produce the dataset from scratch than to access it from HF Datasets!\r\n\r\n### Steps to reproduce the bug\r\n\r\nI have uploaded the dataset that causes this problem [here](https://huggingface.co/datasets/NightMachinery/hf_datasets_bug1).\r\n\r\n```python\r\n#!/usr/bin/env python3\r\nimport sys\r\nimport time\r\nimport torch\r\nfrom datasets import load_dataset\r\n\r\n\r\ndef main(dataset_name):\r\n    # Start the timer\r\n    start_time = time.time()\r\n\r\n    # Load the dataset from Hugging Face Hub\r\n    dataset = load_dataset(dataset_name)\r\n\r\n    # Set the dataset format as torch\r\n    dataset.set_format(type=\"torch\")\r\n\r\n    # Perform an identity map\r\n    dataset = dataset.map(lambda example: example, batched=True, batch_size=20)\r\n\r\n    # End the timer\r\n    end_time = time.time()\r\n\r\n    # Print the time taken\r\n    print(f\"Time taken: {end_time - start_time:.2f} seconds\")\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dataset_name = \"NightMachinery/hf_datasets_bug1\"\r\n    print(f\"dataset_name: {dataset_name}\")\r\n    main(dataset_name)\r\n```\r\n\r\n### Expected behavior\r\n\r\n_\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nHF Datasets data access is extremely slow even when in memory<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Summary\r\n\r\nThis PR addresses [#6013](https://github.com/huggingface/datasets/issues/6013) by reusing unchanged columns from the original dataset in the `map()` method when `input_columns` is specified.\r\n\r\n## What\u2019s Implemented\r\n\r\n- Injected logic at the end of `Dataset.map()` to:\r\n  - Identify untouched columns not in `input_columns` or `remove_columns`\r\n  - Select those columns from the original dataset\r\n  - Concatenate them with the transformed result using `pyarrow.concat_tables`\r\n\r\n## Example Behavior\r\n\r\n```python\r\nds = Dataset.from_dict({\"a\": [1, 2], \"b\": [3, 4]})\r\nds2 = ds.map(lambda x: {\"c\": x[\"a\"] + 10}, input_columns=[\"a\"], remove_columns=[\"a\"])\r\nprint(ds2.column_names)  # Output: ['b', 'c']\r\n````\r\n\r\nColumn `b` is reused from the original dataset.\r\n\r\n## Notes\r\n\r\n* This keeps disk usage and caching minimal by avoiding full dataset duplication.\r\n* Only triggered when `input_columns` is set.\r\n\r\n---\r\n\r\ncc @lhoestq @mariosasko for review \ud83d\ude42\r\n<end_of_turn>\n<start_of_turn>model\nfeat(map): reuse unchanged columns when input_columns specified to reduce disk usage (#6013)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nenable the viewer for datasets like https://huggingface.co/datasets/FrancophonIA/e-calm (there will be more and more apparently)\r\n<end_of_turn>\n<start_of_turn>model\nBasic XML support (mostly copy pasted from text)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAt the moment `.from_generator` can only create a dataset that lives in the cache. The cached dataset cannot be loaded with `load_from_disk` because the cache folder is missing `state.json`. So the only way to convert this cached dataset to a regular is to use `save_to_disk` which needs to create a copy of the cached dataset. For large datasets this can end up wasting a lot of space. In my case the saving operation failed so I am stuck with a large cached dataset and no clear way to convert to a `Dataset` that I can use. The requested feature is to provide a way to be able to load a cached dataset using `.load_from_disk`. Alternatively `.from_generator` can create the dataset at a specified location so that it can be loaded from there with `.load_from_disk`.\n\n### Motivation\n\nI have the following workflow which has exposed some awkwardness about the Datasets saving/caching.\n\n1. I created a cached dataset using `.from_generator` which was cached in a folder. This dataset is rather large (~600GB) with many shards.\n2. I tried to save this dataset using `.save_to_disk` to another location so that I can use later as a `Dataset`. This essentially creates another copy (for a total of 1.2TB!) of what is already in the cache... In my case the saving operation keeps dying for some reason and I am stuck with a cached dataset and no copy.\n3. Now I am trying to \"save\" the existing cached dataset but it is not clear how to access the cached files after `.from_generator` has finished e.g. from a different process. I should not be even looking at the cache but I really do not want to waste another 2hr to generate the set so that if fails agains (I already did this couple of times). \n- I tried `.load_from_disk` but it does not work with cached files and complains that this is not a `Dataset` (!).\n- I looked at `.from_file` which takes one file but the cached file has many (shards) so I am not sure how to make this work. \n- I tried `.load_dataset` but this seems to either try to \"download\" a copy (of a file which is already in the local file system!) which I will then need to save or I need to use `streaming=False` to create an `IterableDataset `which then I need to convert (using the cache) to `Dataset` so that I can save it. With both options I  will end up with 3 copies of the same dataset for a total of ~2TB! I am hoping here is another way to do this...\n\nMaybe I am missing something here: I looked at docs and forums but no luck. I have a bunch of arrow files cached by `Dataset.from_generator` and no clean way to make them into a `Dataset` that I can use. \n\nThis all could be so much easer if `load_from_disk` can recognize the cached files and produce a `Dataset`: after the cache is created I would not have to \"save\" it again and I can just load it when I need.  At the moment `load_from_disk` needs `state.json` which is lacking in the cache folder. So perhaps `.from_generator` could be made to \"finalize\" (e.g. create `state.json`) the dataset once it is done so that it can be loaded easily. Or provide `.from_generator` with a `save_to_dir` parameter in addition to `cache_dir` which can be used for the whole process including creating the `state.json` at the end. \n\nAs a proof of concept I just created `state.json` by hand and `load_from_disk` worked using the cache! So it seems to be the missing piece here.\n\n### Your contribution\n\nTime permitting I can look into `.from_generator` to see if adding  `state.json` is feasible.<end_of_turn>\n<start_of_turn>model\nbetter correspondence between cached and saved datasets created using from_generator<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ntoken is ok to be public since it's only for the hub-ci<end_of_turn>\n<start_of_turn>model\nupdate ci user<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nImprove `Dataset.from_list` docstring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to get the HPLT datasets on the hub. Downloading/re-uploading would be too time- and resource consuming so I wrote [a dataset loader script](https://huggingface.co/datasets/BramVanroy/hplt_mono_v1_2/blob/main/hplt_mono_v1_2.py). I think I am very close but for some reason I always get the error below. It happens during the clean-up phase where the directory cannot be removed because it is not empty.\r\n\r\nMy only guess would be that this may have to do with zstandard\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 1744, in _prepare_split_single\r\n    writer.write(example, key)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 492, in write\r\n    self.write_examples_on_file()\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 434, in write_examples_on_file\r\n    if self.schema\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 409, in schema\r\n    else (pa.schema(self._features.type) if self._features is not None else None)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1643, in type\r\n    return get_nested_type(self)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1209, in get_nested_type\r\n    {key: get_nested_type(schema[key]) for key in schema}\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1209, in <dictcomp>\r\n    {key: get_nested_type(schema[key]) for key in schema}\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1221, in get_nested_type\r\n    value_type = get_nested_type(schema.feature)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1228, in get_nested_type\r\n    return schema()\r\nTypeError: 'str' object is not callable\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 1753, in _prepare_split_single\r\n    num_examples, num_bytes = writer.finalize()\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 588, in finalize\r\n    self.write_examples_on_file()\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 434, in write_examples_on_file\r\n    if self.schema\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/arrow_writer.py\", line 409, in schema\r\n    else (pa.schema(self._features.type) if self._features is not None else None)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1643, in type\r\n    return get_nested_type(self)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1209, in get_nested_type\r\n    {key: get_nested_type(schema[key]) for key in schema}\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1209, in <dictcomp>\r\n    {key: get_nested_type(schema[key]) for key in schema}\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1221, in get_nested_type\r\n    value_type = get_nested_type(schema.feature)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/features/features.py\", line 1228, in get_nested_type\r\n    return schema()\r\nTypeError: 'str' object is not callable\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 959, in incomplete_dir\r\n    yield tmp_dir\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 1005, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 1767, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 1100, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 1605, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 1762, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/pricie/vanroy/.config/JetBrains/PyCharm2023.3/scratches/scratch_5.py\", line 4, in <module>\r\n    ds = load_dataset(\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/load.py\", line 2549, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 985, in download_and_prepare\r\n    with incomplete_dir(self._output_dir) as tmp_output_dir:\r\n  File \"/home/pricie/vanroy/.pyenv/versions/3.10.13/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/home/local/vanroy/dutch-instruction-datasets/.venv/lib/python3.10/site-packages/datasets/builder.py\", line 966, in incomplete_dir\r\n    shutil.rmtree(tmp_dir)\r\n  File \"/home/pricie/vanroy/.pyenv/versions/3.10.13/lib/python3.10/shutil.py\", line 731, in rmtree\r\n    onerror(os.rmdir, path, sys.exc_info())\r\n  File \"/home/pricie/vanroy/.pyenv/versions/3.10.13/lib/python3.10/shutil.py\", line 729, in rmtree\r\n    os.rmdir(path)\r\nOSError: [Errno 39] Directory not empty: '/home/pricie/vanroy/.cache/huggingface/datasets/BramVanroy___hplt_mono_v1_2/ky/1.2.0/7ab138629fe7e9e29fe93ce63d809d5ef9d963273b829f61ab538e012dc9cc47.incomplete'\r\n```\r\n\r\nInterestingly, though, this directory _does_ appear to be empty:\r\n\r\n```shell\r\n> cd /home/pricie/vanroy/.cache/huggingface/datasets/BramVanroy___hplt_mono_v1_2/ky/1.2.0/7ab138629fe7e9e29fe93ce63d809d5ef9d963273b829f61ab538e012dc9cc47.incomplete\r\n\r\n> ls -lah\r\ntotal 0\r\ndrwxr-xr-x. 1 vanroy vanroy   0 Mar  7 12:01 .\r\ndrwxr-xr-x. 1 vanroy vanroy 304 Mar  7 11:52 ..\r\n\r\n> cd ..\r\n> ls\r\n7ab138629fe7e9e29fe93ce63d809d5ef9d963273b829f61ab538e012dc9cc47_builder.lock  7ab138629fe7e9e29fe93ce63d809d5ef9d963273b829f61ab538e012dc9cc47.incomplete\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import load_dataset\r\n\r\n\r\nds = load_dataset(\r\n    \"BramVanroy/hplt_mono_v1_2\",\r\n    \"ky\",\r\n    trust_remote_code=True\r\n)\r\n\r\n```\n\n### Expected behavior\n\nNo error.\n\n### Environment info\n\n- `datasets` version: 2.16.1\r\n- Platform: Linux-5.14.0-284.25.1.el9_2.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.20.2\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.10.0\r\n<end_of_turn>\n<start_of_turn>model\nTypeError: 'str' object is not callable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUp until a couple days ago I was having no issues loading `Helsinki-NLP/europarl` and `Helsinki-NLP/un_pc`, but now suddenly I get the following error:\n\n```\nhuggingface_hub.errors.BadRequestError: (Request ID: ...)\n\nBad request:\n* Invalid input: expected array, received string * at paths * Invalid input: expected boolean, received string * at expand\n\u2716 Invalid input: expected array, received string\n  \u2192 at paths\n\u2716 Invalid input: expected boolean, received string\n  \u2192 at expand\n```\n\nI tried with both `4.0.0` and `3.5.1` since this dataset uses `trust_remote_code`, but I get the same error with both.\n\nWhat can I do to load the dataset? I checked the documentation and GitHub issues here, but couldn't find a solution.\n\n### Steps to reproduce the bug\n\n```python\nimport datasets\nds = datasets.load_dataset(\"Helsinki-NLP/europarl\", \"en-fr\", streaming=True, trust_remote_code=True)[\"train\"]\n```\n\n### Expected behavior\n\nThat the dataset loads as it did a couple days ago.\n\n### Environment info\n\n- `datasets` version: 3.5.1\n- Platform: Linux-4.18.0-513.24.1.el8_9.x86_64-x86_64-with-glibc2.28\n- Python version: 3.11.11\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 20.0.0\n- Pandas version: 2.2.2\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nBadRequestError for loading dataset?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs reported by @natolambert, loading regular JSON files with `datasets` shows poor performance.\r\n\r\nThe cause is that we use the `json` Python standard library instead of other faster libraries. See my old comment: https://github.com/huggingface/datasets/pull/2638#pullrequestreview-706983714\r\n> There are benchmarks that compare different JSON packages, with the Standard Library one among the worst performant: \r\n> - https://github.com/ultrajson/ultrajson#benchmarks\r\n> - https://github.com/ijl/orjson#performance\r\n\r\nI remember having a discussion about this and it was decided that it was better not to include an additional dependency on a 3rd-party library.\r\n\r\nHowever:\r\n- We already depend on `pandas` and `pandas` depends on `ujson`: so we have an indirect dependency on `ujson`\r\n- Even if the above were not the case, we always could include `ujson` as an optional extra dependency, and check at runtime if it is installed to decide which library to use, either json or ujson<end_of_turn>\n<start_of_turn>model\nImprove performance of JSON loader<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix unused DatasetInfosDict code in push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading a `.jsonl` file using `load_dataset(\"json\", data_files=\"data.jsonl\", split=\"train\")`, the function misinterprets the content of a string field  as if it were part of the dataset schema. \n\nIn my case there is a field `body:` with a string value \n```\n\"### Describe the bug (...) ,action: string, datetime: timestamp[s], author: string, (...) Pandas version: 1.3.4\"\n```\nAs a result, I got an exception \n```\n\"TypeError: Couldn't cast array of type timestamp[s] to null\". \n```\nFull stack-trace in the attached file below.\n\nI also attach a minimized dataset (data.json, a single entry) that reproduces the error.\n\n**Observations**(on the minimal example): \n- if I remove _all fields before_ `body`, a different error appears,\n- if I remove _all fields after_ `body`, yet another error appears,\n- if `body` is _the only field_, the error disappears.\n\nSo this might be one complex bug or several edge cases interacting. I haven\u2019t dug deeper.\n\nAlso changing the file extension to `.json` or `.txt` avoids the problem. This suggests **a possible workaround** for the general case: convert `.jsonl` to `.json`. Though I haven\u2019t verified correctness of that workaround yet.\n\nAnyway my understanding is that `load_dataset` with first argument set to \"json\" should properly handle `.jsonl` files. Correct me if I'm wrong.\n\n[stack_trace.txt](https://github.com/user-attachments/files/21004153/stack_trace.txt)\n[data.json](https://github.com/user-attachments/files/21004164/data.json)\n\nP.S.\nI discovered this while going through the HuggingFace tutorial. Specifically [this part](https://huggingface.co/learn/llm-course/chapter5/5?fw=pt). I will try to inform the tutorial team about this issue, as it can be a showstopper for young \ud83e\udd17 adepts.\n\n### Steps to reproduce the bug\n\n1. Download attached [data.json](https://github.com/user-attachments/files/21004164/data.json) file.\n2. Run the following code which should work correctly:\n```\nfrom datasets import load_dataset\nload_dataset(\"json\", data_files=\"data.json\", split=\"train\")\n```\n3. Change extension of the `data` file to `.jsonl` and run:\n```\nfrom datasets import load_dataset\nload_dataset(\"json\", data_files=\"data.jsonl\", split=\"train\")\n```\n\nThis will trigger an error like the one in the attached [stack_trace.txt](https://github.com/user-attachments/files/21004153/stack_trace.txt).\n\nOne can also try removing fields before the `body` field and after it. These actions give different errors.\n\n\n### Expected behavior\n\nParsing data in `.jsonl` format should yield the same result as parsing the same data in `.json` format. In any case, the content of a string field should never be interpreted as part of the dataset schema.\n\n### Environment info\n\ndatasets version: _3.6.0_\npyarrow version: _20.0.0_\nPython version: _3.11.9_\nplatform version: _macOS-15.5-arm64-arm-64bit_<end_of_turn>\n<start_of_turn>model\nFunction load_dataset() misinterprets string field content as part of dataset schema when dealing with `.jsonl` files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix string_to_dict test<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nwhen i used Dataset.map api to process data concurrently, i found that\r\n it gets slower and slower as it gets closer to completion. Then i read the source code of arrow_dataset.py and found that it shard the dataset and use multiprocessing pool to execute each shard.It may cause the slowest task to drag out the entire program's execution time,especially when processing huge dataset.\r\n\r\n### Motivation\r\n\r\nusing work-stealing algorithm instead of sharding and parallel computing to optimize performance. \r\n\r\n### Your contribution\r\n\r\njust an idea.<end_of_turn>\n<start_of_turn>model\nUse work-stealing algorithm when parallel computing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFirst off, I do apologize if this is not the correct repo for submitting this issue. Please direct me to another one if it's more appropriate elsewhere. \n\nThe datasets rest api is inconsistently giving `416 Range Not Satisfiable` when using a range request to get portions of the parquet files. More often than not, I am seeing 416, but other times for an identical request, it gives me the data along with `206 Partial Content` as expected. \n\n\n\n\n### Steps to reproduce the bug\n\nrepeating this request multiple times will return either 416 or 206. \n```sh\n$ curl -v -L -H \"Range: bytes=217875070-218006142\" -o output.parquet \"https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\"\n```\nNote: this is not limited to just the above file, I tried with many different datasets and am able to consistently reproduce issue across multiple datasets.\n\nwhen the 416 is returned, I get the following headers\n```\n< HTTP/2 416\n< content-type: text/html\n< content-length: 49\n< server: CloudFront\n< date: Wed, 16 Jul 2025 14:58:43 GMT\n< expires: Wed, 16 Jul 2025 14:58:43 GMT\n< content-range: bytes */177\n< x-cache: Error from cloudfront\n< via: 1.1 873527676a354c5998cad133525df9c0.cloudfront.net (CloudFront)\n<\n```\n\nthis suggests to me that there is likely a CDN/caching/routing issue happening and the request is not getting routed properly. \n\n\n\nFull verbose output via curl. \n<details>\n\n\u276f curl -v -L -H \"Range: bytes=217875070-218006142\" -o output.parquet \"https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\"\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Host huggingface.co:443 was resolved.\n* IPv6: (none)\n* IPv4: 18.160.102.96, 18.160.102.110, 18.160.102.4, 18.160.102.86\n*   Trying 18.160.102.96:443...\n* Connected to huggingface.co (18.160.102.96) port 443\n* ALPN: curl offers h2,http/1.1\n* (304) (OUT), TLS handshake, Client hello (1):\n} [319 bytes data]\n*  CAfile: /etc/ssl/cert.pem\n*  CApath: none\n* (304) (IN), TLS handshake, Server hello (2):\n{ [122 bytes data]\n* (304) (IN), TLS handshake, Unknown (8):\n{ [19 bytes data]\n* (304) (IN), TLS handshake, Certificate (11):\n{ [3821 bytes data]\n* (304) (IN), TLS handshake, CERT verify (15):\n{ [264 bytes data]\n* (304) (IN), TLS handshake, Finished (20):\n{ [36 bytes data]\n* (304) (OUT), TLS handshake, Finished (20):\n} [36 bytes data]\n* SSL connection using TLSv1.3 / AEAD-AES128-GCM-SHA256 / [blank] / UNDEF\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=huggingface.co\n*  start date: Apr 13 00:00:00 2025 GMT\n*  expire date: May 12 23:59:59 2026 GMT\n*  subjectAltName: host \"huggingface.co\" matched cert's \"huggingface.co\"\n*  issuer: C=US; O=Amazon; CN=Amazon RSA 2048 M02\n*  SSL certificate verify ok.\n* using HTTP/2\n* [HTTP/2] [1] OPENED stream for https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\n* [HTTP/2] [1] [:method: GET]\n* [HTTP/2] [1] [:scheme: https]\n* [HTTP/2] [1] [:authority: huggingface.co]\n* [HTTP/2] [1] [:path: /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet]\n* [HTTP/2] [1] [user-agent: curl/8.7.1]\n* [HTTP/2] [1] [accept: */*]\n* [HTTP/2] [1] [range: bytes=217875070-218006142]\n> GET /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet HTTP/2\n> Host: huggingface.co\n> User-Agent: curl/8.7.1\n> Accept: */*\n> Range: bytes=217875070-218006142\n>\n* Request completely sent off\n< HTTP/2 416\n< content-type: text/html\n< content-length: 49\n< server: CloudFront\n< date: Wed, 16 Jul 2025 14:58:41 GMT\n< expires: Wed, 16 Jul 2025 14:58:41 GMT\n< content-range: bytes */177\n< x-cache: Error from cloudfront\n< via: 1.1 e2f1bed2f82641d6d5439eac20a790ba.cloudfront.net (CloudFront)\n< x-amz-cf-pop: MSP50-P1\n< x-amz-cf-id: Mo8hn-EZLJqE_hoBday8DdhmVXhV3v9-Wg-EEHI6gX_fNlkanVIUBA==\n<\n{ [49 bytes data]\n100    49  100    49    0     0   2215      0 --:--:-- --:--:-- --:--:--  2227\n* Connection #0 to host huggingface.co left intact\n\n(.venv) Daft main*\u200b* \u2261\u276f curl -v -L -H \"Range: bytes=217875070-218006142\" -o output.parquet \"https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\"\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Host huggingface.co:443 was resolved.\n* IPv6: (none)\n* IPv4: 18.160.102.96, 18.160.102.110, 18.160.102.4, 18.160.102.86\n*   Trying 18.160.102.96:443...\n* Connected to huggingface.co (18.160.102.96) port 443\n* ALPN: curl offers h2,http/1.1\n* (304) (OUT), TLS handshake, Client hello (1):\n} [319 bytes data]\n*  CAfile: /etc/ssl/cert.pem\n*  CApath: none\n* (304) (IN), TLS handshake, Server hello (2):\n{ [122 bytes data]\n* (304) (IN), TLS handshake, Unknown (8):\n{ [19 bytes data]\n* (304) (IN), TLS handshake, Certificate (11):\n{ [3821 bytes data]\n* (304) (IN), TLS handshake, CERT verify (15):\n{ [264 bytes data]\n* (304) (IN), TLS handshake, Finished (20):\n{ [36 bytes data]\n* (304) (OUT), TLS handshake, Finished (20):\n} [36 bytes data]\n* SSL connection using TLSv1.3 / AEAD-AES128-GCM-SHA256 / [blank] / UNDEF\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=huggingface.co\n*  start date: Apr 13 00:00:00 2025 GMT\n*  expire date: May 12 23:59:59 2026 GMT\n*  subjectAltName: host \"huggingface.co\" matched cert's \"huggingface.co\"\n*  issuer: C=US; O=Amazon; CN=Amazon RSA 2048 M02\n*  SSL certificate verify ok.\n* using HTTP/2\n* [HTTP/2] [1] OPENED stream for https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\n* [HTTP/2] [1] [:method: GET]\n* [HTTP/2] [1] [:scheme: https]\n* [HTTP/2] [1] [:authority: huggingface.co]\n* [HTTP/2] [1] [:path: /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet]\n* [HTTP/2] [1] [user-agent: curl/8.7.1]\n* [HTTP/2] [1] [accept: */*]\n* [HTTP/2] [1] [range: bytes=217875070-218006142]\n> GET /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet HTTP/2\n> Host: huggingface.co\n> User-Agent: curl/8.7.1\n> Accept: */*\n> Range: bytes=217875070-218006142\n>\n* Request completely sent off\n< HTTP/2 416\n< content-type: text/html\n< content-length: 49\n< server: CloudFront\n< date: Wed, 16 Jul 2025 14:58:42 GMT\n< expires: Wed, 16 Jul 2025 14:58:42 GMT\n< content-range: bytes */177\n< x-cache: Error from cloudfront\n< via: 1.1 bb352451e1eacf85f8786ee3ecd07eca.cloudfront.net (CloudFront)\n< x-amz-cf-pop: MSP50-P1\n< x-amz-cf-id: 9xy-CX9KvlS8Ye4eFr8jXMDobZHFkvdyvkLJGmK_qiwZQywCCwfq7Q==\n<\n{ [49 bytes data]\n100    49  100    49    0     0   2381      0 --:--:-- --:--:-- --:--:--  2450\n* Connection #0 to host huggingface.co left intact\n\n(.venv) Daft main*\u200b* \u2261\u276f curl -v -L -H \"Range: bytes=217875070-218006142\" -o output.parquet \"https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\"\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Host huggingface.co:443 was resolved.\n* IPv6: (none)\n* IPv4: 18.160.102.96, 18.160.102.110, 18.160.102.4, 18.160.102.86\n*   Trying 18.160.102.96:443...\n* Connected to huggingface.co (18.160.102.96) port 443\n* ALPN: curl offers h2,http/1.1\n* (304) (OUT), TLS handshake, Client hello (1):\n} [319 bytes data]\n*  CAfile: /etc/ssl/cert.pem\n*  CApath: none\n* (304) (IN), TLS handshake, Server hello (2):\n{ [122 bytes data]\n* (304) (IN), TLS handshake, Unknown (8):\n{ [19 bytes data]\n* (304) (IN), TLS handshake, Certificate (11):\n{ [3821 bytes data]\n* (304) (IN), TLS handshake, CERT verify (15):\n{ [264 bytes data]\n* (304) (IN), TLS handshake, Finished (20):\n{ [36 bytes data]\n* (304) (OUT), TLS handshake, Finished (20):\n} [36 bytes data]\n* SSL connection using TLSv1.3 / AEAD-AES128-GCM-SHA256 / [blank] / UNDEF\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=huggingface.co\n*  start date: Apr 13 00:00:00 2025 GMT\n*  expire date: May 12 23:59:59 2026 GMT\n*  subjectAltName: host \"huggingface.co\" matched cert's \"huggingface.co\"\n*  issuer: C=US; O=Amazon; CN=Amazon RSA 2048 M02\n*  SSL certificate verify ok.\n* using HTTP/2\n* [HTTP/2] [1] OPENED stream for https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\n* [HTTP/2] [1] [:method: GET]\n* [HTTP/2] [1] [:scheme: https]\n* [HTTP/2] [1] [:authority: huggingface.co]\n* [HTTP/2] [1] [:path: /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet]\n* [HTTP/2] [1] [user-agent: curl/8.7.1]\n* [HTTP/2] [1] [accept: */*]\n* [HTTP/2] [1] [range: bytes=217875070-218006142]\n> GET /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet HTTP/2\n> Host: huggingface.co\n> User-Agent: curl/8.7.1\n> Accept: */*\n> Range: bytes=217875070-218006142\n>\n* Request completely sent off\n< HTTP/2 416\n< content-type: text/html\n< content-length: 49\n< server: CloudFront\n< date: Wed, 16 Jul 2025 14:58:43 GMT\n< expires: Wed, 16 Jul 2025 14:58:43 GMT\n< content-range: bytes */177\n< x-cache: Error from cloudfront\n< via: 1.1 873527676a354c5998cad133525df9c0.cloudfront.net (CloudFront)\n< x-amz-cf-pop: MSP50-P1\n< x-amz-cf-id: wtBgwY4u4YJ2pD1ovM8UV770UiJoqWfs7i7VzschDyoLv5g7swGGmw==\n<\n{ [49 bytes data]\n100    49  100    49    0     0   2273      0 --:--:-- --:--:-- --:--:--  2333\n* Connection #0 to host huggingface.co left intact\n\n(.venv) Daft main*\u200b* \u2261\u276f curl -v -L -H \"Range: bytes=217875070-218006142\" -o output.parquet \"https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\"\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0* Host huggingface.co:443 was resolved.\n* IPv6: (none)\n* IPv4: 18.160.102.96, 18.160.102.110, 18.160.102.4, 18.160.102.86\n*   Trying 18.160.102.96:443...\n* Connected to huggingface.co (18.160.102.96) port 443\n* ALPN: curl offers h2,http/1.1\n* (304) (OUT), TLS handshake, Client hello (1):\n} [319 bytes data]\n*  CAfile: /etc/ssl/cert.pem\n*  CApath: none\n* (304) (IN), TLS handshake, Server hello (2):\n{ [122 bytes data]\n* (304) (IN), TLS handshake, Unknown (8):\n{ [19 bytes data]\n* (304) (IN), TLS handshake, Certificate (11):\n{ [3821 bytes data]\n* (304) (IN), TLS handshake, CERT verify (15):\n{ [264 bytes data]\n* (304) (IN), TLS handshake, Finished (20):\n{ [36 bytes data]\n* (304) (OUT), TLS handshake, Finished (20):\n} [36 bytes data]\n* SSL connection using TLSv1.3 / AEAD-AES128-GCM-SHA256 / [blank] / UNDEF\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=huggingface.co\n*  start date: Apr 13 00:00:00 2025 GMT\n*  expire date: May 12 23:59:59 2026 GMT\n*  subjectAltName: host \"huggingface.co\" matched cert's \"huggingface.co\"\n*  issuer: C=US; O=Amazon; CN=Amazon RSA 2048 M02\n*  SSL certificate verify ok.\n* using HTTP/2\n* [HTTP/2] [1] OPENED stream for https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet\n* [HTTP/2] [1] [:method: GET]\n* [HTTP/2] [1] [:scheme: https]\n* [HTTP/2] [1] [:authority: huggingface.co]\n* [HTTP/2] [1] [:path: /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet]\n* [HTTP/2] [1] [user-agent: curl/8.7.1]\n* [HTTP/2] [1] [accept: */*]\n* [HTTP/2] [1] [range: bytes=217875070-218006142]\n> GET /api/datasets/HuggingFaceTB/smoltalk2/parquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0.parquet HTTP/2\n> Host: huggingface.co\n> User-Agent: curl/8.7.1\n> Accept: */*\n> Range: bytes=217875070-218006142\n>\n* Request completely sent off\n< HTTP/2 302\n< content-type: text/plain; charset=utf-8\n< content-length: 177\n< location: https://huggingface.co/datasets/HuggingFaceTB/smoltalk2/resolve/refs%2Fconvert%2Fparquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0000.parquet\n< date: Wed, 16 Jul 2025 14:58:44 GMT\n< x-powered-by: huggingface-moon\n< cross-origin-opener-policy: same-origin\n< referrer-policy: strict-origin-when-cross-origin\n< x-request-id: Root=1-6877be24-476860f03849cb1a1570c9d8\n< access-control-allow-origin: https://huggingface.co\n< access-control-expose-headers: X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash\n< set-cookie: token=; Path=/; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Secure; SameSite=None\n< set-cookie: token=; Domain=huggingface.co; Path=/; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Secure; SameSite=Lax\n< x-cache: Miss from cloudfront\n< via: 1.1 dd5af138aa8a11d8a70d5ef690ad1a2a.cloudfront.net (CloudFront)\n< x-amz-cf-pop: MSP50-P1\n< x-amz-cf-id: xuSi0X5RpH1OZqQOM8gGQLQLU8eOM6Gbkk-bgIX_qBnTTaa1VNkExA==\n<\n* Ignoring the response-body\n100   177  100   177    0     0   2021      0 --:--:-- --:--:-- --:--:--  2034\n* Connection #0 to host huggingface.co left intact\n* Issue another request to this URL: 'https://huggingface.co/datasets/HuggingFaceTB/smoltalk2/resolve/refs%2Fconvert%2Fparquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0000.parquet'\n* Found bundle for host: 0x600002d54570 [can multiplex]\n* Re-using existing connection with host huggingface.co\n* [HTTP/2] [3] OPENED stream for https://huggingface.co/datasets/HuggingFaceTB/smoltalk2/resolve/refs%2Fconvert%2Fparquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0000.parquet\n* [HTTP/2] [3] [:method: GET]\n* [HTTP/2] [3] [:scheme: https]\n* [HTTP/2] [3] [:authority: huggingface.co]\n* [HTTP/2] [3] [:path: /datasets/HuggingFaceTB/smoltalk2/resolve/refs%2Fconvert%2Fparquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0000.parquet]\n* [HTTP/2] [3] [user-agent: curl/8.7.1]\n* [HTTP/2] [3] [accept: */*]\n* [HTTP/2] [3] [range: bytes=217875070-218006142]\n> GET /datasets/HuggingFaceTB/smoltalk2/resolve/refs%2Fconvert%2Fparquet/Mid/Llama_Nemotron_Post_Training_Dataset_reasoning_r1/0000.parquet HTTP/2\n> Host: huggingface.co\n> User-Agent: curl/8.7.1\n> Accept: */*\n> Range: bytes=217875070-218006142\n>\n* Request completely sent off\n< HTTP/2 302\n< content-type: text/plain; charset=utf-8\n< content-length: 1317\n< location: https://cas-bridge.xethub.hf.co/xet-bridge-us/686fc33898943c873b45c9a0/cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250716T145416Z&X-Amz-Expires=3600&X-Amz-Signature=21a15b50740d73fd8ce82d5105733ca067d2e612ada22570e09e93ebcc7f8842&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%270000.parquet%3B+filename%3D%220000.parquet%22%3B&x-id=GetObject&Expires=1752681256&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjY4MTI1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODZmYzMzODk4OTQzYzg3M2I0NWM5YTAvY2Y4YTNhNTY2NWNmOGIyZmY2NjdmYjUyMzZhMWU1Y2IxM2M3NTgyOTU1Zjk1MzNjODhlMTM4Nzk5N2VmM2FmOSoifV19&Signature=Tl3xorJ-7yaWvG6Y1AhhRlV2Wko9QpoK1tdPOfNZaRbHo%7EdaAkJRJfcLAYD5YzozfHWBZMLlJsaMPJ1MAne21nr5%7E737sE6yLfBwHdP3ZFZhgrLsN%7EvkIWK2GYX543qTg-pVsf3it92w1oWyoyYNQ9srxLfEIuG2AKV2Nu3Ejl7S%7EaAq4Gv4jNemvRTLBFGgYPdUeuavudl4OD4RGkSGTnpzh-P-OBk5WvgpdZZnbb1cRAP73tFHsPDX4%7ETfQIor109G%7E0TB3Jq0wopO9WV0sMQyQs9peZc6bxONiTxb9aHM4yNvWNbVGtlPuC6YS4c9T1e9%7EehdgU4sDOI%7EhpaCvg__&Key-Pair-Id=K2L8F4GPSG1IFC\n< date: Wed, 16 Jul 2025 14:58:44 GMT\n< x-powered-by: huggingface-moon\n< cross-origin-opener-policy: same-origin\n< referrer-policy: strict-origin-when-cross-origin\n< x-request-id: Root=1-6877be24-4f628b292dc8a7a5339c41d3\n< access-control-allow-origin: https://huggingface.co\n< vary: Origin, Accept\n< access-control-expose-headers: X-Repo-Commit,X-Request-Id,X-Error-Code,X-Error-Message,X-Total-Count,ETag,Link,Accept-Ranges,Content-Range,X-Linked-Size,X-Linked-ETag,X-Xet-Hash\n< set-cookie: token=; Path=/; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Secure; SameSite=None\n< set-cookie: token=; Domain=huggingface.co; Path=/; Expires=Thu, 01 Jan 1970 00:00:00 GMT; Secure; SameSite=Lax\n< x-repo-commit: 712df366ffbc959d9f4279bf2da579230b7ca5d8\n< accept-ranges: bytes\n< x-linked-size: 218006142\n< x-linked-etag: \"01736bf26d0046ddec4ab8900fba3f0dc6500b038314b44d0edb73a7c88dec07\"\n< x-xet-hash: cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9\n< link: <https://huggingface.co/api/datasets/HuggingFaceTB/smoltalk2/xet-read-token/712df366ffbc959d9f4279bf2da579230b7ca5d8>; rel=\"xet-auth\", <https://cas-server.xethub.hf.co/reconstruction/cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9>; rel=\"xet-reconstruction-info\"\n< x-cache: Miss from cloudfront\n< via: 1.1 dd5af138aa8a11d8a70d5ef690ad1a2a.cloudfront.net (CloudFront)\n< x-amz-cf-pop: MSP50-P1\n< x-amz-cf-id: 0qXw2sJGrWCLVt7c-Vtn09uE3nu6CrJw9RmAKvNr_flG75muclvlIg==\n<\n* Ignoring the response-body\n100  1317  100  1317    0     0   9268      0 --:--:-- --:--:-- --:--:--  9268\n* Connection #0 to host huggingface.co left intact\n* Issue another request to this URL: 'https://cas-bridge.xethub.hf.co/xet-bridge-us/686fc33898943c873b45c9a0/cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250716T145416Z&X-Amz-Expires=3600&X-Amz-Signature=21a15b50740d73fd8ce82d5105733ca067d2e612ada22570e09e93ebcc7f8842&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%270000.parquet%3B+filename%3D%220000.parquet%22%3B&x-id=GetObject&Expires=1752681256&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjY4MTI1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODZmYzMzODk4OTQzYzg3M2I0NWM5YTAvY2Y4YTNhNTY2NWNmOGIyZmY2NjdmYjUyMzZhMWU1Y2IxM2M3NTgyOTU1Zjk1MzNjODhlMTM4Nzk5N2VmM2FmOSoifV19&Signature=Tl3xorJ-7yaWvG6Y1AhhRlV2Wko9QpoK1tdPOfNZaRbHo%7EdaAkJRJfcLAYD5YzozfHWBZMLlJsaMPJ1MAne21nr5%7E737sE6yLfBwHdP3ZFZhgrLsN%7EvkIWK2GYX543qTg-pVsf3it92w1oWyoyYNQ9srxLfEIuG2AKV2Nu3Ejl7S%7EaAq4Gv4jNemvRTLBFGgYPdUeuavudl4OD4RGkSGTnpzh-P-OBk5WvgpdZZnbb1cRAP73tFHsPDX4%7ETfQIor109G%7E0TB3Jq0wopO9WV0sMQyQs9peZc6bxONiTxb9aHM4yNvWNbVGtlPuC6YS4c9T1e9%7EehdgU4sDOI%7EhpaCvg__&Key-Pair-Id=K2L8F4GPSG1IFC'\n* Host cas-bridge.xethub.hf.co:443 was resolved.\n* IPv6: (none)\n* IPv4: 18.160.181.55, 18.160.181.54, 18.160.181.52, 18.160.181.88\n*   Trying 18.160.181.55:443...\n* Connected to cas-bridge.xethub.hf.co (18.160.181.55) port 443\n* ALPN: curl offers h2,http/1.1\n* (304) (OUT), TLS handshake, Client hello (1):\n} [328 bytes data]\n* (304) (IN), TLS handshake, Server hello (2):\n{ [122 bytes data]\n* (304) (IN), TLS handshake, Unknown (8):\n{ [19 bytes data]\n* (304) (IN), TLS handshake, Certificate (11):\n{ [3818 bytes data]\n* (304) (IN), TLS handshake, CERT verify (15):\n{ [264 bytes data]\n* (304) (IN), TLS handshake, Finished (20):\n{ [36 bytes data]\n* (304) (OUT), TLS handshake, Finished (20):\n} [36 bytes data]\n* SSL connection using TLSv1.3 / AEAD-AES128-GCM-SHA256 / [blank] / UNDEF\n* ALPN: server accepted h2\n* Server certificate:\n*  subject: CN=cas-bridge.xethub.hf.co\n*  start date: Jun  4 00:00:00 2025 GMT\n*  expire date: Jul  3 23:59:59 2026 GMT\n*  subjectAltName: host \"cas-bridge.xethub.hf.co\" matched cert's \"cas-bridge.xethub.hf.co\"\n*  issuer: C=US; O=Amazon; CN=Amazon RSA 2048 M04\n*  SSL certificate verify ok.\n* using HTTP/2\n* [HTTP/2] [1] OPENED stream for https://cas-bridge.xethub.hf.co/xet-bridge-us/686fc33898943c873b45c9a0/cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250716T145416Z&X-Amz-Expires=3600&X-Amz-Signature=21a15b50740d73fd8ce82d5105733ca067d2e612ada22570e09e93ebcc7f8842&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%270000.parquet%3B+filename%3D%220000.parquet%22%3B&x-id=GetObject&Expires=1752681256&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjY4MTI1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODZmYzMzODk4OTQzYzg3M2I0NWM5YTAvY2Y4YTNhNTY2NWNmOGIyZmY2NjdmYjUyMzZhMWU1Y2IxM2M3NTgyOTU1Zjk1MzNjODhlMTM4Nzk5N2VmM2FmOSoifV19&Signature=Tl3xorJ-7yaWvG6Y1AhhRlV2Wko9QpoK1tdPOfNZaRbHo%7EdaAkJRJfcLAYD5YzozfHWBZMLlJsaMPJ1MAne21nr5%7E737sE6yLfBwHdP3ZFZhgrLsN%7EvkIWK2GYX543qTg-pVsf3it92w1oWyoyYNQ9srxLfEIuG2AKV2Nu3Ejl7S%7EaAq4Gv4jNemvRTLBFGgYPdUeuavudl4OD4RGkSGTnpzh-P-OBk5WvgpdZZnbb1cRAP73tFHsPDX4%7ETfQIor109G%7E0TB3Jq0wopO9WV0sMQyQs9peZc6bxONiTxb9aHM4yNvWNbVGtlPuC6YS4c9T1e9%7EehdgU4sDOI%7EhpaCvg__&Key-Pair-Id=K2L8F4GPSG1IFC\n* [HTTP/2] [1] [:method: GET]\n* [HTTP/2] [1] [:scheme: https]\n* [HTTP/2] [1] [:authority: cas-bridge.xethub.hf.co]\n* [HTTP/2] [1] [:path: /xet-bridge-us/686fc33898943c873b45c9a0/cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250716T145416Z&X-Amz-Expires=3600&X-Amz-Signature=21a15b50740d73fd8ce82d5105733ca067d2e612ada22570e09e93ebcc7f8842&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%270000.parquet%3B+filename%3D%220000.parquet%22%3B&x-id=GetObject&Expires=1752681256&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjY4MTI1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODZmYzMzODk4OTQzYzg3M2I0NWM5YTAvY2Y4YTNhNTY2NWNmOGIyZmY2NjdmYjUyMzZhMWU1Y2IxM2M3NTgyOTU1Zjk1MzNjODhlMTM4Nzk5N2VmM2FmOSoifV19&Signature=Tl3xorJ-7yaWvG6Y1AhhRlV2Wko9QpoK1tdPOfNZaRbHo%7EdaAkJRJfcLAYD5YzozfHWBZMLlJsaMPJ1MAne21nr5%7E737sE6yLfBwHdP3ZFZhgrLsN%7EvkIWK2GYX543qTg-pVsf3it92w1oWyoyYNQ9srxLfEIuG2AKV2Nu3Ejl7S%7EaAq4Gv4jNemvRTLBFGgYPdUeuavudl4OD4RGkSGTnpzh-P-OBk5WvgpdZZnbb1cRAP73tFHsPDX4%7ETfQIor109G%7E0TB3Jq0wopO9WV0sMQyQs9peZc6bxONiTxb9aHM4yNvWNbVGtlPuC6YS4c9T1e9%7EehdgU4sDOI%7EhpaCvg__&Key-Pair-Id=K2L8F4GPSG1IFC]\n* [HTTP/2] [1] [user-agent: curl/8.7.1]\n* [HTTP/2] [1] [accept: */*]\n* [HTTP/2] [1] [range: bytes=217875070-218006142]\n> GET /xet-bridge-us/686fc33898943c873b45c9a0/cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20250716%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250716T145416Z&X-Amz-Expires=3600&X-Amz-Signature=21a15b50740d73fd8ce82d5105733ca067d2e612ada22570e09e93ebcc7f8842&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%270000.parquet%3B+filename%3D%220000.parquet%22%3B&x-id=GetObject&Expires=1752681256&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1MjY4MTI1Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82ODZmYzMzODk4OTQzYzg3M2I0NWM5YTAvY2Y4YTNhNTY2NWNmOGIyZmY2NjdmYjUyMzZhMWU1Y2IxM2M3NTgyOTU1Zjk1MzNjODhlMTM4Nzk5N2VmM2FmOSoifV19&Signature=Tl3xorJ-7yaWvG6Y1AhhRlV2Wko9QpoK1tdPOfNZaRbHo%7EdaAkJRJfcLAYD5YzozfHWBZMLlJsaMPJ1MAne21nr5%7E737sE6yLfBwHdP3ZFZhgrLsN%7EvkIWK2GYX543qTg-pVsf3it92w1oWyoyYNQ9srxLfEIuG2AKV2Nu3Ejl7S%7EaAq4Gv4jNemvRTLBFGgYPdUeuavudl4OD4RGkSGTnpzh-P-OBk5WvgpdZZnbb1cRAP73tFHsPDX4%7ETfQIor109G%7E0TB3Jq0wopO9WV0sMQyQs9peZc6bxONiTxb9aHM4yNvWNbVGtlPuC6YS4c9T1e9%7EehdgU4sDOI%7EhpaCvg__&Key-Pair-Id=K2L8F4GPSG1IFC HTTP/2\n> Host: cas-bridge.xethub.hf.co\n> User-Agent: curl/8.7.1\n> Accept: */*\n> Range: bytes=217875070-218006142\n>\n* Request completely sent off\n< HTTP/2 206\n< content-length: 131072\n< date: Mon, 14 Jul 2025 08:40:28 GMT\n< x-request-id: 01K041FDPVA03RR2PRXDZSN30G\n< content-disposition: inline; filename*=UTF-8''0000.parquet; filename=\"0000.parquet\";\n< cache-control: public, max-age=31536000\n< etag: \"cf8a3a5665cf8b2ff667fb5236a1e5cb13c7582955f9533c88e1387997ef3af9\"\n< access-control-allow-origin: *\n< access-control-allow-headers: Content-Range, Content-Type, Content-Disposition, ETag\n< access-control-expose-headers: Accept-Ranges, Content-Range, Content-Type, Content-Disposition, ETag, X-Cache\n< x-cache: Hit from cloudfront\n< via: 1.1 1c857e24a4dc84d2d9c78d5b3463bed6.cloudfront.net (CloudFront)\n< x-amz-cf-pop: MSP50-P2\n< x-amz-cf-id: 3SxFmQa5wLeeXbNiwaAo0_RwoR_n7-SivjsLjDLG-Pwn5UhG2oiEQA==\n< age: 195496\n< content-security-policy: default-src 'none'; sandbox\n< content-range: bytes 217875070-218006141/218006142\n<\n{ [8192 bytes data]\n100  128k  100  128k    0     0   769k      0 --:--:-- --:--:-- --:--:--  769k\n* Connection #1 to host cas-bridge.xethub.hf.co left intact\n\n</details>\n\n### Expected behavior\n\nalways get back a `206` \n\n### Environment info\n\nn/a<end_of_turn>\n<start_of_turn>model\nInconsistent range request behavior for parquet REST api<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n... to save a few seconds when resolving repos with many data files.<end_of_turn>\n<start_of_turn>model\nRemove `os.path.relpath` in `resolve_patterns`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix typo in code example in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhy the datasets is not updating the dill ? \nJust want to know if I update the dill version in dill what will be the repucssion. \nFor now in multiplaces I have to update the library like process requirequire dill 0.4.0 so why not datasets. \nAdding a pr too. \n\n### Steps to reproduce the bug\n\n.\n\n### Expected behavior\n\n.\n\n### Environment info\n\n.<end_of_turn>\n<start_of_turn>model\nDill version update<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR gets multiprocessing to work on mac os:\n\n```python\nfrom datasets import load_dataset\n\nds = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\").take(100)\n\nds = ds.map(lambda x: x, num_proc=4)\n\nds.push_to_hub(\"burtenshaw/dataset-test\", num_proc=4)\n```<end_of_turn>\n<start_of_turn>model\n[FIX] force spawning pool for MacOS<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nFix metrics dead link<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWith the latest 4.0.0 release, huggingface toxicity evaluation module fails with error: `ValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).`\n\n### Steps to reproduce the bug\n\nRepro:\n```\n>>> toxicity.compute(predictions=[\"This is a response\"])\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/serena.ruan/miniconda3/envs/mlflow-310/lib/python3.10/site-packages/evaluate/module.py\", line 467, in compute\n    output = self._compute(**inputs, **compute_kwargs)\n  File \"/Users/serena.ruan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-measurement--toxicity/2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b/toxicity.py\", line 135, in _compute\n    scores = toxicity(predictions, self.toxic_classifier, toxic_label)\n  File \"/Users/serena.ruan/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-measurement--toxicity/2390290fa0bf6d78480143547c6b08f3d4f8805b249df8c7a8e80d0ce8e3778b/toxicity.py\", line 103, in toxicity\n    for pred_toxic in toxic_classifier(preds):\n  File \"/Users/serena.ruan/miniconda3/envs/mlflow-310/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 159, in __call__\n    result = super().__call__(*inputs, **kwargs)\n  File \"/Users/serena.ruan/miniconda3/envs/mlflow-310/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1431, in __call__\n    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n  File \"/Users/serena.ruan/miniconda3/envs/mlflow-310/lib/python3.10/site-packages/transformers/pipelines/base.py\", line 1437, in run_single\n    model_inputs = self.preprocess(inputs, **preprocess_params)\n  File \"/Users/serena.ruan/miniconda3/envs/mlflow-310/lib/python3.10/site-packages/transformers/pipelines/text_classification.py\", line 183, in preprocess\n    return self.tokenizer(inputs, return_tensors=return_tensors, **tokenizer_kwargs)\n  File \"/Users/serena.ruan/miniconda3/envs/mlflow-310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2867, in __call__\n    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n  File \"/Users/serena.ruan/miniconda3/envs/mlflow-310/lib/python3.10/site-packages/transformers/tokenization_utils_base.py\", line 2927, in _call_one\n    raise ValueError(\nValueError: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).\n```\n\n### Expected behavior\n\nThis works before 4.0.0 release\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: macOS-15.5-arm64-arm-64bit\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.33.0\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nToxicity fails with datasets 4.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAfter latest release of `ruff` (https://pypi.org/project/ruff/0.0.284/), we get the following CI error:\r\n```\r\nsrc/datasets/utils/py_utils.py:689:12: E721 Do not compare types, use `isinstance()`\r\n```<end_of_turn>\n<start_of_turn>model\nCI check_code_quality error: E721 Do not compare types, use `isinstance()`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nSome datasets may contain an id-like field, for example the `id` field in [wikimedia/wikipedia](https://huggingface.co/datasets/wikimedia/wikipedia) and the `_id` field in [BeIR/dbpedia-entity](https://huggingface.co/datasets/BeIR/dbpedia-entity). HF datasets support efficient random access via row, but not via this kinds of id fields. I wonder if it is possible to add support for indexing by a custom \"id-like\" field to enable random access via such ids. The ids may be numbers or strings.\r\n\r\n### Motivation\r\n\r\nIn some cases, especially during inference/evaluation, I may want to find out the item that has a specified id, defined by the dataset itself.\r\n\r\nFor example, in a typical re-ranking setting in information retrieval, the user may want to re-rank the set of candidate documents of each query. The input is usually presented in a TREC-style run file, with the following format:\r\n\r\n```\r\n<qid> Q0 <docno> <rank> <score> <tag>\r\n```\r\n\r\nThe re-ranking program should be able to fetch the queries and documents according to the `<qid>` and `<docno>`, which are the original id defined in the query/document datasets. To accomplish this, I have to iterate over the whole HF dataset to get the mapping from real ids to row ids every time I start the program, which is time-consuming. Thus I want HF dataset to provide options for users to index by a custom id column, not by row.\r\n\r\n### Your contribution\r\n\r\nI'm not an expert in this project and I'm afraid that I'm not able to make contributions on the code.<end_of_turn>\n<start_of_turn>model\n[Feature request] Indexing datasets by a customly-defined id field to enable random access dataset items via the id<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### What does this PR do?\r\nThis PR introduces a new `stateful` option to the `dataset.shuffle` method, which defaults to `False`. \r\nWhen enabled, this option allows for resumable shuffling of `IterableDataset` instances, albeit with some additional memory overhead.\r\n\r\nKey points:\r\n* All tests have passed\r\n* Docstrings have been updated to reflect the new functionality\r\n\r\nI'm very looking forward to receiving feedback on this implementation! @lhoestq<end_of_turn>\n<start_of_turn>model\nMake IterableDataset (optionally) resumable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCannot be imported on Google Colab, the import throws the following error:\r\nValueError: pyarrow.lib.IpcWriteOptions size changed, may indicate binary incompatibility. Expected 88 from C header, got 72 from PyObject\n\n### Steps to reproduce the bug\n\n1. `! pip install -U datasets`\r\n2. `import datasets`\n\n### Expected behavior\n\nShould be possible to use the library\n\n### Environment info\n\n- `datasets` version: 2.17.0\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0<end_of_turn>\n<start_of_turn>model\nImport error on Google Colab<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I run down the below code shows this error: AttributeError: module 'numpy' has no attribute '_no_nep50_warning'\r\nI also added this issue in transformers library please check out: [link](https://github.com/huggingface/transformers/issues/28099)\n\n### Steps to reproduce the bug\n\n## Reproduction\r\nHi, please check this line of code, when I run Show attribute error.\r\n\r\n```\r\nfrom datasets import load_dataset\r\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\r\n\r\n# Select an audio file and read it:\r\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\naudio_sample = ds[0][\"audio\"]\r\nwaveform = audio_sample[\"array\"]\r\nsampling_rate = audio_sample[\"sampling_rate\"]\r\n\r\n# Load the Whisper model in Hugging Face format:\r\nprocessor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\r\nmodel = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\r\n\r\n# Use the model and processor to transcribe the audio:\r\ninput_features = processor(\r\n    waveform, sampling_rate=sampling_rate, return_tensors=\"pt\"\r\n).input_features\r\n\r\n# Generate token ids\r\npredicted_ids = model.generate(input_features)\r\n\r\n# Decode token ids to text\r\ntranscription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\r\n\r\ntranscription[0]\r\n\r\n```\r\n\r\n**Attribute Error**\r\n```\r\nAttributeError                            Traceback (most recent call last)\r\nCell In[9], line 6\r\n      4 # Select an audio file and read it:\r\n      5 ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\r\n----> 6 audio_sample = ds[0][\"audio\"]\r\n      7 waveform = audio_sample[\"array\"]\r\n      8 sampling_rate = audio_sample[\"sampling_rate\"]\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/arrow_dataset.py:2795, in Dataset.__getitem__(self, key)\r\n   2793 def __getitem__(self, key):  # noqa: F811\r\n   2794     \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2795     return self._getitem(key)\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/arrow_dataset.py:2780, in Dataset._getitem(self, key, **kwargs)\r\n   2778 formatter = get_formatter(format_type, features=self._info.features, **format_kwargs)\r\n   2779 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n-> 2780 formatted_output = format_table(\r\n   2781     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n   2782 )\r\n   2783 return formatted_output\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:629, in format_table(table, key, formatter, format_columns, output_all_columns)\r\n    627 python_formatter = PythonFormatter(features=formatter.features)\r\n    628 if format_columns is None:\r\n--> 629     return formatter(pa_table, query_type=query_type)\r\n    630 elif query_type == \"column\":\r\n    631     if key in format_columns:\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:396, in Formatter.__call__(self, pa_table, query_type)\r\n    394 def __call__(self, pa_table: pa.Table, query_type: str) -> Union[RowFormat, ColumnFormat, BatchFormat]:\r\n    395     if query_type == \"row\":\r\n--> 396         return self.format_row(pa_table)\r\n    397     elif query_type == \"column\":\r\n    398         return self.format_column(pa_table)\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:437, in PythonFormatter.format_row(self, pa_table)\r\n    435     return LazyRow(pa_table, self)\r\n    436 row = self.python_arrow_extractor().extract_row(pa_table)\r\n--> 437 row = self.python_features_decoder.decode_row(row)\r\n    438 return row\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/formatting/formatting.py:215, in PythonFeaturesDecoder.decode_row(self, row)\r\n    214 def decode_row(self, row: dict) -> dict:\r\n--> 215     return self.features.decode_example(row) if self.features else row\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/features/features.py:1917, in Features.decode_example(self, example, token_per_repo_id)\r\n   1903 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1904     \"\"\"Decode example with custom feature decoding.\r\n   1905 \r\n   1906     Args:\r\n   (...)\r\n   1914         `dict[str, Any]`\r\n   1915     \"\"\"\r\n-> 1917     return {\r\n   1918         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1919         if self._column_requires_decoding[column_name]\r\n   1920         else value\r\n   1921         for column_name, (feature, value) in zip_dict(\r\n   1922             {key: value for key, value in self.items() if key in example}, example\r\n   1923         )\r\n   1924     }\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/features/features.py:1918, in <dictcomp>(.0)\r\n   1903 def decode_example(self, example: dict, token_per_repo_id: Optional[Dict[str, Union[str, bool, None]]] = None):\r\n   1904     \"\"\"Decode example with custom feature decoding.\r\n   1905 \r\n   1906     Args:\r\n   (...)\r\n   1914         `dict[str, Any]`\r\n   1915     \"\"\"\r\n   1917     return {\r\n-> 1918         column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\r\n   1919         if self._column_requires_decoding[column_name]\r\n   1920         else value\r\n   1921         for column_name, (feature, value) in zip_dict(\r\n   1922             {key: value for key, value in self.items() if key in example}, example\r\n   1923         )\r\n   1924     }\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/features/features.py:1339, in decode_nested_example(schema, obj, token_per_repo_id)\r\n   1336 elif isinstance(schema, (Audio, Image)):\r\n   1337     # we pass the token to read and decode files from private repositories in streaming mode\r\n   1338     if obj is not None and schema.decode:\r\n-> 1339         return schema.decode_example(obj, token_per_repo_id=token_per_repo_id)\r\n   1340 return obj\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/datasets/features/audio.py:191, in Audio.decode_example(self, value, token_per_repo_id)\r\n    189 array = array.T\r\n    190 if self.mono:\r\n--> 191     array = librosa.to_mono(array)\r\n    192 if self.sampling_rate and self.sampling_rate != sampling_rate:\r\n    193     array = librosa.resample(array, orig_sr=sampling_rate, target_sr=self.sampling_rate)\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/lazy_loader/__init__.py:78, in attach.<locals>.__getattr__(name)\r\n     76 submod_path = f\"{package_name}.{attr_to_modules[name]}\"\r\n     77 submod = importlib.import_module(submod_path)\r\n---> 78 attr = getattr(submod, name)\r\n     80 # If the attribute lives in a file (module) with the same\r\n     81 # name as the attribute, ensure that the attribute and *not*\r\n     82 # the module is accessible on the package.\r\n     83 if name == attr_to_modules[name]:\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/lazy_loader/__init__.py:77, in attach.<locals>.__getattr__(name)\r\n     75 elif name in attr_to_modules:\r\n     76     submod_path = f\"{package_name}.{attr_to_modules[name]}\"\r\n---> 77     submod = importlib.import_module(submod_path)\r\n     78     attr = getattr(submod, name)\r\n     80     # If the attribute lives in a file (module) with the same\r\n     81     # name as the attribute, ensure that the attribute and *not*\r\n     82     # the module is accessible on the package.\r\n\r\nFile /usr/lib/python3.8/importlib/__init__.py:127, in import_module(name, package)\r\n    125             break\r\n    126         level += 1\r\n--> 127 return _bootstrap._gcd_import(name[level:], package, level)\r\n\r\nFile <frozen importlib._bootstrap>:1014, in _gcd_import(name, package, level)\r\n\r\nFile <frozen importlib._bootstrap>:991, in _find_and_load(name, import_)\r\n\r\nFile <frozen importlib._bootstrap>:975, in _find_and_load_unlocked(name, import_)\r\n\r\nFile <frozen importlib._bootstrap>:671, in _load_unlocked(spec)\r\n\r\nFile <frozen importlib._bootstrap_external>:848, in exec_module(self, module)\r\n\r\nFile <frozen importlib._bootstrap>:219, in _call_with_frames_removed(f, *args, **kwds)\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/librosa/core/audio.py:13\r\n     11 import audioread\r\n     12 import numpy as np\r\n---> 13 import scipy.signal\r\n     14 import soxr\r\n     15 import lazy_loader as lazy\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/scipy/signal/__init__.py:323\r\n    314 from ._spline import (  # noqa: F401\r\n    315     cspline2d,\r\n    316     qspline2d,\r\n   (...)\r\n    319     symiirorder2,\r\n    320 )\r\n    322 from ._bsplines import *\r\n--> 323 from ._filter_design import *\r\n    324 from ._fir_filter_design import *\r\n    325 from ._ltisys import *\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/scipy/signal/_filter_design.py:16\r\n     13 from numpy.polynomial.polynomial import polyval as npp_polyval\r\n     14 from numpy.polynomial.polynomial import polyvalfromroots\r\n---> 16 from scipy import special, optimize, fft as sp_fft\r\n     17 from scipy.special import comb\r\n     18 from scipy._lib._util import float_factorial\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/scipy/optimize/__init__.py:405\r\n      1 \"\"\"\r\n      2 =====================================================\r\n      3 Optimization and root finding (:mod:`scipy.optimize`)\r\n   (...)\r\n    401 \r\n    402 \"\"\"\r\n    404 from ._optimize import *\r\n--> 405 from ._minimize import *\r\n    406 from ._root import *\r\n    407 from ._root_scalar import *\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/scipy/optimize/_minimize.py:26\r\n     24 from ._trustregion_krylov import _minimize_trust_krylov\r\n     25 from ._trustregion_exact import _minimize_trustregion_exact\r\n---> 26 from ._trustregion_constr import _minimize_trustregion_constr\r\n     28 # constrained minimization\r\n     29 from ._lbfgsb_py import _minimize_lbfgsb\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/scipy/optimize/_trustregion_constr/__init__.py:4\r\n      1 \"\"\"This module contains the equality constrained SQP solver.\"\"\"\r\n----> 4 from .minimize_trustregion_constr import _minimize_trustregion_constr\r\n      6 __all__ = ['_minimize_trustregion_constr']\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/scipy/optimize/_trustregion_constr/minimize_trustregion_constr.py:5\r\n      3 from scipy.sparse.linalg import LinearOperator\r\n      4 from .._differentiable_functions import VectorFunction\r\n----> 5 from .._constraints import (\r\n      6     NonlinearConstraint, LinearConstraint, PreparedConstraint, strict_bounds)\r\n      7 from .._hessian_update_strategy import BFGS\r\n      8 from .._optimize import OptimizeResult\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/scipy/optimize/_constraints.py:8\r\n      6 from ._optimize import OptimizeWarning\r\n      7 from warnings import warn, catch_warnings, simplefilter\r\n----> 8 from numpy.testing import suppress_warnings\r\n      9 from scipy.sparse import issparse\r\n     12 def _arr_to_scalar(x):\r\n     13     # If x is a numpy array, return x.item().  This will\r\n     14     # fail if the array has more than one element.\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/numpy/testing/__init__.py:11\r\n      8 from unittest import TestCase\r\n     10 from . import _private\r\n---> 11 from ._private.utils import *\r\n     12 from ._private.utils import (_assert_valid_refcount, _gen_alignment_data)\r\n     13 from ._private import extbuild, decorators as dec\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/numpy/testing/_private/utils.py:480\r\n    476         pprint.pprint(desired, msg)\r\n    477         raise AssertionError(msg.getvalue())\r\n--> 480 @np._no_nep50_warning()\r\n    481 def assert_almost_equal(actual,desired,decimal=7,err_msg='',verbose=True):\r\n    482     \"\"\"\r\n    483     Raises an AssertionError if two items are not equal up to desired\r\n    484     precision.\r\n   (...)\r\n    548 \r\n    549     \"\"\"\r\n    550     __tracebackhide__ = True  # Hide traceback for py.test\r\n\r\nFile /opt/pytorch/lib/python3.8/site-packages/numpy/__init__.py:313, in __getattr__(attr)\r\n    305     raise AttributeError(__former_attrs__[attr])\r\n    307 # Importing Tester requires importing all of UnitTest which is not a\r\n    308 # cheap import Since it is mainly used in test suits, we lazy import it\r\n    309 # here to save on the order of 10 ms of import time for most users\r\n    310 #\r\n    311 # The previous way Tester was imported also had a side effect of adding\r\n    312 # the full `numpy.testing` namespace\r\n--> 313 if attr == 'testing':\r\n    314     import numpy.testing as testing\r\n    315     return testing\r\n\r\nAttributeError: module 'numpy' has no attribute '_no_nep50_warning'\r\n\r\n```\n\n### Expected behavior\n\n``` ' Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.' ```\r\n\r\nAlso, make sure this script is provided for your official website so please update:\r\n[script](https://huggingface.co/docs/transformers/model_doc/whisper)\n\n### Environment info\n\n**System Info**\r\n* transformers -> 4.36.1\r\n* datasets -> 2.15.0\r\n* huggingface_hub -> 0.19.4\r\n* python -> 3.8.10\r\n* accelerate -> 0.25.0\r\n* pytorch -> 2.0.1+cpu\r\n* Using GPU in Script -> No\r\n<end_of_turn>\n<start_of_turn>model\nDataset not loading successfully.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe pile does not stream, a \"File not Found error\" is returned. It looks like the Pile's files have been moved.\n\n### Steps to reproduce the bug\n\nTo reproduce run the following code:\r\n\r\n```\r\nfrom datasets import load_dataset\r\n\r\ndataset = load_dataset('EleutherAI/pile', 'en', split='train', streaming=True)\r\nnext(iter(dataset))\r\n```\r\n\r\nI get the following error:\r\n`FileNotFoundError: https://the-eye.eu/public/AI/pile/train/00.jsonl.zst`\n\n### Expected behavior\n\nReturn the data in a stream.\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Windows-10-10.0.22621-SP0\r\n- Python version: 3.11.5\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nStreaming the Pile: Missing Files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHi there,\r\n\r\nI use streaming and interleaving to combine multiple datasets saved in jsonl files. The size of dataset can vary (from 100ish to 100k-ish). I use dataset.map() and a big batch size to reduce the IO cost. It was working fine with datasets-2.16.1 but this problem shows up after I upgraded to datasets-2.19.2. With  2.21.0 the problem remains.\r\n\r\nPlease see the code below to reproduce the problem.\r\n\r\nThe dataset can iterate correctly if we set either streaming=False or drop_last_batch=False.\r\n\r\nI have to use drop_last_batch=True since it's for distributed training.\r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\n# datasets==2.21.0\r\nimport datasets\r\ndef data_prepare(examples):\r\n    print(examples[\"sentence1\"][0])\r\n    return examples\r\n\r\nbatch_size = 101\r\n# the size of the dataset is 100\r\n# the dataset iterates correctly if we set either streaming=False or drop_last_batch=False \r\ndataset = datasets.load_dataset(\"mteb/biosses-sts\", split=\"test\", streaming=True)\r\ndataset = dataset.map(lambda x: data_prepare(x),\r\n                      drop_last_batch=True,\r\n                      batched=True, batch_size=batch_size)\r\nfor ex in dataset:\r\n    print(ex)\r\n    pass\r\n\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset iterates regardless of the batch size.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.21.0\r\n- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.24.5\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.2.0\r\n<end_of_turn>\n<start_of_turn>model\nStream dataset does not iterate if the batch size is larger than the dataset size (related to drop_last_batch)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nfrom datasets import load_dataset\nlcb_codegen = load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v2\")\n\nor\nconfigs = get_dataset_config_names(\"livecodebench/code_generation_lite\", trust_remote_code=True)\n\nboth error:\nTraceback (most recent call last):\nFile \"\", line 1, in\nFile \"/workspace/miniconda/envs/grpo/lib/python3.10/site-packages/datasets/load.py\", line 2131, in load_dataset\nbuilder_instance = load_dataset_builder(\nFile \"/workspace/miniconda/envs/grpo/lib/python3.10/site-packages/datasets/load.py\", line 1888, in load_dataset_builder\nbuilder_instance: DatasetBuilder = builder_cls(\nTypeError: 'NoneType' object is not callable\n\n### Steps to reproduce the bug\n\nfrom datasets import get_dataset_config_names\n\nconfigs = get_dataset_config_names(\"livecodebench/code_generation_lite\", trust_remote_code=True)\nOR \nlcb_codegen = load_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v2\")\n\n### Expected behavior\n\nload  datasets  livecodebench/code_generation_lite\n\n\n### Environment info\n\n import  datasets\nversion '3.3.2'<end_of_turn>\n<start_of_turn>model\nload_dataset(\"livecodebench/code_generation_lite\", version_tag=\"release_v2\")  TypeError: 'NoneType' object is not callable<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nhttps://github.com/huggingface/datasets/blob/12cfc1196e62847e2e8239fbd727a02cbc86ddec/src/datasets/builder.py#L518-L522\r\n\r\nIf `config_name` is `None`, `DEFAULT_CONFIG_NAME` should be select. But once users pass `config_kwargs` to their customized `BuilderConfig`, the logic is ignored, and dataset cannot select the default config from multiple configs.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndatasets.load_dataset('/dataset/with/multiple/config'')  # Ok\r\ndatasets.load_dataset('/dataset/with/multiple/config', some_field_in_config='some')  # Err\r\n```\n\n### Expected behavior\n\nDefault config behavior should be consistent.\n\n### Environment info\n\n- `datasets` version: 2.14.3\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.15\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\ndefault config name doesn't work when config kwargs are specified.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPR #7634 replaced the Sequence feature with List in 4.0.0, so datasets saved with version 4.0.0 with that feature cannot be loaded by earlier versions. There is no clear option in 4.0.0 to use the legacy feature type to preserve backward compatibility.\n\nWhy is this a problem? I have a complex preprocessing and training pipeline dependent on 3.6.0; we manage a very large number of separate datasets that get concatenated during training. If just one of those datasets is saved with 4.0.0, they become unusable, and we have no way of \"fixing\" them. I can load them in 4.0.0 but I can't re-save with the legacy feature type, and I can't load it in 3.6.0 for obvious reasons.\n\nPerhaps I'm missing something here, since the PR says that backward compatibility is preserved; if so, it's not obvious to me how.<end_of_turn>\n<start_of_turn>model\nReplacement of \"Sequence\" feature with \"List\" breaks backward compatibility<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhile generating the docs, we get an error when some parameter defaults to a `NamedSplit` value, like:\r\n```python\r\ndef call_function(split=Split.TRAIN):\r\n    ...\r\n```\r\nThe error is: ValueError: Equality not supported between split train and <class 'inspect._empty'>\r\n\r\nSee: https://github.com/huggingface/datasets/actions/runs/9869660902/job/27254359863?pr=7015\r\n```\r\nBuilding the MDX files:  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258b| 58/60 [00:00<00:00, 91.94it/s] \r\nTraceback (most recent call last):\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/build_doc.py\", line 197, in build_mdx_files\r\n    content, new_anchors, source_files, errors = resolve_autodoc(\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/build_doc.py\", line 123, in resolve_autodoc\r\n    doc = autodoc(\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/autodoc.py\", line 499, in autodoc\r\n    method_doc, check = document_object(\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/autodoc.py\", line 395, in document_object\r\n    signature = format_signature(obj)\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/autodoc.py\", line 126, in format_signature\r\n    if param.default != inspect._empty:\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/datasets/splits.py\", line 136, in __ne__\r\n    return not self.__eq__(other)\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/datasets/splits.py\", line 379, in __eq__\r\n    raise ValueError(f\"Equality not supported between split {self} and {other}\")\r\nValueError: Equality not supported between split train and <class 'inspect._empty'>\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/runner/work/datasets/datasets/.venv/bin/doc-builder\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/commands/doc_builder_cli.py\", line 47, in main\r\n    args.func(args)\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/commands/build.py\", line 102, in build_command\r\n    build_doc(\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/build_doc.py\", line 367, in build_doc\r\n    anchors_mapping, source_files_mapping = build_mdx_files(\r\n  File \"/home/runner/work/datasets/datasets/.venv/lib/python3.10/site-packages/doc_builder/build_doc.py\", line 230, in build_mdx_files\r\n    raise type(e)(f\"There was an error when converting {file} to the MDX format.\\n\" + e.args[0]) from e\r\nValueError: There was an error when converting ../datasets/docs/source/package_reference/main_classes.mdx to the MDX format.\r\nEquality not supported between split train and <class 'inspect._empty'>\r\n```<end_of_turn>\n<start_of_turn>model\nDocs are not generated when a parameter defaults to a NamedSplit value<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nExample discussed in detail here: https://huggingface.co/datasets/sasha/birdsnap/discussions/1\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset, VerificationMode\r\ndataset = load_dataset(\r\n    'sasha/birdsnap',\r\n    split=\"train\",\r\n    verification_mode=VerificationMode.ALL_CHECKS,\r\n    streaming=True # I recommend using streaming=True when reproducing, as this dataset is large\r\n)\r\nfor idx, row in enumerate(dataset):\r\n    # Iterating to 9287 took 7 minutes for me\r\n    # If you already have the data locally cached and set streaming=False, you see the same error just by with dataset[9287]\r\n    pass\r\n    # error at 9287 OSError: image file is truncated (45 bytes not processed)\r\n    # note that we can't avoid the error using a try/except + continue inside the loop\r\n```\n\n### Expected behavior\n\nAble to escape errors in casting to Image() without killing the whole loop\n\n### Environment info\n\n- `datasets` version: 2.15.0\r\n- Platform: Linux-5.15.0-84-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.5\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 2.1.3\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nIf an image in a dataset is corrupted, we get unescapable error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHello,\r\n\r\nI have a question regarding the map function in the Hugging Face datasets.\r\n\r\nThe situation is as follows: when I load a jsonl file using load_dataset(..., streaming=False), and then utilize the map function to process it, I specify that the returned example should be of type Torch.tensor. However, I noticed that after applying the map function, the datatype automatically changes to List, which leads to errors in my program.\r\n\r\nI attempted to use load_dataset(..., streaming=True), and this issue no longer occurs. I'm not entirely clear on why this happens. Could you please provide some insights into this?\r\n\r\n### Steps to reproduce the bug\r\n\r\n1.dataset =  load_dataset(xxx, streaming = False)\r\n2. dataset.map(function), function will return torch.Tensor.\r\n3. you will find the format of data in dataset is List.\r\n\r\n### Expected behavior\r\n\r\nI expected to receieve the format of data is torch.Tensor.\r\n\r\n### Environment info\r\n\r\n2.18.0<end_of_turn>\n<start_of_turn>model\nA Question About the Map Function<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nthe format is not relevant for imagefolder, and makes the viewer fail to process datasets on HF (so many that the viewer takes more time to process new datasets)<end_of_turn>\n<start_of_turn>model\nRemove .h5 from imagefolder extensions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nMore robust preupload retry mechanism<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`get_deletions_and_dataset_card` stuck at error a commit has happened error since push to hub for http error 412 for tag 4.1.0. The error does not exists in 4.0.0. \n\n### Steps to reproduce the bug\n\nCreate code to use push_to_hub, ran twice each time with different content for datasets.Dataset.\nThe code will stuck in time.sleep loop for `get_deletions_and_dataset_card`. If error is explicitly printed, the error is HTTP 412.\n\n### Expected behavior\n\nNew datasets overwrite existing one on repo.\n\n### Environment info\n\ndatasets 4.1.0<end_of_turn>\n<start_of_turn>model\npush_to_hub not overwriting but stuck in a loop when there are existing commits<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRe-enable raising error from huggingface-hub FutureWarning in CI, which was disabled by PR:\r\n- #6876\r\n\r\nNote that this can only be done once transformers releases the fix:\r\n- https://github.com/huggingface/transformers/pull/31007<end_of_turn>\n<start_of_turn>model\nRe-enable raising error from huggingface-hub FutureWarning in CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen it fails, `preupload_lfs_files` throws a [`RuntimeError`](https://github.com/huggingface/huggingface_hub/blob/5eefebee2c150a2df950ab710db350e96c711433/src/huggingface_hub/_commit_api.py#L402) error and chains the original HTTP error. This PR modifies the retry mechanism's error handling to account for that.\r\n\r\nFix https://github.com/huggingface/datasets/issues/6392<end_of_turn>\n<start_of_turn>model\nFix shard retry mechanism in `push_to_hub`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi everyone, I uploaded my dataset https://huggingface.co/datasets/PRAIG/SMB a few months ago while I was waiting for a conference acceptance response. Without modifying anything in the dataset repository now the Dataset viewer is not rendering the metadata.jsonl annotations, neither it is being downloaded when using load_dataset. Can you please help? Thank you in advance.\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\nds = load_dataset(\"PRAIG/SMB\")\nds = ds[\"train\"]\n\n### Expected behavior\n\n It is expected to have all the metadata available in the jsonl file. Fields like: \"score_id\", \"original_width\", \"original_height\", \"regions\"... among others.\n\n### Environment info\n\ndatasets==3.6.0, python 3.13.3 (but he problem is already in the huggingface dataset page)<end_of_turn>\n<start_of_turn>model\nMy already working dataset (when uploaded few months ago) now is ignoring metadata.jsonl<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThis code fails to load the dataset it just saved:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\nMODEL = \"google-bert/bert-base-cased\"\r\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\r\n\r\ndataset = load_dataset(\"yelp_review_full\")\r\n\r\ndef tokenize_function(examples):\r\n    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\r\n\r\ntokenized_datasets = dataset.map(tokenize_function, batched=True)\r\ntokenized_datasets.save_to_disk(\"dataset\")\r\n\r\ntokenized_datasets = load_dataset(\"dataset/\")  # raises\r\n```\r\n\r\nIt raises `ValueError: Couldn't infer the same data file format for all splits. Got {NamedSplit('train'): ('arrow', {}), NamedSplit('test'): ('json', {})}`.\r\n\r\nI believe this bug is caused by the [logic that tries to infer dataset format](https://github.com/huggingface/datasets/blob/9af8dd3de7626183a9a9ec8973cebc672d690400/src/datasets/load.py#L556). It counts the most common file extension. However, a small dataset can fit in a single `.arrow` file and have two JSON metadata files, causing the format to be inferred as JSON:\r\n\r\n```shell\r\n$ ls -l dataset/test\r\n-rw-r--r-- 1 sliedes sliedes 191498784 Jul  1 13:55 data-00000-of-00001.arrow\r\n-rw-r--r-- 1 sliedes sliedes      1730 Jul  1 13:55 dataset_info.json\r\n-rw-r--r-- 1 sliedes sliedes       249 Jul  1 13:55 state.json\r\n```\n\n### Steps to reproduce the bug\n\nExecute the code above.\n\n### Expected behavior\n\nThe dataset is loaded successfully.\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-6.9.3-arch1-1-x86_64-with-glibc2.39\r\n- Python version: 3.12.4\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0\r\n<end_of_turn>\n<start_of_turn>model\n`load_dataset` fails to load dataset saved by `save_to_disk`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUse Qwen1.5-0.5B-Chat as an easy example for multi-GPU\r\n\r\nthe previous example was using a model for translation and the way it was setup was not really the right way to use the model.<end_of_turn>\n<start_of_turn>model\nBetter multi-gpu example<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTranslate Docs into Arabic  issue-number : #7093\r\n[Arabic Docs](https://github.com/AhmedAlmaghz/datasets/blob/main/docs/source/ar/index.mdx)\r\n[English Docs](https://github.com/AhmedAlmaghz/datasets/blob/main/docs/source/en/index.mdx)\r\n\r\n@stevhliu<end_of_turn>\n<start_of_turn>model\nAdd Arabic Docs to Datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe issue is visible only with the latest `datasets==3.2.0`.\r\n\r\nWhen using image dataset the Python process gets aborted right before the exit with the following error:\r\n```\r\nFatal Python error: PyGILState_Release: thread state 0x7fa1f409ade0 must be current when releasing\r\nPython runtime state: finalizing (tstate=0x0000000000ad2958)\r\n\r\nThread 0x00007fa33d157740 (most recent call first):\r\n  <no Python frame>\r\n\r\nExtension modules: numpy.core._multiarray_umath, numpy.core._multiarray_tests, numpy.linalg._umath_linalg, numpy.fft._pocketfft_internal, numpy.random._common, numpy.random.bit_generator, numpy.random._boun\r\nded_integers, numpy.random._mt19937, numpy.random.mtrand, numpy.random._philox, numpy.random._pcg64, numpy.random._sfc64, numpy.random._generator, pyarrow.lib, pandas._libs.tslibs.ccalendar, pandas._libs.ts\r\nlibs.np_datetime, pandas._libs.tslibs.dtypes, pandas._libs.tslibs.base, pandas._libs.tslibs.nattype, pandas._libs.tslibs.timezones, pandas._libs.tslibs.fields, pandas._libs.tslibs.timedeltas, pandas._libs.t\r\nslibs.tzconversion, pandas._libs.tslibs.timestamps, pandas._libs.properties, pandas._libs.tslibs.offsets, pandas._libs.tslibs.strptime, pandas._libs.tslibs.parsing, pandas._libs.tslibs.conversion, pandas._l\r\nibs.tslibs.period, pandas._libs.tslibs.vectorized, pandas._libs.ops_dispatch, pandas._libs.missing, pandas._libs.hashtable, pandas._libs.algos, pandas._libs.interval, pandas._libs.lib, pyarrow._compute, pan\r\ndas._libs.ops, pandas._libs.hashing, pandas._libs.arrays, pandas._libs.tslib, pandas._libs.sparse, pandas._libs.internals, pandas._libs.indexing, pandas._libs.index, pandas._libs.writers, pandas._libs.join,\r\n pandas._libs.window.aggregations, pandas._libs.window.indexers, pandas._libs.reshape, pandas._libs.groupby, pandas._libs.json, pandas._libs.parsers, pandas._libs.testing, charset_normalizer.md, requests.pa\r\nckages.charset_normalizer.md, requests.packages.chardet.md, yaml._yaml, markupsafe._speedups, PIL._imaging, torch._C, torch._C._dynamo.autograd_compiler, torch._C._dynamo.eval_frame, torch._C._dynamo.guards\r\n, torch._C._dynamo.utils, torch._C._fft, torch._C._linalg, torch._C._nested, torch._C._nn, torch._C._sparse, torch._C._special, sentencepiece._sentencepiece, sklearn.__check_build._check_build, psutil._psut\r\nil_linux, psutil._psutil_posix, scipy._lib._ccallback_c, scipy.sparse._sparsetools, _csparsetools, scipy.sparse._csparsetools, scipy.linalg._fblas, scipy.linalg._flapack, scipy.linalg.cython_lapack, scipy.l\r\ninalg._cythonized_array_utils, scipy.linalg._solve_toeplitz, scipy.linalg._decomp_lu_cython, scipy.linalg._matfuncs_sqrtm_triu, scipy.linalg.cython_blas, scipy.linalg._matfuncs_expm, scipy.linalg._decomp_up\r\ndate, scipy.sparse.linalg._dsolve._superlu, scipy.sparse.linalg._eigen.arpack._arpack, scipy.sparse.linalg._propack._spropack, scipy.sparse.linalg._propack._dpropack, scipy.sparse.linalg._propack._cpropack,\r\n scipy.sparse.linalg._propack._zpropack, scipy.sparse.csgraph._tools, scipy.sparse.csgraph._shortest_path, scipy.sparse.csgraph._traversal, scipy.sparse.csgraph._min_spanning_tree, scipy.sparse.csgraph._flo\r\nw, scipy.sparse.csgraph._matching, scipy.sparse.csgraph._reordering, scipy.special._ufuncs_cxx, scipy.special._ufuncs, scipy.special._specfun, scipy.special._comb, scipy.special._ellip_harm_2, scipy.spatial\r\n._ckdtree, scipy._lib.messagestream, scipy.spatial._qhull, scipy.spatial._voronoi, scipy.spatial._distance_wrap, scipy.spatial._hausdorff, scipy.spatial.transform._rotation, scipy.optimize._group_columns, s\r\ncipy.optimize._trlib._trlib, scipy.optimize._lbfgsb, _moduleTNC, scipy.optimize._moduleTNC, scipy.optimize._cobyla, scipy.optimize._slsqp, scipy.optimize._minpack, scipy.optimize._lsq.givens_elimination, sc\r\nipy.optimize._zeros, scipy.optimize._highs.cython.src._highs_wrapper, scipy.optimize._highs._highs_wrapper, scipy.optimize._highs.cython.src._highs_constants, scipy.optimize._highs._highs_constants, scipy.l\r\ninalg._interpolative, scipy.optimize._bglu_dense, scipy.optimize._lsap, scipy.optimize._direct, scipy.integrate._odepack, scipy.integrate._quadpack, scipy.integrate._vode, scipy.integrate._dop, scipy.integr\r\nate._lsoda, scipy.interpolate._fitpack, scipy.interpolate._dfitpack, scipy.interpolate._bspl, scipy.interpolate._ppoly, scipy.interpolate.interpnd, scipy.interpolate._rbfinterp_pythran, scipy.interpolate._r\r\ngi_cython, scipy.special.cython_special, scipy.stats._stats, scipy.stats._biasedurn, scipy.stats._levy_stable.levyst, scipy.stats._stats_pythran, scipy._lib._uarray._uarray, scipy.stats._ansari_swilk_statis\r\ntics, scipy.stats._sobol, scipy.stats._qmc_cy, scipy.stats._mvn, scipy.stats._rcont.rcont, scipy.stats._unuran.unuran_wrapper, scipy.ndimage._nd_image, _ni_label, scipy.ndimage._ni_label, sklearn.utils._isf\r\ninite, sklearn.utils.sparsefuncs_fast, sklearn.utils.murmurhash, sklearn.utils._openmp_helpers, sklearn.metrics.cluster._expected_mutual_info_fast, sklearn.preprocessing._csr_polynomial_expansion, sklearn.p\r\nreprocessing._target_encoder_fast, sklearn.metrics._dist_metrics, sklearn.metrics._pairwise_distances_reduction._datasets_pair, sklearn.utils._cython_blas, sklearn.metrics._pairwise_distances_reduction._bas\r\ne, sklearn.metrics._pairwise_distances_reduction._middle_term_computer, sklearn.utils._heap, sklearn.utils._sorting, sklearn.metrics._pairwise_distances_reduction._argkmin, sklearn.metrics._pairwise_distanc\r\nes_reduction._argkmin_classmode, sklearn.utils._vector_sentinel, sklearn.metrics._pairwise_distances_reduction._radius_neighbors, sklearn.metrics._pairwise_distances_reduction._radius_neighbors_classmode, s\r\nklearn.metrics._pairwise_fast, PIL._imagingft, google._upb._message, h5py._errors, h5py.defs, h5py._objects, h5py.h5, h5py.utils, h5py.h5t, h5py.h5s, h5py.h5ac, h5py.h5p, h5py.h5r, h5py._proxy, h5py._conv,\r\nh5py.h5z, h5py.h5a, h5py.h5d, h5py.h5ds, h5py.h5g, h5py.h5i, h5py.h5o, h5py.h5f, h5py.h5fd, h5py.h5pl, h5py.h5l, h5py._selector, _cffi_backend, pyarrow._parquet, pyarrow._fs, pyarrow._azurefs, pyarrow._hdfs\r\n, pyarrow._gcsfs, pyarrow._s3fs, multidict._multidict, propcache._helpers_c, yarl._quoting_c, aiohttp._helpers, aiohttp._http_writer, aiohttp._http_parser, aiohttp._websocket, frozenlist._frozenlist, xxhash\r\n._xxhash, pyarrow._json, pyarrow._acero, pyarrow._csv, pyarrow._dataset, pyarrow._dataset_orc, pyarrow._parquet_encryption, pyarrow._dataset_parquet_encryption, pyarrow._dataset_parquet, regex._regex, scipy\r\n.io.matlab._mio_utils, scipy.io.matlab._streams, scipy.io.matlab._mio5_utils, PIL._imagingmath, PIL._webp (total: 236)\r\nAborted (core dumped)\r\n```an \n\n### Steps to reproduce the bug\n\nInstall `datasets==3.2.0`\r\n\r\nRun the following script:\r\n```python\r\nimport datasets\r\n\r\n\r\nDATASET_NAME = \"phiyodr/InpaintCOCO\"\r\nNUM_SAMPLES = 10\r\n\r\n\r\ndef preprocess_fn(example):\r\n    return {\r\n        \"prompts\": example[\"inpaint_caption\"],\r\n        \"images\": example[\"coco_image\"],\r\n        \"masks\": example[\"mask\"],\r\n    }\r\n\r\n\r\ndefault_dataset = datasets.load_dataset(\r\n    DATASET_NAME, split=\"test\", streaming=True\r\n).filter(lambda example: example[\"inpaint_caption\"] != \"\").take(NUM_SAMPLES)\r\ntest_data = default_dataset.map(\r\n    lambda x: preprocess_fn(x), remove_columns=default_dataset.column_names\r\n)\r\n\r\nfor data in test_data:\r\n    print(data[\"prompts\"])\r\n\r\n``\n\n### Expected behavior\n\nThe script should not hang or crash.\n\n### Environment info\n\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.15.0-50-generic-x86_64-with-glibc2.31\r\n- Python version: 3.11.0\r\n- `huggingface_hub` version: 0.25.1\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.2.0<end_of_turn>\n<start_of_turn>model\nPython process aborded with GIL issue when using image dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHello guys, \r\n\r\nI have a dataset and I didn't know I couldn't upload more than 32 splits. Now, my dataset viewer is not working. I don't have the dataset locally on my node anymore and recreating would take a week. And I have to publish the dataset coming Monday. I read about the practice, how I can resolve it and avoid this issue in the future. But, at the moment I need a hard fix for two of my datasets. \r\n\r\nAnd I don't want to mess or change anything and allow everyone in public to see the dataset and interact with it. Can you please help me?\r\n\r\nhttps://huggingface.co/datasets/laion/Wikipedia-X\r\nhttps://huggingface.co/datasets/laion/Wikipedia-X-Full<end_of_turn>\n<start_of_turn>model\nDataset viewer not working! Failure due to more than 32 splits.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHi, I would like the option to ignore keys/columns when loading a dataset from files (e.g. jsonl).\n\n### Motivation\n\nI am working on a dataset which is built on jsonl. It seems the dataset is unclean and a column has different types in each row. I can't clean this or remove the column (It is not my data and it is too big for me to clean and save on my own hardware). \nI would like the option to just ignore this column when using `load_dataset`, since i don't need it.\nI tried to look if this is already possible but couldn't find a solution. if there is I would love some help. If it is not currently possible, I would love this feature\n\n### Your contribution\n\nI don't think I can help this time, unfortunately.<end_of_turn>\n<start_of_turn>model\nAdd option to ignore keys/columns when loading a dataset from jsonl(or any other data format)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## What's in this PR?\r\n\r\nAs of a previous PR at #5902, I've seen that the CI was automatically trigger on any file, in that case when modifying a Jupyter Notebook (.ipynb), which IMO could be skipped, as the modification on the Jupyter Notebook has no effect/impact on the `ci.yml` outcome. So this PR controls the paths that trigger the `ci.yml` to avoid wasting resources when not needed.\r\n\r\n## What's pending in this PR?\r\n\r\nI would like to confirm whether this should affect both `push` and `pull_request`, since just modifications in those files won't change the `ci.yml` outcome, so maybe it's worth skipping it too in the `push` trigger.<end_of_turn>\n<start_of_turn>model\nRelax `ci.yml` trigger for `pull_request` based on modified paths<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nthis should fix the CI which currently uses transformers on 3.9 while it's now unsupported<end_of_turn>\n<start_of_turn>model\nDefine CI future<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd an argument in `save_to_disk` regarding batch size, which would be passed to `shard` and other methods.\n\n### Motivation\n\nThe `Dataset.save_to_disk` method currently calls `shard` without passing a `writer_batch_size` argument, thus implicitly using the default value (1000). This can result in RAM saturation when using a lot of processes on long text sequences or other modalities, or for specific IO configs.\n\n### Your contribution\n\nI would be glad to submit a PR, as long as it does not imply extensive tests refactoring.<end_of_turn>\n<start_of_turn>model\nAllow to choose the `writer_batch_size` when using `save_to_disk`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollowing changes at https://github.com/fsspec/filesystem_spec/pull/1497 for `fsspec>=2024.2.0`<end_of_turn>\n<start_of_turn>model\nFix fsspec tqdm callback<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReplace `disable=False` with `disable=None` in the `tqdm` bars to disable them in non-interactive environments (by default).\r\n\r\nFor more info, see a [similar PR](https://github.com/huggingface/huggingface_hub/pull/2000) in `huggingface_hub`.<end_of_turn>\n<start_of_turn>model\nDisable `tqdm` bars in non-interactive environments<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes def log for dill version >= 0.3.9 \r\nhttps://pypi.org/project/dill/\r\n\r\nThis project uses dill with the release of version 0.3.9 the datasets lib. <end_of_turn>\n<start_of_turn>model\nfix: dill default for version bigger 0.3.8<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nThis returns error.\r\n```python\r\nfrom datasets import Dataset\r\n\r\ndsdf = Dataset.from_dict({\"x\": [[1, 2], [3, 4, 5]], \"y\": [\"a\", \"b\"]})\r\nDataset.from_polars(dsdf.to_polars())\r\n```\r\nValueError: Arrow type large_list<item: int64> does not have a datasets dtype equivalent.\n\n### Motivation\n\nWhen datasets contain Sequence data type, it will be converted to Arrow type large_list. However, the reverse (from large_list to Sequence) does not work.\n\n### Your contribution\n\nNo<end_of_turn>\n<start_of_turn>model\nConvert polars DataFrame back to datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nChange default compression type from `None` to \"infer\", to align with pandas' defaults.\r\n\r\nDocumentation asks the user to supply `to_json_kwargs` with arguments suitable for pandas' `to_json` method. At the same time, while pandas' by default uses [\"infer\"](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html) for compression, datasets enforce `None` as default. This, likely, confuses user, as they expect the same behaviour, i.e they expect that if they name their output file as \"dataset.jsonl.zst\" then the compression would be inferred as \"zstd\" and file will be compressed before writing.\r\n\r\nMoreover, while it is probably outside of the scope of this pull request, `compression` argument needs to be capable of taking `dict` as input (along with `str`), as it does in pandas, in order to allow user to specify compression parameters. Current implementation will likely fail with `NotImplementedError`, as it expects either `None` or `str` specifying compression algo.<end_of_turn>\n<start_of_turn>model\nChange default compression argument for JsonDatasetWriter<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\n\ud83e\uddea Do not mock create_commit<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nIt is currently quite easy to cause massive slowdowns when using datasets and not familiar with the underlying data conversions by e.g. making bad choices of formatting.\r\n\r\nWould it be more user-friendly to set defaults that avoid this as much as possible? e.g. format Array features as numpy arrays rather than python lists\n\n### Motivation\n\nDefault array formatting leads to slow performance: e.g.\r\n\r\n```python\r\nimport numpy as np\r\nfrom datasets import Dataset, Features, Array3D\r\nfeatures=Features(**{\"array0\": Array3D((None, 10, 10), dtype=\"float32\"), \"array1\": Array3D((None,10,10), dtype=\"float32\")})\r\ndataset = Dataset.from_dict({f\"array{i}\": [np.zeros((x,10,10), dtype=np.float32) for x in [2000,1000]*25] for i in range(2)}, features=features)\r\n```\r\n\r\n```python\r\nt0 = time.time()\r\nfor ex in ds:\r\n   pass\r\nt1 = time.time()\r\n```\r\n~1.4 s\r\n\r\n```python\r\nds = dataset.to_iterable_dataset()\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\n```\r\n~10s\r\n\r\n```python\r\nds = dataset.with_format(\"numpy\")\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\n```\r\n~0.04s\r\n\r\n```python\r\nds = dataset.to_iterable_dataset().with_format(\"numpy\")\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\n```\r\n~0.04s\n\n### Your contribution\n\nMay be able to contribute<end_of_turn>\n<start_of_turn>model\nConvert Array features to numpy arrays rather than lists by default<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nResolved for Issue#6152<end_of_turn>\n<start_of_turn>model\nResolved for empty datafiles<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nPytorch is migrating video processing to torchcodec and it's pretty cool. It would be nice to migrate both the audio and video features to use torchcodec instead of torchaudio/video.\n\n### Motivation\n\nMy use case is I'm working on a multimodal AV model, and what's nice about torchcodec is I can extract the audio tensors directly from MP4 files. Also, I can easily resample video data to whatever fps I like on the fly. I haven't found an easy/efficient way to do this with torchvision.\n\n### Your contribution\n\nI\u2019m modifying the Video dataclass to use torchcodec in place of the current backend, starting from a stable commit for a project I\u2019m working on. If it ends up working well, I\u2019m happy to open a PR on main.<end_of_turn>\n<start_of_turn>model\nVideo and audio decoding with torchcodec<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis code should not return empty data files\r\n\r\n```python\r\nfrom datasets import load_dataset_builder\r\n\r\nrevision = \"3d406e70bc21c3ca92a9a229b4c6fc3ed88279fd\"\r\nb = load_dataset_builder(\"bigcode/the-stack-v2-dedup\", data_dir=\"data/Dockerfile\", revision=revision)\r\nprint(b.config.data_files)\r\n```\r\n\r\nPreviously it would return no data files because it would apply the YAML `data_files: data/**/train-*` pattern to this directory\r\n\r\ncc @anton-l <end_of_turn>\n<start_of_turn>model\nFix data_files when passing data_dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\ndatasets version\uff1a 2.10.1\r\nI `load_dataset `and `save_to_disk` sucessfully on windows10( **and I `load_from_disk(/LLM/data/wiki)` succcesfully on windows10**), and I copy the dataset `/LLM/data/wiki`\r\ninto a ubuntu system, but when I `load_from_disk(/LLM/data/wiki)` on ubuntu,  something weird happens:\r\n\r\n\r\n```\r\nload_from_disk('/LLM/data/wiki')\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/datasets/load.py\", line 1874, in load_from_disk\r\n    return DatasetDict.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/datasets/dataset_dict.py\", line 1309, in load_from_disk\r\n    dataset_dict[k] = Dataset.load_from_disk(\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/datasets/arrow_dataset.py\", line 1543, in load_from_disk\r\n    fs_token_paths = fsspec.get_fs_token_paths(dataset_path, storage_options=storage_options)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/fsspec/core.py\", line 610, in get_fs_token_paths\r\n    chain = _un_chain(urlpath0, storage_options or {})\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/fsspec/core.py\", line 325, in _un_chain\r\n    cls = get_filesystem_class(protocol)\r\n  File \"/usr/local/miniconda3/lib/python3.8/site-packages/fsspec/registry.py\", line 232, in get_filesystem_class\r\n    raise ValueError(f\"Protocol not known: {protocol}\")\r\nValueError: Protocol not known: /LLM/data/wiki\r\n```\r\nIt seems that something went wrong on the arrow file?\r\nHow can I solve this , since currently I can not save_to_disk on ubuntu system\r\n\r\n### Steps to reproduce the bug\r\n\r\ndatasets version\uff1a 2.10.1\r\n\r\n### Expected behavior\r\n\r\ndatasets version\uff1a 2.10.1\r\n\r\n### Environment info\r\n\r\ndatasets version\uff1a 2.10.1<end_of_turn>\n<start_of_turn>model\nload_dataset save_to_disk load_from_disk error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nOnce root issue is fixed, remove temporary pin of fsspec < 2023.10.0 introduced by:\r\n- #6331\r\n\r\nRelated to issue:\r\n- #6330\r\n\r\nAs @ZachNagengast suggested, the issue might be related to:\r\n- https://github.com/fsspec/filesystem_spec/pull/1381<end_of_turn>\n<start_of_turn>model\nSupport fsspec 2023.10.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDiffusers currently asks for a max fsspec version of `2025.3.0`. This change updates it to the current latest version. This change is mainly required to resolve conflicts with other packages in an environment. In my particular case, `aider-chat` which is a part of my environment installs `2025.5.1` which is incompatible with `datasets`.<end_of_turn>\n<start_of_turn>model\nUpdate fsspec max version to current release 2025.7.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSaving a dataset as parquet fails with a `ValueError: Table schema does not match schema used to create file`  if the dataset was obtained out of a `.select_columns()` call with columns selected out of order.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\ndataset = datasets.Dataset.from_dict(\r\n    {\r\n        \"x1\": [1, 2, 3],\r\n        \"x2\": [10, 11, 12],\r\n    }\r\n)\r\n\r\nds = dataset.select_columns([\"x2\", \"x1\"])\r\n\r\nds.to_parquet(\"demo.parquet\")\r\n```\r\n\r\n```shell\r\n>>>\r\nValueError: Table schema does not match schema used to create file: \r\ntable:\r\nx2: int64\r\nx1: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x2\": {\"dtype\": \"int64\", \"_type\": \"V' + 53 vs. \r\nfile:\r\nx1: int64\r\nx2: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x1\": {\"dtype\": \"int64\", \"_type\": \"V' + 53\r\n```\r\n\r\n--- \r\n\r\nI think this is because after the `.select_columns()` call with out of order columns, the output dataset features' schema ends up being out of sync with the schema of the arrow table backing it. \r\n\r\n```python\r\nds.features.arrow_schema\r\n>>>\r\nx1: int64\r\nx2: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x1\": {\"dtype\": \"int64\", \"_type\": \"V' + 53\r\n\r\nds.data.schema\r\n>>>\r\nx2: int64\r\nx1: int64\r\n-- schema metadata --\r\nhuggingface: '{\"info\": {\"features\": {\"x2\": {\"dtype\": \"int64\", \"_type\": \"V' + 53\r\n```\r\n\r\n\r\nSo when we call `.to_parquet()`, the call behind the scenes to `datasets.io.parquet.ParquetDatasetWriter(...).write()` which initialises the backend `pyarrow.parquet.ParquetWriter` with `schema = self.dataset.features.arrow_schema` triggers `pyarrow` on write when [it checks](https://github.com/apache/arrow/blob/11b140a734a516e436adaddaeb35d23f30dcce44/python/pyarrow/parquet/core.py#L1086-L1090) that the `ParquetWriter` schema matches the schema of the table being written \ud83d\ude4c \r\n\r\nhttps://github.com/huggingface/datasets/blob/6ed837325cb539a5deb99129e5ad181d0269e050/src/datasets/io/parquet.py#L139-L141\r\n\n\n### Expected behavior\n\nThe dataset gets successfully saved as parquet. \r\n\r\n*In the same way as it does if saving it as csv:\r\n\r\n```python\r\nimport datasets\r\n\r\ndataset = datasets.Dataset.from_dict(\r\n    {\r\n        \"x1\": [1, 2, 3],\r\n        \"x2\": [10, 11, 12],\r\n    }\r\n)\r\n\r\nds = dataset.select_columns([\"x2\", \"x1\"])\r\n\r\nds.to_csv(\"demo.csv\")\r\n```\n\n### Environment info\n\n`python==3.11`\r\n`datasets==2.13.1`\r\n<end_of_turn>\n<start_of_turn>model\nValueError: Table schema does not match schema used to create file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ne.g. if we use `map` in arrow format and transform the table, the returned table might have new columns but the metadata might be wrong<end_of_turn>\n<start_of_turn>model\nUse schema metadata only if it matches features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nLet\u2019s say I have a dataset with 5 samples with values [1, 2, 3, 4, 5], with 2 GPUs (for DDP) and batch size of 2. This dataset is an `IterableDataset` since I am streaming it.\r\n\r\nNow I split the dataset using `split_dataset_by_node` to ensure it doesn\u2019t get repeated. And since it\u2019s already splitted, I don\u2019t have to use `DistributedSampler` (also they don't work with iterable datasets anyway)?\r\n\r\nBut in this case I noticed that the:\r\n\r\nFirst iteraton:\r\nfirst GPU will get \u2192 [1, 2]\r\nfirst GPU will get \u2192 [3, 4]\r\n\r\nSecond iteraton:\r\nfirst GPU will get \u2192 [5]\r\nfirst GPU will get \u2192 Nothing\r\n\r\nwhich actually creates an issue since in case of `DistributedSampler`, the samples are repeated internally to ensure non of the GPUs at any iteration is missing any data for gradient sync.\r\n\r\nSo my questions are:\r\n\r\n1. Here since splitting is happening before hand, how to make sure each GPU get\u2019s a batch at each iteration to avoid gradient sync issues?\r\n2. Do we need to use `DistributedSampler`? If yes, how?\r\n3. in the docstrings of `split_dataset_by_node`, this is mentioned: *\"If the dataset has a number of shards that is a factor of `world_size` (i.e. if `dataset.n_shards % world_size == 0`), then the shards are evenly assigned across the nodes, which is the most optimized. Otherwise, each node keeps 1 example out of `world_size`, skipping the other examples.\"* Can you explain the last part here?\r\n4. If `dataset.n_shards % world_size != 0`, is it possible to shard the streaming dataset on the fly to avoid the case where data is missing?\r\n\r\n### Motivation\r\n\r\nSomehow streaming datasets should work with DDP since for big LLMs a lot of data is required and DDP/multi-node is mostly used to train such models and streaming can actually help solve the data part of it.\r\n\r\n### Your contribution\r\n\r\nYes, I can help in submitting the PR once we get mutual understanding on how it should behave.<end_of_turn>\n<start_of_turn>model\nstreaming datasets doesn't work properly with multi-node<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nmy attempt at #6010  \r\nnot sure if this is the right way to go about it, I will wait for your feedback <end_of_turn>\n<start_of_turn>model\nDataset string representation enhancement<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nSo I was trying to download a dataset and save it as parquet and I follow the [tutorial](https://huggingface.co/docs/datasets/filesystems#download-and-prepare-a-dataset-into-a-cloud-storage) of Huggingface. However, during the excution I face a FileNotFoundError.\r\n\r\nI debug the code and it seems there is a bug there:\r\nSo first it creates a .incomplete folder and before moving its contents the following code deletes the directory\r\n[Code](https://github.com/huggingface/datasets/blob/98fdc9e78e6d057ca66e58a37f49d6618aab8130/src/datasets/builder.py#L984)\r\nhence as a result I face with: \r\n\r\n``` FileNotFoundError: [Errno 2] No such file or directory: '~/data/Parquet/.incomplete '```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import load_dataset_builder\r\nfrom pathlib import Path\r\n\r\nparquet_dir = \"~/data/Parquet/\" \r\nPath(parquet_dir).mkdir(parents=True, exist_ok=True)\r\nbuilder = load_dataset_builder(\r\n    \"rotten_tomatoes\",\r\n)\r\nbuilder.download_and_prepare(parquet_dir, file_format=\"parquet\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nDownloads the files and saves as parquet\r\n\r\n### Environment info\r\n\r\nUbuntu,\r\nPython 3.10\r\n```\r\ndatasets 2.19.1\r\n```<end_of_turn>\n<start_of_turn>model\nDatasetbuilder Local Download FileNotFoundError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n**emotions = load_dataset('emotion')**\r\n\r\n_UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte_\n\n### Steps to reproduce the bug\n\nload_dataset('emotion') \n\n### Expected behavior\n\nsuccese\n\n### Environment info\n\npy3.10\r\ntransformers  4.41.0.dev0\r\ndatasets     2.19.0<end_of_turn>\n<start_of_turn>model\nload_dataset('emotion')    UnicodeDecodeError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nthis removes the dependency on Soundfile completely<end_of_turn>\n<start_of_turn>model\nAudio: use TorchCodec instead of Soundfile for encoding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI manage a somewhat large and popular Hugging Face dataset known as the [Open Australian Legal Corpus](https://huggingface.co/datasets/umarbutler/open-australian-legal-corpus). I recently updated my corpus to be stored in a json lines file where each line is an array and each element represents a value at a particular column. Previously, my corpus was stored as a json lines file where each line was a dictionary and the keys were the fields.\r\n\r\nEssentially, a line in my json lines file used to look like this:\r\n```json\r\n{\"version_id\":\"\",\"type\":\"\",\"jurisdiction\":\"\",\"source\":\"\",\"citation\":\"\",\"url\":\"\",\"when_scraped\":\"\",\"text\":\"\"}\r\n```\r\n\r\nAnd now it looks like this:\r\n```json\r\n[\"\",\"\",\"\",\"\",\"\",\"\",\"\",\"\"]\r\n```\r\n\r\nThis saves 65 bytes per document and allows me very quickly serialise and deserialise documents via `msgspec`.\r\n\r\nAfter making this change, I found that `datasets` was incapable of deserialising my Corpus without a custom loading script, even if I ensured that the `dataset_info` field in my dataset card contained the desired names of my features.\r\n\r\nI would like to request that functionality be added to support this format which is more memory-efficent and faster than using dictionaries.\n\n### Motivation\n\nThe [documentation](https://huggingface.co/docs/datasets/en/dataset_script) for creating dataset loading scripts asserts that:\r\n> In the next major release, the new safety features of \ud83e\udd17 Datasets will disable running dataset loading scripts by default, and you will have to pass trust_remote_code=True to load datasets that require running a dataset script.\r\n\r\nI would rather not require my users to pass `trust_remote_code=True` which means that I will need built-in support for this format.\n\n### Your contribution\n\nI would be happy to submit a PR for this if this is something you would incorporate into `datasets` and if I can be pointed to where the code would need to go.<end_of_turn>\n<start_of_turn>model\nSupport the deserialization of json lines files comprised of lists<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nAdd `num_proc=` to `.push_to_hub()` (Dataset and IterableDataset)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd `flatten_indices` to `DatasetDict` for convinience<end_of_turn>\n<start_of_turn>model\nAdd `flatten_indices` to `DatasetDict`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe dataset generated from local file works fine.\r\n```py\r\nroot = \"/home/data/train\"\r\nfile_list1 = glob(os.path.join(root, \"*part1.flac\"))\r\nfile_list2 = glob(os.path.join(root, \"*part2.flac\"))\r\n\r\nds = (\r\n    Dataset.from_dict({\"part1\": file_list1, \"part2\": file_list2})\r\n    .cast_column(\"part1\", Audio(sampling_rate=None, mono=False))\r\n    .cast_column(\"part2\", Audio(sampling_rate=None, mono=False))\r\n)\r\n\r\nids = ds.to_iterable_dataset(128)\r\nids = ids.shuffle(buffer_size=10000, seed=42)\r\ndataloader = DataLoader(ids, num_workers=4, batch_size=8, persistent_workers=True)\r\n\r\nfor batch in dataloader:\r\n    break\r\n```\r\n\r\nBut after saving it to disk and then loading it from disk, I cannot get data as expected.\r\n```py\r\nroot = \"/home/data/train\"\r\nfile_list1 = glob(os.path.join(root, \"*part1.flac\"))\r\nfile_list2 = glob(os.path.join(root, \"*part2.flac\"))\r\n\r\nds = (\r\n    Dataset.from_dict({\"part1\": file_list1, \"part2\": file_list2})\r\n    .cast_column(\"part1\", Audio(sampling_rate=None, mono=False))\r\n    .cast_column(\"part2\", Audio(sampling_rate=None, mono=False))\r\n)\r\n\r\nds.save_to_disk(\"./train\")\r\n\r\nds = datasets.load_from_disk(\"./train\")\r\n\r\nids = ds.to_iterable_dataset(128)\r\nids = ids.shuffle(buffer_size=10000, seed=42)\r\ndataloader = DataLoader(ids, num_workers=4, batch_size=8, persistent_workers=True)\r\n\r\nfor batch in dataloader:\r\n    break\r\n```\r\n\r\nAfter a long time waiting, an error occurs:\r\n```\r\nLoading dataset from disk: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 165/165 [00:00<00:00, 6422.18it/s]\r\nTraceback (most recent call last):\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1133, in _try_get_data\r\n    data = self._data_queue.get(timeout=timeout)\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/multiprocessing/queues.py\", line 113, in get\r\n    if not self._poll(timeout):\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/multiprocessing/connection.py\", line 257, in poll\r\n    return self._poll(timeout)\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/multiprocessing/connection.py\", line 424, in _poll\r\n    r = wait([self], timeout)\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\r\n    ready = selector.select(timeout)\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/selectors.py\", line 416, in select\r\n    fd_event_list = self._selector.poll(timeout)\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py\", line 66, in handler\r\n    _error_if_any_worker_fails()\r\nRuntimeError: DataLoader worker (pid 3490529) is killed by signal: Killed. \r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/hanzerui/.vscode-server/extensions/ms-python.debugpy-2024.9.12011011/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/__main__.py\", line 39, in <module>\r\n    cli.main()\r\n  File \"/home/hanzerui/.vscode-server/extensions/ms-python.debugpy-2024.9.12011011/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 430, in main\r\n    run()\r\n  File \"/home/hanzerui/.vscode-server/extensions/ms-python.debugpy-2024.9.12011011/bundled/libs/debugpy/adapter/../../debugpy/launcher/../../debugpy/../debugpy/server/cli.py\", line 284, in run_file\r\n    runpy.run_path(target, run_name=\"__main__\")\r\n  File \"/home/hanzerui/.vscode-server/extensions/ms-python.debugpy-2024.9.12011011/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 321, in run_path\r\n    return _run_module_code(code, init_globals, run_name,\r\n  File \"/home/hanzerui/.vscode-server/extensions/ms-python.debugpy-2024.9.12011011/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 135, in _run_module_code\r\n    _run_code(code, mod_globals, init_globals,\r\n  File \"/home/hanzerui/.vscode-server/extensions/ms-python.debugpy-2024.9.12011011/bundled/libs/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_runpy.py\", line 124, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/hanzerui/workspace/NetEase/test/test_datasets.py\", line 60, in <module>\r\n    for batch in dataloader:\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\r\n    data = self._next_data()\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1329, in _next_data\r\n    idx, data = self._get_data()\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1295, in _get_data\r\n    success, data = self._try_get_data()\r\n  File \"/home/hanzerui/.conda/envs/mss/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1146, in _try_get_data\r\n    raise RuntimeError(f'DataLoader worker (pid(s) {pids_str}) exited unexpectedly') from e\r\nRuntimeError: DataLoader worker (pid(s) 3490529) exited unexpectedly\r\n```\r\n\r\nIt seems that streaming is not supported by `laod_from_disk`, so does that mean I cannot convert it to iterable?\n\n### Steps to reproduce the bug\n\n1. Create a `Dataset` from local files with `from_dict`\r\n2. Save it to disk with `save_to_disk`\r\n3. Load it from disk with `load_from_disk`\r\n4. Convert to iterable with `to_iterable_dataset`\r\n5. Loop the dataset\n\n### Expected behavior\n\nGet items faster than the original dataset generated from dict.\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-6.5.0-41-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.23.2\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.5.0<end_of_turn>\n<start_of_turn>model\nCannot get item after loading from disk and then converting to iterable.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported at https://huggingface.co/datasets/tbone5563/tar_images/discussions/1\r\n\r\n```python\r\n>>> from datasets import load_dataset\r\n>>> ds = load_dataset(\"tbone5563/tar_images\")\r\nDownloading\u2007data:\u2007100%\r\n\u20071.41G/1.41G\u2007[00:48<00:00,\u200717.2MB/s]\r\nDownloading\u2007data:\u2007100%\r\n\u2007619M/619M\u2007[00:11<00:00,\u200757.4MB/s]\r\nGenerating\u2007train\u2007split:\u2007\r\n\u2007970/0\u2007[00:02<00:00,\u2007534.94\u2007examples/s]\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1747                 _time = time.time()\r\n-> 1748                 for key, record in generator:\r\n   1749                     if max_shard_size is not None and writer._num_bytes > max_shard_size:\r\n\r\n7 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/webdataset/webdataset.py](https://localhost:8080/#) in _generate_examples(self, tar_paths, tar_iterators)\r\n    108                 for field_name in image_field_names + audio_field_names:\r\n--> 109                     example[field_name] = {\"path\": example[\"__key__\"] + \".\" + field_name, \"bytes\": example[field_name]}\r\n    110                 yield f\"{tar_idx}_{example_idx}\", example\r\n\r\nKeyError: 'png'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nDatasetGenerationError                    Traceback (most recent call last)\r\n[<ipython-input-2-8e0fbb7badc9>](https://localhost:8080/#) in <cell line: 3>()\r\n      1 from datasets import load_dataset\r\n      2 \r\n----> 3 ds = load_dataset(\"tbone5563/tar_images\")\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/load.py](https://localhost:8080/#) in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2607 \r\n   2608     # Download and prepare data\r\n-> 2609     builder_instance.download_and_prepare(\r\n   2610         download_config=download_config,\r\n   2611         download_mode=download_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n   1025                         if num_proc is not None:\r\n   1026                             prepare_split_kwargs[\"num_proc\"] = num_proc\r\n-> 1027                         self._download_and_prepare(\r\n   1028                             dl_manager=dl_manager,\r\n   1029                             verification_mode=verification_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs)\r\n   1787 \r\n   1788     def _download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs):\r\n-> 1789         super()._download_and_prepare(\r\n   1790             dl_manager,\r\n   1791             verification_mode,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1120             try:\r\n   1121                 # Prepare split will record examples associated to the split\r\n-> 1122                 self._prepare_split(split_generator, **prepare_split_kwargs)\r\n   1123             except OSError as e:\r\n   1124                 raise OSError(\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\r\n   1625             job_id = 0\r\n   1626             with pbar:\r\n-> 1627                 for job_id, done, content in self._prepare_split_single(\r\n   1628                     gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\r\n   1629                 ):\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/builder.py](https://localhost:8080/#) in _prepare_split_single(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\r\n   1782             if isinstance(e, SchemaInferenceError) and e.__context__ is not None:\r\n   1783                 e = e.__context__\r\n-> 1784             raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\n   1785 \r\n   1786         yield job_id, True, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\r\n\r\nDatasetGenerationError: An error occurred while generating the dataset\r\n```<end_of_turn>\n<start_of_turn>model\nWebdataset: KeyError: 'png' on some datasets when streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI have already logged in Huggingface using CLI with my valid token. Now trying to download the datasets using following code:\n\n\n    from transformers import WhisperProcessor, WhisperForConditionalGeneration, WhisperTokenizer, Trainer, TrainingArguments, DataCollatorForSeq2Seq\n    from datasets import load_dataset, DatasetDict, Audio\n    \n    def load_and_preprocess_dataset():\n        dataset = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"bn\")\n        dataset = dataset.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"segment\", \"up_votes\"])\n        dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n        dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n        dataset = DatasetDict({\n            \"train\": dataset[\"train\"],\n            \"test\": dataset[\"test\"]\n        })\n        return dataset\n    \n    load_and_preprocess_dataset()\n\n\nI am getting following error:\n\n    Downloading\u2007data:\u2007100%\n    \u200725/25\u2007[00:01<00:00,\u200725.31files/s]\n    ---------------------------------------------------------------------------\n    HTTPError                                 Traceback (most recent call last)\n    File ~/github/bangla-asr/.venv/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409, in hf_raise_for_status(response, endpoint_name)\n        408 try:\n    --> 409     response.raise_for_status()\n        410 except HTTPError as e:\n    \n    File ~/github/bangla-asr/.venv/lib/python3.11/site-packages/requests/models.py:1024, in Response.raise_for_status(self)\n       1023 if http_error_msg:\n    -> 1024     raise HTTPError(http_error_msg, response=self)\n    \n    HTTPError: 403 Client Error: BlockSIEL for url: https://hf.co/api/s3proxy?GET=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf65687d8a6928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250dc638866f22bf1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D621e731d4fd6d08afbf568379797746ab8e2b853b6728ff5e1122fef6e56880b%26X-Amz-SignedHeaders%3Dhost%26response-content-disposition%3Dinline%253B%2520filename%252A%253DUTF-8%2527%2527bn_validated_1.tar%253B%2520filename%253D%2522bn_validated_1.tar%2522%253B%26response-content-type%3Dapplication%252Fx-tar%26x-id%3DGetObject&HEAD=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf65687d8a6928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250dc638866f22bf1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D15254fb79d30b0dc36b94a28138e675e0e00bb475b8a3ae774418500b095a661%26X-Amz-SignedHeaders%3Dhost&sign=eyJhbGciOiJIUzI1NiJ9.eyJyZWRpcmVjdF9kb21haW4iOiJoZi1odWItbGZzLXVzLWVhc3QtMS5zMy51cy1lYXN0LTEuYW1hem9uYXdzLmNvbSIsImlhdCI6MTc0NDExOTgyNSwiZXhwIjoxNzQ0MjA2MjI1LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.5sJzudFDU3SmOdOLlwmQCOfQFf2r7y9590HoX8WBkRk\n    \n    The above exception was the direct cause of the following exception:\n    \n    HfHubHTTPError                            Traceback (most recent call last)\n    Cell In[16], line 15\n          9     dataset = DatasetDict({\n         10         \"train\": dataset[\"train\"],\n         11         \"test\": dataset[\"test\"]\n         12     })\n         13     return dataset\n    ---> 15 load_and_preprocess_dataset()\n         17 # def setup_model():\n         18 #     processor = WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n    ...\n        475     range_header = response.request.headers.get(\"Range\")\n    \n    HfHubHTTPError: 403 Forbidden: None.\n    Cannot access content at: https://hf.co/api/s3proxy?GET=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf6568724a6928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250dc638786f22bf1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D621e731d4fd6d08afbf568379797746ab394b853b6728ff5e1122fef6e56880b%26X-Amz-SignedHeaders%3Dhost%26response-content-disposition%3Dinline%253B%2520filename%252A%253DUTF-8%2527%2527bn_validated_1.tar%253B%2520filename%253D%2522bn_validated_1.tar%2522%253B%26response-content-type%3Dapplication%252Fx-tar%26x-id%3DGetObject&HEAD=https%3A%2F%2Fhf-hub-lfs-us-east-1.s3.us-east-1.amazonaws.com%2Frepos%2Fa3%2F86%2Fa386bf65687ab76928c1ea57c383aa3faade32f5171150e25af3fc1cfc273db8%2F67f1ac9cabd539bfbff3acbc549b60647833a250d2338866f222f1b64e68806d%3FX-Amz-Algorithm%3DAWS4-HMAC-SHA256%26X-Amz-Content-Sha256%3DUNSIGNED-PAYLOAD%26X-Amz-Credential%3DAKIA2JU7TKAQLC2QXPN7%252F20250408%252Fus-east-1%252Fs3%252Faws4_request%26X-Amz-Date%3D20250408T134345Z%26X-Amz-Expires%3D3600%26X-Amz-Signature%3D15254fb79d30b0dc36b94a28138e675e0e00bb475b8a3ae774418500b095a661%26X-Amz-SignedHeaders%3Dhost&sign=eyJhbGciOiJIUzI1NiJ9.eyJyZWRpcmVjds9kb21haW4iOiJoZi1odWItbGZzLXVzLWVhc3QtMS5zMy51cy1lYXN0LTEuYW1hem9uYXdzLmNvbSIsImlhdCI6MTc0NDExOT2yNSwiZXhwIjoxNzQ0MjA2MjI1LCJpc3MiOiJodHRwczovL2h1Z2dpbmdmYWNlLmNvIn0.5sJzudFDU3SmOdOLlwmQdOfQFf2r7y9590HoX8WBkRk.\n    Make sure your token has the correct permissions.\n\n\n**What's wrong with the code?** Please note that the error is happening only when I am running from my office network due to probably proxy. Which URL, I need to take a proxy exception?<end_of_turn>\n<start_of_turn>model\nHfHubHTTPError: 403 Forbidden: None. Cannot access content at: https://hf.co/api/s3proxy<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi there, thanks for the great library! We have been using it a lot in torchtune and it's been a huge help for us. \r\n\r\nRegarding the bug: the same call to `load_dataset` errors with `ExpectedMoreSplits` in 2.19.0 after working fine in 2.18.0. Full details given in the repro below.\n\n### Steps to reproduce the bug\n\nOn 2.18.0, things work fine:\r\n```\r\n# First clear the locally cached dataset\r\nrm -r ~/.cache/huggingface/datasets/lvwerra___stack-exchange-paired\r\npip install \"datasets==2.18.0\"\r\npython3\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset('lvwerra/stack-exchange-paired', split='train', data_dir='data/rl')\r\n```\r\n\r\nOn 2.19.0, they do not:\r\n```\r\n# First clear the locally cached dataset\r\nrm -r ~/.cache/huggingface/datasets/lvwerra___stack-exchange-paired\r\npip install \"datasets==2.19.0\"\r\npython3\r\n>>> from datasets import load_dataset\r\n>>> dataset = load_dataset('lvwerra/stack-exchange-paired', split='train', data_dir='data/rl')\r\n```\r\n\r\nThe stack trace I see from the 2.19.0 version of load_dataset can be seen [here](https://gist.github.com/ebsmothers/f9b1f1949bee7030a8d7bb8a491550d2).\r\n\r\n(Maybe unsurprising but) notably if I do not delete the cache first I am able to load the dataset successfully. So based on this I suspect the cause is somewhere in the download logic.\n\n### Expected behavior\n\nDownload the dataset successfully :)\n\n### Environment info\n\n- `datasets` version: 2.19.0\r\n- Platform: Linux-5.12.0-0_fbk16_zion_7661_geb00762ce6d2-x86_64-with-glibc2.34\r\n- Python version: 3.11.9\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 16.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nExpectedMoreSplits error on load_dataset when upgrading to 2.19.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nhttps://github.com/huggingface/datasets/pull/6464 mentions a `num_proc` param while loading dataset from disk, but can't find that in the documentation and code anywhere\n\n### Motivation\n\nMake loading large datasets from disk faster\n\n### Your contribution\n\nHappy to contribute if given pointers<end_of_turn>\n<start_of_turn>model\nConcurrent loading in `load_from_disk` - `num_proc` as a param <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix docs relative links<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhen I run the following code, an error occurs: \"TypeError: argument of type 'module' is not iterable\"\n```python\nprint(\"open\" in globals()[\"__builtins__\"])\n```\nTraceback (most recent call last):\n  File \"./main.py\", line 2, in <module>\n    print(\"open\" in globals()[\"__builtins__\"])\n          ^^^^^^^^^^^^^^^^^^^^^^\nTypeError: argument of type 'module' is not iterable\n\nBut this code runs fine in datasets, I don't understand why\n[src/datasets/utils/patching.py#L96](https://github.com/huggingface/datasets/blob/3.6.0/src/datasets/utils/patching.py#L96)<end_of_turn>\n<start_of_turn>model\n\"open\" in globals()[\"__builtins__\"], an error occurs: \"TypeError: argument of type 'module' is not iterable\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI thought https://github.com/huggingface/datasets/pull/7252 might be applicable but looking at it maybe not.\n\nSay we have a process, eg. a database query, that can return data in slightly different order each time. So, the initial query needs to be run by a single thread (not to mention running multiple times incurs more cost too). But the results are also big enough that we don't want to materialize it entirely and instead stream it with an IterableDataset.\n\nBut after we have the results we want to split it up across workers to parallelize processing.\n\nIs something like this possible to do?\n\nHere's a failed attempt. The end result should be that each of the shards has unique data, but unfortunately with this attempt the generator gets run once in each shard and the results end up with duplicates...\n\n```\nimport random\nimport datasets\n\n\ndef gen():\n  print('RUNNING GENERATOR!')\n  items = list(range(10))\n  random.shuffle(items)\n  yield from items\n\n\nds = datasets.IterableDataset.from_generator(gen)\n\nprint('dataset contents:')\nfor item in ds:\n  print(item)\nprint()\n\nprint('dataset contents (2):')\nfor item in ds:\n  print(item)\nprint()\n\n\nnum_shards = 3\n\n\ndef sharded(shard_id):\n  for i, example in enumerate(ds):\n    if i % num_shards in shard_id:\n      yield example\n\n\nds1 = datasets.IterableDataset.from_generator(\n  sharded, gen_kwargs={'shard_id': list(range(num_shards))}\n)\n\nfor shard in range(num_shards):\n  print('shard', shard)\n  for item in ds1.shard(num_shards, shard):\n    print(item)\n```<end_of_turn>\n<start_of_turn>model\nIs it possible to shard a single-sharded IterableDataset?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nCreating a dataset with ArrayXD features leads to errors when downloading from hub due to DatasetCardData removing the Nones\r\n\r\n@lhoestq \r\n\r\n### Steps to reproduce the bug\r\n\r\n```python\r\nimport numpy as np\r\nfrom datasets import Array2D, Dataset, Features, load_dataset\r\n\r\ndef examples_generator():\r\n    for i in range(4):\r\n        yield {\r\n            \"array_1d\": np.zeros((10,1), dtype=\"uint16\"),\r\n            \"array_2d\": np.zeros((10, 1), dtype=\"uint16\"),\r\n        }\r\n\r\nfeatures = Features(array_1d=Array2D((None,1), \"uint16\"), array_2d=Array2D((None, 1), \"uint16\"))\r\ndataset = Dataset.from_generator(examples_generator, features=features)\r\ndataset.push_to_hub(\"alex-hh/test_array_1d2d\")\r\nds = load_dataset(\"alex-hh/test_array_1d2d\")\r\n```\r\n\r\nSource of error appears to be DatasetCardData.to_dict invoking DatasetCardData._remove_none\r\n```python\r\nfrom huggingface_hub import DatasetCardData\r\nfrom datasets.info import DatasetInfosDict\r\n\r\ndataset_card_data = DatasetCardData()\r\nDatasetInfosDict({\"default\": dataset.info.copy()}).to_dataset_card_data(dataset_card_data)\r\nprint(dataset_card_data.to_dict())  # removes Nones in shape\r\n```\r\n\r\n### Expected behavior\r\n\r\nShould be possible to load datasets saved with shape None in leading dimension\r\n\r\n### Environment info\r\n\r\n3.0.2 and latest huggingface_hub<end_of_turn>\n<start_of_turn>model\nArrayXD with None as leading dim incompatible with DatasetCardData<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\npython script is:\r\n```\r\n\r\nfrom datasets import load_dataset\r\ncache_dir = 'path/to/your/cache/directory'\r\ndataset = load_dataset('allenai/c4','allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train',  use_auth_token=False, cache_dir=cache_dir)\r\n```\r\n\r\n\r\nthe script success when datasets version is 2.14.7. \r\nwhen using 2.16.1, error occurs\r\n`\r\nValueError: BuilderConfig 'allenai--c4' not found. Available: ['default']`\n\n### Steps to reproduce the bug\n\n1. pip install datasets==2.16.1\r\n2. run python script:\r\n```\r\n\r\nfrom datasets import load_dataset\r\ncache_dir = 'path/to/your/cache/directory'\r\ndataset = load_dataset('allenai/c4','allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train',  use_auth_token=False, cache_dir=cache_dir)\r\n```\n\n### Expected behavior\n\nthe dataset should be loaded successful in the latest version. \n\n### Environment info\n\ndatasets 2.16.1<end_of_turn>\n<start_of_turn>model\nLatest version 2.16.1, when load dataset error occurs. ValueError: BuilderConfig 'allenai--c4' not found. Available: ['default']<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nlikely related to #6460 \r\n\r\nusing `datasets.load_dataset(\"json\", data_dir= ... )` with multiple `.jsonl` files will error if one of the files (maybe the first file?) contains a full column of empty data. \r\n\r\n### Steps to reproduce the bug\r\n\r\nreal world example:\r\ndata is available in this [PR-branch](https://github.com/Vipitis/shadertoys-dataset/pull/3/commits/cb1e7157814f74acb09d5dc2f1be3c0a868a9933). Because my files are chunked by months, some months contain all empty data for some columns, just by chance - these are `[]`. Otherwise it's all the same structure. \r\n\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"json\", data_dir=\"./data/annotated/api\")\r\n```\r\n\r\nyou get a long error trace, where in the middle it says something like \r\n```cs\r\nTypeError: Couldn't cast array of type struct<id: int64, src: string, ctype: string, channel: int64, sampler: struct<filter: string, wrap: string, vflip: string, srgb: string, internal: string>, published: int64> to null\r\n```\r\n\r\ntoy example: (on request)\r\n\r\n\r\n### Expected behavior\r\n\r\nSome suggestions \r\n1. give a better error message to the user\r\n2. consider all files before deciding on a data structure for a given column.\r\n3. if you encounter a new structure, and can't cast that to null, replace the null-hypothesis. (maybe something for pyarrow)\r\n\r\nas a workaround I have lazily implemented the following (essentially step 2)\r\n```python\r\nimport os \r\nimport jsonlines\r\nimport datasets\r\n\r\napi_files = os.listdir(\"./data/annotated/api\")\r\n\r\napi_files = [f\"./data/annotated/api/{f}\" for f in api_files]\r\n\r\napi_file_contents = []\r\nfor f in api_files:\r\n    with jsonlines.open(f) as reader:\r\n        for obj in reader:\r\n            api_file_contents.append(obj)\r\n\r\nds = datasets.Dataset.from_list(api_file_contents)\r\n```\r\nthis works fine for my usecase, but is potentially slower and less memory efficient for really large datasets (where this is unlikely to happen in the first place).\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.20.0\r\n- Platform: Windows-10-10.0.19041-SP0\r\n- Python version: 3.9.4\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nload_dataset with multiple jsonlines files interprets datastructure too early<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTo fix https://discuss.huggingface.co/t/error-when-setting-up-the-dataset-viewer-streamingrowserror/51276.\r\n<end_of_turn>\n<start_of_turn>model\nAllow hyphen in split name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a multilingual dataset that contains a lot of subsets. Each subset corresponds to a pair of languages, you can see here an example with 250 subsets: [https://hf.co/datasets/loicmagne/open-subtitles-250-bitext-mining](). As part of the MTEB benchmark, we may need to load all the subsets of the dataset. The dataset is relatively small and contains only ~45MB of data, but when I try to load every subset, it takes 15 minutes from the HF hub and 13 minutes from the cache\r\n\r\nThis issue https://github.com/huggingface/datasets/issues/5499 also referenced this overhead, but I'm wondering if there is anything I can do to speedup loading different subsets of the same dataset, both when loading from disk and from the HF hub? Currently each subset is stored in a jsonl file\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\n\r\nfor subset in ['ka-ml', 'br-sr', 'bg-br', 'kk-lv', 'br-sk', 'br-fi', 'eu-ze_zh', 'kk-nl', 'kk-vi', 'ja-kk', 'br-sv', 'kk-zh_cn', 'kk-ms', 'br-et', 'br-hu', 'eo-kk', 'br-tr', 'ko-tl', 'te-zh_tw', 'br-hr', 'br-nl', 'ka-si', 'br-cs', 'br-is', 'br-ro', 'br-de', 'et-kk', 'fr-hy', 'br-no', 'is-ko', 'br-da', 'br-en', 'eo-lt', 'is-ze_zh', 'eu-ko', 'br-it', 'br-id', 'eu-zh_cn', 'is-ja', 'br-sl', 'br-gl', 'br-pt_br', 'br-es', 'br-pt', 'is-th', 'fa-is', 'br-ca', 'eu-ka', 'is-zh_cn', 'eu-ur', 'id-kk', 'br-sq', 'eu-ja', 'uk-ur', 'is-zh_tw', 'ka-ko', 'eu-zh_tw', 'eu-th', 'eu-is', 'is-tl', 'br-eo', 'eo-ze_zh', 'eu-te', 'ar-kk', 'eo-lv', 'ko-ze_zh', 'ml-ze_zh', 'is-lt', 'br-fr', 'ko-te', 'kk-sl', 'eu-fa', 'eo-ko', 'ka-ze_en', 'eo-eu', 'ta-zh_tw', 'eu-lv', 'ko-lv', 'lt-tl', 'eu-si', 'hy-ru', 'ar-is', 'eu-lt', 'eu-tl', 'eu-uk', 'ka-ze_zh', 'si-ze_zh', 'el-is', 'bn-is', 'ko-ze_en', 'eo-si', 'cs-kk', 'is-uk', 'eu-ze_en', 'ta-ze_zh', 'is-pl', 'is-mk', 'eu-ta', 'ko-lt', 'is-lv', 'fa-ko', 'bn-ko', 'hi-is', 'bn-ze_zh', 'bn-eu', 'bn-ja', 'is-ml', 'eu-ru', 'ko-ta', 'is-vi', 'ja-tl', 'eu-mk', 'eu-he', 'ka-zh_tw', 'ka-zh_cn', 'si-tl', 'is-kk', 'eu-fi', 'fi-ko', 'is-ur', 'ka-th', 'ko-ur', 'eo-ja', 'he-is', 'is-tr', 'ka-ur', 'et-ko', 'eu-vi', 'is-sk', 'gl-is', 'fr-is', 'is-sq', 'hu-is', 'fr-kk', 'eu-sq', 'is-ru', 'ja-ka', 'fi-tl', 'ka-lv', 'fi-is', 'is-si', 'ar-ko', 'ko-sl', 'ar-eu', 'ko-si', 'bg-is', 'eu-hu', 'ko-sv', 'bn-hu', 'kk-ro', 'eu-hi', 'ka-ms', 'ko-th', 'ko-sr', 'ko-mk', 'fi-kk', 'ka-vi', 'eu-ml', 'ko-ml', 'de-ko', 'fa-ze_zh', 'eu-sk', 'is-sl', 'et-is', 'eo-is', 'is-sr', 'is-ze_en', 'kk-pt_br', 'hr-hy', 'kk-pl', 'ja-ta', 'is-ms', 'hi-ze_en', 'is-ro', 'ko-zh_cn', 'el-eu', 'ka-pl', 'ka-sq', 'eu-sl', 'fa-ka', 'ko-no', 'si-ze_en', 'ko-uk', 'ja-ze_zh', 'hu-ko', 'kk-no', 'eu-pl', 'is-pt_br', 'bn-lv', 'tl-zh_cn', 'is-nl', 'he-ko', 'ko-sq', 'ta-th', 'lt-ta', 'da-ko', 'ca-is', 'is-ta', 'bn-fi', 'ja-ml', 'lv-si', 'eu-sv', 'ja-te', 'bn-ur', 'bn-ca', 'bs-ko', 'bs-is', 'eu-sr', 'ko-vi', 'ko-zh_tw', 'et-tl', 'kk-tr', 'eo-vi', 'is-it', 'ja-ko', 'eo-et', 'id-is', 'bn-et', 'bs-eu', 'bn-lt', 'tl-uk', 'bn-zh_tw', 'da-eu', 'el-ko', 'no-tl', 'ko-sk', 'is-pt', 'hu-kk', 'si-zh_tw', 'si-te', 'ka-ru', 'lt-ml', 'af-ja', 'bg-eu', 'eo-th', 'cs-is', 'pl-ze_zh', 'el-kk', 'kk-sv', 'ka-nl', 'ko-pl', 'bg-ko', 'ka-pt_br', 'et-eu', 'tl-zh_tw', 'ka-pt', 'id-ko', 'fi-ze_zh', 'he-kk', 'ka-tr']:\r\n    load_dataset('loicmagne/open-subtitles-250-bitext-mining', subset)\r\n```\n\n### Expected behavior\n\nFaster loading?\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 2.18.0\r\n- Platform: Linux-6.5.0-27-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.22.2\r\n- PyArrow version: 15.0.2\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2023.5.0\r\n<end_of_turn>\n<start_of_turn>model\nHigh overhead when loading lots of subsets from the same dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\n\u0634\u0628\u06a9\u0647 \u0647\u0627\u06cc \u0645\u062a\u0646 \u0628\u0647 \u06af\u0641\u062a\u0627\u0631 \u0627\u0628\u062a\u062f\u0627 \u0645\u062a\u0646 \u062f\u0627\u062f\u0647 \u0634\u062f\u0647 \u0631\u0627 \u0628\u0647 \u0628\u0627\u0632\u0646\u0645\u0627\u06cc\u06cc \u0645\u06cc\u0627\u0646\u06cc<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe following code\r\n\r\n```python\r\nimport datasets as hf\r\nds1 = hf.Dataset.from_list([{'i': i} for i in [0,1]])\r\n#ds1 = ds1.to_iterable_dataset()\r\nds2 = ds1.map(\r\n    lambda i: {'i': i+1},\r\n    input_columns = ['i'],\r\n    remove_columns = ['i']\r\n)   \r\nlist(ds2)\r\n```\r\n\r\nproduces\r\n\r\n```python\r\n[{'i': 1}, {'i': 2}]\r\n```\r\n\r\nas expected. If the line that converts `ds1` to iterable is uncommented so that the `ds2` is a map of an `IterableDataset`, the result is \r\n\r\n```python\r\n[{},{}]\r\n```\r\n\r\nI expected the output to be the same as before. It seems that in the second case the removed column is not added back into the output.\r\n\r\nThe issue seems to be [here](https://github.com/huggingface/datasets/blob/6c6a82a573f946c4a81069f56446caed15cee9c2/src/datasets/iterable_dataset.py#L1093): the columns are removed after the mapping which is not what we want (or what the [documentation says](https://github.com/huggingface/datasets/blob/6c6a82a573f946c4a81069f56446caed15cee9c2/src/datasets/iterable_dataset.py#L2370)) because we want the columns removed from the transformed example but then added if the map produced them.\r\n\r\nThis is `datasets==3.2.0` and `python==3.10`\r\n\r\n\n\n### Steps to reproduce the bug\n\nsee above\n\n### Expected behavior\n\nsee above\n\n### Environment info\n\nsee above<end_of_turn>\n<start_of_turn>model\nDifferent behaviour of IterableDataset.map vs Dataset.map with remove_columns<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`Dataset.to_dict` seems to ignore the decoding instruction passed in features.\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\nimport numpy as np\r\nfrom PIL import Image\r\n\r\nimg = np.random.randint(0, 256, (5, 5, 3), dtype=np.uint8)\r\nimg = Image.fromarray(img)\r\n\r\nfeatures = datasets.Features({\"image\": datasets.Image(decode=True)})\r\ndataset = datasets.Dataset.from_dict({\"image\": [img]}, features=features)\r\nprint({key: dataset[key] for key in dataset.column_names})\r\n# {'image': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=5x5 at 0x7EFBC80E15B0>]}\r\nprint(dataset.to_dict())\r\n# {'image': [{'bytes': b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x00\\x05\\x00\\x00\\x00\\x05\\x08\\x02\\x00\\x00\\x00\\x02\\r\\xb1\\xb2\\x00\\x00\\x00[IDATx\\x9c\\x01P\\x00\\xaf\\xff\\x01\\x13\\x1b<7\\xe7\\xe0\\xdc^6\\xed\\x04\\xc7M\\xd2\\x9f\\x00X\\x1b\\xb0?\\x1ba\\x15\\xc5 o\\xd0\\x80\\xbe\\x19/\\x01\\xec\\x95\\x1f\\x9f\\xffj\\xfa1\\xa7\\xc4X\\xea\\xbe\\xa4g\\x00\\xc4\\x15\\xdeC\\xc7 \\xbbaqe\\xc8\\xb9\\xa9q\\xe7\\x00,?M\\xc0)\\xdaD`}\\xb1\\xdci\\x1e\\xafC\\xa9]%.@\\xa6\\xf0\\xb3\\x00\\x00\\x00\\x00IEND\\xaeB`\\x82', 'path': None}]}\r\n```\r\n\r\n\r\n\n\n### Expected behavior\n\nI would expect `{key: dataset[key] for key in dataset.column_names}` and `dataset.to_dict()` to be equivalent. If the previous behavior is expected, then it should be stated [in the doc](https://huggingface.co/docs/datasets/v2.14.4/en/package_reference/main_classes#datasets.Dataset.to_dict).\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: Linux-6.2.0-31-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n- Pillow 9.5.0\r\n- numpy 1.25.2<end_of_turn>\n<start_of_turn>model\n`Dataset.to_dict()` ignore `decode=True` with Image feature<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhile loading a dataset with multiple splits I get an error saying `Couldn't find file at <URL>`\r\n\r\nI am loading the dataset like so, nothing out of the ordinary.\r\nThis dataset needs a token to access it.\r\n```\r\ntoken=\"hf_myhftoken-sdhbdsjgkhbd\"\r\nload_dataset(\"speechcolab/gigaspeech\", \"test\", cache_dir=f\"gigaspeech/test\", token=token)\r\n```\r\n\r\nI get the following error\r\n![Screenshot 2024-04-19 at 11 03 07\u202fPM](https://github.com/huggingface/datasets/assets/35369637/8dce757f-08ff-45dd-85b5-890fced7c5bc)\r\n\r\nNow you can see that the URL that it is trying to reach has the JSON object of the dataset split appended to the base URL. I think this may be due to a newly introduced issue.\r\n\r\nI did not have this issue with the previous version of the datasets. Everything was fine for me yesterday and after the release 12 hours ago, this seems to have broken. Also, the dataset in question runs custom code and I checked and there have been no commits to the dataset on Huggingface in 6 months.\r\n\r\n### Steps to reproduce the bug\r\n\r\nSince this happened with one particular dataset for me, I am listing steps to use that dataset.\r\n1. Open https://huggingface.co/datasets/speechcolab/gigaspeech and fill the form to get access.\r\n2. Create a token on your huggingface account with read access.\r\n3. Run the following line, substituing `<your_token_here>` with your token.\r\n```\r\nload_dataset(\"speechcolab/gigaspeech\", \"test\", cache_dir=f\"gigaspeech/test\", token=\"<your_token_here>\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nBe able to load the dataset in question.\r\n\r\n### Environment info\r\n\r\ndatasets == 2.19.0\r\npython == 3.10\r\nkernel == Linux 6.1.58+<end_of_turn>\n<start_of_turn>model\nLoading a remote dataset fails in the last release (v2.19.0)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix parallel push_to_hub in dataset_dict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHello! I want to create a dataset of parquet files, with audios stored as separate .mp3 files. However, it says \"No such file or directory\" (see the reproducing code).\r\n\r\n### Steps to reproduce the bug\r\n\r\nCreating a dataset\r\n```\r\nfrom pathlib import Path\r\nfrom datasets import Dataset, load_dataset, Audio\r\n\r\nPath('my_dataset/audio').mkdir(parents=True, exist_ok=True)\r\nPath('my_dataset/audio/file.mp3').touch(exist_ok=True)\r\nDataset.from_list(\r\n    [{'audio': {'path': 'audio/file.mp3'}}]\r\n).to_parquet('my_dataset/data.parquet')\r\n```\r\n\r\nResult:\r\n```\r\n# my_dataset\r\n# \u251c\u2500\u2500 audio\r\n# \u2502   \u2514\u2500\u2500 file.mp3\r\n# \u2514\u2500\u2500 data.parquet\r\n```\r\n\r\nTrying to load the dataset\r\n```\r\ndataset = (\r\n    load_dataset('my_dataset', split='train')\r\n    .cast_column('audio', Audio(sampling_rate=16_000))\r\n)\r\ndataset[0]\r\n\r\n>>> FileNotFoundError: [Errno 2] No such file or directory: 'audio/file.mp3'\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expect the dataset to load correctly.\r\n\r\nI've found 2 workarounds, but they are not very good:\r\n1. I can specify an absolute path to the audio, however, when I move the folder or upload to HF it will stop working.\r\n2. I can set `'path': 'file.mp3'`, and load with `load_dataset('my_dataset', data_dir='audio')` - it seems to work, but does this mean that anyone from Hugging Face who wants to use this dataset should also pass the `data_dir` argument, otherwise it won't work?\r\n\r\n### Environment info\r\n\r\ndatasets 3.1.0, Ubuntu 24.04.1<end_of_turn>\n<start_of_turn>model\nCannot create a dataset with relative audio path<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen trying to get config names or load any dataset within the open-llm-leaderboard ecosystem (`open-llm-leaderboard/details_`) I receive the DataFilesNotFoundError. For the last month or so I've been loading datasets from the leaderboard almost everyday; yesterday was the first time I started seeing this.\n\n### Steps to reproduce the bug\n\nThis snippet has three cells:\r\n1. Loads the modules\r\n2. Tries to get config names\r\n3. Tries to load the dataset\r\n\r\nI've chosen \"davidkim205\"'s Rhea-72b-v0.5 model because it is one of the best performers on the leaderboard should likely have no dataset issues:\r\n\r\n```python\r\nIn [1]: from datasets import load_dataset, get_dataset_config_names\r\n\r\nIn [2]: get_dataset_config_names(\"open-llm-leaderboard/details_davidkim205__Rhea\r\n   ...: -72b-v0.5\")\r\n---------------------------------------------------------------------------\r\nDataFilesNotFoundError                    Traceback (most recent call last)\r\nCell In[2], line 1\r\n----> 1 get_dataset_config_names(\"open-llm-leaderboard/details_davidkim205__Rhea-72b-v0.5\")\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/inspect.py:347, in get_dataset_config_names(path, revision, download_config, download_mode, dynamic_modules_path, data_files, **download_kwargs)\r\n    291 def get_dataset_config_names(\r\n    292     path: str,\r\n    293     revision: Optional[Union[str, Version]] = None,\r\n   (...)\r\n    298     **download_kwargs,\r\n    299 ):\r\n    300     \"\"\"Get the list of available config names for a particular dataset.\r\n    301 \r\n    302     Args:\r\n   (...)\r\n    345     ```\r\n    346     \"\"\"\r\n--> 347     dataset_module = dataset_module_factory(\r\n    348         path,\r\n    349         revision=revision,\r\n    350         download_config=download_config,\r\n    351         download_mode=download_mode,\r\n    352         dynamic_modules_path=dynamic_modules_path,\r\n    353         data_files=data_files,\r\n    354         **download_kwargs,\r\n    355     )\r\n    356     builder_cls = get_dataset_builder_class(dataset_module, dataset_name=os.path.basename(path))\r\n    357     return list(builder_cls.builder_configs.keys()) or [\r\n    358         dataset_module.builder_kwargs.get(\"config_name\", builder_cls.DEFAULT_CONFIG_NAME or \"default\")\r\n    359     ]\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:1821, in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\r\n   1812     return LocalDatasetModuleFactoryWithScript(\r\n   1813         combined_path,\r\n   1814         download_mode=download_mode,\r\n   1815         dynamic_modules_path=dynamic_modules_path,\r\n   1816         trust_remote_code=trust_remote_code,\r\n   1817     ).get_module()\r\n   1818 elif os.path.isdir(path):\r\n   1819     return LocalDatasetModuleFactoryWithoutScript(\r\n   1820         path, data_dir=data_dir, data_files=data_files, download_mode=download_mode\r\n-> 1821     ).get_module()\r\n   1822 # Try remotely\r\n   1823 elif is_relative_path(path) and path.count(\"/\") <= 1:\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:1039, in LocalDatasetModuleFactoryWithoutScript.get_module(self)\r\n   1033     patterns = get_data_patterns(base_path)\r\n   1034 data_files = DataFilesDict.from_patterns(\r\n   1035     patterns,\r\n   1036     base_path=base_path,\r\n   1037     allowed_extensions=ALL_ALLOWED_EXTENSIONS,\r\n   1038 )\r\n-> 1039 module_name, default_builder_kwargs = infer_module_for_data_files(\r\n   1040     data_files=data_files,\r\n   1041     path=self.path,\r\n   1042 )\r\n   1043 data_files = data_files.filter_extensions(_MODULE_TO_EXTENSIONS[module_name])\r\n   1044 # Collect metadata files if the module supports them\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:597, in infer_module_for_data_files(data_files, path, download_config)\r\n    595     raise ValueError(f\"Couldn't infer the same data file format for all splits. Got {split_modules}\")\r\n    596 if not module_name:\r\n--> 597     raise DataFilesNotFoundError(\"No (supported) data files found\" + (f\" in {path}\" if path else \"\"))\r\n    598 return module_name, default_builder_kwargs\r\n\r\nDataFilesNotFoundError: No (supported) data files found in open-llm-leaderboard/details_davidkim205__Rhea-72b-v0.5\r\n\r\nIn [3]: data = load_dataset(\"open-llm-leaderboard/details_davidkim205__Rhea-72b-\r\n   ...: v0.5\", \"harness_winogrande_5\")\r\n---------------------------------------------------------------------------\r\nDataFilesNotFoundError                    Traceback (most recent call last)\r\nCell In[3], line 1\r\n----> 1 data = load_dataset(\"open-llm-leaderboard/details_davidkim205__Rhea-72b-v0.5\", \"harness_winogrande_5\")\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:2587, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   2582 verification_mode = VerificationMode(\r\n   2583     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\r\n   2584 )\r\n   2586 # Create a dataset builder\r\n-> 2587 builder_instance = load_dataset_builder(\r\n   2588     path=path,\r\n   2589     name=name,\r\n   2590     data_dir=data_dir,\r\n   2591     data_files=data_files,\r\n   2592     cache_dir=cache_dir,\r\n   2593     features=features,\r\n   2594     download_config=download_config,\r\n   2595     download_mode=download_mode,\r\n   2596     revision=revision,\r\n   2597     token=token,\r\n   2598     storage_options=storage_options,\r\n   2599     trust_remote_code=trust_remote_code,\r\n   2600     _require_default_config_name=name is None,\r\n   2601     **config_kwargs,\r\n   2602 )\r\n   2604 # Return iterable dataset in case of streaming\r\n   2605 if streaming:\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:2259, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\r\n   2257     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   2258     download_config.storage_options.update(storage_options)\r\n-> 2259 dataset_module = dataset_module_factory(\r\n   2260     path,\r\n   2261     revision=revision,\r\n   2262     download_config=download_config,\r\n   2263     download_mode=download_mode,\r\n   2264     data_dir=data_dir,\r\n   2265     data_files=data_files,\r\n   2266     cache_dir=cache_dir,\r\n   2267     trust_remote_code=trust_remote_code,\r\n   2268     _require_default_config_name=_require_default_config_name,\r\n   2269     _require_custom_configs=bool(config_kwargs),\r\n   2270 )\r\n   2271 # Get dataset builder class from the processing script\r\n   2272 builder_kwargs = dataset_module.builder_kwargs\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:1821, in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\r\n   1812     return LocalDatasetModuleFactoryWithScript(\r\n   1813         combined_path,\r\n   1814         download_mode=download_mode,\r\n   1815         dynamic_modules_path=dynamic_modules_path,\r\n   1816         trust_remote_code=trust_remote_code,\r\n   1817     ).get_module()\r\n   1818 elif os.path.isdir(path):\r\n   1819     return LocalDatasetModuleFactoryWithoutScript(\r\n   1820         path, data_dir=data_dir, data_files=data_files, download_mode=download_mode\r\n-> 1821     ).get_module()\r\n   1822 # Try remotely\r\n   1823 elif is_relative_path(path) and path.count(\"/\") <= 1:\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:1039, in LocalDatasetModuleFactoryWithoutScript.get_module(self)\r\n   1033     patterns = get_data_patterns(base_path)\r\n   1034 data_files = DataFilesDict.from_patterns(\r\n   1035     patterns,\r\n   1036     base_path=base_path,\r\n   1037     allowed_extensions=ALL_ALLOWED_EXTENSIONS,\r\n   1038 )\r\n-> 1039 module_name, default_builder_kwargs = infer_module_for_data_files(\r\n   1040     data_files=data_files,\r\n   1041     path=self.path,\r\n   1042 )\r\n   1043 data_files = data_files.filter_extensions(_MODULE_TO_EXTENSIONS[module_name])\r\n   1044 # Collect metadata files if the module supports them\r\n\r\nFile ~/open-llm-bda/venv/lib/python3.11/site-packages/datasets/load.py:597, in infer_module_for_data_files(data_files, path, download_config)\r\n    595     raise ValueError(f\"Couldn't infer the same data file format for all splits. Got {split_modules}\")\r\n    596 if not module_name:\r\n--> 597     raise DataFilesNotFoundError(\"No (supported) data files found\" + (f\" in {path}\" if path else \"\"))\r\n    598 return module_name, default_builder_kwargs\r\n\r\nDataFilesNotFoundError: No (supported) data files found in open-llm-leaderboard/details_davidkim205__Rhea-72b-v0.5\r\n```\n\n### Expected behavior\n\nNo exceptions from `get_dataset_config_names` or `load_dataset`\n\n### Environment info\n\n- `datasets` version: 2.19.0\r\n- Platform: Linux-6.5.0-1018-aws-aarch64-with-glibc2.35\r\n- Python version: 3.11.8\r\n- `huggingface_hub` version: 0.23.0\r\n- PyArrow version: 16.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nDataFilesNotFoundError for datasets in the open-llm-leaderboard<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/6071<end_of_turn>\n<start_of_turn>model\nFix fsspec storage_options from load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdd streaming support after https://github.com/huggingface/datasets/pull/7690, cc @klamike :)\r\n\r\n## Details\r\n\r\nin `datasets` loaders, `open()` is extended to work with files that are on disk but also on HF. Files on HF are streamed using HTTP range requests using the `HfFileSystem` implementation in the `huggingface_hub` library.<end_of_turn>\n<start_of_turn>model\nAllow streaming hdf5 files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThere is a misalignment between the format of the `data_files` specified in the configs metadata YAML (CSV):\r\n```yaml\r\nconfigs:\r\n  - config_name: default\r\n    data_files:\r\n      - split: train\r\n        path: data.csv\r\n``` \r\nand the inferred builder (JSON). Note there are multiple JSON files in the repo, but they do not appear in the configs metadata YAML.\r\n\r\nSee: https://huggingface.co/datasets/freddyaboulton/chatinterface_with_image_csv/discussions/1\r\n\r\nCC: @freddyaboulton @polinaeterna <end_of_turn>\n<start_of_turn>model\nMisalignment between file format specified in configs metadata YAML and the inferred builder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReported by @vttrifonov at https://github.com/huggingface/datasets/pull/6704#issuecomment-2071168296:\r\n> This change is breaking in \r\n> https://github.com/huggingface/datasets/blob/f96e74d5c633cd5435dd526adb4a74631eb05c43/src/datasets/arrow_dataset.py#L1515\r\n> when the input is `pathlib.Path`. The issue is that `url_to_fs` expects a `str` and cannot deal with `Path`. `get_fs_token_paths` converts to `str` so it is not a problem\r\n\r\nThis change was introduced in:\r\n- #6704 <end_of_turn>\n<start_of_turn>model\nLoad and save from/to disk no longer accept pathlib.Path<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to fine-tune the t5 model on the paraphrasing task. While running the same code without-\r\n\r\nmodel = get_peft_model(model, config)\r\n\r\nthe model trains without any issues. However, using the model returned from get_peft_model raises the following error due to datasets-\r\n\r\nIndexError: Invalid key: 47682 is out of bounds for size 0.\r\n\r\nI had raised this in https://github.com/huggingface/peft/issues/1299#issue-2056173386 and they suggested that I raise it here.\r\n\r\nHere is the complete error-\r\n\r\nIndexError Traceback (most recent call last)\r\nin <cell line: 1>()\r\n----> 1 trainer.train()\r\n\r\n11 frames\r\n[/usr/local/lib/python3.10/dist-packages/transformers/trainer.py](https://localhost:8080/#) in train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\r\n1553 hf_hub_utils.enable_progress_bars()\r\n1554 else:\r\n-> 1555 return inner_training_loop(\r\n1556 args=args,\r\n1557 resume_from_checkpoint=resume_from_checkpoint,\r\n\r\n[/usr/local/lib/python3.10/dist-packages/transformers/trainer.py](https://localhost:8080/#) in _inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\r\n1836\r\n1837 step = -1\r\n-> 1838 for step, inputs in enumerate(epoch_iterator):\r\n1839 total_batched_samples += 1\r\n1840 if rng_to_sync:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/accelerate/data_loader.py](https://localhost:8080/#) in iter(self)\r\n446 # We iterate one batch ahead to check when we are at the end\r\n447 try:\r\n--> 448 current_batch = next(dataloader_iter)\r\n449 except StopIteration:\r\n450 yield\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py](https://localhost:8080/#) in next(self)\r\n628 # TODO(https://github.com/pytorch/pytorch/issues/76750)\r\n629 self._reset() # type: ignore[call-arg]\r\n--> 630 data = self._next_data()\r\n631 self._num_yielded += 1\r\n632 if self._dataset_kind == _DatasetKind.Iterable and \\\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py](https://localhost:8080/#) in _next_data(self)\r\n672 def _next_data(self):\r\n673 index = self._next_index() # may raise StopIteration\r\n--> 674 data = self._dataset_fetcher.fetch(index) # may raise StopIteration\r\n675 if self._pin_memory:\r\n676 data = _utils.pin_memory.pin_memory(data, self._pin_memory_device)\r\n\r\n[/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py](https://localhost:8080/#) in fetch(self, possibly_batched_index)\r\n47 if self.auto_collation:\r\n48 if hasattr(self.dataset, \"getitems\") and self.dataset.getitems:\r\n---> 49 data = self.dataset.getitems(possibly_batched_index)\r\n50 else:\r\n51 data = [self.dataset[idx] for idx in possibly_batched_index]\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in getitems(self, keys)\r\n2802 def getitems(self, keys: List) -> List:\r\n2803 \"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\r\n-> 2804 batch = self.getitem(keys)\r\n2805 n_examples = len(batch[next(iter(batch))])\r\n2806 return [{col: array[i] for col, array in batch.items()} for i in range(n_examples)]\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in getitem(self, key)\r\n2798 def getitem(self, key): # noqa: F811\r\n2799 \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\r\n-> 2800 return self._getitem(key)\r\n2801\r\n2802 def getitems(self, keys: List) -> List:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py](https://localhost:8080/#) in _getitem(self, key, **kwargs)\r\n2782 format_kwargs = format_kwargs if format_kwargs is not None else {}\r\n2783 formatter = get_formatter(format_type, features=self._info.features, **format_kwargs)\r\n-> 2784 pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\r\n2785 formatted_output = format_table(\r\n2786 pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py](https://localhost:8080/#) in query_table(table, key, indices)\r\n581 else:\r\n582 size = indices.num_rows if indices is not None else table.num_rows\r\n--> 583 _check_valid_index_key(key, size)\r\n584 # Query the main table\r\n585 if indices is None:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py](https://localhost:8080/#) in _check_valid_index_key(key, size)\r\n534 elif isinstance(key, Iterable):\r\n535 if len(key) > 0:\r\n--> 536 _check_valid_index_key(int(max(key)), size=size)\r\n537 _check_valid_index_key(int(min(key)), size=size)\r\n538 else:\r\n\r\n[/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py](https://localhost:8080/#) in _check_valid_index_key(key, size)\r\n524 if isinstance(key, int):\r\n525 if (key < 0 and key + size < 0) or (key >= size):\r\n--> 526 raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\n527 return\r\n528 elif isinstance(key, slice):\r\n\r\nIndexError: Invalid key: 47682 is out of bounds for size 0\n\n### Steps to reproduce the bug\n\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\r\n\r\n#defining model name for tokenizer and model loading\r\nmodel_name= \"t5-small\"\r\n#loading the tokenizer\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\n\r\ndef preprocess_function(data, tokenizer):\r\ninputs = [f\"Paraphrase this sentence: {doc}\" for doc in data[\"text\"]]\r\nmodel_inputs = tokenizer(inputs, max_length=150, truncation=True)\r\nlabels = [ast.literal_eval(i)[0] for i in data['paraphrases']]\r\nlabels = tokenizer(labels, max_length=150, truncation=True)\r\nmodel_inputs[\"labels\"] = labels[\"input_ids\"]\r\nreturn model_inputs\r\n\r\ntrain_dataset = load_dataset(\"humarin/chatgpt-paraphrases\", split=\"train\").shuffle(seed=42).select(range(50000))\r\nval_dataset = load_dataset(\"humarin/chatgpt-paraphrases\", split=\"train\").shuffle(seed=42).select(range(50000,55000))\r\n\r\ntokenized_train = train_dataset.map(lambda batch: preprocess_function(batch, tokenizer), batched=True)\r\ntokenized_val = val_dataset.map(lambda batch: preprocess_function(batch, tokenizer), batched=True)\r\n\r\ndef print_trainable_parameters(model):\r\n\"\"\"\r\nPrints the number of trainable parameters in the model.\r\n\"\"\"\r\ntrainable_params = 0\r\nall_param = 0\r\nfor _, param in model.named_parameters():\r\nall_param += param.numel()\r\nif param.requires_grad:\r\ntrainable_params += param.numel()\r\nprint(\r\nf\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\r\n)\r\n\r\nconfig = LoraConfig(\r\nr=16, #attention heads\r\nlora_alpha=32, #alpha scaling\r\nlora_dropout=0.05,\r\nbias=\"none\",\r\ntask_type=\"Seq2Seq\"\r\n)\r\n\r\n#loading the model\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\r\n\r\nmodel = get_peft_model(model, config)\r\nprint_trainable_parameters(model)\r\n\r\n#loading the data collator\r\ndata_collator = DataCollatorForSeq2Seq(\r\ntokenizer=tokenizer,\r\nmodel=model,\r\nlabel_pad_token_id=-100,\r\npadding=\"longest\"\r\n)\r\n\r\n#defining the training arguments\r\ntraining_args = Seq2SeqTrainingArguments(\r\noutput_dir=os.getcwd(),\r\nevaluation_strategy=\"epoch\",\r\nsave_strategy=\"epoch\",\r\nlearning_rate=2e-5,\r\nper_device_train_batch_size=16,\r\nper_device_eval_batch_size=16,\r\nweight_decay=1e-3,\r\nsave_total_limit=3,\r\nload_best_model_at_end=True,\r\nnum_train_epochs=1,\r\npredict_with_generate=True\r\n)\r\n\r\ndef compute_metric_with_extra(tokenizer):\r\ndef compute_metrics(eval_preds):\r\nmetric = evaluate.load('rouge')\r\npreds, labels = eval_preds\r\n\r\n    # decode preds and labels\r\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\r\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\r\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\r\n\r\n    # rougeLSum expects newline after each sentence\r\n    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\r\n    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\r\n\r\n    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\r\n    return result\r\n\r\nreturn compute_metrics\r\ntrainer = Seq2SeqTrainer(\r\nmodel=model,\r\nargs=training_args,\r\ntrain_dataset=tokenized_train,\r\neval_dataset=tokenized_val,\r\ntokenizer=tokenizer,\r\ndata_collator=data_collator,\r\ncompute_metrics= compute_metric_with_extra(tokenizer)\r\n)\r\n\r\ntrainer.train()\n\n### Expected behavior\n\nI would want the trainer to train normally as it was before I used-\r\n\r\nmodel = get_peft_model(model, config)\r\n\r\n\n\n### Environment info\n\ndatasets version- 2.16.0\r\npeft version- 0.7.1\r\ntransformers version- 4.35.2\r\naccelerate version- 0.25.0\r\npython- 3.10.12\r\nenviroment- google colab<end_of_turn>\n<start_of_turn>model\nIndexError: Invalid key: 47682 is out of bounds for size 0 while using PEFT<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nList of formats and libraries we can use to load the data in `datasets`:\n\n- [ ] DICOMs: pydicom\n- [x] NIfTIs: nibabel\n- [ ] WFDB: wfdb\n\ncc @zaRizk7 for viz\n\nFeel free to comment / suggest other formats and libs you'd like to see or to share your interest in one of the mentioned format<end_of_turn>\n<start_of_turn>model\nSupport scientific data formats<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nHi !\r\n\r\nI'm currently working with a large (~150GB) unnormalized dataset at work.\r\nThe dataset is available on a read-only filesystem internally, and I use a [loading script](https://huggingface.co/docs/datasets/dataset_script) to retreive it.\r\n\r\nI want to normalize the features of the dataset, meaning I need to compute the mean and standard deviation metric for each feature of the entire dataset. I cannot load the entire dataset to RAM as it is too big, so following [this discussion on the huggingface discourse](https://discuss.huggingface.co/t/copy-columns-in-a-dataset-and-compute-statistics-for-a-column/22157) I am using a [map operation](https://huggingface.co/docs/datasets/v2.14.0/en/package_reference/main_classes#datasets.Dataset.map) to first compute the metrics and a second map operation to apply them on the dataset.\r\n\r\nThe problem lies in the second mapping, as it gets stuck at ~99%. By checking what the process does (using `htop` and `strace`) it seems to be doing a lot of I/O operations, and I'm not sure why.\r\n\r\nObviously, I could always normalize the dataset externally and then load it using a loading script. However, since the internal dataset is updated fairly frequently, using the library to perform normalization automatically would make it much easier for me.\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nI'm able to reproduce the problem using the following scripts:\r\n\r\n```python\r\n# random_data.py\r\n\r\nimport datasets\r\nimport torch\r\n\r\n_VERSION = \"1.0.0\"\r\n\r\n\r\nclass RandomDataset(datasets.GeneratorBasedBuilder):\r\n    def _info(self):\r\n        return datasets.DatasetInfo(\r\n            version=_VERSION,\r\n            supervised_keys=None,\r\n            features=datasets.Features(\r\n                {\r\n                    \"positions\": datasets.Array2D(\r\n                        shape=(30000, 3),\r\n                        dtype=\"float32\",\r\n                    ),\r\n                    \"normals\": datasets.Array2D(\r\n                        shape=(30000, 3),\r\n                        dtype=\"float32\",\r\n                    ),\r\n                    \"features\": datasets.Array2D(\r\n                        shape=(30000, 6),\r\n                        dtype=\"float32\",\r\n                    ),\r\n                    \"scalars\": datasets.Sequence(\r\n                        feature=datasets.Value(\"float32\"),\r\n                        length=20,\r\n                    ),\r\n                },\r\n            ),\r\n        )\r\n\r\n    def _split_generators(self, dl_manager):\r\n        return [\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TRAIN,  # type: ignore\r\n                gen_kwargs={\"nb_samples\": 1000},\r\n            ),\r\n            datasets.SplitGenerator(\r\n                name=datasets.Split.TEST,  # type: ignore\r\n                gen_kwargs={\"nb_samples\": 100},\r\n            ),\r\n        ]\r\n\r\n    def _generate_examples(self, nb_samples: int):\r\n        for idx in range(nb_samples):\r\n            yield idx, {\r\n                \"positions\": torch.randn(30000, 3),\r\n                \"normals\": torch.randn(30000, 3),\r\n                \"features\": torch.randn(30000, 6),\r\n                \"scalars\": torch.randn(20),\r\n            }\r\n```\r\n\r\n```python\r\n# main.py\r\n\r\nimport datasets\r\nimport torch\r\n\r\n\r\ndef apply_mean_std(\r\n    dataset: datasets.Dataset,\r\n    means: dict[str, torch.Tensor],\r\n    stds: dict[str, torch.Tensor],\r\n) -> dict[str, torch.Tensor]:\r\n    \"\"\"Normalize the dataset using the mean and standard deviation of each feature.\r\n\r\n    Args:\r\n        dataset (`Dataset`): A huggingface dataset.\r\n        mean (`dict[str, Tensor]`): A dictionary containing the mean of each feature.\r\n        std (`dict[str, Tensor]`): A dictionary containing the standard deviation of each feature.\r\n\r\n    Returns:\r\n        dict: A dictionary containing the normalized dataset.\r\n    \"\"\"\r\n    result = {}\r\n\r\n    for key in means.keys():\r\n        # extract data from dataset\r\n        data: torch.Tensor = dataset[key]  # type: ignore\r\n\r\n        # extract mean and std from dict\r\n        mean = means[key]  # type: ignore\r\n        std = stds[key]  # type: ignore\r\n\r\n        # normalize data\r\n        normalized_data = (data - mean) / std\r\n\r\n        result[key] = normalized_data\r\n\r\n    return result\r\n\r\n\r\n# get dataset\r\nds = datasets.load_dataset(\r\n    path=\"random_data.py\",\r\n    split=\"train\",\r\n).with_format(\"torch\")\r\n\r\n# compute mean (along last axis)\r\nmeans = {key: torch.zeros(ds[key][0].shape[-1]) for key in ds.column_names}\r\nmeans_sq = {key: torch.zeros(ds[key][0].shape[-1]) for key in ds.column_names}\r\n\r\nfor batch in ds.iter(batch_size=8):\r\n    for key in ds.column_names:\r\n        data = batch[key]\r\n        batch_size = data.shape[0]\r\n        data = data.reshape(-1, data.shape[-1])\r\n        means[key] += data.mean(dim=0) / len(ds) * batch_size\r\n        means_sq[key] += (data**2).mean(dim=0) / len(ds) * batch_size\r\n\r\n# compute std (along last axis)\r\nstds = {key: torch.sqrt(means_sq[key] - means[key] ** 2) for key in ds.column_names}\r\n\r\n# normalize each feature of the dataset\r\nds_normalized = ds.map(\r\n    desc=\"Applying mean/std\",  # type: ignore\r\n    function=apply_mean_std,\r\n    batched=False,\r\n    fn_kwargs={\r\n        \"means\": means,\r\n        \"stds\": stds,\r\n    },\r\n)\r\n```\r\n\r\n### Expected behavior\r\n\r\nUsing the previous scripts, the `ds_normalized` mapping completes in ~5 minutes, but any subsequent use of `ds_normalized` is really really slow, for example reapplying `apply_mean_std` to `ds_normalized` takes forever. This is very strange, I'm sure I must be missing something, but I would still expect this to be faster.\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.13.1\r\n- Platform: Linux-3.10.0-1160.66.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.0\r\n- Pandas version: 2.0.2<end_of_turn>\n<start_of_turn>model\nMapping gets stuck at 99%<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI\u2019m not sure if this is a bug or a feature and I just don\u2019t fully understand how dataset loading is to work, but it appears there may be a bug with how locally stored Image() are being accessed. I\u2019ve uploaded a new dataset to hugging face (rmdig/rocky_mountain_snowpack) but I\u2019ve come into a ton of trouble trying to have the images handled properly (at least in the way I\u2019d expect them to be handled). \n\nI find that I cannot use relative paths for loading images remotely from the Hugging Face repo or from a local repository. Any time I do it always simply appends my current working directory to the dataset. As a result to use the datasets library with my dataset I have to change my working directory to the dataset folder or abandon the dataset object structure, which I cannot imagine you intended. As a result I have to use URL\u2019s since an absolute path on my system obviously wouldn\u2019t work for others. The URL works ok, but despite me having it locally downloaded, it appears to be redownloading the dataset every time I train my snowGAN model on it (and often times I\u2019m coming into HTTPS errors for over requesting the data).\n\nOr maybe image relative paths aren't intended to be loaded directly through your datasets library as images and should be kept as strings for the user to handle? If so I feel like you\u2019re missing out on some pretty seamless functionality\n\n### Steps to reproduce the bug\n\n1. Download a local copy of the dataset (rmdig/rocky_mountain_snowpack) through git or whatever you prefer.\n2. Alter the README.md YAML for file_path (the relative path to each image) to be type Image instead of type string\n\n`\n---\ndataset_info:\n  features:\n    - name: image\n      dtype: Image\n    - name: file_path\n      dtype: Image\n`\n\n3. Initialize the dataset locally, make sure your working directory is not the dataset directory root \n\n`dataset = datasets.load_dataset(\u2018path/to/local/rocky_mountain_snowpack/\u2018)`\n \n4. Call to one of the samples and you\u2019ll get an error that the image was not found in current/working/directory/preprocessed/cores/image_1.png. Showing that it\u2019s simply looking in the current working directory + relative path\n\n`\n>>> dataset['train'][0]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 2859, in __getitem__\n    return self._getitem(key)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/arrow_dataset.py\", line 2841, in _getitem\n    formatted_output = format_table(\n                       ^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/formatting/formatting.py\", line 657, in format_table\n    return formatter(pa_table, query_type=query_type)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/formatting/formatting.py\", line 410, in __call__\n    return self.format_row(pa_table)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/formatting/formatting.py\", line 459, in format_row\n    row = self.python_features_decoder.decode_row(row)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/formatting/formatting.py\", line 223, in decode_row\n    return self.features.decode_example(row, token_per_repo_id=self.token_per_repo_id) if self.features else row\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/features/features.py\", line 2093, in decode_example\n    column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/features/features.py\", line 1405, in decode_nested_example\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/datasets/features/image.py\", line 171, in decode_example\n    image = PIL.Image.open(path)\n            ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/dennyschaedig/miniconda3/lib/python3.12/site-packages/PIL/Image.py\", line 3277, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/Users/dennyschaedig/Datasets/preprocessed/cores/image_1.png'\n`\n\n### Expected behavior\n\nI expect the datasets and Image() to load the locally hosted data using path/to/local/rocky_mountain_snowpack/ (that I pass in with my datasets.load_dataset() or the you all handle on the backend) call + relative path. \n\nInstead it appears to load from my current working directory + relative path.\n\n### Environment info\n\nTested on\u2026\n\nWindows 11, Ubuntu Linux 22.04 and Mac Sequoia 15.5 Silicone M2\n\ndatasets version 4.0.0\n\nPython 3.12 and 3.13<end_of_turn>\n<start_of_turn>model\nDataset Repo Paths to Locally Stored Images Not Being Appended to Image Path<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI have a multi label dataset and I'd like to be able to class encode the column and store the mapping directly in the features just as I can with a single label column.  `class_encode_column` currently does not support multi labels.\r\n\r\nHere's an example of what I'd like to encode:\r\n\r\n```\r\ndata = {\r\n    'text': ['one', 'two', 'three', 'four'],\r\n    'labels': [['a', 'b'], ['b'], ['b', 'c'], ['a', 'd']]\r\n}\r\n\r\ndataset = Dataset.from_dict(data)\r\ndataset = dataset.class_encode_column('labels')\r\n```\r\n\r\nI did some digging into the code base to evaluate the feasibility of this (note I'm very new to this code base) and from what I noticed the `ClassLabel` feature is still stored as an underlying raw data type of int so I thought a `MultiLabel` feature could similarly be stored as a Sequence of ints, thus not requiring significant serialization / conversion work to / from arrow.\r\n\r\nI did a POC of this [here](https://github.com/huggingface/datasets/commit/15443098e9ce053943172f7ec6fce3769d7dff6e) and included a simple test case (please excuse all the commented out tests, going for speed of POC here and didn't want to fight IDE to debug a single test).   In the test I just assert that `num_classes` is the same to show that things are properly serializing, but if you break after loading from disk you'll see the dataset correct and the dataset feature is as expected.\r\n\r\nAfter digging more I did notice a few issues\r\n- After loading from disk I noticed type of the `labels` class is `Sequence` not `MultiLabel` (though the added `feature` attribute came through).  This doesn't happen for `ClassLabel` but I couldn't find the encode / decode code paths that handle this.\r\n- I subclass `Sequence` in `MultiLabel` to leverage existing serialization, but this does miss the custom encode logic that `ClassLabel` has.  I'm not sure of the best way to approach this as I haven't fully understood the encode / decode flow for datasets.  I suspect my simple implementation will need some improvement as it'll require a significant amount of repeated logic to mimic `ClassLabel` behavior.\r\n\r\n\n\n### Motivation\n\nSee above - would like to support multi label class encodings.\n\n### Your contribution\n\nThis would be a big help for us and we're open to contributing but I'll likely need some guidance on how to implement to fit the encode / decode flow.  Some suggestions on tests / would be great too, I'm guessing in addition to the class encode tests (that I'll need to expand) we'll need encode / decode tests.<end_of_turn>\n<start_of_turn>model\nMulti label class encoding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nit seems that sth wrong with my terrible \"bug body\" life, When i run this code, \"import datasets\"\r\ni meet this error FileNotFoundError: [Errno 2] No such file or directory: 'nul'\r\n![image](https://github.com/huggingface/datasets/assets/73683903/3973c120-ebb1-42b7-bede-b9de053e861d)\r\n![image](https://github.com/huggingface/datasets/assets/73683903/0496adff-a7a7-4dcb-929e-ec11ede71f04)\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1.import datasets\r\n\r\n### Expected behavior\r\n\r\ni just run a single line code and stuct in this bug \r\n\r\n### Environment info\r\n\r\nOS: Windows10\r\nDatasets==2.15.0\r\npython=3.10<end_of_turn>\n<start_of_turn>model\nFileNotFoundError: [Errno 2] No such file or directory: 'nul'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI've noticed that when you're not working on the main branch, there are sometimes errors in the files returned. After some investigation, I realized that the revision was not properly passed  everywhere. This PR proposes a fix.<end_of_turn>\n<start_of_turn>model\nAdd missing `revision` argument<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nHi Datasets team! \r\n\r\nIs it possible to add the ability to describe only selected fields of the dataset files in `README.md`? For example, I have this open dataset ([open-llm-leaderboard/results](https://huggingface.co/datasets/open-llm-leaderboard/results?row=0)) and I want to describe only some fields in order not to overcomplicate the Dataset Preview and filter out some fields \n\n### Motivation\n\nThe `Results` dataset for the Open LLM Leaderboard contains json files with a complex nested structure. I would like to add `README.md` there to use the SQL console, for example. But if I describe the structure of this dataset completely, it will overcomplicate the use of Dataset Preview and the total number of columns will exceed 50 \n\n### Your contribution\n\nI'm afraid I'm not familiar with the project structure, so I won't be able to open a PR, but I'll try to help with something else if possible<end_of_turn>\n<start_of_turn>model\nDescribe only selected fields in README<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI propose adding a core maintainer list to the `CONTRIBUTING.md` file.\n\n### Motivation\n\nThe Transformers and Liger-Kernel projects maintain lists of core maintainers for each module. \nHowever, the Datasets project doesn't have such a list. \n\n### Your contribution\n\nI have nothing to add here.<end_of_turn>\n<start_of_turn>model\nAdding Core Maintainer List to CONTRIBUTING.md<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nSimplify downloading and streaming datasets locally. Specifically, perhaps add an option to `load_dataset(..., streaming=\"download_first\")` or add better support for streaming symlinked or arrow files.\r\n\r\n### Motivation\r\n\r\nI have downloaded FineWeb-edu locally and currently trying to stream the dataset from the local files. I have both the raw parquet files using `hugginface-cli download --repo-type dataset HuggingFaceFW/fineweb-edu` and the processed arrow files using `load_dataset(\"HuggingFaceFW/fineweb-edu\")`. \r\n\r\nStreaming the files locally does not work well for both file types for two different reasons.\r\n\r\n**Arrow files**\r\n\r\nWhen running `load_dataset(\"arrow\", data_files={\"train\": \"~/.cache/huggingface/datasets/HuggingFaceFW___fineweb-edu/default/0.0.0/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/fineweb-edu-train-*.arrow\"})` resolving the data files is fast, but because `arrow` is not included in the known [extensions file list](https://github.com/huggingface/datasets/blob/ce4a0c573920607bc6c814605734091b06b860e7/src/datasets/utils/file_utils.py#L738) , all files are opened and scanned to determine the compression type. Adding `arrow` to the known extension types resolves this issue. \r\n\r\n**Parquet files**\r\n\r\nWhen running `load_dataset(\"arrow\", data_files={\"train\": \"~/.cache/huggingface/hub/dataset-HuggingFaceFW___fineweb-edu/snapshots/5b89d1ea9319fe101b3cbdacd89a903aca1d6052/data/CC-MAIN-*/train-*.parquet\"})` the paths do not get resolved because the parquet files are symlinked from the blobs (which contain all files in case there are different versions). This occurs because the [pattern matching](https://github.com/huggingface/datasets/blob/ce4a0c573920607bc6c814605734091b06b860e7/src/datasets/data_files.py#L389) checks if the path is a file and does not check for symlinks. Symlinks (at least on my machine) are of type \"other\". \r\n\r\n\r\n### Your contribution\r\n\r\nI have created a PR for fixing arrow file streaming and symlinks. However, I have not checked locally if the tests work or new tests need to be added.\r\n\r\nIMO, the easiest option would be to add a `streaming=download_first` option, but I'm afraid that exceeds my current knowledge of how the datasets library works. https://github.com/huggingface/datasets/pull/7083<end_of_turn>\n<start_of_turn>model\nMore easily support streaming local files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses https://github.com/huggingface/datasets/issues/6252<end_of_turn>\n<start_of_turn>model\nTranspose images with EXIF Orientation tag<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUsing `*` is ~2X faster according to [benchmark](https://colab.research.google.com/gist/ttsugriy/c964a2604edf70c41911b10335729b6a/for-vs-mult.ipynb) with just 4 patterns. This doesn't matter much since this tiny difference is not going to be noticeable, but why not?<end_of_turn>\n<start_of_turn>model\nUse more efficient and idiomatic way to construct list.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have noticed that on my dataset named [BrunoHays/Accueil_UBS](https://huggingface.co/datasets/BrunoHays/Accueil_UBS), since the version 3.4.0, every column except audio is missing when I load the dataset.\n\nInterestingly, the dataset viewer still shows the correct columns\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import load_dataset\nds = load_dataset(\"BrunoHays/Accueil_UBS\", streaming=True)\nprint(next(iter(ds[\"test\"])).keys())\n```\nWith datasets >= 3.4.0: \n-> dict_keys(['audio'])\nWith datasets == 3.3.2:\n-> dict_keys(['audio', 'id', 'speaker', 'sentence', 'raw_sentence', 'start_timestamp', 'end_timestamp', 'overlap'])\n\n\n### Expected behavior\n\nAll the columns should be present\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: macOS-14.6.1-x86_64-i386-64bit\n- Python version: 3.10.15\n- `huggingface_hub` version: 0.30.1\n- PyArrow version: 16.1.0\n- Pandas version: 1.5.3\n- `fsspec` version: 2023.10.0\n<end_of_turn>\n<start_of_turn>model\nColumns in the dataset obtained though load_dataset do not correspond to the one in the dataset viewer since 3.4.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThanks to @janineguo 's work in https://github.com/huggingface/datasets/pull/5919 which was needed to support HfFileSystem.\r\n\r\nSwitching to `HfFileSystem` will help implementing optimization in data files resolution\r\n\r\n## Implementation details\r\n\r\nI replaced all the from_hf_repo and from_local_or_remote in data_files.py to only use a new `from_patterns` which works for any fsspec path, including hf:// paths, https:// URLs and local paths. This simplifies the codebase since there is no logic duplication anymore when it comes to data files resolution.\r\n\r\nI added `_prepare_path_and_storage_options` which returns the right storage_options to use given a path and a `DownloadConfig`. This is the only place where the logic depends on the filesystem type that must be used.\r\n\r\nI also removed the `get_metadata_data_files_list ` and `get_patterns_and_data_files` functions added recently, since data files resolution is now handled using a common interface.\r\n\r\n## New features\r\n\r\nhf:// paths are now supported in data_files\r\n\r\n## Breaking changes\r\n\r\nDataFilesList and DataFilesDict:\r\n- use `str` paths instead of `Union[Path, Url]`\r\n- require posix paths for windows paths\r\n\r\nclose https://github.com/huggingface/datasets/issues/6017<end_of_turn>\n<start_of_turn>model\nUse new hffs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollowing up on https://github.com/huggingface/datasets/pull/7840 I asked claude code to check for undocumented parameters for other features and it found:\n- mode parameter on video is documented but unused: https://github.com/huggingface/datasets/blob/main/src/datasets/features/video.py#L48-L49\n- the same goes for the mode parameter on the pdf feature: https://github.com/huggingface/datasets/blob/main/src/datasets/features/pdf.py#L47-L48\n\nI assume checking if these modes can be supported and otherwise removing them is the way to go here.\n<end_of_turn>\n<start_of_turn>model\nDOC: `mode` parameter on pdf and video features unused<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThere is a small typo on https://github.com/huggingface/datasets/blob/d5468836fe94e8be1ae093397dd43d4a2503b926/src/datasets/dataset_dict.py#L938\r\n\r\nIt should be `caching is enabled`. \r\n\r\n### Steps to reproduce the bug\r\n\r\nPlease visit \r\n\r\nhttps://github.com/huggingface/datasets/blob/d5468836fe94e8be1ae093397dd43d4a2503b926/src/datasets/dataset_dict.py#L938\r\n\r\n\r\n### Expected behavior\r\n\r\n`caching is enabled`\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.17.1\r\n- Platform: Linux-5.15.0-101-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.7\r\n- `huggingface_hub` version: 0.20.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nSmall typo on the documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nas the tittle says ...\n\n### Steps to reproduce the bug\n\n`sort` seems to be normal.\r\n\r\n```python\r\nfrom datasets import Dataset\r\nimport random\r\n\r\nnums = [{\"k\":random.choice(range(0,1000))} for _ in range(100000)]\r\n\r\nds = Dataset.from_list(nums)\r\n\r\nprint(\"start sort\")\r\n\r\nds = ds.sort(\"k\")\r\n\r\nprint(\"finish sort\")\r\n```\r\n\r\nbut `sort` after `filter` is extremely slow.\r\n\r\n```python\r\nfrom datasets import Dataset\r\nimport random\r\n\r\nnums = [{\"k\":random.choice(range(0,1000))} for _ in range(100000)]\r\n\r\nds = Dataset.from_list(nums)\r\n\r\nds = ds.filter(lambda x:x > 100, input_columns=\"k\")\r\n\r\nprint(\"start sort\")\r\n\r\nds = ds.sort(\"k\")\r\n\r\nprint(\"finish sort\")\r\n```\r\n\n\n### Expected behavior\n\nIs this a bug, or is it a misuse of the `sort` function?\n\n### Environment info\n\n- `datasets` version: 2.20.0\r\n- Platform: Linux-3.10.0-1127.19.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\n`sort` after `filter` unreasonably  slow<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ne.g. to download images or call an inference API like HF or vLLM\r\n\r\n```python\r\nimport asyncio\r\nimport random\r\n\r\nfrom datasets import Dataset\r\n\r\n\r\nasync def f(x):\r\n    await asyncio.sleep(random.random())\r\n\r\nds = Dataset.from_dict({\"data\": range(100)})\r\nds.map(f)\r\n# Map: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:01<00:00, 99.81 examples/s]\r\n```\r\n\r\nTODO\r\n- [x] clean code (right now it's a big copy paste)\r\n- [x] batched\r\n- [x] Dataset.map()\r\n- [x] IterableDataset.map()\r\n- [x] Dataset.filter()\r\n- [x] IterableDataset.filter()\r\n- [x] test\r\n- [x] docs<end_of_turn>\n<start_of_turn>model\nSupport async functions in map()<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I run my code using the \"torchrun\" command, when the code reaches the \"save_to_disk\" part, suddenly I get the following warning and error messages:\r\nBecause the dataset is too large, the \"save_to_disk\" function splits it into 70 parts for saving. However, an error occurs suddenly when it reaches the 14th shard.\r\nWARNING: torch.distributed.elastic.multiprocessing.api: Sending process 2224968 closing signal SIGTERM\r\nERROR: torch.distributed.elastic.multiprocessing.api: failed (exitcode: -7). traceback: Signal 7 (SIGBUS) received by PID 2224967.\n\n### Steps to reproduce the bug\n\nds_shard = ds_shard.map(map_fn, *args, **kwargs)\r\nds_shard.save_to_disk(ds_shard_filepaths[rank])\r\n\r\nSaving the dataset (14/70 shards):  20%|\u2588\u2588        | 875350/4376702 [00:19<01:53, 30863.15 examples/s]\r\nWARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2224968 closing signal SIGTERM\r\nERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: -7) local_rank: 0 (pid: 2224967) of binary: /home/bingxing2/home/scx6964/.conda/envs/ariya235/bin/python\r\nTraceback (most recent call last):\r\n  File \"/home/bingxing2/home/scx6964/.conda/envs/ariya235/bin/torchrun\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/bingxing2/home/scx6964/.conda/envs/ariya235/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\r\n    return f(*args, **kwargs)\r\n  File \"/home/bingxing2/home/scx6964/.conda/envs/ariya235/lib/python3.10/site-packages/torch/distributed/run.py\", line 794, in main\r\n    run(args)\r\n  File \"/home/bingxing2/home/scx6964/.conda/envs/ariya235/lib/python3.10/site-packages/torch/distributed/run.py\", line 785, in run\r\n    elastic_launch(\r\n  File \"/home/bingxing2/home/scx6964/.conda/envs/ariya235/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\r\n    return launch_agent(self._config, self._entrypoint, list(args))\r\n  File \"/home/bingxing2/home/scx6964/.conda/envs/ariya235/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\r\n    raise ChildFailedError(\r\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \r\n==========================================================\r\nrun.py FAILED\r\n----------------------------------------------------------\r\nFailures:\r\n  <NO_OTHER_FAILURES>\r\n----------------------------------------------------------\r\nRoot Cause (first observed failure):\r\n[0]:\r\n  time      : 2023-12-08_20:09:04\r\n  rank      : 0 (local_rank: 0)\r\n  exitcode  : -7 (pid: 2224967)\r\n  error_file: <N/A>\r\n  traceback : Signal 7 (SIGBUS) received by PID 2224967\r\n\r\n\n\n### Expected behavior\n\nI hope it can save successfully without any issues, but it seems there is a problem.\n\n### Environment info\n\n`datasets` version: 2.14.6\r\n- Platform: Linux-4.19.90-24.4.v2101.ky10.aarch64-aarch64-with-glibc2.28\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 14.0.0\r\n- Pandas version: 2.1.2<end_of_turn>\n<start_of_turn>model\nusing torchrun, save_to_disk suddenly shows SIGTERM<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nhfhubhttperror: 500 server error: internal server error for url: https://huggingface.co/api/models/unsloth/deepseek-r1-distill-qwen-32b-bnb-4bit/commits/main (request id: root=1-67fe23fa-3a2150eb444c2a823c388579;de3aed68-c397-4da5-94d4-6565efd3b919) internal error - we're working hard to fix this as soon as possible!\n\n### Steps to reproduce the bug\n\nunsloth/DeepSeek-R1-Distill-Qwen-32B server error\n\n### Expected behavior\n\nNetwork repair\n\n### Environment info\n\nThe web side is also unavailable<end_of_turn>\n<start_of_turn>model\nunsloth/DeepSeek-R1-Distill-Qwen-32B server error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nRunning the tests for the latest release fails due to missing `shared_datadir` fixture.\n\n### Steps to reproduce the bug\n\nRunning `pytest` while building a package for Arch Linux leads to these errors:\n\n```\n==================================== ERRORS ====================================\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>1] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>2] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>3] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>4] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>5] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>6] _________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n_______________ ERROR at setup of test_dataset_with_pdf_feature ________________\n[gw44] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 34\n  @require_pdfplumber\n  def test_dataset_with_pdf_feature(shared_datadir):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:34\n_________ ERROR at setup of test_pdf_feature_encode_example[<lambda>0] _________\n[gw46] linux -- Python 3.13.2 /build/python-datasets/src/datasets-3.5.0/test-env/bin/python\nfile /build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py, line 8\n  @require_pdfplumber\n  @pytest.mark.parametrize(\n      \"build_example\",\n      [\n          lambda pdf_path: pdf_path,\n          lambda pdf_path: open(pdf_path, \"rb\").read(),\n          lambda pdf_path: {\"path\": pdf_path},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": None},\n          lambda pdf_path: {\"path\": pdf_path, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"path\": None, \"bytes\": open(pdf_path, \"rb\").read()},\n          lambda pdf_path: {\"bytes\": open(pdf_path, \"rb\").read()},\n      ],\n  )\n  def test_pdf_feature_encode_example(shared_datadir, build_example):\nE       fixture 'shared_datadir' not found\n>       available fixtures: _hf_gated_dataset_repo_txt_data, arrow_file, arrow_path, audio_file, bz2_csv_path, bz2_file, cache, capfd, capfdbinary, caplog, capsys, capsysbinary, ci_hfh_hf_hub_url, ci_hub_config, cleanup_repo, csv2_path, csv_path, data_dir_with_hidden_files, dataset, dataset_dict, disable_implicit_token, disable_tqdm_output, doctest_namespace, geoparquet_path, gz_file, hf_api, hf_gated_dataset_repo_txt_data, hf_private_dataset_repo_txt_data, hf_private_dataset_repo_txt_data_, hf_private_dataset_repo_zipped_img_data, hf_private_dataset_repo_zipped_img_data_, hf_private_dataset_repo_zipped_txt_data, hf_private_dataset_repo_zipped_txt_data_, hf_token, image_file, json_dict_of_lists_path, json_list_of_dicts_path, jsonl2_path, jsonl_312_path, jsonl_gz_path, jsonl_path, jsonl_str_path, lz4_file, mock_fsspec, mockfs, monkeypatch, parquet_path, pytestconfig, record_property, record_testsuite_property, record_xml_attribute, recwarn, set_ci_hub_access_token, set_sqlalchemy_silence_uber_warning, set_test_cache_config, set_update_download_counts_to_false, seven_zip_file, sqlite_path, tar_file, tar_jsonl_path, tar_nested_jsonl_path, temporary_repo, tensor_file, testrun_uid, text2_path, text_dir, text_dir_with_unsupported_extension, text_file, text_file_content, text_gz_path, text_path, text_path_with_unicode_new_lines, tmp_path, tmp_path_factory, tmpdir, tmpdir_factory, tmpfs, worker_id, xml_file, xz_file, zero_time_out_for_remote_code, zip_csv_path, zip_csv_with_dir_path, zip_file, zip_image_path, zip_jsonl_path, zip_jsonl_with_dir_path, zip_nested_jsonl_path, zip_text_path, zip_text_with_dir_path, zip_unsupported_ext_path, zip_uppercase_csv_path, zstd_file\n>       use 'pytest --fixtures [testpath]' for help on them.\n\n/build/python-datasets/src/datasets-3.5.0/tests/features/test_pdf.py:8\n```\n\n### Expected behavior\n\nAll fixtures used in tests should be available.\n\n### Environment info\n\nArch Linux build system, building the [python-datasets](https://gitlab.archlinux.org/archlinux/packaging/packages/python-datasets) package.\n\nThere are actually [many deselected tests](https://gitlab.archlinux.org/archlinux/packaging/packages/python-datasets/-/blob/6f97957f0c326cc7b3da6b7f12326305bcaef374/PKGBUILD#L66-148) which were failing on previous releases, but these errors popped up in 3.5.0.<end_of_turn>\n<start_of_turn>model\n`shared_datadir` fixture is missing<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI added a new DataFilesSet class to disallow duplicate data files.\r\nI also deprecated DataFilesList.\r\n\r\nEDIT: actually I might just add drop_duplicates=True to `.from_patterns`\r\n\r\nclose https://github.com/huggingface/datasets/issues/6259\r\nclose https://github.com/huggingface/datasets/issues/6272\r\n\r\nTODO:\r\n- [ ] tests\r\n- [ ] preserve data files order<end_of_turn>\n<start_of_turn>model\nNo data files duplicates<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ne.g. dashes '-' are not allowed in split names\r\n\r\nThis should add an error message on datasets with unsupported split names like https://huggingface.co/datasets/open-source-metrics/test\r\n\r\ncc @AndreaFrancis <end_of_turn>\n<start_of_turn>model\nRaise error on bad split name<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Discussed in https://github.com/huggingface/datasets/discussions/6768\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **RitchieP** April  1, 2024</sup>\r\nCurrently, I have a dataset hosted on Huggingface with a custom script [here](https://huggingface.co/datasets/RitchieP/VerbaLex_voice).\r\n\r\nI'm loading my dataset as below.\r\n```py\r\nfrom datasets import load_dataset, IterableDatasetDict\r\n\r\ndataset = IterableDatasetDict()\r\n\r\ndataset[\"train\"] = load_dataset(\"RitchieP/VerbaLex_voice\", \"ar\", split=\"train\", use_auth_token=True, streaming=True)\r\ndataset[\"test\"] = load_dataset(\"RitchieP/VerbaLex_voice\", \"ar\", split=\"test\", use_auth_token=True, streaming=True)\r\n```\r\n\r\nAnd when I try to see the data I have loaded with\r\n```py\r\nlist(dataset[\"train\"].take(1))\r\n```\r\n\r\nAnd it gives me this stack trace\r\n```\r\n---------------------------------------------------------------------------\r\nFileNotFoundError                         Traceback (most recent call last)\r\nCell In[2], line 1\r\n----> 1 list(dataset[\"train\"].take(1))\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py:1388, in IterableDataset.__iter__(self)\r\n   1385         yield formatter.format_row(pa_table)\r\n   1386     return\r\n-> 1388 for key, example in ex_iterable:\r\n   1389     if self.features:\r\n   1390         # `IterableDataset` automatically fills missing columns with None.\r\n   1391         # This is done with `_apply_feature_types_on_example`.\r\n   1392         example = _apply_feature_types_on_example(\r\n   1393             example, self.features, token_per_repo_id=self._token_per_repo_id\r\n   1394         )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py:1044, in TakeExamplesIterable.__iter__(self)\r\n   1043 def __iter__(self):\r\n-> 1044     yield from islice(self.ex_iterable, self.n)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py:234, in ExamplesIterable.__iter__(self)\r\n    233 def __iter__(self):\r\n--> 234     yield from self.generate_examples_fn(**self.kwargs)\r\n\r\nFile ~/.cache/huggingface/modules/datasets_modules/datasets/RitchieP--VerbaLex_voice/9465eaee58383cf9d7c3e14111d7abaea56398185a641b646897d6df4e4732f7/VerbaLex_voice.py:127, in VerbaLexVoiceDataset._generate_examples(self, local_extracted_archive_paths, archives, meta_path)\r\n    125 for i, audio_archive in enumerate(archives):\r\n    126     print(audio_archive)\r\n--> 127     for path, file in audio_archive:\r\n    128         _, filename = os.path.split(path)\r\n    129         if filename in metadata:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:869, in _IterableFromGenerator.__iter__(self)\r\n    868 def __iter__(self):\r\n--> 869     yield from self.generator(*self.args, **self.kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:919, in ArchiveIterable._iter_from_urlpath(cls, urlpath, download_config)\r\n    915 @classmethod\r\n    916 def _iter_from_urlpath(\r\n    917     cls, urlpath: str, download_config: Optional[DownloadConfig] = None\r\n    918 ) -> Generator[Tuple, None, None]:\r\n--> 919     compression = _get_extraction_protocol(urlpath, download_config=download_config)\r\n    920     # Set block_size=0 to get faster streaming\r\n    921     # (e.g. for hf:// and https:// it uses streaming Requests file-like instances)\r\n    922     with xopen(urlpath, \"rb\", download_config=download_config, block_size=0) as f:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:400, in _get_extraction_protocol(urlpath, download_config)\r\n    398 urlpath, storage_options = _prepare_path_and_storage_options(urlpath, download_config=download_config)\r\n    399 try:\r\n--> 400     with fsspec.open(urlpath, **(storage_options or {})) as f:\r\n    401         return _get_extraction_protocol_with_magic_number(f)\r\n    402 except FileNotFoundError:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/fsspec/core.py:100, in OpenFile.__enter__(self)\r\n     97 def __enter__(self):\r\n     98     mode = self.mode.replace(\"t\", \"\").replace(\"b\", \"\") + \"b\"\r\n--> 100     f = self.fs.open(self.path, mode=mode)\r\n    102     self.fobjects = [f]\r\n    104     if self.compression is not None:\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/fsspec/spec.py:1307, in AbstractFileSystem.open(self, path, mode, block_size, cache_options, compression, **kwargs)\r\n   1305 else:\r\n   1306     ac = kwargs.pop(\"autocommit\", not self._intrans)\r\n-> 1307     f = self._open(\r\n   1308         path,\r\n   1309         mode=mode,\r\n   1310         block_size=block_size,\r\n   1311         autocommit=ac,\r\n   1312         cache_options=cache_options,\r\n   1313         **kwargs,\r\n   1314     )\r\n   1315     if compression is not None:\r\n   1316         from fsspec.compression import compr\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:180, in LocalFileSystem._open(self, path, mode, block_size, **kwargs)\r\n    178 if self.auto_mkdir and \"w\" in mode:\r\n    179     self.makedirs(self._parent(path), exist_ok=True)\r\n--> 180 return LocalFileOpener(path, mode, fs=self, **kwargs)\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:302, in LocalFileOpener.__init__(self, path, mode, autocommit, fs, compression, **kwargs)\r\n    300 self.compression = get_compression(path, compression)\r\n    301 self.blocksize = io.DEFAULT_BUFFER_SIZE\r\n--> 302 self._open()\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/fsspec/implementations/local.py:307, in LocalFileOpener._open(self)\r\n    305 if self.f is None or self.f.closed:\r\n    306     if self.autocommit or \"w\" not in self.mode:\r\n--> 307         self.f = open(self.path, mode=self.mode)\r\n    308         if self.compression:\r\n    309             compress = compr[self.compression]\r\n\r\nFileNotFoundError: [Errno 2] No such file or directory: '/kaggle/working/h'\r\n```\r\nAfter looking into the stack trace, and referring to the source codes, it looks like its trying to access a directory in the notebook's environment and I don't understand why.\r\n\r\nNot sure if its a bug in Datasets library, so I'm opening a discussions first. Feel free to ask for more information if needed. Appreciate any help in advance!</div>\r\n\r\nHi, referring to the discussion title above, after further digging, I think it's an issue within the datasets library. But not quite sure where it is. \r\n\r\nIf you require any more info or actions from me, please let me know. Appreciate any help in advance!<end_of_turn>\n<start_of_turn>model\nDatasets FileNotFoundError when trying to generate examples.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen initializing `FilesIterable` with a list of file paths using `FilesIterable.from_paths`, it will discard all the files after a hidden file.\r\nThe problem is in [this line](https://github.com/huggingface/datasets/blob/88896a7b28610ace95e444b94f9a4bc332cc1ee3/src/datasets/download/download_manager.py#L233C26-L233C26) where `return` should be replaced by `continue`. \n\n### Steps to reproduce the bug\n\nhttps://colab.research.google.com/drive/1SQlxs4y_LSo1Q89KnFoYDSyyKEISun_J#scrollTo=93K4_blkW-8-\n\n### Expected behavior\n\nThe script should print all the files except the hidden one.\n\n### Environment info\n\n- `datasets` version: 2.14.1\r\n- Platform: Linux-5.15.109+-x86_64-with-glibc2.35\r\n- Python version: 3.10.6\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nFilesIterable skips all the files after a hidden file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPlace columns \"__key__\" and \"__url__\" in last position in the Dataset Viewer since they are not the main content\r\n\r\nbefore:\r\n\r\n<img width=\"1012\" alt=\"image\" src=\"https://github.com/user-attachments/assets/b556c1fe-2674-4ba0-9643-c074aa9716fd\" />\r\n<end_of_turn>\n<start_of_turn>model\nWebdataset special columns in last position<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nProposition to fix #5806.\r\n\r\nAdded an optional parameter `return_file_name` in the dataset builder config. When set to `True`, the function will include the file name corresponding to the sample in the returned output.\r\n\r\nThere is a difference between arrow-based and folder-based datasets to return the file name:\r\n- for arrow-based: a column is concatenated after the table is cast.\r\n- for folder-based: `dataset.info.features` has the entry `file_name` and the original file name is passed to the `sample_metadata` dictionary. \r\n\r\nThe difference in behavior might be a concern, also I do not know whether the `file_name` should return the original file path or the downloaded one for folder-based datasets.\r\n\r\nI added some tests for the datasets that already had a test file.<end_of_turn>\n<start_of_turn>model\nAdd return_file_name in load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nWhile fixing the Windows errors in #6362, I noticed that `PermissionError` can still easily be thrown on the session exit by the temporary cache directory's finalizer (we would also have to keep track of intermediate datasets, copies, etc.). ~~Due to the low usage of `datasets` on Windows, this PR takes a simpler approach to the issue than https://github.com/huggingface/datasets/pull/2403 - it tries to delete the temporary cache directory, and if this fails, logs a warning message about using a `delete-temp-cache` CLI command to delete it manually. The problematic references are freed after the session exits, so the CLI command should then succeed.~~ This PR implements `Dataset.__setstate__` to register datasets with temporary cache files for deletion. \r\n<end_of_turn>\n<start_of_turn>model\nMore robust temporary directory deletion<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate requests >=2.32.1 to fix vulnerability.<end_of_turn>\n<start_of_turn>model\nUpdate requests >=2.32.1 to fix vulnerability<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n        1.  dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir=training_args.cache_dir, split='train[:1%]')\r\n        2.  dataset = load_dataset(\"togethercomputer/RedPajama-Data-1T-Sample\", cache_dir=training_args.cache_dir)\r\n\r\nThe only difference I would expect these calls to have is the size of the dataset.\r\n\r\nBut, while 2. returns a dictionary with \"train\" key in it, 1. returns a dataset WITHOUT any initial \"train\" keyword.\r\n\r\nBoth calls are to be used within exactly the same context. They should return identically structured datasets of different size.\n\n### Steps to reproduce the bug\n\nSee above.\n\n### Expected behavior\n\nExpect both calls to return the same structured Dataset structure but with different number of elements, i.e. call 1. should have 1% of the data of the call 2.0\n\n### Environment info\n\nUbuntu 20.04\r\ngcc 9.x.x.\r\n\r\nIt is really irrelevant.<end_of_turn>\n<start_of_turn>model\nDifferent objects are returned from calls that should be returning the same kind of object.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe following Python code tries to download a private dataset and fails with the error `datasets.exceptions.DatasetNotFoundError: Dataset 'ClimatePolicyRadar/all-document-text-data-weekly' doesn't exist on the Hub or cannot be accessed.`. Downloading a public dataset doesn't work.\r\n\r\n``` py\r\nfrom datasets import load_dataset\r\n_ = load_dataset(\"ClimatePolicyRadar/all-document-text-data-weekly\")\r\n```\r\n\r\nThis seems to be just an issue with my machine config as the code above works with a colleague's machine. So far I have tried:\r\n\r\n- logging back out and in from the Huggingface CLI using `huggingface-cli logout`\r\n- manually removing the token cache at `/Users/kalyan/.cache/huggingface/token` (found using `huggingface-cli env`)\r\n- manually passing a token in `load_dataset`\r\n\r\nMy output of `huggingface-cli whoami`:\r\n\r\n```\r\nkdutia\r\norgs:  ClimatePolicyRadar\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\npython\r\nPython 3.12.2 (main, Feb  6 2024, 20:19:44) [Clang 15.0.0 (clang-1500.1.0.2.5)] on darwin\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> _ = load_dataset(\"ClimatePolicyRadar/all-document-text-data-weekly\")\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/kalyan/Library/Caches/pypoetry/virtualenvs/open-data-cnKQNmjn-py3.12/lib/python3.12/site-packages/datasets/load.py\", line 2074, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\n                       ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/kalyan/Library/Caches/pypoetry/virtualenvs/open-data-cnKQNmjn-py3.12/lib/python3.12/site-packages/datasets/load.py\", line 1795, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/Users/kalyan/Library/Caches/pypoetry/virtualenvs/open-data-cnKQNmjn-py3.12/lib/python3.12/site-packages/datasets/load.py\", line 1659, in dataset_module_factory\r\n    raise e1 from None\r\n  File \"/Users/kalyan/Library/Caches/pypoetry/virtualenvs/open-data-cnKQNmjn-py3.12/lib/python3.12/site-packages/datasets/load.py\", line 1597, in dataset_module_factory\r\n    raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e\r\ndatasets.exceptions.DatasetNotFoundError: Dataset 'ClimatePolicyRadar/all-document-text-data-weekly' doesn't exist on the Hub or cannot be accessed.\r\n>>>\r\n```\r\n\r\n### Expected behavior\r\n\r\nThe dataset downloads successfully.\r\n\r\n### Environment info\r\n\r\nFrom `huggingface-cli env`:\r\n\r\n```\r\n- huggingface_hub version: 0.25.1\r\n- Platform: macOS-14.2.1-arm64-arm-64bit\r\n- Python version: 3.12.2\r\n- Running in iPython ?: No\r\n- Running in notebook ?: No\r\n- Running in Google Colab ?: No\r\n- Running in Google Colab Enterprise ?: No\r\n- Token path ?: /Users/kalyan/.cache/huggingface/token\r\n- Has saved token ?: True\r\n- Who am I ?: kdutia\r\n- Configured git credential helpers: osxkeychain\r\n- FastAI: N/A\r\n- Tensorflow: N/A\r\n- Torch: N/A\r\n- Jinja2: 3.1.4\r\n- Graphviz: N/A\r\n- keras: N/A\r\n- Pydot: N/A\r\n- Pillow: N/A\r\n- hf_transfer: N/A\r\n- gradio: N/A\r\n- tensorboard: N/A\r\n- numpy: 2.1.1\r\n- pydantic: N/A\r\n- aiohttp: 3.10.8\r\n- ENDPOINT: https://huggingface.co\r\n- HF_HUB_CACHE: /Users/kalyan/.cache/huggingface/hub\r\n- HF_ASSETS_CACHE: /Users/kalyan/.cache/huggingface/assets\r\n- HF_TOKEN_PATH: /Users/kalyan/.cache/huggingface/token\r\n- HF_HUB_OFFLINE: False\r\n- HF_HUB_DISABLE_TELEMETRY: False\r\n- HF_HUB_DISABLE_PROGRESS_BARS: None\r\n- HF_HUB_DISABLE_SYMLINKS_WARNING: False\r\n- HF_HUB_DISABLE_EXPERIMENTAL_WARNING: False\r\n- HF_HUB_DISABLE_IMPLICIT_TOKEN: False\r\n- HF_HUB_ENABLE_HF_TRANSFER: False\r\n- HF_HUB_ETAG_TIMEOUT: 10\r\n- HF_HUB_DOWNLOAD_TIMEOUT: 10\r\n```\r\n\r\nfrom `datasets-cli env`:\r\n\r\n```\r\n- `datasets` version: 3.0.1\r\n- Platform: macOS-14.2.1-arm64-arm-64bit\r\n- Python version: 3.12.2\r\n- `huggingface_hub` version: 0.25.1\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.6.1\r\n```<end_of_turn>\n<start_of_turn>model\ndatasets.exceptions.DatasetNotFoundError for private dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSeems that `dataset` doesn't work with python==3.14. The root cause seems to be something with a `deel` API that was changed.\n```\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\n```\n\n### Steps to reproduce the bug\n\n(on a new folder)\nuv init \nuv python pin 3.14\nuv add datasets\nuv run python \n\n(in REPL)\nimport datasets\ndatasets.load_dataset(\"cais/mmlu\", \"all\")   # will fail on any dataset\n\n```\n>>> datasets.load_dataset(\"cais/mmlu\", \"all\")\nTraceback (most recent call last):\n  File \"<python-input-2>\", line 1, in <module>\n    datasets.load_dataset(\"cais/mmlu\", \"all\")\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^                                                                                                                                \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1397, in load_dataset\n    builder_instance = load_dataset_builder(                          \n        path=path,                                                                                                                                                           \n    ...<10 lines>...                                                               \n        **config_kwargs,                     \n    )                                                                                                                                                                        \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1185, in load_dataset_builder\n    builder_instance._use_legacy_cache_dir_if_possible(dataset_module)                                                                                                       \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^                                                                                                       \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 615, in _use_legacy_cache_dir_if_possible\n    self._check_legacy_cache2(dataset_module) or self._check_legacy_cache() or None\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^                                                                                                                                \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 487, in _check_legacy_cache2\n    config_id = self.config.name + \"-\" + Hasher.hash({\"data_files\": self.config.data_files})\n                                         ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                  \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/fingerprint.py\", line 188, in hash\n    return cls.hash_bytes(dumps(value))  \n                          ~~~~~^^^^^^^                                                                                                                                       \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 120, in dumps\n    dump(obj, file)             \n    ~~~~^^^^^^^^^^^                                                                                                                                                          \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 114, in dump\n    Pickler(file, recurse=True).dump(obj)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^                                                                                                                                    \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 428, in dump\n    StockPickler.dump(self, obj)                                       \n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^                                                                                                                                             \n  File \"/Users/zmoshe/.local/uv/python/cpython-3.14.0rc2-macos-aarch64-none/lib/python3.14/pickle.py\", line 498, in dump\n    self.save(obj)                                  \n    ~~~~~~~~~^^^^^                                                                                                                                                           \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 70, in save\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                      \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 422, in save\n    StockPickler.save(self, obj, save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                         \n  File \"/Users/zmoshe/.local/uv/python/cpython-3.14.0rc2-macos-aarch64-none/lib/python3.14/pickle.py\", line 572, in save\n    f(self, obj)  # Call unbound method with explicit self\n    ~^^^^^^^^^^^                                                                  \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 1262, in save_module_dict\n    StockPickler.save_dict(pickler, obj)                                                                                                                                     \n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/Users/zmoshe/.local/uv/python/cpython-3.14.0rc2-macos-aarch64-none/lib/python3.14/pickle.py\", line 1064, in save_dict\n    self._batch_setitems(obj.items(), obj)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\n```\n\n### Expected behavior\n\nshould work.\n\n### Environment info\n\ndatasets==v4.3.0\npython==3.14\n<end_of_turn>\n<start_of_turn>model\ndatasets doesn't work with python 3.14<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nLarge datasets like Laion400m are provided as embeddings. The provided methods in load_dataset are not straightforward for loading embedding files, i.e. img_emb_XX.npy ; XX = 0 to 99\r\n\n\n### Motivation\n\nThe trial and experimentation is the key pivot of HF. It would be great if HF can load embeddings files s,ealessly.\n\n### Your contribution\n\nI cam write the loader with some help.<end_of_turn>\n<start_of_turn>model\nMethod to load Laion400m<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRequire Pillow >= 9.4.0 to avoid AttributeError when loading image dataset.\r\n\r\nThe `PIL.Image.ExifTags` that we use in our code was implemented in Pillow-9.4.0: https://github.com/python-pillow/Pillow/commit/24a5405a9f7ea22f28f9c98b3e407292ea5ee1d3\r\n\r\nThe bug #6881 was introduced in datasets-2.19.0 by this PR:\r\n- #6739\r\n\r\nFix #6881.<end_of_turn>\n<start_of_turn>model\nRequire Pillow >= 9.4.0 to avoid AttributeError when loading image dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi, i\u2019m trying to create a HF dataset from a list using Dataset.from_list.\r\n\r\nEach sample in the list is a dict with the same keys (which will be my features). The values for each feature are a list of dictionaries, and each such dictionary has a different set of keys. However, the datasets library standardizes all dictionaries under a feature and adds all possible keys (with None value) from all the dictionaries under that feature.\r\n\r\nHow can I keep the same set of keys as in the original list for each dictionary under a feature?\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import Dataset\r\n\r\n# Define a function to generate a sample with \"tools\" feature\r\ndef generate_sample():\r\n    # Generate random sample data\r\n    sample_data = {\r\n        \"text\": \"Sample text\",\r\n        \"feature_1\": []\r\n    }\r\n    \r\n    # Add feature_1 with random keys for this sample\r\n    feature_1 = [{\"key1\": \"value1\"}, {\"key2\": \"value2\"}]  # Example feature_1 with random keys\r\n    sample_data[\"feature_1\"].extend(feature_1)\r\n    \r\n    return sample_data\r\n\r\n# Generate multiple samples\r\nnum_samples = 10\r\nsamples = [generate_sample() for _ in range(num_samples)]\r\n\r\n# Create a Hugging Face Dataset\r\ndataset = Dataset.from_list(samples)\r\ndataset[0]\r\n```\r\n\r\n```{'text': 'Sample text', 'feature_1': [{'key1': 'value1', 'key2': None}, {'key1': None, 'key2': 'value2'}]}```\n\n### Expected behavior\n\n```{'text': 'Sample text', 'feature_1': [{'key1': 'value1'}, {'key2': 'value2'}]}```\n\n### Environment info\n\n- `datasets` version: 2.19.1\r\n- Platform: Linux-5.15.0-1040-nvidia-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.23.0\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nList of dictionary features get standardized<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nhttps://github.com/rajasekarnp1/neural-audio-upscaler/tree/main\n\n### Motivation\n\n ai model develpment and audio \n\n### Your contribution\n\n   ai model develpment and audio <end_of_turn>\n<start_of_turn>model\nhelp is appreciated<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\n```\nIn [6]: ds = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:1000]\")\n\nIn [7]: ds = datasets.load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"train_sft[:1_000]\")\n[dozens of frames skipped]\nFile /usr/local/lib/python3.10/dist-packages/datasets/arrow_reader.py:444, in _str_to_read_instruction(spec)\n    442 res = _SUB_SPEC_RE.match(spec)\n    443 if not res:\n--> 444     raise ValueError(f\"Unrecognized instruction format: {spec}\")\nValueError: Unrecognized instruction format: train_sft[:1_000]\n```\n\nIt took me a while to understand what the problem was. But apparently `pyarrow` doesn't allow python numbers that may include `_` as in `1_000`. The `_` aids readability since `10_000_000` vs `10000000` is obviously easier to grasp of what the actual number is.\n\nFeature request:\n\nideally `datasets` being a python module will do the right thing and convert python numbers into whatever pyarrow supports - in this case stripping `_`s.\n\nSecond best it'd err and tell the user that using numbers with `_` in split slices is not acceptible, so that the user won't have to deal with a huge pyarrow assert they know nothing about.\n\nThank you!<end_of_turn>\n<start_of_turn>model\ndeal with python `10_000` legal number in slice syntax<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.14.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRename audio_classificiation.py to audio_classification.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nUsing interleave_datasets with multiple dataloader workers and a seed set causes the same dataset sampling order across all workers.\n\nShould the seed be modulated with the worker id?\n\n### Steps to reproduce the bug\n\nSee above\n\n### Expected behavior\n\nSee above\n\n### Environment info\n\n- `datasets` version: 3.5.1\n- Platform: macOS-15.4.1-arm64-arm-64bit\n- Python version: 3.12.9\n- `huggingface_hub` version: 0.30.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\ninterleave_datasets seed with multiple workers<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6333.<end_of_turn>\n<start_of_turn>model\nSupport fsspec 2023.10.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR resolves #7472 for DatasetDict \r\n\r\nThe previously merged PR #7483 added `try_original_type` to ArrowDataset, but DatasetDict misses `try_original_type`\r\n\r\nCc: @lhoestq <end_of_turn>\n<start_of_turn>model\nAdd try_original_type to DatasetDict.map<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n![Image](https://github.com/user-attachments/assets/3d958e43-28dc-4467-9333-5990c7af3b3f)\n\n### Steps to reproduce the bug\n\n![Image](https://github.com/user-attachments/assets/3067eeca-a54d-4956-b0fd-3fc5ea93dabb)\n\n### Expected behavior\n\n![Image](https://github.com/user-attachments/assets/77d32936-488f-4572-9365-bfb4170e555b)\n\n### Environment info\n\n![Image](https://github.com/user-attachments/assets/230335c4-825f-4db1-b07d-4776ef63ead8)<end_of_turn>\n<start_of_turn>model\nvllm\u6279\u91cf\u63a8\u7406\u62a5\u9519<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Related Issue\r\nCloses #3113\r\n\r\n### What does this PR do?\r\n\r\nThis PR introduces a new dataset loader module called **`h5folder`** to support loading datasets stored in **HDF5 (.h5)** format.\r\n\r\nIt allows users to do:\r\n\r\n```python\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"h5folder\", data_dir=\"path/to/\")\r\n````\r\n\r\n### \ud83e\udde9 Design Overview\r\n\r\n* Implemented inside `datasets/packaged_modules/h5folder/h5folder.py`\r\n* Based on the `GeneratorBasedBuilder` API\r\n* Uses `h5py` to read HDF5 files and yield examples\r\n* Expects datasets such as `id`, `data`, and `label` inside `data.h5`\r\n* Converts numpy arrays to Python types before yielding\r\n\r\n### \ud83e\uddea Example `.h5` Structure (for local testing)\r\n\r\n```python\r\nimport h5py\r\nimport numpy as np\r\n\r\nwith h5py.File(\"data.h5\", \"w\") as f:\r\n    f.create_dataset(\"id\", data=np.arange(100))\r\n    f.create_dataset(\"data\", data=np.random.randn(100, 10))\r\n    f.create_dataset(\"label\", data=np.random.randint(0, 2, size=100))\r\n```\r\n\r\n### \u2705 Testing\r\n\r\n- The loader logic follows the structure of existing modules like `imagefolder`\r\n- Will rely on Hugging Face CI to validate integration\r\n- Manually testing planned once merged or during feedback\r\n\r\n### \ud83d\udcc1 Files Added\r\n\r\n* `datasets/src/datasets/packaged_modules/h5folder/h5folder.py`\r\n\r\n### \ud83d\udccc Component(s) Affected\r\n\r\n* `area/datasets`\r\n* `area/load`\r\n\r\n### \ud83d\udce6 Release Note Classification\r\n\r\n* `rn/feature` \u2013 Adds support for loading `.h5` datasets via `load_dataset(\"h5folder\", ...)`\r\n\r\n---\r\n\r\nLet me know if any changes or improvements are needed \u2014 happy to iterate. Thanks for reviewing!\r\n<end_of_turn>\n<start_of_turn>model\nfeat: Add h5folder dataset loader for HDF5 support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis should fix the version compatibility issue when using `huggingface_hub` < 0.21.2 and latest fsspec (>=2023.12.0).\r\n\r\nSee my comment: https://github.com/huggingface/datasets/pull/6687#issuecomment-1976493336\r\n>> EDIT: the fix has been released in `huggingface_hub` 0.21.2 - I removed my commits that were using `huggingface_hub@main`\r\n>\r\n>Please note that people using `huggingface_hub` < 0.21.2 and latest `fsspec` will have issues when using `datasets`:\r\n>- https://github.com/huggingface/lighteval/actions/runs/8139147047/job/22241658122?pr=86\r\n>- https://github.com/huggingface/lighteval/pull/84\r\n\r\nCC: @clefourrier \r\n<end_of_turn>\n<start_of_turn>model\nBump huggingface-hub lower version to 0.21.2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nAs also raised by @cyyever in https://github.com/huggingface/datasets/pull/7296 and @NeilGirdhar in https://github.com/huggingface/datasets/commit/d5468836fe94e8be1ae093397dd43d4a2503b926#commitcomment-140952162 , `datasets` has a problematic version constraint on `fsspec`.\r\n\r\nIn our case this causes (unnecessary?) troubles due to a race condition bug in that version of the corresponding `gcsfs` plugin, that causes deadlocks: https://github.com/fsspec/gcsfs/pull/643\r\n\r\nWe just use a version override to ignore the constraint from `datasets`, but imho the version constraint could just be removed in the first place?\r\n\r\nThe last few PRs bumping the upper bound were basically uneventful:\r\n\r\n* https://github.com/huggingface/datasets/pull/7219\r\n* https://github.com/huggingface/datasets/pull/6921\r\n* https://github.com/huggingface/datasets/pull/6747\r\n\r\n### Steps to reproduce the bug\r\n\r\n-\r\n\r\n### Expected behavior\r\n\r\nInstalling `fsspec>=2024.10.0` along `datasets` should be possible without overwriting constraints.\r\n\r\n### Environment info\r\n\r\nAll recent datasets versions<end_of_turn>\n<start_of_turn>model\nRemove upper bound for fsspec<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\n`to_parquet` currently saves the dataset as one massive, monolithic parquet file, rather than as several small parquet files. It should shard large datasets automatically.\r\n\r\n### Motivation\r\n\r\nThis default behavior makes me very sad because a program I ran for 6 hours saved its results using `to_parquet`, putting the entire billion+ row dataset into a 171 GB *single shard parquet file* which pyarrow, apache spark, etc. all cannot work with without completely exhausting the memory of my system. I was previously able to work with larger-than-memory parquet files, but not this one. I *assume* the reason why this is happening is because it is a single shard. Making sharding the default behavior puts datasets in parity with other frameworks, such as spark, which automatically shard when a large dataset is saved as parquet. \r\n\r\n### Your contribution\r\n\r\nI could change the logic here https://github.com/huggingface/datasets/blob/bf6f41e94d9b2f1c620cf937a2e85e5754a8b960/src/datasets/io/parquet.py#L109-L158\r\nto use `pyarrow.dataset.write_dataset`, which seems to support sharding, or periodically open new files. We would only shard if the user passed in a path rather than file handle. <end_of_turn>\n<start_of_turn>model\nSave Dataset as Sharded Parquet <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Description\n\nThe current documentation example for `datasets.Dataset.map()` demonstrates batched processing but uses a `tokenizer` object without defining or importing it. This causes an error every time it's copied.\n\nHere is the problematic line:\n\n```python\n# process a batch of examples\n>>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n```\nThis assumes the user has already set up a tokenizer, which contradicts the goal of having self-contained, copy-paste-friendly examples. \n\n## Problem\nUsers who copy and run the example as-is will encounter:\n\n```python\nNameError: name 'tokenizer' is not defined\n```\n\nThis breaks the flow for users and violates HuggingFace's documentation principle that examples should \"work as expected\" when copied directly. \n\n## Proposal\nUpdate the example to include the required tokenizer setup using the Transformers library, like so:\n\n```python\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\nds_tokenized = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n```\nThis will help new users understand the workflow and apply the method correctly.\n\n## Note\nThis PR complements ongoing improvements like #7700, which clarifies multiprocessing in .map(). My change focuses on undefined tokenizer \u2014 causes NameError\n\n\n\n\n<end_of_turn>\n<start_of_turn>model\n[Docs] map() example uses undefined `tokenizer` \u2014 causes NameError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ncloses #7567\r\n\r\nAdd `shift_rngs` method to `ExamplesIterable` that is called directly after sharding. If a generator is available (not the case for all subclasses) we update the seed of the generator by shifting by the worker_id.\r\n\r\n~This is just the fix for `shuffle`, in the corresponding issue `interleave_datasets` is mentioned as well, which won't be fixed with this approach.~\r\n\r\nEDIT: This is a fix for `shuffle` and `interleave_datasets`. Adding recursivity to `shift_rngs` solved `interleave_datasets` as well. Not sure though if this is completely safe or if we could destroy something with that. I don't think so but could be wrong and appreciate some guidance from the maintainers. I also checked, on a single_worker we are always handing over `index=0` so that case preserves the seed the user specified. <end_of_turn>\n<start_of_turn>model\nFix random seed on shuffle and interleave_datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix this error\r\n\r\n```python\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/datasets/utils/file_utils.py\", line 934, in xopen\r\n    file_obj = fsspec.open(file, mode=mode, *args, **kwargs).open()\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/fsspec/core.py\", line 147, in open\r\n    return self.__enter__()\r\n           ~~~~~~~~~~~~~~^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/fsspec/core.py\", line 105, in __enter__\r\n    f = self.fs.open(self.path, mode=mode)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/fsspec/spec.py\", line 1338, in open\r\n    f = self._open(\r\n        path,\r\n    ...<4 lines>...\r\n        **kwargs,\r\n    )\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 275, in _open\r\n    return HfFileSystemFile(self, path, mode=mode, revision=revision, block_size=block_size, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 950, in __init__\r\n    self.resolved_path = fs.resolve_path(path, revision=revision)\r\n                         ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 198, in resolve_path\r\n    repo_and_revision_exist, err = self._repo_and_revision_exist(repo_type, repo_id, revision)\r\n                                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_file_system.py\", line 125, in _repo_and_revision_exist\r\n    self._api.repo_info(\r\n    ~~~~~~~~~~~~~~~~~~~^\r\n        repo_id, revision=revision, repo_type=repo_type, timeout=constants.HF_HUB_ETAG_TIMEOUT\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    )\r\n    ^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py\", line 2864, in repo_info\r\n    return method(\r\n        repo_id,\r\n    ...<4 lines>...\r\n        files_metadata=files_metadata,\r\n    )\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/hf_api.py\", line 2721, in dataset_info\r\n    r = get_session().get(path, headers=headers, timeout=timeout, params=params)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 602, in get\r\n    return self.request(\"GET\", url, **kwargs)\r\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 589, in request\r\n    resp = self.send(prep, **send_kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/sessions.py\", line 703, in send\r\n    r = adapter.send(request, **kwargs)\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py\", line 95, in send\r\n    return super().send(request, *args, **kwargs)\r\n           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/requests/adapters.py\", line 690, in send\r\n    raise ReadTimeout(e, request=request)\r\nrequests.exceptions.ReadTimeout: (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: e7e1ae72-54a0-4ce4-b011-144fb7a3fb06)')\r\n```\r\n\r\nwhich could also be related to\r\n\r\n```python\r\n  File \"/workdir/.venv/lib/python3.13/site-packages/datasets/utils/file_utils.py\", line 1364, in _iter_from_urlpaths\r\n    raise FileNotFoundError(urlpath)\r\nFileNotFoundError: hf://datasets/.../train-00013-of-00031.parquet\r\n```<end_of_turn>\n<start_of_turn>model\nRetry open hf file<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCannot decode audio data.\n\n\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import load_dataset\n\ndataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\nprint(dataset[0][\"audio\"][\"array\"])\n```\n\n1st round run, got\n```\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/features/audio.py\", line 172, in decode_example\n    raise ImportError(\"To support decoding audio data, please install 'torchcodec'.\")\nImportError: To support decoding audio data, please install 'torchcodec'.\n```\n\nAfter `pip install torchcodec` and run, got\n```\n  File \"/usr/local/lib/python3.12/dist-packages/torchcodec/_core/_metadata.py\", line 16, in <module>\n    from torchcodec._core.ops import (\n  File \"/usr/local/lib/python3.12/dist-packages/torchcodec/_core/ops.py\", line 84, in <module>\n    load_torchcodec_shared_libraries()\n  File \"/usr/local/lib/python3.12/dist-packages/torchcodec/_core/ops.py\", line 69, in load_torchcodec_shared_libraries\n    raise RuntimeError(\nRuntimeError: Could not load libtorchcodec. Likely causes:\n          1. FFmpeg is not properly installed in your environment. We support\n             versions 4, 5, 6 and 7.\n          2. The PyTorch version (2.8.0a0+5228986c39.nv25.06) is not compatible with\n             this version of TorchCodec. Refer to the version compatibility\n             table:\n             https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n          3. Another runtime dependency; see exceptions below.\n        The following exceptions were raised as we tried to load libtorchcodec:\n\n[start of libtorchcodec loading traceback]\nFFmpeg version 7: libavutil.so.59: cannot open shared object file: No such file or directory\nFFmpeg version 6: libavutil.so.58: cannot open shared object file: No such file or directory\nFFmpeg version 5: libavutil.so.57: cannot open shared object file: No such file or directory\nFFmpeg version 4: libavutil.so.56: cannot open shared object file: No such file or directory\n[end of libtorchcodec loading traceback].\n```\n\nAfter `apt update && apt install ffmpeg -y`, got\n```\nTraceback (most recent call last):\n  File \"/workspace/jiqing/test_datasets.py\", line 4, in <module>\n    print(dataset[0][\"audio\"][\"array\"])\n          ~~~~~~~^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 2859, in __getitem__\n    return self._getitem(key)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/arrow_dataset.py\", line 2841, in _getitem\n    formatted_output = format_table(\n                       ^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\", line 657, in format_table\n    return formatter(pa_table, query_type=query_type)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\", line 410, in __call__\n    return self.format_row(pa_table)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\", line 459, in format_row\n    row = self.python_features_decoder.decode_row(row)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/formatting/formatting.py\", line 223, in decode_row\n    return self.features.decode_example(row, token_per_repo_id=self.token_per_repo_id) if self.features else row\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/features/features.py\", line 2093, in decode_example\n    column_name: decode_nested_example(feature, value, token_per_repo_id=token_per_repo_id)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/features/features.py\", line 1405, in decode_nested_example\n    return schema.decode_example(obj, token_per_repo_id=token_per_repo_id) if obj is not None else None\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/datasets/features/audio.py\", line 198, in decode_example\n    audio = AudioDecoder(bytes, stream_index=self.stream_index, sample_rate=self.sampling_rate)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torchcodec/decoders/_audio_decoder.py\", line 62, in __init__\n    self._decoder = create_decoder(source=source, seek_mode=\"approximate\")\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torchcodec/decoders/_decoder_utils.py\", line 33, in create_decoder\n    return core.create_from_bytes(source, seek_mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torchcodec/_core/ops.py\", line 144, in create_from_bytes\n    return create_from_tensor(buffer, seek_mode)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/torch/_ops.py\", line 756, in __call__\n    return self._op(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\nNotImplementedError: Could not run 'torchcodec_ns::create_from_tensor' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'torchcodec_ns::create_from_tensor' is only available for these backends: [Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMTIA, AutogradMAIA, AutogradMeta, Tracer, AutocastCPU, AutocastMTIA, AutocastMAIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at /dev/null:214 [kernel]\nBackendSelect: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at /__w/torchcodec/torchcodec/pytorch/torchcodec/src/torchcodec/_core/custom_ops.cpp:694 [kernel]\nFuncTorchDynamicLayerBackMode: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\nFunctionalize: registered at /opt/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\nNamed: registered at /opt/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at /opt/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at /opt/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at /opt/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:104 [backend fallback]\nAutogradOther: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradCPU: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:67 [backend fallback]\nAutogradCUDA: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:75 [backend fallback]\nAutogradXLA: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:87 [backend fallback]\nAutogradMPS: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:95 [backend fallback]\nAutogradXPU: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:71 [backend fallback]\nAutogradHPU: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:108 [backend fallback]\nAutogradLazy: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:91 [backend fallback]\nAutogradMTIA: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:79 [backend fallback]\nAutogradMAIA: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:83 [backend fallback]\nAutogradMeta: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:99 [backend fallback]\nTracer: registered at /opt/pytorch/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\nAutocastCPU: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\nAutocastMTIA: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\nAutocastMAIA: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\nAutocastXPU: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:542 [backend fallback]\nAutocastMPS: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\nAutocastCUDA: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\nFuncTorchBatched: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at /opt/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:208 [backend fallback]\nPythonTLSSnapshot: registered at /opt/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\nPreDispatch: registered at /opt/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\nPythonDispatcher: registered at /opt/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n```\n\n### Expected behavior\n\nThe result is \n```\n[0.00238037 0.0020752  0.00198364 ... 0.00042725 0.00057983 0.0010376 ]\n```\non `datasets==3.6.0`\n\n### Environment info\n\n[NV official docker image](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch): `nvcr.io/nvidia/pytorch:25.06-py3` \n\n```\n- `datasets` version: 4.0.0\n- Platform: Linux-5.4.292-1.el8.elrepo.x86_64-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- `huggingface_hub` version: 0.34.2\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0\n```<end_of_turn>\n<start_of_turn>model\nload_dataset() in 4.0.0 failed when decoding audio<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix conda deps<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nBasically https://github.com/huggingface/datasets/pull/7718 just for binary view instead of string view<end_of_turn>\n<start_of_turn>model\nAdd pyarrow's binary view to features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI try to upload a very large dataset of images, and get the following error:\r\n```\r\nFile /fsx-multigen/yuvalkirstain/miniconda/envs/pickapic/lib/python3.10/site-packages/huggingface_hub/hf_api.py:2712, in HfApi.create_commit(self, repo_id, operations, commit_message, commit_description, token, repo_type, revision, create_pr, num_threads, parent_commit, run_as_future)\r\n   2710 try:\r\n   2711     commit_resp = get_session().post(url=commit_url, headers=headers, data=data, params=params)\r\n-> 2712     hf_raise_for_status(commit_resp, endpoint_name=\"commit\")\r\n   2713 except RepositoryNotFoundError as e:\r\n   2714     e.append_to_message(_CREATE_COMMIT_NO_REPO_ERROR_MESSAGE)\r\n\r\nFile /fsx-multigen/yuvalkirstain/miniconda/envs/pickapic/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:301, in hf_raise_for_status(response, endpoint_name)\r\n    297     raise BadRequestError(message, response=response) from e\r\n    299 # Convert `HTTPError` into a `HfHubHTTPError` to display request information\r\n    300 # as well (request id and/or server error message)\r\n--> 301 raise HfHubHTTPError(str(e), response=response) from e\r\n\r\nHfHubHTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/api/datasets/yuvalkirstain/pickapic_v2/commit/main (Request ID: Root=1-65112399-12d63f7d7f28bfa40a36a0fd)\r\n\r\nYou have exceeded our hourly quotas for action: commit. We invite you to retry later.\r\n```\r\n\r\nthis makes it much less convenient to host large datasets on HF hub.\n\n### Steps to reproduce the bug\n\nUpload a very large dataset of images\n\n### Expected behavior\n\nthe upload to work well\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.15.0-1033-aws-x86_64-with-glibc2.31\r\n- Python version: 3.10.11\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nHfHubHTTPError - exceeded our hourly quotas for action: commit<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI tried to download dataset but have got this error:\n\nfrom datasets import load_dataset\nload_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True) \n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[8], line 4\n      1 from datasets import load_dataset\n----> 4 load_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)\n\nFile c:\\Users\\ege_g\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1392, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\n   1387 verification_mode = VerificationMode(\n   1388     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\n   1389 )\n   1391 # Create a dataset builder\n-> 1392 builder_instance = load_dataset_builder(\n   1393     path=path,\n   1394     name=name,\n   1395     data_dir=data_dir,\n   1396     data_files=data_files,\n   1397     cache_dir=cache_dir,\n   1398     features=features,\n   1399     download_config=download_config,\n   1400     download_mode=download_mode,\n   1401     revision=revision,\n   1402     token=token,\n   1403     storage_options=storage_options,\n   1404     **config_kwargs,\n   1405 )\n   1407 # Return iterable dataset in case of streaming\n   1408 if streaming:\n\nFile c:\\Users\\ege_g\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1132, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\n   1130 if features is not None:\n   1131     features = _fix_for_backward_compatible_features(features)\n-> 1132 dataset_module = dataset_module_factory(\n   1133     path,\n   1134     revision=revision,\n   1135     download_config=download_config,\n   1136     download_mode=download_mode,\n   1137     data_dir=data_dir,\n   1138     data_files=data_files,\n   1139     cache_dir=cache_dir,\n   1140 )\n   1141 # Get dataset builder class\n   1142 builder_kwargs = dataset_module.builder_kwargs\n\nFile c:\\Users\\ege_g\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:1031, in dataset_module_factory(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\n   1026             if isinstance(e1, FileNotFoundError):\n   1027                 raise FileNotFoundError(\n   1028                     f\"Couldn't find any data file at {relative_to_absolute_path(path)}. \"\n   1029                     f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\n   1030                 ) from None\n-> 1031             raise e1 from None\n   1032 else:\n   1033     raise FileNotFoundError(f\"Couldn't find any data file at {relative_to_absolute_path(path)}.\")\n\nFile c:\\Users\\ege_g\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\load.py:989, in dataset_module_factory(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\n    981 try:\n    982     api.hf_hub_download(\n    983         repo_id=path,\n    984         filename=filename,\n   (...)\n    987         proxies=download_config.proxies,\n    988     )\n--> 989     raise RuntimeError(f\"Dataset scripts are no longer supported, but found {filename}\")\n    990 except EntryNotFoundError:\n    991     # Use the infos from the parquet export except in some cases:\n    992     if data_dir or data_files or (revision and revision != \"main\"):\n\nRuntimeError: Dataset scripts are no longer supported, but found common_voice_11_0.py\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\nload_dataset(\"mozilla-foundation/common_voice_11_0\", \"en\", split=\"test\", streaming=True)\n\n\n### Expected behavior\n\nits supposed to download this dataset.\n\n### Environment info\n\nPython 3.12 , Windows 11 <end_of_turn>\n<start_of_turn>model\ncommon_voice_11_0.py failure in dataset library<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAttempting to `load_dataset` from a path starting with `~` (as a shorthand for the user's home directory) seems not to be fully working - at least as far as the `parquet` dataset builder is concerned.\r\n\r\n(the same file can be loaded correctly if providing its absolute path instead)\r\n\r\nI think that this is very similar to https://github.com/huggingface/datasets/issues/5757, but for `data_files` rather than `data_dir`\n\n### Steps to reproduce the bug\n\n```python\r\nimport datasets\r\n\r\n# save a parquet file at ~/path/to/data.parquet\r\n\r\ndata_files = \"~/path/to/data.parquet\"\r\ndataset = datasets.load_dataset(\"parquet\", data_files=data_files)\r\n```\r\n```\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 12671.61it/s]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 22671.91it/s]\r\nGenerating train split: 0 examples [00:00, ? examples/s]\r\nTraceback (most recent call last):\r\n  File \".venv/lib/python3.11/site-packages/datasets/builder.py\", line 1949, in _prepare_split_single\r\n    num_examples, num_bytes = writer.finalize()\r\n                              ^^^^^^^^^^^^^^^^^\r\n  File \".venv/lib/python3.11/site-packages/datasets/arrow_writer.py\", line 598, in finalize\r\n    raise SchemaInferenceError(\"Please pass `features` or at least one example when writing data\")\r\ndatasets.arrow_writer.SchemaInferenceError: Please pass `features` or at least one example when writing data\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \".venv/lib/python3.11/site-packages/datasets/load.py\", line 2133, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \".venv/lib/python3.11/site-packages/datasets/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \".venv/lib/python3.11/site-packages/datasets/builder.py\", line 1049, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \".venv/lib/python3.11/site-packages/datasets/builder.py\", line 1813, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(\r\n  File \".venv/lib/python3.11/site-packages/datasets/builder.py\", line 1958, in _prepare_split_single\r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n\r\n```\n\n### Expected behavior\n\nCan use `~` shorthand in paths when loading local (parquet) datasets.\n\n### Environment info\n\n`datasets                               2.14.3`\r\n<end_of_turn>\n<start_of_turn>model\nTilde (~) is not supported for data_files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUpdate LICENSE<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFirst, I cd workdir.\r\nThen, I just use load_dataset(\"json\", data_file={\"train\":\"/a/b/c/.d/train/train.json\", \"test\":\"/a/b/c/.d/train/test.json\"})\r\nthat couldn't work and \r\n<FileNotFoundError: Unable to find \r\n'/a/b/c/.d/train/train.jsonl' at \r\n/a/b/c/.d/>\r\n\r\nAnd I debug, it is fine in version2.1.2\r\n\r\nSo there maybe a bug in path join.\r\n\r\nHere is the whole bug report:\r\n/x/datasets/loa \u2502\r\n\u2502 d.py:1656 in load_dataset                                                    \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   1653 \u2502   ignore_verifications = ignore_verifications or save_infos         \u2502\r\n\u2502   1654 \u2502                                                                     \u2502\r\n\u2502   1655 \u2502   # Create a dataset builder                                        \u2502\r\n\u2502 \u2771 1656 \u2502   builder_instance = load_dataset_builder(                          \u2502\r\n\u2502   1657 \u2502   \u2502   path=path,                                                    \u2502\r\n\u2502   1658 \u2502   \u2502   name=name,                                                    \u2502\r\n\u2502   1659 \u2502   \u2502   data_dir=data_dir,                                            \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 x/datasets/loa \u2502\r\n\u2502 d.py:1439 in load_dataset_builder                                            \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   1436 \u2502   if use_auth_token is not None:                                    \u2502\r\n\u2502   1437 \u2502   \u2502   download_config = download_config.copy() if download_config e \u2502\r\n\u2502   1438 \u2502   \u2502   download_config.use_auth_token = use_auth_token               \u2502\r\n\u2502 \u2771 1439 \u2502   dataset_module = dataset_module_factory(                          \u2502\r\n\u2502   1440 \u2502   \u2502   path,                                                         \u2502\r\n\u2502   1441 \u2502   \u2502   revision=revision,                                            \u2502\r\n\u2502   1442 \u2502   \u2502   download_config=download_config,                              \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 x/datasets/loa \u2502\r\n\u2502 d.py:1097 in dataset_module_factory                                          \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   1094 \u2502                                                                     \u2502\r\n\u2502   1095 \u2502   # Try packaged                                                    \u2502\r\n\u2502   1096 \u2502   if path in _PACKAGED_DATASETS_MODULES:                            \u2502\r\n\u2502 \u2771 1097 \u2502   \u2502   return PackagedDatasetModuleFactory(                          \u2502\r\n\u2502   1098 \u2502   \u2502   \u2502   path,                                                     \u2502\r\n\u2502   1099 \u2502   \u2502   \u2502   data_dir=data_dir,                                        \u2502\r\n\u2502   1100 \u2502   \u2502   \u2502   data_files=data_files,                                    \u2502\r\n\u2502                                                                              \u2502\r\n\u2502x/datasets/loa \u2502\r\n\u2502 d.py:743 in get_module                                                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502    740 \u2502   \u2502   \u2502   if self.data_dir is not None                              \u2502\r\n\u2502    741 \u2502   \u2502   \u2502   else get_patterns_locally(str(Path().resolve()))          \u2502\r\n\u2502    742 \u2502   \u2502   )                                                             \u2502\r\n\u2502 \u2771  743 \u2502   \u2502   data_files = DataFilesDict.from_local_or_remote(              \u2502\r\n\u2502    744 \u2502   \u2502   \u2502   patterns,                                                 \u2502\r\n\u2502    745 \u2502   \u2502   \u2502   use_auth_token=self.download_config.use_auth_token,       \u2502\r\n\u2502    746 \u2502   \u2502   \u2502   base_path=str(Path(self.data_dir).resolve()) if self.data \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 x/datasets/dat \u2502\r\n\u2502 a_files.py:590 in from_local_or_remote                                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   587 \u2502   \u2502   out = cls()                                                    \u2502\r\n\u2502   588 \u2502   \u2502   for key, patterns_for_key in patterns.items():                 \u2502\r\n\u2502   589 \u2502   \u2502   \u2502   out[key] = (                                               \u2502\r\n\u2502 \u2771 590 \u2502   \u2502   \u2502   \u2502   DataFilesList.from_local_or_remote(                    \u2502\r\n\u2502   591 \u2502   \u2502   \u2502   \u2502   \u2502   patterns_for_key,                                  \u2502\r\n\u2502   592 \u2502   \u2502   \u2502   \u2502   \u2502   base_path=base_path,                               \u2502\r\n\u2502   593 \u2502   \u2502   \u2502   \u2502   \u2502   allowed_extensions=allowed_extensions,             \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /x/datasets/dat \u2502\r\n\u2502 a_files.py:558 in from_local_or_remote                                       \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   555 \u2502   \u2502   use_auth_token: Optional[Union[bool, str]] = None,             \u2502\r\n\u2502   556 \u2502   ) -> \"DataFilesList\":                                              \u2502\r\n\u2502   557 \u2502   \u2502   base_path = base_path if base_path is not None else str(Path() \u2502\r\n\u2502 \u2771 558 \u2502   \u2502   data_files = resolve_patterns_locally_or_by_urls(base_path, pa \u2502\r\n\u2502   559 \u2502   \u2502   origin_metadata = _get_origin_metadata_locally_or_by_urls(data \u2502\r\n\u2502   560 \u2502   \u2502   return cls(data_files, origin_metadata)                        \u2502\r\n\u2502   561                                                                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /x/datasets/dat \u2502\r\n\u2502 a_files.py:195 in resolve_patterns_locally_or_by_urls                        \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   192 \u2502   \u2502   if is_remote_url(pattern):                                     \u2502\r\n\u2502   193 \u2502   \u2502   \u2502   data_files.append(Url(pattern))                            \u2502\r\n\u2502   194 \u2502   \u2502   else:                                                          \u2502\r\n\u2502 \u2771 195 \u2502   \u2502   \u2502   for path in _resolve_single_pattern_locally(base_path, pat \u2502\r\n\u2502   196 \u2502   \u2502   \u2502   \u2502   data_files.append(path)                                \u2502\r\n\u2502   197 \u2502                                                                      \u2502\r\n\u2502   198 \u2502   if not data_files:                                                 \u2502\r\n\u2502                                                                              \u2502\r\n\u2502 /x/datasets/dat \u2502\r\n\u2502 a_files.py:145 in _resolve_single_pattern_locally                            \u2502\r\n\u2502                                                                              \u2502\r\n\u2502   142 \u2502   \u2502   error_msg = f\"Unable to find '{pattern}' at {Path(base_path).r \u2502\r\n\u2502   143 \u2502   \u2502   if allowed_extensions is not None:                             \u2502\r\n\u2502   144 \u2502   \u2502   \u2502   error_msg += f\" with any supported extension {list(allowed \u2502\r\n\u2502 \u2771 145 \u2502   \u2502   raise FileNotFoundError(error_msg)                             \u2502\r\n\u2502   146 \u2502   return sorted(out)                                                 \u2502\r\n\u2502   147                         \n\n### Steps to reproduce the bug\n\n1. Version=2.3.2\r\n2. In shell, cd workdir.(cd /a/b/c/.d/)\r\n3. load_dataset(\"json\", data_file={\"train\":\"/a/b/c/.d/train/train.json\", \"test\":\"/a/b/c/.d/train/test.json\"})\n\n### Expected behavior\n\nfix it please~\n\n### Environment info\n\n2.3.2<end_of_turn>\n<start_of_turn>model\nversion2.3.2   load_dataset()data_files can't include .xxxx in path<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nPushing a dataset with n + m features to a repo which was deleted, but contained n features, will fail.\n\n### Steps to reproduce the bug\n\n1. Create a dataset with n features per row\r\n2. `dataset.push_to_hub(YOUR_PATH, SPLIT, token=TOKEN)`\r\n3. Go on the hub, delete the repo at `YOUR_PATH`\r\n4. Update your local dataset to have n + m features per row\r\n5. `dataset.push_to_hub(YOUR_PATH, SPLIT, token=TOKEN)` will fail because of a mismatch in features number\r\n\n\n### Expected behavior\n\nStep 5 should work or display a message to indicate the cache has not been cleared\n\n### Environment info\n\n- `datasets` version: 2.12.0\r\n- Platform: Linux-5.15.0-88-generic-x86_64-with-glibc2.31\r\n- Python version: 3.10.10\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.0.0\r\n<end_of_turn>\n<start_of_turn>model\nCaching problem when deleting a dataset <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThere seems to be a weird interaction between Deepspeed, the Dataset.from_list method and trl's RewardTrainer. On a multi-GPU setup (10 A100s), training always hangs at the very end of training until it times out. The training itself works fine until the end of training and running the same script with Deepspeed on a single GPU works without hangig. The issue persisted across a wide range of Deepspeed configs and training arguments. The issue went away when storing the exact same dataset as a JSON and using `dataset = load_dataset(\"json\", ...)`. Here is my training script:\n\n```python\nimport pickle\nimport os\nimport random\nimport warnings\n\nimport torch\nfrom datasets import load_dataset, Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom trl import RewardConfig, RewardTrainer, ModelConfig\n\n####################################### Reward model #################################################\n\n# Explicitly set arguments\nmodel_name_or_path = \"Qwen/Qwen2.5-1.5B\"\noutput_dir = \"Qwen2-0.5B-Reward-LoRA\"\nper_device_train_batch_size = 2\nnum_train_epochs = 5\ngradient_checkpointing = True\nlearning_rate = 1.0e-4\nlogging_steps = 25\neval_strategy = \"steps\"\neval_steps = 50\nmax_length = 2048\ntorch_dtype = \"auto\"\ntrust_remote_code = False\n\nmodel_args = ModelConfig(\n    model_name_or_path=model_name_or_path,\n    model_revision=None,\n    trust_remote_code=trust_remote_code,\n    torch_dtype=torch_dtype,\n    lora_task_type=\"SEQ_CLS\", # Make sure task type is seq_cls\n)\n\ntraining_args = RewardConfig(\n    output_dir=output_dir,\n    per_device_train_batch_size=per_device_train_batch_size,\n    num_train_epochs=num_train_epochs,\n    gradient_checkpointing=gradient_checkpointing,\n    learning_rate=learning_rate,\n    logging_steps=logging_steps,\n    eval_strategy=eval_strategy,\n    eval_steps=eval_steps,\n    max_length=max_length,\n    gradient_checkpointing_kwargs=dict(use_reentrant=False),\n    center_rewards_coefficient = 0.01,\n    fp16=False,\n    bf16=True,\n    save_strategy=\"no\",\n    dataloader_num_workers=0,\n    # deepspeed=\"./configs/deepspeed_config.json\",\n)\n\n################\n# Model & Tokenizer\n################\n\nmodel_kwargs = dict(\n    revision=model_args.model_revision,\n    use_cache=False if training_args.gradient_checkpointing else True,\n    torch_dtype=model_args.torch_dtype,\n)\ntokenizer = AutoTokenizer.from_pretrained(\n    model_args.model_name_or_path, use_fast=True\n)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_args.model_name_or_path, num_labels=1, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n)\n# Align padding tokens between tokenizer and model\nmodel.config.pad_token_id = tokenizer.pad_token_id\n\n# If post-training a base model, use ChatML as the default template\nif tokenizer.chat_template is None:\n    model, tokenizer = setup_chat_format(model, tokenizer)\n\nif model_args.use_peft and model_args.lora_task_type != \"SEQ_CLS\":\n    warnings.warn(\n        \"You are using a `task_type` that is different than `SEQ_CLS` for PEFT. This will lead to silent bugs\"\n        \" Make sure to pass --lora_task_type SEQ_CLS when using this script with PEFT.\",\n        UserWarning,\n    )\n\n##############\n# Load dataset\n##############\n\n\nwith open('./prefs.pkl', 'rb') as fh:\n    loaded_data = pickle.load(fh)\n\nrandom.shuffle(loaded_data)\n\ndataset = []\nfor a_wins, a, b in loaded_data:\n    if a_wins == 0:\n        a, b = b, a\n    dataset.append({'chosen': a, 'rejected': b})\n\n\ndataset = Dataset.from_list(dataset)\n\n# Split the dataset into training and evaluation sets\ntrain_eval_split = dataset.train_test_split(test_size=0.15, shuffle=True, seed=42)\n\n# Access the training and evaluation datasets\ntrain_dataset = train_eval_split['train']\neval_dataset = train_eval_split['test']\n\n\n##########\n# Training\n##########\n\n\ntrainer = RewardTrainer(\n    model=model,\n    processing_class=tokenizer,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n)\n\ntrainer.train()\n```\n\nReplacing `dataset = Dataset.from_list(dataset)` with \n```python\nwith open('./prefs.json', 'w') as fh:\n    json.dump(dataset, fh)\ndataset = load_dataset(\"json\", data_files=\"./prefs.json\", split='train')\n```\nresolves the issue.<end_of_turn>\n<start_of_turn>model\nDeepspeed reward training hangs at end of training with Dataset.from_list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThis code:\n\n```python\ndef train_iterable_gen():\n        images = np.array(load_image(\"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\").resize((128, 128)))\n        yield {\n            \"images\": np.expand_dims(images, axis=0),\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [{\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\" }]\n                },\n                {\n                    \"role\": \"assistant\",\n                    \"content\": [{\"type\": \"text\", \"text\": \"duck\" }]\n                }\n            ]\n        }\n    train_ds = Dataset.from_generator(train_iterable_gen,\n                                      features=Features({\n                                           'images': [datasets.Image(mode=None, decode=True, id=None)],\n                                           'messages': [{'content': [{'text': datasets.Value(dtype='string', id=None), 'type': datasets.Value(dtype='string', id=None) }], 'role': datasets.Value(dtype='string', id=None)}]\n                                           } )\n                                           )\n```\n\nworks as I'd expect; if I iterate the dataset then the `images` column returns a `List[PIL.Image.Image]`, i.e. `'images': [<PIL.PngImagePlugin.PngImageFile image mode=RGB size=128x128 at 0x77EFB7EF4680>]`.\n\nBut if I change `Dataset` to `IterableDataset`, the `images` column changes into `'images': [{'path': None, 'bytes': ..]`\n\n### Steps to reproduce the bug\n\nThe code above +\n\n```python\ndef load_image(url):\n        response = requests.get(url)\n        image = Image.open(io.BytesIO(response.content))\n        return image\n```\n\nI'm feeding it to SFTTrainer\n\n### Expected behavior\n\nDataset and IterableDataset would behave the same\n\n### Environment info\n\n```yaml\nrequires-python = \">=3.12\"\ndependencies = [\n    \"av>=14.1.0\",\n    \"boto3>=1.36.7\",\n    \"datasets>=3.3.2\",\n    \"docker>=7.1.0\",\n    \"google-cloud-storage>=2.19.0\",\n    \"grpcio>=1.70.0\",\n    \"grpcio-tools>=1.70.0\",\n    \"moviepy>=2.1.2\",\n    \"open-clip-torch>=2.31.0\",\n    \"opencv-python>=4.11.0.86; sys_platform == 'darwin'\",\n    \"opencv-python-headless>=4.11.0.86; sys_platform == 'linux'\",\n    \"pandas>=2.2.3\",\n    \"pillow>=10.4.0\",\n    \"plotly>=6.0.0\",\n    \"py-spy>=0.4.0\",\n    \"pydantic>=2.10.6\",\n    \"pydantic-settings>=2.7.1\",\n    \"pymysql>=1.1.1\",\n    \"ray[data,default,serve,train,tune]>=2.43.0\",\n    \"torch>=2.6.0\",\n    \"torchmetrics>=1.6.1\",\n    \"torchvision>=0.21.0\",\n    \"transformers[torch]@git+https://github.com/huggingface/transformers\",\n    \"wandb>=0.19.4\",\n    # https://github.com/Dao-AILab/flash-attention/issues/833\n    \"flash-attn @ https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.3/flash_attn-2.7.3+cu12torch2.6cxx11abiFALSE-cp312-cp312-linux_x86_64.whl; sys_platform == 'linux'\",\n    \"trl@https://github.com/huggingface/trl.git\",\n    \"peft>=0.14.0\",\n]\n```<end_of_turn>\n<start_of_turn>model\nList of images behave differently on IterableDataset and Dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix ci for pyarrow 18<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nHi,\r\n\r\nI like your idea of adapting the datasets library to be usable with JAX. Thank you for that.\r\n\r\nHowever, in your [setup.py](https://github.com/huggingface/datasets/blob/main/setup.py), you enforce old versions of JAX <= 0.3... It is very cumbersome !\r\n\r\nWhat is the rationale for such a limitation ? Can you remove it please ?\r\n\r\nThanks,\r\n\r\n### Motivation\r\n\r\nThis library is unusable with new versions of JAX ?\r\n\r\n### Your contribution\r\n\r\nYes.<end_of_turn>\n<start_of_turn>model\nSupport for newer versions of JAX<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n![image](https://github.com/huggingface/datasets/assets/47747764/f58044fb-ddda-48b6-ba68-7bbfef781630)\r\n\n\n### Steps to reproduce the bug\n\njust loading datasets \n\n### Expected behavior\n\nhow can I fix it\n\n### Environment info\n\npip install /mnt/cluster/zhangfan/study_info/LLaMA-Factory/peft-0.6.0-py3-none-any.whl\r\npip install huggingface_hub-0.19.4-py3-none-any.whl tokenizers-0.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl transformers-4.36.1-py3-none-any.whl pyarrow_hotfix-0.6-py3-none-any.whl datasets-2.15.0-py3-none-any.whl tyro-0.5.18-py3-none-any.whl trl-0.7.4-py3-none-any.whl\r\n\r\ndone<end_of_turn>\n<start_of_turn>model\n OverflowError: value too large to convert to int32_t <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix dataset name for community Hub script-datasets by passing explicit dataset_name to HubDatasetModuleFactoryWithScript.\r\n\r\nFix #6854.\r\n\r\nCC: @Wauplin <end_of_turn>\n<start_of_turn>model\nFix dataset name for community Hub script-datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix batch_size default description in `to_polars` docstrings.<end_of_turn>\n<start_of_turn>model\nFix batch_size default description in to_polars docstrings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix for https://github.com/huggingface/datasets/issues/7220 as suggested in discussion, in preference to #7221 \r\n\r\n(only concern would be on effect on type checking with custom feature types that aren't covered by FeatureType?)<end_of_turn>\n<start_of_turn>model\nsupport for custom feature encoding/decoding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAllow indexing datasets with a scalar numpy integer type.\n\n### Motivation\n\nIndexing a dataset with a scalar numpy.int* object raises a TypeError. This is due to the test in `datasets/formatting/formatting.py:key_to_query_type`\n\n``` python\ndef key_to_query_type(key: Union[int, slice, range, str, Iterable]) -> str:\n    if isinstance(key, int):\n        return \"row\"\n    elif isinstance(key, str):\n        return \"column\"\n    elif isinstance(key, (slice, range, Iterable)):\n        return \"batch\"\n    _raise_bad_key_type(key)\n```\n\nIn the row case, it checks if key is an int, which returns false when key is integer like but not a builtin python integer type. This is counterintuitive because a numpy array of np.int64s can be used for the batch case.\n\nFor example:\n\n``` python\nimport numpy as np\n\nimport datasets\n\ndataset = datasets.Dataset.from_dict({\"a\": [1, 2, 3, 4], \"b\": [5, 6, 7, 8]})\n\n# Regular indexing\ndataset[0]\ndataset[:2]\n\n# Indexing with numpy data types (expect same results)\nidx = np.asarray([0, 1])\ndataset[idx]  # Succeeds when using an array of np.int64 values\ndataset[idx[0]]  # Fails with TypeError when using scalar np.int64\n```\n\nFor the user, this can be solved by wrapping `idx[0]` in `int` but the test could also be changed in `key_to_query_type` to accept a less strict definition of int.\n\n``` diff\n+import numbers\n+\ndef key_to_query_type(key: Union[int, slice, range, str, Iterable]) -> str:\n+   if isinstance(key, numbers.Integral):\n-   if isinstance(key, int):\n        return \"row\"\n    elif isinstance(key, str):\n        return \"column\"\n    elif isinstance(key, (slice, range, Iterable)):\n        return \"batch\"\n    _raise_bad_key_type(key)\n```\n\nLooking at how others do it, pandas has an `is_integer` definition that it checks which uses `is_integer_object` defined in `pandas/_libs/utils.pxd`:\n\n``` cython\ncdef inline bint is_integer_object(object obj) noexcept:\n    \"\"\"\n    Cython equivalent of\n\n    `isinstance(val, (int, np.integer)) and not isinstance(val, (bool, np.timedelta64))`\n\n    Parameters\n    ----------\n    val : object\n\n    Returns\n    -------\n    is_integer : bool\n\n    Notes\n    -----\n    This counts np.timedelta64 objects as integers.\n    \"\"\"\n    return (not PyBool_Check(obj) and isinstance(obj, (int, cnp.integer))\n            and not is_timedelta64_object(obj))\n```\n\nThis would be less flexible as it explicitly checks for numpy integer, but worth noting that they had the need to ensure the key is not a bool.\n\n### Your contribution\n\nI can submit a pull request with the above changes after checking that indexing succeeds with the numpy integer type. Or if there is a different integer check that would be preferred I could add that.\n\nIf there is a reason not to want this behavior that is fine too.\n<end_of_turn>\n<start_of_turn>model\nRow indexing a dataset with numpy integers<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNew fix to #7594\r\nThis PR adds support for the columns argument in the JSON dataset builder.\r\nAdded columns parameter to JsonConfig.\r\nApplied column filtering after table creation, filling missing columns with None.\r\n\r\nExtended tests to cover:\r\n- Selecting a subset of columns\r\n- Handling missing requested columns\r\n- Column selection on list-of-strings case<end_of_turn>\n<start_of_turn>model\nAdd columns support to JSON loader<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n`from_polars` dataset cannot cast column to Image/Audio/Video, while it works on `from_pandas` and `from_dict`\n\n### Steps to reproduce the bug\n\n```python3\nimport datasets\nimport pandas as pd\nimport polars as pl\n\nimage_path = \"./sample.png\"\n\n# polars\ndf = pl.DataFrame({\"image_path\": [image_path]})\ndataset = datasets.Dataset.from_polars(df)\ndataset = dataset.cast_column(\"image_path\", datasets.Image())\n\n# # raises Error\npyarrow.lib.ArrowNotImplementedError: Unsupported cast from large_string to struct using function cast_struct\n\n\n# pandas \ndf = pd.DataFrame({\"image_path\": [image_path]})\ndataset = datasets.Dataset.from_pandas(df)\ndataset = dataset.cast_column(\"image_path\", datasets.Image())\n\n# # pass\n{'image_path': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=338x277 at 0x7FBA719D4050>}\n\n\n# dict\ndataset = datasets.Dataset.from_dict({\"image_path\": [image_path]})\ndataset = dataset.cast_column(\"image_path\", datasets.Image())\n\n# # pass\n{'image_path': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=338x277 at 0x7FBA719D4050>}\n```\n\n\n### Expected behavior\n\n`from_polars` case shouldn't raise error and have the same outputs as `from_pandas` and `from_dict`\n\n### Environment info\n\n```\n# Name                    Version                   Build  Channel\ndatasets                  4.0.0                    pypi_0    pypi\npandas                    2.3.1                    pypi_0    pypi\npolars                    1.32.3                   pypi_0    pypi\n```<end_of_turn>\n<start_of_turn>model\npolars dataset cannot cast column to Image/Audio/Video<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading a large dataset (>1000GB) from S3 I run into the following error:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/s3fs/core.py\", line 113, in _error_wrapper\r\n    return await func(*args, **kwargs)\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/aiobotocore/client.py\", line 383, in _make_api_call\r\n    raise error_class(parsed_response, operation_name)\r\nbotocore.exceptions.ClientError: An error occurred (RequestTimeTooSkewed) when calling the GetObject operation: The difference between the request time and the current time is too large.\r\nThe above exception was the direct cause of the following exception:\r\nTraceback (most recent call last):\r\n  File \"/home/alp/phoneme-classification.monorepo/aws_sagemaker/data_processing/inspect_final_dataset.py\", line 13, in <module>\r\n    dataset = load_from_disk(\"s3://speech-recognition-processed-data/whisper/de/train_data/\", storage_options=storage_options)\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/datasets/load.py\", line 1902, in load_from_disk\r\n    return Dataset.load_from_disk(dataset_path, keep_in_memory=keep_in_memory, storage_options=storage_options)\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py\", line 1686, in load_from_disk\r\n    fs.download(src_dataset_path, [dest_dataset_path.as](http://dest_dataset_path.as/)_posix(), recursive=True)\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/fsspec/spec.py\", line 1480, in download\r\n    return self.get(rpath, lpath, recursive=recursive, **kwargs)\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/fsspec/asyn.py\", line 121, in wrapper\r\n    return sync(self.loop, func, *args, **kwargs)\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/fsspec/asyn.py\", line 106, in sync\r\n    raise return_result\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/fsspec/asyn.py\", line 61, in _runner\r\n    result[0] = await coro\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/fsspec/asyn.py\", line 604, in _get\r\n    return await _run_coros_in_chunks(\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/fsspec/asyn.py\", line 257, in _run_coros_in_chunks\r\n    await asyncio.gather(*chunk, return_exceptions=return_exceptions),\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 408, in wait_for\r\n    return await fut\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/s3fs/core.py\", line 1193, in _get_file\r\n    body, content_length = await _open_file(range=0)\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/s3fs/core.py\", line 1184, in _open_file\r\n    resp = await self._call_s3(\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/s3fs/core.py\", line 348, in _call_s3\r\n    return await _error_wrapper(\r\n  File \"/home/alp/.local/lib/python3.10/site-packages/s3fs/core.py\", line 140, in _error_wrapper\r\n    raise err\r\nPermissionError: The difference between the request time and the current time is too large.\r\n```\r\n\r\nThe usual problem for this error is that the time on my local machine is out of sync with the current time. However, this is not the case here. I checked the time and even reset it with no success. See resources here:\r\n- https://stackoverflow.com/questions/4770635/s3-error-the-difference-between-the-request-time-and-the-current-time-is-too-la\r\n- https://stackoverflow.com/questions/25964491/aws-s3-upload-fails-requesttimetooskewed\r\n\r\nThe error does not appear when loading a smaller dataset (e.g. our test set) from the same s3 path.\n\n### Steps to reproduce the bug\n\n1. Create large dataset\r\n2. Try loading it from s3 using:\r\n```\r\ndataset = load_from_disk(\"s3://...\", storage_options=storage_options)  \r\n```\n\n### Expected behavior\n\nLoad dataset without running into this error. \n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.15.0-91-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.19.3\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\n`load_from_disk` with large dataset from S3 runs into `botocore.exceptions.ClientError`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses #7527 \r\n\r\nNote that this implementation **will only merge the last PR in the case that they get split up by `push_to_hub`**. See https://github.com/huggingface/datasets/discussions/7555 for more details.<end_of_turn>\n<start_of_turn>model\nAdd `--merge-pull-request` option for `convert_to_parquet`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nTrying to download Hugging Face datasets using Hugging Face CLI raises error. This error only started after December 27th, 2024.  For example:\r\n```\r\nhuggingface-cli download --repo-type dataset gboleda/wikicorpus\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/test_venv/bin/huggingface-cli\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/commands/huggingface_cli.py\", line 51, in main\r\n    service.run()\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/commands/download.py\", line 146, in run\r\n    print(self._download())  # Print path to downloaded files\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/commands/download.py\", line 180, in _download\r\n    return snapshot_download(\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/_snapshot_download.py\", line 164, in snapshot_download\r\n    repo_info = api.repo_info(repo_id=repo_id, repo_type=repo_type, revision=revision, token=token)\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2491, in repo_info\r\n    return method(\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 2366, in dataset_info\r\n    return DatasetInfo(**data)\r\n  File \"/home/ubuntu/test_venv/lib/python3.10/site-packages/huggingface_hub/hf_api.py\", line 799, in __init__\r\n    self.tags = kwargs.pop(\"tags\")\r\nKeyError: 'tags'\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\n1. huggingface-cli download --repo-type dataset gboleda/wikicorpus\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nThere should be no error.\r\n\r\n### Environment info\r\n\r\n\r\n- `datasets` version: 2.19.1\r\n- Platform: Linux-6.8.0-1015-aws-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.23.5\r\n- PyArrow version: 18.1.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.3.1<end_of_turn>\n<start_of_turn>model\nHuggingFace CLI dataset download raises error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(and also reorder the hffs args to improve caching)\r\n\r\nWhen using `DataLoader(iterable_dataset, num_workers=...)` the dataset is pickled and passed to the worker. However previously the resulting dataset would be in a process with an empty hffs cache. By keeping the cache attached to `IterableDataset`, the cached hffs instances are pickled with the dataset and re-populates the cache in the DataLoader workers\r\n\r\nthis requires https://github.com/huggingface/huggingface_hub/pull/3443 to work effectively though, otherwise the unpickled hffs cache would start empty\r\n\r\n\r\ncc @andimarafioti @ltmeyer<end_of_turn>\n<start_of_turn>model\nKeep hffs cache in workers when streaming<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRemove unused global variables in `audio.py`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe laion coco dataset is not available now. How to download it\r\n\r\nhttps://huggingface.co/datasets/laion/laion-coco<end_of_turn>\n<start_of_turn>model\nHow to download the laion-coco dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI noticed this numbering scheme not matching up in a different project and wanted to raise it as an issue for discussion, what is the actual proper way to have these stored?\r\n\r\n<img width=\"425\" alt=\"image\" src=\"https://github.com/huggingface/datasets/assets/1981179/3ffa2144-7c9a-446f-b521-a5e9db71e7ce\">\r\n\r\nThe `-SSSSS-of-NNNNN` seems to be used widely across the codebase. The section that creates the part in my screenshot is here https://github.com/huggingface/datasets/blob/main/src/datasets/arrow_dataset.py#L5287\r\nThere are also some edits to this section in the single commit branch.\n\n### Steps to reproduce the bug\n\n1. Upload a dataset that requires at least two parquet files in it\r\n2. Observe the naming scheme\n\n### Expected behavior\n\nThe couple options here are of course **1. keeping it as is**\r\n\r\n**2. Starting the index at 1:**\r\ntrain-00001-of-00002-{hash}.parquet\r\ntrain-00002-of-00002-{hash}.parquet\r\n\r\n**3. My preferred option** (which would solve my specific issue), dropping the total entirely:\r\ntrain-00000-{hash}.parquet\r\ntrain-00001-{hash}.parquet\r\n\r\nThis also solves an issue that will occur with an `append` variable for `push_to_hub` (see https://github.com/huggingface/datasets/issues/6290) where as you add a new parquet file, you need to rename everything in the repo as well. \r\n\r\nHowever, I know there are parts of the repo that use 0 as the starting file or may require the total, so raising the question for discussion.\r\n\n\n### Environment info\n\n- `datasets` version: 2.14.6.dev0\r\n- Platform: macOS-14.0-arm64-arm-64bit\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.18.0\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nParquet uploads off-by-one naming scheme<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading a dataset from disk, concatenating it, and starting the training process, the RAM usage progressively increases until the kernel terminates the process due to excessive memory consumption.\n\nhttps://github.com/huggingface/datasets/issues/2276\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import  DatasetDict, concatenate_datasets\n\ndataset = DatasetDict.load_from_disk(\"data\")\n\n...\n...\n\ncombined_dataset = concatenate_datasets(\n        [dataset[split] for split in dataset]\n    )\n\n#start SentenceTransformer training\n```\n\n### Expected behavior\n\nI would not expect RAM utilization to increase after concatenation. Removing the concatenation step resolves the issue\n\n### Environment info\n\nsentence-transformers==3.1.1\ndatasets==3.2.0\n\npython3.10<end_of_turn>\n<start_of_turn>model\nExcessive RAM Usage After Dataset Concatenation concatenate_datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR adds support for specifying image, video, audio, and pdf features using `pathlib.Path`.<end_of_turn>\n<start_of_turn>model\nSupport pathlib.Path for feature input<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n\u26a0\ufe0f (WIP) \u26a0\ufe0f\r\n\r\n### What this PR does\r\n\r\nThis PR adds a `Music` feature for the symbolic music modality, in particular [MIDI](https://en.wikipedia.org/wiki/Musical_Instrument_Digital_Interface) and [abc](https://en.wikipedia.org/wiki/ABC_notation) files.\r\n\r\n###\u00a0Motivations\r\n\r\nThese two file formats are widely used in the [Music Information Retrieval (MIR)](https://en.wikipedia.org/wiki/Music_information_retrieval) for tasks such as music generation, music transcription, music synthesis or music transcription. Having a dedicated feature in the datasets library would allow to both encourage researchers to share datasets of this modality as well as making them more easily usable for end users, benefitting from the perks of the library.\r\n\r\nThese file formats are supported by [symusic](https://github.com/Yikai-Liao/symusic), a lightweight Python library with C bindings (using nanobind) allowing to efficiently read, write and manipulate them. The library is actively developed, and can in the future also implement other file formats such as [musicXML](https://en.wikipedia.org/wiki/MusicXML). As such, this PR relies on it.\r\n\r\nThe music data can then easily be tokenized with appropriate tokenizers such as [MidiTok](https://github.com/Natooz/MidiTok) or converted to pianorolls matrices by symusic.\r\n\r\n**Jul 16th 2024:**\r\n\r\n* the tests for the `Music` feature are currently failing due to non-supported access to the LazyBatch in `test_dataset_with_music_feature_map` and `test_dataset_with_music_feature_map_resample_music` (see TODOs). I am a beginner with pyArrow, I'll take any advice to make this work;\r\n* additional tests including the `Music` feature with parquet and WebDataset should be implemented. As of right now, I am waiting for your feedback before taking further steps;\r\n* a `MusicFolder` should also be implemented to comply with the usages of the `Image` and `Audio` features, waiting for your feedback too.\r\n\r\nCCing @lhoestq and @albertvillanova <end_of_turn>\n<start_of_turn>model\nAdding `Music` feature for symbolic music modality (MIDI, abc)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI would like to request that manual downloads of data files from Hugging Face dataset repositories count as downloads of a dataset. According to the documentation for the Hugging Face Hub, that is currently not the case: https://huggingface.co/docs/hub/en/datasets-download-stats\n\n### Motivation\n\nThis would ensure that downloads are accurately reported to end users.\n\n### Your contribution\n\nN/A<end_of_turn>\n<start_of_turn>model\nManual downloads should count as downloads<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nwhen resolve features of `IterableDataset`, got `pyarrow.lib.ArrowInvalid: Can only convert 1-dimensional array values` error.\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"test.py\", line 6\r\n    iter_ds = iter_ds._resolve_features()\r\n  File \"lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 2876, in _resolve_features\r\n    features = _infer_features_from_batch(self.with_format(None)._head())\r\n  File \"lib/python3.10/site-packages/datasets/iterable_dataset.py\", line 63, in _infer_features_from_batch\r\n    pa_table = pa.Table.from_pydict(batch)\r\n  File \"pyarrow/table.pxi\", line 1813, in pyarrow.lib._Tabular.from_pydict\r\n  File \"pyarrow/table.pxi\", line 5339, in pyarrow.lib._from_pydict\r\n  File \"pyarrow/array.pxi\", line 374, in pyarrow.lib.asarray\r\n  File \"pyarrow/array.pxi\", line 344, in pyarrow.lib.array\r\n  File \"pyarrow/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowInvalid: Can only convert 1-dimensional array values\r\n```\n\n### Steps to reproduce the bug\n\n```python\r\nfrom datasets import Dataset\r\nimport numpy as np\r\n\r\n# create list of numpy\r\niter_ds = Dataset.from_dict({'a': [[[1, 2, 3], [1, 2, 3]]]}).to_iterable_dataset().map(lambda x: {'a': [np.array(x['a'])]})\r\niter_ds = iter_ds._resolve_features()  # errors here\r\n```\n\n### Expected behavior\n\nfeatures can be successfully resolved\n\n### Environment info\n\n- `datasets` version: 2.21.0\r\n- Platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- `huggingface_hub` version: 0.23.4\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.0\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nIterableDataset: cannot resolve features from list of numpy arrays<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently, IterableDataset doesn't support setting num_worker in .map(), which results in slow processing here. Could we add support for it? As .map() can be run in the batch fashion (e.g., batch_size is default to 1000 in datasets), it seems to be doable for IterableDataset as the regular Dataset.\n\n### Motivation\n\nImproving data processing efficiency\n\n### Your contribution\n\nTesting<end_of_turn>\n<start_of_turn>model\nSupport of num_workers (multiprocessing) in map for IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhile importing from packages getting the error\r\nCode:\r\n\r\n```\r\nimport os\r\nimport torch\r\nfrom datasets import load_dataset, Dataset\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\n    TrainingArguments,\r\n    pipeline,\r\n    logging\r\n)\r\nfrom peft import LoraConfig, PeftModel\r\nfrom trl import SFTTrainer\r\nfrom huggingface_hub import login\r\nimport pandas as pd\r\n```\r\n\r\nError:\r\n````\r\n---------------------------------------------------------------------------\r\nImportError                               Traceback (most recent call last)\r\nCell In[5], line 14\r\n      4 from transformers import (\r\n      5     AutoModelForCausalLM,\r\n      6     AutoTokenizer,\r\n   (...)\r\n     11     logging\r\n     12 )\r\n     13 from peft import LoraConfig, PeftModel\r\n---> 14 from trl import SFTTrainer\r\n     15 from huggingface_hub import login\r\n     16 import pandas as pd\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/trl/__init__.py:21\r\n      8 from .import_utils import (\r\n      9     is_diffusers_available,\r\n     10     is_npu_available,\r\n   (...)\r\n     13     is_xpu_available,\r\n     14 )\r\n     15 from .models import (\r\n     16     AutoModelForCausalLMWithValueHead,\r\n     17     AutoModelForSeq2SeqLMWithValueHead,\r\n     18     PreTrainedModelWrapper,\r\n     19     create_reference_model,\r\n     20 )\r\n---> 21 from .trainer import (\r\n     22     DataCollatorForCompletionOnlyLM,\r\n     23     DPOTrainer,\r\n     24     IterativeSFTTrainer,\r\n     25     PPOConfig,\r\n     26     PPOTrainer,\r\n     27     RewardConfig,\r\n     28     RewardTrainer,\r\n     29     SFTTrainer,\r\n     30 )\r\n     33 if is_diffusers_available():\r\n     34     from .models import (\r\n     35         DDPOPipelineOutput,\r\n     36         DDPOSchedulerOutput,\r\n     37         DDPOStableDiffusionPipeline,\r\n     38         DefaultDDPOStableDiffusionPipeline,\r\n     39     )\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/trl/trainer/__init__.py:44\r\n     42 from .ppo_trainer import PPOTrainer\r\n     43 from .reward_trainer import RewardTrainer, compute_accuracy\r\n---> 44 from .sft_trainer import SFTTrainer\r\n     45 from .training_configs import RewardConfig\r\n\r\nFile /opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:23\r\n     21 import torch.nn as nn\r\n     22 from datasets import Dataset\r\n---> 23 from datasets.arrow_writer import SchemaInferenceError\r\n     24 from datasets.builder import DatasetGenerationError\r\n     25 from transformers import (\r\n     26     AutoModelForCausalLM,\r\n     27     AutoTokenizer,\r\n   (...)\r\n     33     TrainingArguments,\r\n     34 )\r\n\r\nImportError: cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (/opt/conda/lib/python3.10/site-packages/datasets/arrow_writer.py\r\n\r\n````\r\n\r\ntransformers version: 4.36.2\r\npython version: 3.10.12\r\ndatasets version: 2.16.1\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. Install packages\r\n```\r\n!pip install -U datasets trl accelerate peft bitsandbytes transformers trl huggingface_hub\r\n```\r\n\r\n2. import packages\r\n```\r\nimport os\r\nimport torch\r\nfrom datasets import load_dataset, Dataset\r\nfrom transformers import (\r\n    AutoModelForCausalLM,\r\n    AutoTokenizer,\r\n    BitsAndBytesConfig,\r\n    HfArgumentParser,\r\n    TrainingArguments,\r\n    pipeline,\r\n    logging\r\n)\r\nfrom peft import LoraConfig, PeftModel\r\nfrom trl import SFTTrainer\r\nfrom huggingface_hub import login\r\nimport pandas as pd\r\n```\r\n\r\n### Expected behavior\r\n\r\nNo error while importing\r\n\r\n### Environment info\r\n\r\n- `datasets` version: 2.16.0\r\n- Platform: Linux-5.15.133+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.20.1\r\n- PyArrow version: 11.0.0\r\n- Pandas version: 2.1.4\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\nImportError: cannot import name 'SchemaInferenceError' from 'datasets.arrow_writer' (/opt/conda/lib/python3.10/site-packages/datasets/arrow_writer.py)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nA sliced + pickled ConcatenationTable could end up with a different schema than the original schema, if the slice only contains blocks with only a subset of the columns.\r\n\r\nThis can lead to issues when saving datasets from a concatenation of datasets with mixed schemas\r\n\r\nReported in https://discuss.huggingface.co/t/datasetdict-save-to-disk-with-num-proc-1-seems-to-hang-with-error/75595<end_of_turn>\n<start_of_turn>model\nFix sliced ConcatenationTable pickling with mixed schemas vertically<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nJSON lines with missing struct fields raise TypeError: Couldn't cast array of type.\r\n\r\nSee example: https://huggingface.co/datasets/wikimedia/structured-wikipedia/discussions/5\r\n\r\nOne would expect that the struct missing fields are added with null values.<end_of_turn>\n<start_of_turn>model\nJSON lines with missing struct fields raise TypeError: Couldn't cast array<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThere is a bug when using load_dataset with dataset version at 3.0.1 .\r\nPlease see below in the \"steps to reproduce the bug\".\r\nTo resolve the bug, I had to downgrade to version 2.21.0\r\nOS: Ubuntu 24 (AWS instance)\r\nPython: same bug under 3.12 and 3.10\r\n\r\n\r\nThe error I had was:\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/ubuntu/miniconda3/envs/maxence_env/lib/python3.10/site-packages/datasets/load.py\", line 2096, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/ubuntu/miniconda3/envs/maxence_env/lib/python3.10/site-packages/datasets/builder.py\", line 924, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/ubuntu/miniconda3/envs/maxence_env/lib/python3.10/site-packages/datasets/builder.py\", line 1647, in _download_and_prepare\r\n    super()._download_and_prepare(\r\n  File \"/home/ubuntu/miniconda3/envs/maxence_env/lib/python3.10/site-packages/datasets/builder.py\", line 977, in _download_and_prepare\r\n    split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n  File \"/home/ubuntu/.cache/huggingface/modules/datasets_modules/datasets/mozilla-foundation--common_voice_6_0/cb17afd34f5799f97e8f48398748f83006335b702bd785f9880797838d541b81/common_voice_6_0.py\", line 159, in _split_generators\r\n    archive_path = dl_manager.download(self._get_bundle_url(self.config.name, bundle_url_template))\r\n  File \"/home/ubuntu/miniconda3/envs/maxence_env/lib/python3.10/site-packages/datasets/download/download_manager.py\", line 150, in download\r\n    download_config = self.download_config.copy()\r\n  File \"/home/ubuntu/miniconda3/envs/maxence_env/lib/python3.10/site-packages/datasets/download/download_config.py\", line 73, in copy\r\n    return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\nTypeError: DownloadConfig.__init__() got an unexpected keyword argument 'ignore_url_params'\n\n### Steps to reproduce the bug\n\n1. install dataset with ```pip install datasets --upgrade```\r\n2. launch python; from datasets import loaad_dataset\r\n3. run load_dataset(\"mozilla-foundation/common_voice_6_0\")\r\n4. exit python\r\n5. uninstall datasets; then ```pip install datasets==2.21.0```\r\n6. launch python; from datasets import loaad_dataset\r\n7. run load_dataset(\"mozilla-foundation/common_voice_6_0\")\r\n8. Everything runs great now\n\n### Expected behavior\n\nBe able to download a dataset without error\n\n### Environment info\n\nCopy-and-paste the text below in your GitHub issue.\r\n\r\n- `datasets` version: 3.0.1\r\n- Platform: Linux-6.8.0-1017-aws-x86_64-with-glibc2.39\r\n- Python version: 3.12.4\r\n- `huggingface_hub` version: 0.26.0\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.3\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nincompatibily issue when using load_dataset with datasets==3.0.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix batch_size default description in `to_polars` docstrings.<end_of_turn>\n<start_of_turn>model\nFix batch_size default description in to_polars docstrings<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nthis removes the dependency on Soundfile completely<end_of_turn>\n<start_of_turn>model\nAudio: use TorchCodec instead of Soundfile for encoding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi! When I load this dataset, it fails with a pyarrow error. I'm using datasets 4.1.1, though I also see this with datasets 4.1.2\n\nTo reproduce:\n\n```\nimport datasets\n\nds = datasets.load_dataset(path=\"metr-evals/malt-public\", name=\"irrelevant_detail\")\n```\n\nError:\n```\nTraceback (most recent call last):\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 1815, in _prepare_split_single\n    for _, table in generator:\n                    ^^^^^^^^^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/packaged_modules/parquet/parquet.py\", line 93, in _generate_tables\n    for batch_idx, record_batch in enumerate(\n                                   ~~~~~~~~~^\n        parquet_fragment.to_batches(\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n        )\n        ^\n    ):\n    ^\n  File \"pyarrow/_dataset.pyx\", line 3904, in _iterator\n  File \"pyarrow/_dataset.pyx\", line 3494, in pyarrow._dataset.TaggedRecordBatchIterator.__next__\n  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\npyarrow.lib.ArrowNotImplementedError: Nested data conversions not implemented for chunked array outputs\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/neev/scratch/test_hf.py\", line 3, in <module>\n    ds = datasets.load_dataset(path=\"metr-evals/malt-public\", name=\"irrelevant_detail\")\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/load.py\", line 1412, in load_dataset\n    builder_instance.download_and_prepare(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        download_config=download_config,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<3 lines>...\n        storage_options=storage_options,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 894, in download_and_prepare\n    self._download_and_prepare(\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        dl_manager=dl_manager,\n        ^^^^^^^^^^^^^^^^^^^^^^\n    ...<2 lines>...\n        **download_and_prepare_kwargs,\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    )\n    ^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 970, in _download_and_prepare\n    self._prepare_split(split_generator, **prepare_split_kwargs)\n    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 1702, in _prepare_split\n    for job_id, done, content in self._prepare_split_single(\n                                 ~~~~~~~~~~~~~~~~~~~~~~~~~~^\n        gen_kwargs=gen_kwargs, job_id=job_id, **_prepare_split_args\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ):\n    ^\n  File \"/Users/neev/scratch/.venv/lib/python3.13/site-packages/datasets/builder.py\", line 1858, in _prepare_split_single\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\ndatasets.exceptions.DatasetGenerationError: An error occurred while generating the dataset\n```\n\n### Steps to reproduce the bug\n\nTo reproduce:\n\n```\nimport datasets\n\nds = datasets.load_dataset(path=\"metr-evals/malt-public\", name=\"irrelevant_detail\")\n```\n\n### Expected behavior\n\nThe dataset loads\n\n### Environment info\n\nDatasets: 4.1.1\nPython: 3.13\nPlatform: Macos<end_of_turn>\n<start_of_turn>model\nCannot load dataset, fails with nested data conversions not implemented for chunked array outputs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI can't load dataset subsets of 'pragmeval'.\r\n\r\nThe funny thing is I ran the dataset author's [colab notebook](https://colab.research.google.com/drive/1sg--LF4z7XR1wxAOfp0-3d4J6kQ9nj_A?usp=sharing) and it works just fine. I tried to install exactly the same packages that are installed on colab using poetry, so my environment info only differs from the one from colab in linux version - I still get the same bug outside colab.\n\n### Steps to reproduce the bug\n\nInstall dependencies with poetry\r\n\r\npyproject.toml\r\n```\r\n[tool.poetry]\r\nname = \"project\"\r\nversion = \"0.1.0\"\r\ndescription = \"\"\r\nauthors = []\r\n\r\n[tool.poetry.dependencies]\r\npython = \"^3.10\"\r\ndatasets = \"2.16.0\"\r\npandas = \"1.5.3\"\r\npyarrow = \"10.0.1\"\r\nhuggingface-hub = \"0.19.4\"\r\nfsspec = \"2023.6.0\"\r\n\r\n\r\n[build-system]\r\nrequires = [\"poetry-core\"]\r\nbuild-backend = \"poetry.core.masonry.api\"\r\n```\r\n\r\n`poetry run python -c \"import datasets; print(datasets.get_dataset_config_names('pragmeval'))`\r\n\r\nprints ['default']\n\n### Expected behavior\n\nThe command should print\r\n\r\n```\r\n['emergent',\r\n 'emobank-arousal',\r\n 'emobank-dominance',\r\n 'emobank-valence',\r\n 'gum',\r\n 'mrda',\r\n 'pdtb',\r\n 'persuasiveness-claimtype',\r\n 'persuasiveness-eloquence',\r\n 'persuasiveness-premisetype',\r\n 'persuasiveness-relevance',\r\n 'persuasiveness-specificity',\r\n 'persuasiveness-strength',\r\n 'sarcasm',\r\n 'squinky-formality',\r\n 'squinky-implicature',\r\n 'squinky-informativeness',\r\n 'stac',\r\n 'switchboard',\r\n 'verifiability']\r\n```\n\n### Environment info\n\n- `datasets` version: 2.16.0\r\n- Platform: Linux-6.2.0-37-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- `huggingface_hub` version: 0.19.4\r\n- PyArrow version: 10.0.1\r\n- Pandas version: 1.5.3\r\n- `fsspec` version: 2023.6.0<end_of_turn>\n<start_of_turn>model\n'Repo card metadata block was not found' when loading a pragmeval dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI saved a dataset with two splits to disk with `DatasetDict.save_to_disk`. The train is bigger and ended up in 10 shards, whereas the test split only resulted in 1 split.\r\n\r\nNow when trying to load the dataset, an error is raised that not all splits have the same data format:\r\n\r\n> ValueError: Couldn't infer the same data file format for all splits. Got {NamedSplit('train'): ('arrow', {}), NamedSplit('test'): ('json', {})}\r\n\r\nThis is not expected because both splits are saved as arrow files.\r\n\r\nI did some debugging and found that this is the case because the list of data_files includes a `state.json` file. \r\n\r\nNow this means for train split I get 10 \".arrow\" and 1 \".json\" file. Since datasets picks based on the most common extension this is correctly inferred as \"arrow\". In the test split, there is 1 .arrow and 1 .json file. Given the function description:\r\n\r\n> It picks the module based on the most common file extension.\r\n    In case of a draw \".parquet\" is the favorite, and then alphabetical order.\r\n\r\nThis is not quite true though, because in a tie the extensions are actually based on reverse-alphabetical order:\r\n```\r\nfor (ext, _), _ in sorted(extensions_counter.items(), key=sort_key, *reverse=True*):\r\n```\r\n\r\nWhich thus leads to the module wrongly inferred as \"json\", whereas it should be \"arrow\", matching the train split.\r\n\r\nI first thought about adding \"state.json\" in the list of excluded files for the inference: https://github.com/huggingface/datasets/blob/main/src/datasets/load.py#L513. However, I think from digging into the code it looks like the right thing to do is to exclude it in the list of `data_files` to start with, because it is more of a metadata than a data file.<end_of_turn>\n<start_of_turn>model\nFix data file module inference<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nUse object detection images from `huggingface/documentation-images`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix cast on sliced `FixedSizeListArray`s.\r\n\r\nFix #5866<end_of_turn>\n<start_of_turn>model\nFix `FixedSizeListArray` casting<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix typos in docs introduced by:\r\n- #6956\r\n\r\nTypos:\r\n- `comparisions` => `comparisons`\r\n- two consecutive sentences both ending in colon\r\n- split one sentence into two\r\n\r\nSorry, I did not have time to review that PR.\r\n\r\nCC: @lhoestq <end_of_turn>\n<start_of_turn>model\nFix typos in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe model was built to do fine tunning on BERT model for relation extraction.\r\n\r\ntrainer.train() returns an error message ```TypeError: vars() argument must have __dict__ attribute``` when it has `train_dataset` generated from `torch.utils.data.TensorDataset`\r\n\r\nHowever, in the document, the required data format is `torch.utils.data.TensorDataset`. \r\n![image](https://github.com/huggingface/datasets/assets/49014051/36fa34ac-3127-4c64-9580-9ab736136d83)\r\n\r\nTransformers trainer is supposed to accept the train_dataset in the format of torch.utils.data.TensorDataset, but it returns error message *\"TypeError: vars() argument must have __dict__ attribute\"*\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-30-5df728c929a2> in <cell line: 1>()\r\n----> 1 trainer.train()\r\n      2 trainer.evaluate(test_dataset)\r\n\r\n9 frames\r\n/usr/local/lib/python3.10/dist-packages/transformers/data/data_collator.py in <listcomp>(.0)\r\n    107 \r\n    108     if not isinstance(features[0], Mapping):\r\n--> 109         features = [vars(f) for f in features]\r\n    110     first = features[0]\r\n    111     batch = {}\r\n\r\nTypeError: vars() argument must have __dict__ attribute\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nCreate train_dataset using `torch.utils.data.TensorDataset`, for instance,\r\n\r\n```train_dataset = torch.utils.data.TensorDataset(train_input_ids, train_attention_masks, train_labels)```\r\n\r\nFeed this `train_dataset` to your trainer and run trainer.train\r\n\r\n```\r\ntrainer = Trainer(model,\r\n                  training_args,\r\n                  train_dataset=train_dataset,\r\n                  eval_dataset=dev_dataset,\r\n                  compute_metrics=compute_metrics,\r\n                  )\r\n```\r\n\r\n### Expected behavior\r\n\r\nTrainer should start training\r\n\r\n### Environment info\r\n\r\nIt is running on Google Colab\r\n\r\n- `datasets` version: 2.14.6\r\n- Platform: Linux-5.15.120+-x86_64-with-glibc2.35\r\n- Python version: 3.10.12\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 1.5.3<end_of_turn>\n<start_of_turn>model\nTensorDataset format does not work with Trainer from transformers<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIn `src/datasets/load.py`, we can use unpacking rather than concatenating two lists for improved time and memory performance. It\u2019s a small improvement in absolute terms, but a consistent and measurable one:\r\n\r\n```diff\r\n- ALL_ALLOWED_EXTENSIONS = list(_EXTENSION_TO_MODULE.keys()) + [\".zip\"]\r\n+ ALL_ALLOWED_EXTENSIONS = [*_EXTENSION_TO_MODULE.keys(), \".zip\"]\r\n```\r\n\r\nBenchmarking shows approximately 32.3% time improvement and 30.6% memory improvement.\r\n\r\nExample benchmarking script:\r\n```python\r\n#!/usr/bin/env python3\r\n\"\"\"\r\nBenchmark script to test performance of list(_EXTENSION_TO_MODULE.keys()) vs [*_EXTENSION_TO_MODULE.keys()]\r\n\"\"\"\r\nimport time\r\nimport tracemalloc\r\nfrom statistics import mean, stdev\r\n\r\n# Simulate _EXTENSION_TO_MODULE - based on actual size from datasets\r\n_EXTENSION_TO_MODULE = {\r\n    f\".ext{i}\": f\"module{i}\" for i in range(20)  # Realistic size\r\n}\r\n\r\ndef method_old():\r\n    \"\"\"Current implementation using list()\"\"\"\r\n    return list(_EXTENSION_TO_MODULE.keys()) + [\".zip\"]\r\n\r\ndef method_new():\r\n    \"\"\"Proposed implementation using unpacking\"\"\"\r\n    return [*_EXTENSION_TO_MODULE.keys(), \".zip\"]\r\n\r\ndef benchmark_time(func, iterations=100000):\r\n    \"\"\"Benchmark execution time\"\"\"\r\n    times = []\r\n    for _ in range(10):  # Multiple runs for accuracy\r\n        start = time.perf_counter()\r\n        for _ in range(iterations):\r\n            func()\r\n        end = time.perf_counter()\r\n        times.append((end - start) / iterations * 1_000_000)  # microseconds\r\n    \r\n    return mean(times), stdev(times)\r\n\r\ndef benchmark_memory(func):\r\n    \"\"\"Benchmark peak memory usage\"\"\"\r\n    tracemalloc.start()\r\n    func()\r\n    current, peak = tracemalloc.get_traced_memory()\r\n    tracemalloc.stop()\r\n    return peak\r\n\r\nif __name__ == \"__main__\":\r\n    print(\"Benchmarking list() vs unpacking performance...\\n\")\r\n    \r\n    # Time benchmarks\r\n    old_time, old_std = benchmark_time(method_old)\r\n    new_time, new_std = benchmark_time(method_new)\r\n    \r\n    print(f\"Time Performance (\u00b5s per operation):\")\r\n    print(f\"  list() approach:     {old_time:.3f} \u00b1 {old_std:.3f}\")\r\n    print(f\"  unpacking approach:  {new_time:.3f} \u00b1 {new_std:.3f}\")\r\n    print(f\"  Improvement:         {((old_time - new_time) / old_time * 100):.1f}% faster\")\r\n    \r\n    # Memory benchmarks\r\n    old_mem = benchmark_memory(method_old)\r\n    new_mem = benchmark_memory(method_new)\r\n    \r\n    print(f\"\\nMemory Usage (bytes):\")\r\n    print(f\"  list() approach:     {old_mem}\")\r\n    print(f\"  unpacking approach:  {new_mem}\")\r\n    print(f\"  Reduction:           {old_mem - new_mem} bytes ({((old_mem - new_mem) / old_mem * 100):.1f}% less)\")\r\n    \r\n    # Verify identical results\r\n    assert method_old() == method_new(), \"Results should be identical!\"\r\n    print(f\"\\n\u2713 Both methods produce identical results\")\r\n```\r\n\r\nResults:\r\n```\r\nBenchmarking list() vs unpacking performance...\r\n\r\nTime Performance (\u00b5s per operation):\r\n  list() approach:     0.213 \u00b1 0.020\r\n  unpacking approach:  0.144 \u00b1 0.002\r\n  Improvement:         32.3% faster\r\n\r\nMemory Usage (bytes):\r\n  list() approach:     392\r\n  unpacking approach:  272\r\n  Reduction:           120 bytes (30.6% less)\r\n\r\n\u2713 Both methods produce identical results\r\n```<end_of_turn>\n<start_of_turn>model\nRefactor: use unpacking in load.py for time and memory improvement<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nLoading image data in a huggingface dataset results in very slow read speeds, approximately 1000 times longer than reading the same data from a pytorch dataset. This applies even when the dataset is loaded into RAM using a `keep_in_memory=True` flag.\n\nThe following script reproduces the result with random data, but it applies equally to datasets that are loaded from the hub.\n\n### Steps to reproduce the bug\n\nThe following script should reproduce the behavior\n```\nimport torch\nimport time\nfrom datasets import Dataset\n\nimages = torch.randint(0, 255, (1000, 3, 224, 224), dtype=torch.uint8)\nlabels = torch.randint(0, 200, (1000,), dtype=torch.uint8)\n\npt_dataset = torch.utils.data.TensorDataset(images, labels)\n\nhf_dataset = Dataset.from_dict({'image': images, 'label':labels})\nhf_dataset.set_format('torch', dtype=torch.uint8)\nhf_in_memory = hf_dataset.map(lambda x: x, keep_in_memory=True)\n\n# measure access speeds\ndef time_access(dataset, img_col):\n    start_time = time.time()\n    for i in range(1000):\n        _ = dataset[i][img_col].shape\n    end_time = time.time()\n    return end_time - start_time\n\n\nprint(f\"In-memory Tensor access: {time_access(pt_dataset, 0):.4f} seconds\")\nprint(f\"HF Dataset access: {time_access(hf_dataset, 'image'):.4f} seconds\")\nprint(f\"In-memory HF Dataset access: {time_access(hf_in_memory, 'image'):.4f} seconds\")\n```\n\n\n\n### Expected behavior\n\nFor me, the above script produces\n```\nIn-memory Tensor access: 0.0025 seconds\nHF Dataset access: 2.9317 seconds\nIn-memory HF Dataset access: 2.8082 seconds\n```\n\nI think that this difference is larger than expected.\n\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: macOS-14.7.7-arm64-arm-64bit\n- Python version: 3.12.11\n- `huggingface_hub` version: 0.34.3\n- PyArrow version: 18.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0<end_of_turn>\n<start_of_turn>model\ndatasets massively slows data reads, even in memory<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTo use `datasets` offline, one can use the `HF_DATASETS_OFFLINE` environment variable. This PR makes `HF_HUB_OFFLINE` the recommended environment variable for offline training. Goal is to be more consistent with the rest of HF ecosystem and have a single config value to set.\r\n\r\nThe changes are backward-compatible meaning that:\r\n- `HF_DATASETS_OFFLINE` environment is still taken into account, though not documented\r\n- `datasets.config.HF_DATASETS_OFFLINE` still exists, though it is not used anymore (in favor of `datasets.config.HF_HUB_OFFLINE`)\r\n\r\n\r\n**Note:** it might break things in downstream libraries if they were monkeypatching `datasets.config.HF_DATASETS_OFFLINE` in their CI tests (for instance). Not much of a problem IMO.<end_of_turn>\n<start_of_turn>model\nUse `HF_HUB_OFFLINE` instead of `HF_DATASETS_OFFLINE`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSome warning messages didn't quite sound like warnings so I modified their logging levels to info.<end_of_turn>\n<start_of_turn>model\nModify levels of some logging messages<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemoves the temporary pin introduced in #6264 <end_of_turn>\n<start_of_turn>model\nUnpin `tensorflow` maximum version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes: https://github.com/huggingface/datasets/issues/6900\r\n\r\nNot sure if this needs any addition stuff before merging<end_of_turn>\n<start_of_turn>model\nFix WebDatasets KeyError for user-defined Features when a field is missing in an example<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Summary\r\nFixes #7818 \r\n\r\nThis PR resolves the `ValueError: Unable to avoid copy while creating an array` error that occurs when using `train_test_split` with `stratify_by_column` parameter in NumPy 2.0+.\r\n\r\n## Changes\r\n- Wrapped the stratify column array access with `np.asarray()` in `arrow_dataset.py`\r\n- This allows NumPy 2.0 to make a copy when the Arrow array is non-contiguous in memory\r\n\r\n## Testing\r\n- \u2705 Tested with NumPy 2.3.4 - stratified splits work correctly\r\n- \u2705 Tested with NumPy 1.26.4 - backward compatibility maintained\r\n- \u2705 Verified class balance is preserved in stratified splits\r\n- \u2705 Non-stratified splits continue to work as expected\r\n\r\n<end_of_turn>\n<start_of_turn>model\nresolves the ValueError: Unable to avoid copy while creating an array<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAs discussed [here](https://github.com/huggingface/datasets/issues/5965), there are some cases where datasets is unable to automatically promote columns during mapping. The fix is to explicitly provide a `features` definition so pyarrow can configure itself with the right column types from the outset.\r\n\r\nThis PR provides an alternative approach, which is functionally equivalent to specifying features but a bit cleaner within a larger mapping pipeline. It allows clients to typehint the return variable coming from the mapper function - if we find one of these type annotations specified, and no explicit features have been passed in, we'll try to convert it into a Features map. If the map function runs and casting is unable to succeed, it will raise a DatasetTransformationNotAllowedError that indicates the typehint may be to blame. It works for batched and non-batched mapping functions.\r\n\r\nCurrently supported column types:\r\n- builtins primitives: string, int, float, bool\r\n- dictionaries, lists (nested and one-deep)\r\n- Optional types and None-Unions (synonymous with optional types)\r\n\r\nIt's used like:\r\n\r\n```python\r\nclass DatasetTyped(TypedDict):\r\n  texts: list[str]\r\n\r\ndef dataset_typed_map(batch) -> DatasetTyped:\r\n  return {\"texts\": [text.split() for text in batch[\"raw_text\"]]}\r\n\r\ndataset = {\"raw_text\": [\"\", \"This is a test\", \"This is another test\"]}\r\n\r\nwith Dataset.from_dict(dataset) as dset:\r\n  new_dataset = dset.map(\r\n    dataset_typed_map,\r\n    batched=True,\r\n    batch_size=1,\r\n    num_proc=1,\r\n)\r\n```\r\n\r\nOpen questions:\r\n- Should logging indicate we have automatically guessed these types? Or proceed quietly until we hit an error (as is the current implementation).<end_of_turn>\n<start_of_turn>model\nAdd column type guessing from map return function<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFor some reason when I try to resume the upload of my dataset, it is very slow to reach the index of the shard from which to resume the uploading.\r\n\r\nFrom my understanding, the problem is in this part of the code:\r\n\r\narrow_dataset.py\r\n```python\r\nfor index, shard in logging.tqdm(\r\n            enumerate(itertools.chain([first_shard], shards_iter)),\r\n            desc=\"Pushing dataset shards to the dataset hub\",\r\n            total=num_shards,\r\n            disable=not logging.is_progress_bar_enabled(),\r\n        ):\r\n            shard_path_in_repo = path_in_repo(index, shard)\r\n            # Upload a shard only if it doesn't already exist in the repository\r\n            if shard_path_in_repo not in data_files:\r\n```\r\n\r\nIn particular, iterating the generator is slow during the call:\r\n```python\r\nself._select_contiguous(start, length, new_fingerprint=new_fingerprint)\r\n```\r\n\r\nI wonder if it is possible to avoid calling this function for shards that are already uploaded and just start from the correct shard index.\n\n### Steps to reproduce the bug\n\n1. Start the upload\r\n```python\r\ndataset = load_dataset(\"imagefolder\", data_dir=DATA_DIR, split=\"train\", drop_labels=True)    \r\ndataset.push_to_hub(\"repo/name\")\r\n```\r\n2. Stop and restart the upload after hundreds of shards\n\n### Expected behavior\n\nSkip the uploaded shards faster.\n\n### Environment info\n\n- `datasets` version: 2.5.1\r\n- Platform: Linux-4.18.0-193.el8.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.8.16\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.2\r\n<end_of_turn>\n<start_of_turn>model\nSkipping shard in the remote repo and resume upload<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n## Actual\r\n\r\nWhen creating a multi-split dataset using generators like \r\n\r\n```python\r\ndatasets.DatasetDict({\r\n  \"val\": datasets.Dataset.from_generator(\r\n      generator=generator_val,\r\n      features=features\r\n  ),\r\n  \"test\": datasets.Dataset.from_generator(\r\n      generator=generator_test,\r\n      features=features,\r\n  )\r\n})\r\n```\r\n\r\nIt displays (for both test and val)\r\n\r\n```\r\nGenerating train split\r\n```\r\n\r\n\r\n## Expected\r\n\r\nI would like to be able to improve this behavior by doing\r\n\r\n```python\r\ndatasets.DatasetDict({\r\n  \"val\": datasets.Dataset.from_generator(\r\n      generator=generator_val,\r\n      features=features,\r\n      split=\"val\"\r\n  ),\r\n  \"test\": datasets.Dataset.from_generator(\r\n      generator=generator_test,\r\n      features=features,\r\n      split=\"test\"\r\n  )\r\n})\r\n```\r\n\r\n\r\nIt would display\r\n\r\n```\r\nGenerating val split\r\n```\r\n\r\nand \r\n\r\n```\r\nGenerating test split\r\n```\r\n\r\n## Proposal\r\n\r\nCurrent PR is adding an explicit `split` argument and replace the implicit \"train\" split in the following classes/function : \r\n* Generator\r\n* from_generator\r\n* AbstractDatasetInputStream\r\n* GeneratorDatasetInputStream\r\n\r\nPlease share your feedbacks<end_of_turn>\n<start_of_turn>model\nadd split argument to Generator<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThere ~10 unnecessary `/api/datasets/user/dataset/revision`calls due to multithreading in data files resolution.\r\n\r\nI disabled multithreading, which was actually not useful anymore since `HfFileSystem` has been using caching for a while now.<end_of_turn>\n<start_of_turn>model\nLess api calls when resolving data_files<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`save_to_disk` creates this file, but also [`HugginFaceDatasetSever`](https://github.com/gradio-app/gradio/blob/26fef8c7f85a006c7e25cdbed1792df19c512d02/gradio/flagging.py#L214), so this is needed to avoid issues such as [this one](https://discord.com/channels/879548962464493619/1149295819938349107/1149295819938349107).<end_of_turn>\n<start_of_turn>model\nIgnore `dataset_info.json` in data files resolution<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix casting list array to fixed size list.\r\n\r\nThis bug was introduced in [datasets-2.17.0](https://github.com/huggingface/datasets/releases/tag/2.17.0) by PR: https://github.com/huggingface/datasets/pull/6283/files#diff-1cb2b66aa9311d729cfd83013dad56cf5afcda35b39dfd0bfe9c3813a049eab0R1899\r\n- #6283\r\n\r\nFix #7020.<end_of_turn>\n<start_of_turn>model\nFix casting list array to fixed size list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nDon't accept gzip encoding from github, otherwise some files are not streamable + seekable.\r\n\r\nfix https://huggingface.co/datasets/code_x_glue_cc_code_to_code_trans/discussions/2#64c0e0c1a04a514ba6303e84\r\n\r\nand making sure https://github.com/huggingface/datasets/issues/2918 works as well<end_of_turn>\n<start_of_turn>model\nNo gzip encoding from github<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix conda deps<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to load YODAS2 dataset with datasets==3.6.0\n```\nfrom datasets import load_dataset\nnext(iter(load_dataset('espnet/yodas2', name='ru000', split='train', streaming=True, trust_remote_code=True)))\n```\nAnd get `UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa8 in position 1: invalid start byte`\n\nThe cause of the error is the following:\n\n```\nfrom datasets.utils.file_utils import xopen\n\nfilepath = 'https://huggingface.co/datasets/espnet/yodas2/resolve/c9674490249665d658f527e2684848377108d82c/data/ru000/text/00000000.json'\nxopen(filepath, 'r').read()\n>>> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa8 in position 1: invalid start byte\n```\n\nAnd the cause of this is the following:\n\n```\nimport fsspec\n\nfsspec.open(\n    'hf://datasets/espnet/yodas2@c9674490249665d658f527e2684848377108d82c/data/ru000/text/00000000.json',\n    mode='r',\n    hf={'token': None, 'endpoint': 'https://huggingface.co'},\n).open().read()\n>>> UnicodeDecodeError: 'utf-8' codec can't decode byte 0xa8 in position 1: invalid start byte\n```\n\nIs it true that streaming=True loading is not supported anymore for trust_remote_code=True, even with datasets==3.6.0? This breaks backward compatibility.\n\n### Steps to reproduce the bug\n\n```\nfrom datasets import load_dataset\nnext(iter(load_dataset('espnet/yodas2', name='ru000', split='train', streaming=True)))\n```\n\n### Expected behavior\n\nNo errors expected\n\n### Environment info\n\ndatasets==3.6.0, ubuntu 24.04<end_of_turn>\n<start_of_turn>model\nxopen: invalid start byte for streaming dataset with trust_remote_code=True<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nshape returns (number of rows, number of columns)<end_of_turn>\n<start_of_turn>model\nUpdate dataset_dict.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd an `overwrite` argument to the `push_to_hub` method.\n\n### Motivation\n\nI want to overwrite a repo without deleting it on Hugging Face. Is this possible? I couldn't find anything in the documentation or tutorials.\n\n### Your contribution\n\nI can create a PR.<end_of_turn>\n<start_of_turn>model\n`push_to_hub` overwrite argument<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI currently have multiple datasets (train+validation) saved as 50MB shards. For one dataset the validation pair is small enough to fit into a single shard and this apparently causes problems when loading the dataset. I created the datasets using a DatasetDict, saved them as 50MB arrow files for streaming and then load each dataset. I have no problem loading any of the other datasets with more than 1 arrow file/shard. \n\nThe error indicates the training set got loaded in arrow format (correct) and the validation set in json (incorrect). This seems to be because some of the metadata files are considered as dataset files. \n```\nError loading /nfs/dataset_pt-uk: Couldn't infer the same data file format for all splits. Got {NamedSplit('train'): ('arrow', {}), NamedSplit('validation'): ('json', {})} \n```\n\n![Image](https://github.com/user-attachments/assets/f6e7596a-dd53-46a9-9a23-4e9cac2ac049)\n\nConcretely, there is a mismatch between the metadata created by the `DatasetDict.save_to_file` and the builder for `datasets.load_dataset`:\n\nhttps://github.com/huggingface/datasets/blob/e71b0b19d79c7531f9b9bea7c09916b5f6157f42/src/datasets/data_files.py#L107\n\nThe `folder_based_builder` lists all files and with 1 arrow file the json files (that are actually metadata) are in the majority.\nhttps://github.com/huggingface/datasets/blob/e71b0b19d79c7531f9b9bea7c09916b5f6157f42/src/datasets/packaged_modules/folder_based_builder/folder_based_builder.py#L58\n\n### Steps to reproduce the bug\n\nCreate a dataset with metadata and 1 arrow file in validation and multiple arrow files in the training set, following the above description. In my case, I saved the files via:\n\n```python\n        dataset = DatasetDict({\n            'train': train_dataset,\n            'validation': val_dataset\n        })\n        \n        dataset.save_to_disk(output_path, max_shard_size=\"50MB\")\n```\n\n### Expected behavior\n\nThe dataset would get loaded.\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Linux-6.14.0-22-generic-x86_64-with-glibc2.41\n- Python version: 3.12.7\n- `huggingface_hub` version: 0.31.1\n- PyArrow version: 18.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\n`load_dataset` defaults to json file format for datasets with 1 shard<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nExperiencing persistent 401 Unauthorized errors when attempting to upload datasets to Hugging Face Hub using the `datasets` library. The error occurs specifically with the CAS (Content Addressable Storage) service during the upload process. Tried using HF_HUB_DISABLE_XET=1. It seems to work for smaller files. \n\nExact error message : \n```\nProcessing Files (0 / 0)                : |                                                                                                                                 |  0.00B /  0.00B             2025-09-10T09:44:35.657565Z ERROR  Fatal Error: \"cas::upload_xorb\" api call failed (request id 01b[...]XXX): HTTP status client error (401 Unauthorized) for url (https://cas-server.xethub.hf.co/xorb/default/7f3abdc[...]XXX)\n    at /home/runner/work/xet-core/xet-core/cas_client/src/retry_wrapper.rs:113\n\nProcessing Files (0 / 0)                :   0%|                                                                                                                             |  0.00B /  184kB,  0.00B/s  \nNew Data Upload                         :   0%|                                                                                                                             |  0.00B /  184kB,  0.00B/s  \n\u274c Failed to push some_dataset: Data processing error: CAS service error : Reqwest Error: HTTP status client error (401 Unauthorized), domain: https://cas-server.xethub.hf.co/xorb/default/7f3abdc[...]XXX\n```\n\nWorkaround Attempts\n\n1. **Disabled XET**: Set `HF_HUB_DISABLE_XET=1` environment variable\n2. **Updated hf-xet**: Use `hf-xet==1.1.9` rather than latest\n3. **Verified Authentication**: Confirmed HF token is valid and has write permissions\n4. **Tested with Smaller Datasets**: \n   - 100 samples: \u2705 **SUCCESS** (uploaded successfully)\n   - 10,000 samples: \u274c **FAILS** (401 Unauthorized)\n\n### Steps to reproduce the bug\n\n   ```python\n   from datasets import Dataset, DatasetDict\n   \n   # Create dataset (example with 10,000 samples)\n   dataset = Dataset.from_dict({\n       \"question\": questions,\n       \"answer\": answers,\n       # ... other fields\n   })\n   \n   # Split into train/test\n   dataset_dict = dataset.train_test_split(test_size=0.1)\n   \n   # Upload to Hub\n   dataset_dict.push_to_hub(\"Org/some-dataset\")\n   ```\n\n### Expected behavior\n\n## Expected Behavior\n- Dataset should upload successfully to Hugging Face Hub\n- Progress bars should complete without authentication errors\n- Dataset should be accessible at the specified repository URL\n\n## Actual Behavior\n- Upload fails consistently with 401 Unauthorized error\n- Error occurs specifically during CAS service interaction\n- No progress is made on the upload (0% completion)\n- Dataset is created on Hugging Face Hub with no data folder\n\n### Environment info\n\n\n- **Platform**: SageMaker (AWS)\n- **Python Version**: 3.12\n- **Libraries**: \n  - `datasets` library (latest version)\n  - `hf-xet==1.1.9` (attempted fix)\n- **Authentication**: Hugging Face token configured\n- **Dataset Size**: ~10,000 samples, works for smaller sizes (e.g. 100)<end_of_turn>\n<start_of_turn>model\nHugging Face Hub Dataset Upload CAS Error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n```\r\n---------------------------------------------------------------------------\r\nKeyError                                  Traceback (most recent call last)\r\n[<ipython-input-9-bd197b36b6a0>](https://localhost:8080/#) in <cell line: 1>()\r\n----> 1 dataset['train']['/workspace/data']\r\n\r\n3 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py](https://localhost:8080/#) in _check_valid_column_key(key, columns)\r\n    518 def _check_valid_column_key(key: str, columns: List[str]) -> None:\r\n    519     if key not in columns:\r\n--> 520         raise KeyError(f\"Column {key} not in the dataset. Current columns in the dataset: {columns}\")\r\n    521 \r\n    522 \r\n\r\nKeyError: \"Column train not in the dataset. Current columns in the dataset: ['image', '/workspace/data']\"\r\n```\n\n### Steps to reproduce the bug\n\nplease find the notebook for reference: https://colab.research.google.com/drive/10lZ_zLtU4itYVmIVTvIEVbjfOtCZaAZy?usp=sharing\n\n### Expected behavior\n\nadd column to the dataset\n\n### Environment info\n\ncolab pro<end_of_turn>\n<start_of_turn>model\nAdd new column to dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nLook at https://huggingface.co/datasets/severo/doc-yaml-2\r\n\r\nIt's a reproduction of the example given in the docs at https://huggingface.co/docs/hub/datasets-manual-configuration\r\n\r\n```\r\nYou can select multiple files per split using a list of paths:\r\n\r\nmy_dataset_repository/\r\n\u251c\u2500\u2500 README.md\r\n\u251c\u2500\u2500 data/\r\n\u2502   \u251c\u2500\u2500 abc.csv\r\n\u2502   \u2514\u2500\u2500 def.csv\r\n\u2514\u2500\u2500 holdout/\r\n    \u2514\u2500\u2500 ghi.csv\r\n\r\n\r\n---\r\nconfigs:\r\n- config_name: default\r\n  data_files:\r\n  - split: train\r\n    path:\r\n    - \"data/abc.csv\"\r\n    - \"data/def.csv\"\r\n  - split: test\r\n    path: \"holdout/ghi.csv\"\r\n---\r\n```\r\n\r\nIt raises the following error:\r\n\r\n```\r\nError code:   ConfigNamesError\r\nException:    FileNotFoundError\r\nMessage:      Couldn't find a dataset script at /src/services/worker/severo/doc-yaml-2/doc-yaml-2.py or any data file in the same directory. Couldn't find 'severo/doc-yaml-2' on the Hugging Face Hub either: FileNotFoundError: Unable to find 'hf://datasets/severo/doc-yaml-2@938a0578fb4c6bc9da7d80b06a3ba39c2834b0c2/data/def.csv' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.arrow', '.txt', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']\r\nTraceback:    Traceback (most recent call last):\r\n                File \"/src/services/worker/src/worker/job_runners/dataset/config_names.py\", line 65, in compute_config_names_response\r\n                  for config in sorted(get_dataset_config_names(path=dataset, token=hf_token))\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/inspect.py\", line 351, in get_dataset_config_names\r\n                  dataset_module = dataset_module_factory(\r\n                File \"/src/services/worker/.venv/lib/python3.9/site-packages/datasets/load.py\", line 1507, in dataset_module_factory\r\n                  raise FileNotFoundError(\r\n              FileNotFoundError: Couldn't find a dataset script at /src/services/worker/severo/doc-yaml-2/doc-yaml-2.py or any data file in the same directory. Couldn't find 'severo/doc-yaml-2' on the Hugging Face Hub either: FileNotFoundError: Unable to find 'hf://datasets/severo/doc-yaml-2@938a0578fb4c6bc9da7d80b06a3ba39c2834b0c2/data/def.csv' with any supported extension ['.csv', '.tsv', '.json', '.jsonl', '.parquet', '.arrow', '.txt', '.blp', '.bmp', '.dib', '.bufr', '.cur', '.pcx', '.dcx', '.dds', '.ps', '.eps', '.fit', '.fits', '.fli', '.flc', '.ftc', '.ftu', '.gbr', '.gif', '.grib', '.h5', '.hdf', '.png', '.apng', '.jp2', '.j2k', '.jpc', '.jpf', '.jpx', '.j2c', '.icns', '.ico', '.im', '.iim', '.tif', '.tiff', '.jfif', '.jpe', '.jpg', '.jpeg', '.mpg', '.mpeg', '.msp', '.pcd', '.pxr', '.pbm', '.pgm', '.ppm', '.pnm', '.psd', '.bw', '.rgb', '.rgba', '.sgi', '.ras', '.tga', '.icb', '.vda', '.vst', '.webp', '.wmf', '.emf', '.xbm', '.xpm', '.BLP', '.BMP', '.DIB', '.BUFR', '.CUR', '.PCX', '.DCX', '.DDS', '.PS', '.EPS', '.FIT', '.FITS', '.FLI', '.FLC', '.FTC', '.FTU', '.GBR', '.GIF', '.GRIB', '.H5', '.HDF', '.PNG', '.APNG', '.JP2', '.J2K', '.JPC', '.JPF', '.JPX', '.J2C', '.ICNS', '.ICO', '.IM', '.IIM', '.TIF', '.TIFF', '.JFIF', '.JPE', '.JPG', '.JPEG', '.MPG', '.MPEG', '.MSP', '.PCD', '.PXR', '.PBM', '.PGM', '.PPM', '.PNM', '.PSD', '.BW', '.RGB', '.RGBA', '.SGI', '.RAS', '.TGA', '.ICB', '.VDA', '.VST', '.WEBP', '.WMF', '.EMF', '.XBM', '.XPM', '.aiff', '.au', '.avr', '.caf', '.flac', '.htk', '.svx', '.mat4', '.mat5', '.mpc2k', '.ogg', '.paf', '.pvf', '.raw', '.rf64', '.sd2', '.sds', '.ircam', '.voc', '.w64', '.wav', '.nist', '.wavex', '.wve', '.xi', '.mp3', '.opus', '.AIFF', '.AU', '.AVR', '.CAF', '.FLAC', '.HTK', '.SVX', '.MAT4', '.MAT5', '.MPC2K', '.OGG', '.PAF', '.PVF', '.RAW', '.RF64', '.SD2', '.SDS', '.IRCAM', '.VOC', '.W64', '.WAV', '.NIST', '.WAVEX', '.WVE', '.XI', '.MP3', '.OPUS', '.zip']\r\n```<end_of_turn>\n<start_of_turn>model\nTrouble loading files defined in YAML explicitly<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen performing map, it's nice to be able to pass the new feature type, and indeed required by interleave and concatenate datasets.\r\n\r\nHowever, this can cause a major slowdown for certain types of array features due to the features being re-encoded.\r\n\r\nThis is separate to the slowdown reported in #7206 \r\n\r\n### Steps to reproduce the bug\r\n\r\n```\r\nfrom datasets import Dataset, Features, Array3D, Sequence, Value\r\nimport numpy as np\r\nimport time\r\nfeatures=Features(**{\"array0\": Sequence(feature=Value(\"float32\"), length=-1), \"array1\": Sequence(feature=Value(\"float32\"), length=-1)})\r\ndataset = Dataset.from_dict({f\"array{i}\": [np.zeros((x,), dtype=np.float32) for x in [5000,10000]*25] for i in range(2)}, features=features)\r\n```\r\n\r\n```\r\nds = dataset.to_iterable_dataset()\r\nds = ds.with_format(\"numpy\").map(lambda x: x)\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\n```\r\n~1.5 s on main\r\n\r\n\r\n```\r\nds = dataset.to_iterable_dataset()\r\nds = ds.with_format(\"numpy\").map(lambda x: x, features=features)\r\nt0 = time.time()\r\nfor ex in ds:\r\n    pass\r\nt1 = time.time()\r\n```\r\n~ 3 s on main\r\n\r\n### Expected behavior\r\n\r\nI'm not 100% sure whether passing new feature types to formatted outputs of map should be supported or not, but assuming it should, then there should be a cost-free way to specify the new feature type - knowing feature type is required by interleave_datasets and concatenate_datasets for example\r\n\r\n### Environment info\r\n\r\n3.0.2<end_of_turn>\n<start_of_turn>model\nIterable dataset map with explicit features causes slowdown for Sequence features<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen persistent workers are enabled, the epoch that's set via the IterableDataset instance held by the training process is ignored by the workers as they are disconnected across processes.\r\n\r\nPyTorch samplers for non-iterable datasets have a mechanism to sync this, datasets.IterableDataset does not.\r\n\r\nIn my own use of IterableDatasets I usually track the epoch count which crosses process boundaries in a multiprocessing.Value \r\n\r\n### Steps to reproduce the bug\r\n\r\nUse a streaming dataset (Iterable) w/ the recommended pattern below and `persistent_workers=True` in the torch DataLoader. \r\n\r\n```\r\nfor epoch in range(epochs):\r\n    shuffled_dataset.set_epoch(epoch)\r\n    for example in shuffled_dataset:\r\n        ...\r\n```\r\n\r\n### Expected behavior\r\n\r\nWhen the canonical bit of code above is used with `num_workers > 0` and `persistent_workers=True`, the epoch set via `set_epoch()` is propagated to the IterableDataset instances in the worker processes\r\n\r\n\r\n### Environment info\r\n\r\nN/A<end_of_turn>\n<start_of_turn>model\nIterableDataset `set_epoch` is ignored when DataLoader `persistent_workers=True`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen running the BeamBasedBuilder with a GCS path specified in the cache_dir, the following ValueError occurs:\r\n\r\n```\r\nValueError: Unable to get filesystem from specified path, please use the correct path or ensure the required dependency is installed, e.g., pip install apache-beam[gcp]. Path specified: gcs://my-bucket/huggingface_datasets/my_beam_dataset/default/0.0.0/my_beam_dataset-train [while running 'train/Save to parquet/Write/WriteImpl/InitializeWrite']\r\n```\r\n\r\nSame error occurs after running `pip install apache-beam[gcp]` as instructed.\n\n### Steps to reproduce the bug\n\nPut `my_beam_dataset.py`:\r\n\r\n```python\r\nimport datasets\r\n\r\n\r\nclass MyBeamDataset(datasets.BeamBasedBuilder):\r\n    def _info(self):\r\n        features = datasets.Features({\"value\": datasets.Value(\"int64\")})\r\n        return datasets.DatasetInfo(features=features)\r\n\r\n    def _split_generators(self, dl_manager, pipeline):\r\n        return [datasets.SplitGenerator(name=datasets.Split.TRAIN, gen_kwargs={})]\r\n\r\n    def _build_pcollection(self, pipeline):\r\n        import apache_beam as beam\r\n\r\n        return pipeline | beam.Create([{\"value\": i} for i in range(10)])\r\n```\r\n\r\nRun:\r\n```bash\r\ndatasets-cli run_beam my_beam_dataset.py --cache_dir=gs://my-bucket/huggingface_datasets/ --beam_pipeline_options=\"runner=DirectRunner\"\r\n```\n\n### Expected behavior\n\nRunning the BeamBasedBuilder with a GCS cache path without any errors.\n\n### Environment info\n\n- `datasets` version: 2.14.4\r\n- Platform: macOS-13.4-arm64-arm-64bit\r\n- Python version: 3.9.17\r\n- Huggingface_hub version: 0.16.4\r\n- PyArrow version: 9.0.0\r\n- Pandas version: 2.0.3<end_of_turn>\n<start_of_turn>model\nValueError when running BeamBasedBuilder with GCS path in cache_dir<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nManually fix dependency conflict on `typing-extensions` version originated by `apache-beam` + `pydantic` (now a dependency of `huggingface-hub`).\r\nThis is a temporary hot fix of our CI build documentation until we stop using `apache-beam`.\r\nFix #6406.<end_of_turn>\n<start_of_turn>model\nFix dependency conflict within CI build documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n#### **Problem Description**\r\nCurrently, the Hugging Face Dataset Viewer automatically interprets dataset fields for datasets created with the `datasets` library. However, for datasets pushed directly via `git`, the Viewer:\r\n- Defaults to generic columns like `label` with `null` values if no explicit mapping is provided.\r\n- Does not allow dataset creators to configure field mappings or suppress default fields unless the dataset is recreated and pushed using the `datasets` library.\r\n\r\nThis creates a limitation for creators who:\r\n- Use custom workflows to prepare datasets (e.g., manifest files with audio-transcription mappings).\r\n- Push large datasets directly via `git` and cannot easily restructure them to conform to the `datasets` library format.\r\n\r\n#### **Proposed Solution**\r\nIntroduce a feature that allows dataset creators to manually configure the Dataset Viewer behavior for datasets not created with the `datasets` library. This could be achieved by:\r\n1. **Using the YAML Metadata in `README.md`:**\r\n   - Add support for defining the dataset's field mappings directly in the `README.md` YAML section.\r\n   - Example:\r\n   \r\n     ```yaml\r\n     viewer:\r\n       fields:\r\n         - name: \"audio\"\r\n           type: \"audio_path\" / \"text\"\r\n           source: \"manifest['audio']\"\r\n         - name: \"bambara_transcription\"\r\n           type: \"text\"\r\n           source: \"manifest['bambara']\"\r\n         - name: \"french_translation\"\r\n           type: \"text\"\r\n           source: \"manifest['french']\"\r\n     ```\r\nWith manifest being a csv or json like format file in the repository so that the viewer understands that it should look for the values of each field in that file. \r\n\r\n#### **Benefits**\r\n- Improves flexibility for dataset creators who push datasets via `git`.\r\n- Enhances dataset discoverability and usability on the Hugging Face Hub by allowing creators to present meaningful field mappings without restructuring their data.\r\n- Reduces overhead for creators of large or complex datasets.\r\n\r\n#### **Examples of Use Case**\r\n- An audio dataset with transcriptions in multiple languages stored in a `manifest.json` file, where the user wants the Viewer to:\r\n  - Display the `audio` column and Explicitly map features that he defined such as `bambara_transcription` and `french_translation` from the manifest.<end_of_turn>\n<start_of_turn>model\nAllow manual configuration of Dataset Viewer for datasets not created with the `datasets` library<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove token arg from CLI examples.\r\n\r\nFix #6838.\r\n\r\nCC: @Wauplin <end_of_turn>\n<start_of_turn>model\nRemove token arg from CLI examples<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nminor video docs on how to install<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove all metrics, as part of the 3.0 release.\r\n\r\nNote they are deprecated since 2.5.0 version.<end_of_turn>\n<start_of_turn>model\nRemove metrics<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen I try to load `cnn_dailymail` dataset, it takes longer than usual and when I checked the dataset it's 3x bigger than it's supposed to be.\r\n\r\nCheck https://huggingface.co/datasets/cnn_dailymail: it says 287k rows for train. But when I check length of train split it says 861339.\r\n\r\nAlso I checked data:\r\n\r\n```\r\n>>> ds['train']['highlights'][0]                                           \r\n\"Harry Potter star Daniel Radcliffe gets \u00a320M fortune as he turns 18 Monday . Young actor says he has no plans to fritter his cash away . Radcliffe's earnings from first five Potter films have been held in trust fund .\"````\r\n\r\n>>> ds['train']['highlights'][0]                                           \r\n\"Harry Potter star Daniel Radcliffe gets \u00a320M fortune as he turns 18 Monday . Young actor says he has no plans to fritter his cash away . Radcliffe's earnings from first five Potter films have been held in trust fund .\"````\r\n\r\n>>> ds['train']['highlights'][287113]\r\n\"Harry Potter star Daniel Radcliffe gets \u00a320M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\"````\r\n\r\n>>> ds['train']['highlights'][574226]\r\n\"Harry Potter star Daniel Radcliffe gets \u00a320M fortune as he turns 18 Monday .\\nYoung actor says he has no plans to fritter his cash away .\\nRadcliffe's earnings from first five Potter films have been held in trust fund .\"\r\n```\r\n\r\nThe datasets seems to be updated 6 days ago to convert it to Parquet. Probably, there is some issue with backward compatability.\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1.\r\n```\r\nfrom datasets import load_dataset\r\nds = load_dataset('cnn_dailymail', '3.0.0')\r\nlen(ds['train'])\r\n```\r\n\r\n### Expected behavior\r\n\r\nIt should not repeat itself.\r\n\r\n### Environment info\r\n\r\ndatasets==2.13.2\r\nPython==3.7.13<end_of_turn>\n<start_of_turn>model\ncnn_dailymail repeats itself<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport Python 3.11.\r\n\r\nFix #7178.<end_of_turn>\n<start_of_turn>model\nSupport Python 3.11<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nUpdate yanked version of minimum requests requirement.\r\n\r\nVersion 2.32.1 was yanked: https://pypi.org/project/requests/2.32.1/<end_of_turn>\n<start_of_turn>model\nUpdate yanked version of minimum requests requirement<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nhttps://github.com/huggingface/datasets/issues/6393 has plagued me on and off for a very long time. I have had various workarounds (one time combining two filter calls into one filter call removed the issue, another time making rank 0 go first resolved a cache race condition, one time i think upgrading the version of something resolved it). I don't know hf datasets well enough to fully understand the root cause, but I _think_ this PR fixes it.\r\n\r\nEvidence: I have an LLM Foundry training yaml/script (datasets version 3.2.0) that results in a hang ~1/10 times (for a baseline for this testing, it was 2/36 runs that hung). I also reran with the latest datasets version (3.3.1) and got 4/36 hung. Installing datasets from this PR, I was able to successful run the script 144 times without a hang occurring. Assuming the base probability is 1/10, this should be more than enough times to have confidence it works.\r\n\r\nAfter adding some logging, I could see that the code hung during the __exit__ of the mp pool context manager, after all shards had been processed, and the tqdm context manager had exited.\r\n\r\nMy best explanation: When multiprocessing pool __exit__ is called, it calls pool.terminate, which forcefully exits all the processes (and calls code related to this that I haven't looked at closely). I'm guessing this forceful termination has a bad interaction with some multithreading/multiprocessing that hf datasets does. If we instead call pool.close and pool.join before the pool.terminate happens, perhaps whatever that bad interaction is is able to complete gracefully, and then terminate call proceeds without issue.\r\n\r\nIf this PR seems good to you, I'd be very appreciative if you were able to do a patch release including it. Thank you!\r\n\r\n@lhoestq <end_of_turn>\n<start_of_turn>model\nAttempt to fix multiprocessing hang by closing and joining the pool before termination<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n(wip and experimental)\r\n\r\nadding the `Video` type based on `VideoReader` from `decord`\r\n\r\n```python\r\n>>>from datasets import load_dataset\r\n>>> ds = load_dataset(\"path/to/videos\", split=\"train\").with_format(\"torch\")\r\n>>> print(ds[0][\"video\"])\r\n<decord.video_reader.VideoReader object at 0x337a47910>\r\n>>> print(ds[0][\"video\"][0])\r\ntensor([[[73, 73, 73],\r\n         [73, 73, 73],\r\n         [73, 73, 73],\r\n         ...,\r\n         [23, 23, 23],\r\n         [23, 23, 23],\r\n         [23, 23, 23]]], dtype=torch.uint8)\r\n```\r\n\r\nthe storage is the same as for audio and images: `{\"path\": pa.string(), \"bytes\": pa.binary()}` and I did a small to keep the hf:// URL in the \"path\" field if possible, this way the viewer can link to fiels on the hub if possible<end_of_turn>\n<start_of_turn>model\nVideo support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 2.20.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPin dill<0.3.9 to fix CI for deps-latest.\r\n\r\nNote that dill-0.3.9 was released yesterday Sep 29, 2024:\r\n- https://pypi.org/project/dill/0.3.9/\r\n- https://github.com/uqfoundation/dill/releases/tag/0.3.9\r\n\r\nFix #7183.<end_of_turn>\n<start_of_turn>model\nPin dill<0.3.9 to fix CI<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nIf this tokenization function is used with IterableDatasets and no sample is as big as the context length, `input_batch` will be an empty list.\r\n```\r\ndef tokenize(batch, tokenizer, context_length):\r\n    outputs = tokenizer(\r\n        batch[\"text\"],\r\n        truncation=True,\r\n        max_length=context_length,\r\n        return_overflowing_tokens=True,\r\n        return_length=True\r\n    )\r\n    input_batch = []\r\n    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\r\n        if length == context_length:\r\n            input_batch.append(input_ids)\r\n    return {\"input_ids\": input_batch}\r\n\r\ndataset.map(tokenize, batched=True, batch_size=batch_size, fn_kwargs={\"context_length\": context_length, \"tokenizer\": tokenizer}, remove_columns=dataset.column_names)\r\n```\r\n\r\nThis will throw the following error: UnboundLocalError: local variable 'batch_idx' referenced before assignment, because the for loop was not executed a single time\r\n\r\n```\r\nfor batch_idx, example in enumerate(_batch_to_examples(transformed_batch)):\r\n     yield new_key, example\r\ncurrent_idx += batch_idx + 1\r\n```\r\n\r\nSome of the possible solutions\r\n\r\n```\r\nfor batch_idx, example in enumerate(_batch_to_examples(transformed_batch)):\r\n     yield new_key, example\r\ntry:\r\n     current_idx += batch_idx + 1\r\nexcept:\r\n     current_idx += 1\r\n```\r\n\r\nor \r\n\r\n```\r\nbatch_idx = 0\r\nfor batch_idx, example in enumerate(_batch_to_examples(transformed_batch)):\r\n     yield new_key, example\r\ncurrent_idx += batch_idx + 1\r\n```<end_of_turn>\n<start_of_turn>model\nFix UnboundLocalError if preprocessing returns an empty list<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi,\n\nI'm trying to load a dataset on Kaggle TPU image. There is some known compat issue with progress bar on Kaggle, so I'm trying to disable the progress bar globally. This does not work as you can see in [here](https://www.kaggle.com/code/windmaple/hf-datasets-issue).\n\nIn contract, disabling progress bar for snapshot_download() works as expected as in [here](https://www.kaggle.com/code/windmaple/snapshot-download-error).\n\n### Steps to reproduce the bug\n\nSee this [notebook](https://www.kaggle.com/code/windmaple/hf-datasets-issue). \n\nThere is sth. wrong with `shell_paraent`.\n\n### Expected behavior\n\nThe downloader should disable progress bar and move forward w/ no error.\n\n### Environment info\n\nThe latest version as I did:\n\n!pip install -U datasets ipywidgets ipykernel<end_of_turn>\n<start_of_turn>model\ndisable_progress_bar() not working as expected<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThe sharding of IterableDatasets with respect to distributed and dataloader worker processes appears problematic with significant performance traps and inconsistencies wrt to distributed train processes vs worker processes.\r\n\r\nSplitting across num_workers (per train process loader processes) and world_size (distributed training processes) appears inconsistent.\r\n* worker split: https://github.com/huggingface/datasets/blob/9d6d16117a30ba345b0236407975f701c5b288d4/src/datasets/iterable_dataset.py#L1266-L1283\r\n* distributed split: https://github.com/huggingface/datasets/blob/9d6d16117a30ba345b0236407975f701c5b288d4/src/datasets/iterable_dataset.py#L1335-L1356\r\n\r\nIn the case of the distributed split, there is a modulus check that flips between two very different behaviours, why is this different than splitting across the data loader workers? For IterableDatasets the DataLoaders worker processes are independent, so whether it's workers within one train process or across a distributed world the shards should be distributed the same, across `world_size * num_worker` independent workers in either case... \r\n\r\nFurther, the fallback case when the `n_shards % world_size == 0` check fails is a rather extreme change. I argue it is not desirable to do that implicitly, it should be an explicit case for specific scenarios (ie reliable validation). A train scenario would likely be much better handled with improved wrapping / stopping behaviour to eg also fix #6437. Changing from stepping shards to stepping samples means that every single process reads ALL of the shards. This was never an intended default for sharded training, shards gain their performance advantage in large scale distributed training by explicitly avoiding the need to have every process overlapping in the data they read, by default, only the data allocated to each process via their assigned shards should be read in each pass of the dataset.\r\n\r\nUsing a large scale CLIP example, some of the larger datasets have 10-20k shards across 100+TB of data. Training with 1000 GPUs we are switching between reading 100 terabytes per epoch to 100 petabytes if say change 20k % 1000 and drop one gpu-node to 20k % 992.\r\n\r\nThe 'step over samples' case might be worth the overhead in specific validation scenarios where gaurantees of at least/most once samples seen are more important and do not make up a significant portion of train time or are done in smaller world sizes outside of train.\r\n\r\n### Steps to reproduce the bug\r\n\r\nN/A\r\n\r\n### Expected behavior\r\n\r\nWe have an iterable dataset with N shards, to split across workers\r\n* shuffle shards (same seed across all train processes)\r\n* step shard iterator across distributed processes\r\n* step shard iterator across dataloader worker processes\r\n* shuffle samples in every worker via shuffle buffer (different seed in each worker, but ideally controllable (based on base seed + worker id + epoch).\r\n* end up with (possibly uneven) number of shards per worker but each shard only ever accessed by 1 worker per pass (epoch)\r\n\r\n\r\n\r\n### Environment info\r\n\r\nN/A<end_of_turn>\n<start_of_turn>model\nIterableDataset sharding logic needs improvement<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nhttps://huggingface.co/datasets/Samsung/samsum dataset not found error 404\n\nOriginated from https://github.com/meta-llama/llama-cookbook/issues/948\n\n### Steps to reproduce the bug\n\ngo to website https://huggingface.co/datasets/Samsung/samsum\nsee the error\n\nalso downloading it with python throws \n```\nCouldn't find 'Samsung/samsum' on the Hugging Face Hub either: FileNotFoundError: Samsung/samsum@f00baf5a7d4abfec6820415493bcb52c587788e6/samsum.py (repository not found)\n```\n\n### Expected behavior\n\nDataset exists\n\n### Environment info\n\n```\n- `datasets` version: 3.2.0\n- Platform: macOS-15.4.1-arm64-arm-64bit\n- Python version: 3.12.2\n- `huggingface_hub` version: 0.26.5\n- PyArrow version: 16.1.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.9.0\n```<end_of_turn>\n<start_of_turn>model\nNo Samsum dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nThe issue is related to ray data https://github.com/ray-project/ray/issues/49008 which it requires to check if the dataset is the original one just after `load_dataset` and parquet files are already available on hf hub.\r\n\r\nThe solution used now is to get the dataset name, config and split, then `load_dataset` again and check the fingerprint. But it's unable to get the correct dataset name if it contains username. So how to get the dataset name with username prefix, or is there another way to query if a dataset is the original one with parquet available?\r\n\r\n@lhoestq \r\n\r\n### Motivation\r\n\r\nhttps://github.com/ray-project/ray/issues/49008\r\n\r\n### Your contribution\r\n\r\nWould like to fix that.<end_of_turn>\n<start_of_turn>model\nHow to get the original dataset name with username?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRelated to https://github.com/huggingface/transformers/issues/27034 and https://github.com/huggingface/huggingface_hub/pull/1782.\r\n\r\n**TL;DR:** `hashlib` is not a secure library for cryptography-related stuff. We are only using `hashlib` for non-security-related purposes in `datasets` so it's fine. From Python 3.9 we set can `usedforsecurity=False` in any `hashlib` method which is mandatory for companies that forbid the use of `hashlib` for security purposes. This PR fixes that.\r\n\r\n**Note:** before merging this we need to release a new tokenizers version that would allow the newest `huggingface_hub` version (see https://github.com/huggingface/tokenizers/pull/1385). Otherwise it might create friction to users that want to install `datasets` + `tokenizers` at the same time.<end_of_turn>\n<start_of_turn>model\nSet `usedforsecurity=False` in hashlib methods (FIPS compliance)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes issue #7700\r\n\r\nThis PR makes num_proc=0 behave like None in Dataset.map(), disabling multiprocessing.\r\nIt improves UX by aligning with DataLoader(num_workers=0) behavior.\r\nThe num_proc docstring is also updated to clearly explain valid values and behavior.\r\n\r\n@SunMarc\r\n<end_of_turn>\n<start_of_turn>model\nnum_proc=0 behave like None, num_proc=1 uses one worker (not main process) and clarify num_proc documentation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nGracefully cancel async tasks<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nCurrently, when using dataset.cast_column(\"image\", Image(decode=True)), the pipeline throws an error and halts if any image in the dataset is invalid or corrupted (e.g., truncated files, incorrect formats, unreachable URLs). This behavior disrupts large-scale processing where a few faulty samples are common.\nreference : https://discuss.huggingface.co/t/handle-errors-when-loading-images-404-corrupted-etc/50318/5\n                  https://discuss.huggingface.co/t/handling-non-existing-url-in-image-dataset-while-cast-column/69185\n                  \nProposed Feature\n\nIntroduce a mechanism (e.g., a continue_on_error=True flag or global error handling mode) in Image(decode=True) that:\n\nSkips invalid images and sets them as None, or\n\nLogs the error but allows the rest of the dataset to be processed without interruption.\n\n\nExample Usage\n\nfrom datasets import load_dataset, Image\n\ndataset = load_dataset(\"my_dataset\")\ndataset = dataset.cast_column(\"image\", Image(decode=True, continue_on_error=True))\n\nBenefits\n\nEnsures robust large-scale image dataset processing.\n\nImproves developer productivity by avoiding custom retry/error-handling code.\n\nAligns with best practices in dataset preprocessing pipelines that tolerate minor data corruption.\n\n\nPotential Implementation Options\n\nInternally wrap the decoding in a try/except block.\n\nReturn None or a placeholder on failure.\n\nOptionally allow custom error callbacks or logging.\n\n### Motivation\n\nRobustness: Large-scale image datasets often contain a small fraction of corrupt files or unreachable URLs. Halting on the first error forces users to write custom workarounds or preprocess externally.\n\nSimplicity: A built-in flag removes boilerplate try/except logic around every decode step.\n\nPerformance: Skipping invalid samples inline is more efficient than a two-pass approach (filter then decode).\n\n### Your contribution\n\n\n1. API Change\nExtend datasets.features.Image(decode=True) to accept continue_on_error: bool = False.\n\n2. Behavior\nIf continue_on_error=False (default), maintain current behavior: any decode error raises an exception.\nIf continue_on_error=True, wrap decode logic in try/except:\nOn success: store the decoded image.\nOn failure: log a warning (e.g., via logging.warning) and set the field to None (or a sentinel value).\n\n\n3. Optional Enhancements\nAllow a callback hook:\nImage(decode=True, continue_on_error=True, on_error=lambda idx, url, exc: ...)\nEmit metrics or counts of skipped images.<end_of_turn>\n<start_of_turn>model\nGraceful Error Handling for cast_column(\"image\", Image(decode=True)) in Hugging Face Datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd Arabic Docs to datasets\r\n[Datasets Arabic](https://github.com/AhmedAlmaghz/datasets/blob/main/docs/source/ar/index.mdx)\n\n### Motivation\n\n@AhmedAlmaghz\r\nhttps://github.com/AhmedAlmaghz/datasets/blob/main/docs/source/ar/index.mdx\n\n### Your contribution\n\n@AhmedAlmaghz\r\nhttps://github.com/AhmedAlmaghz/datasets/blob/main/docs/source/ar/index.mdx<end_of_turn>\n<start_of_turn>model\nAdd Arabic Docs to datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a bug in the `generate_examples` function where `datasets.Value` features with a `float` dtype were incorrectly generated using `np.random.randint`. This resulted in integer values being cast to float, which is not representative of true floating-point data.\r\n\r\n\r\n**Key changes include:**\r\n\r\n* Added explicit handling for `float` features using `np.random.rand` to generate continuous values.\r\n* Introduced fail-fast type checks for unsupported dtypes to improve robustness.\r\n* Added validation for sequence features to ensure `seq_shapes` is provided.\r\n\r\n### Before Fix\r\n\r\nFloat features were generated incorrectly as integers cast to float:\r\n\r\n```text\r\n- Example 0:\r\n- int_feature: 0\r\n- float_feature: 9.0  <-- Incorrect: An integer disguised as a float\r\n- string_feature: The small grey turtle was surprisingly fast...\r\n- seq_feature: [0.3048 0.4291 0.4283]\r\n```\r\n\r\n### After Fix\r\n\r\nFloat features are now correctly generated as continuous numbers in the range [0, 1):\r\n```text\r\n+ Example 0:\r\n+ int_feature: 0\r\n+ float_feature: 0.0183  <-- Correct: A true random float\r\n+ string_feature: The small grey turtle was surprisingly fast...\r\n+ seq_feature: [0.9237 0.7972 0.8526]\r\n```\r\n#### Note: This PR is a follow-up/fix of the previously closed PR #7769 for clarity and context.\r\n<end_of_turn>\n<start_of_turn>model\nFix: Correct float feature generation in `generate_examples` <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis is needed to enable page pruning with DataFusion, which will be useful for the Dataset Viewer.\r\n\r\nIndeed page pruning with DataFusion allows to download only certain pages of a row group, reducing the I/O required to read just a few rows.\r\n\r\nBut while data page v1 generally works, it's not easy with DataFusion to do page pruning on datasets with nested data. This is because rows can span multiple pages in v1, contrary to v2.\r\n\r\ncc @severo for viz<end_of_turn>\n<start_of_turn>model\nParquet: use data page v2 for efficient page pruning<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIn datasets 4.0.0 release, `load_dataset()` returns different audio samples compared to earlier versions, this breaks integration tests that depend on consistent sample data across different environments (first and second envs specified below).\n\n\n\n### Steps to reproduce the bug\n\n```python\nfrom datasets import Audio, load_dataset\n\nds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\nds = ds.cast_column(\"audio\", Audio(24000))\nsample= ds[0][\"audio\"][\"array\"]\nprint(sample)\n\n# sample in 3.6.0 \n[0.00231914 0.00245417 0.00187414 ... 0.00061956 0.00101157 0.00076325]\n\n\n# sample in 4.0.0 \narray([0.00238037, 0.00220794, 0.00198703, ..., 0.00057983, 0.00085863,\n       0.00115309], dtype=float32)\n```\n\n### Expected behavior\n\nThe same dataset should load identical samples across versions to maintain reproducibility.\n\n### Environment info\n\nFirst env:\n- datasets version: 3.6.0\n- Platform: Windows-10-10.0.26100-SP0\n- Python: 3.11.0\n\nSecond env:\n- datasets  version: 4.0.0\n- Platform: Linux-6.1.123+-x86_64-with-glibc2.35\n- Python: 3.11.13<end_of_turn>\n<start_of_turn>model\nload_dataset() in 4.0.0 returns different audio samples compared to earlier versions breaking reproducibility<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCi is broken with error `Failed to fetch wheel: llvmlite==0.34.0`: https://github.com/huggingface/datasets/actions/runs/10466825281/job/28984414269\r\n```\r\nRun uv pip install --system \"datasets[tests_numpy2] @ .\"\r\nResolved 150 packages in 4.42s\r\nerror: Failed to prepare distributions\r\n  Caused by: Failed to fetch wheel: llvmlite==0.34.0\r\n  Caused by: Build backend failed to build wheel through `build_wheel()` with exit status: 1\r\n--- stdout:\r\nrunning bdist_wheel\r\n/home/runner/.cache/uv/builds-v0/.tmpcyKh8S/bin/python /home/runner/.cache/uv/built-wheels-v3/pypi/llvmlite/0.34.0/wrk1bNwq1gleSiznvrSEZ/llvmlite-0.34.0.tar.gz/ffi/build.py\r\nLLVM version...\r\n--- stderr:\r\nTraceback (most recent call last):\r\n  File \"/home/runner/.cache/uv/built-wheels-v3/pypi/llvmlite/0.34.0/wrk1bNwq1gleSiznvrSEZ/llvmlite-0.34.0.tar.gz/ffi/build.py\", line 105, in main_posix\r\n    out = subprocess.check_output([llvm_config, '--version'])\r\n  File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/subprocess.py\", line 421, in check_output\r\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\r\n  File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/subprocess.py\", line 503, in run\r\n    with Popen(*popenargs, **kwargs) as process:\r\n  File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/subprocess.py\", line 971, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/opt/hostedtoolcache/Python/3.10.14/x64/lib/python3.10/subprocess.py\", line 1863, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'llvm-config'\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/runner/.cache/uv/built-wheels-v3/pypi/llvmlite/0.34.0/wrk1bNwq1gleSiznvrSEZ/llvmlite-0.34.0.tar.gz/ffi/build.py\", line 191, in <module>\r\n    main()\r\n  File \"/home/runner/.cache/uv/built-wheels-v3/pypi/llvmlite/0.34.0/wrk1bNwq1gleSiznvrSEZ/llvmlite-0.34.0.tar.gz/ffi/build.py\", line 181, in main\r\n    main_posix('linux', '.so')\r\n  File \"/home/runner/.cache/uv/built-wheels-v3/pypi/llvmlite/0.34.0/wrk1bNwq1gleSiznvrSEZ/llvmlite-0.34.0.tar.gz/ffi/build.py\", line 107, in main_posix\r\n    raise RuntimeError(\"%s failed executing, please point LLVM_CONFIG \"\r\nRuntimeError: llvm-config failed executing, please point LLVM_CONFIG to the path for llvm-config\r\nerror: command '/home/runner/.cache/uv/builds-v0/.tmpcyKh8S/bin/python' failed with exit code 1\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken for numpy-2: Failed to fetch wheel: llvmlite==0.34.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR updates the method for setting the permissions on `cache_path` after calling `shutil.move`. The call to `shutil.move` may not preserve permissions if the source and destination are on different filesystems. Reading and resetting umask can cause race conditions, so directly read what permissions were set for the `temp_file` instead.\r\n\r\nThis fixes https://github.com/huggingface/datasets/issues/7536.<end_of_turn>\n<start_of_turn>model\nAvoid global umask for setting file mode.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nmodified `MappedExamplesIterable` and `test_iterable_dataset.py::test_mapped_examples_iterable_with_indices`\r\n\r\nfix #7345\r\n\r\n@lhoestq <end_of_turn>\n<start_of_turn>model\nchanges to MappedExamplesIterable to resolve #7345<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhat could be causing this?\r\n```\r\n$ python3\r\nPython 3.8.13 (default, Mar 28 2022, 11:38:47) \r\n[GCC 7.5.0] :: Anaconda, Inc. on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> from datasets import load_dataset\r\n>>> load_dataset(\"imagenet-1k\")\r\nDownloading builder script: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4.72k/4.72k [00:00<00:00, 7.51MB/s]\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 85.4k/85.4k [00:00<00:00, 510kB/s]\r\nDownloading extra modules: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 46.4k/46.4k [00:00<00:00, 300kB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.1G/29.1G [19:36<00:00, 24.8MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.3G/29.3G [08:38<00:00, 56.5MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.0G/29.0G [09:26<00:00, 51.2MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29.2G/29.2G [09:38<00:00, 50.6MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 29.2G/29.2G [09:37<00:00, 44.1MB/s^Downloading data:   0%|                     | 106M/29.1G [00:05<23:49, 20.3MB/s]\r\n```\n\n### Steps to reproduce the bug\n\nSee above commands/code\n\n### Expected behavior\n\nimagenet-1k is downloaded\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: Linux-6.2.0-34-generic-x86_64-with-glibc2.17\r\n- Python version: 3.8.13\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 14.0.0\r\n- Pandas version: 1.5.2<end_of_turn>\n<start_of_turn>model\nimagenet-1k downloads over and over<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI custom a BuilderConfig and GeneratorBasedBuilder.\r\nHere is the code for BuilderConfig\r\n```\r\nclass UIEConfig(datasets.BuilderConfig):\r\n\r\n    def __init__(\r\n            self,\r\n            *args,\r\n            data_dir=None,\r\n            instruction_file=None,\r\n            instruction_strategy=None,\r\n            task_config_dir=None,\r\n            num_examples=None,\r\n            max_num_instances_per_task=None,\r\n            max_num_instances_per_eval_task=None,\r\n            over_sampling=None,\r\n            **kwargs\r\n    ):\r\n        super().__init__(*args, **kwargs)\r\n        self.data_dir = data_dir\r\n        self.num_examples = num_examples\r\n        self.over_sampling = over_sampling\r\n        self.instructions = self._parse_instruction(instruction_file)\r\n        self.task_configs = self._parse_task_config(task_config_dir)\r\n        self.instruction_strategy = instruction_strategy\r\n        self.max_num_instances_per_task = max_num_instances_per_task\r\n        self.max_num_instances_per_eval_task = max_num_instances_per_eval_task\r\n```\r\nBesides, here is the code for GeneratorBasedBuilder.\r\n```\r\nclass UIEInstructions(datasets.GeneratorBasedBuilder):\r\n    VERSION = datasets.Version(\"2.0.0\")\r\n    BUILDER_CONFIG_CLASS = UIEConfig\r\n    BUILDER_CONFIGS = [\r\n        UIEConfig(name=\"default\", description=\"Default config for NaturalInstructions\")\r\n    ]\r\n    DEFAULT_CONFIG_NAME = \"default\"\r\n```\r\n\r\nHere is the load_dataset\r\n```\r\nraw_datasets = load_dataset(\r\n        os.path.join(CURRENT_DIR, \"uie_dataset.py\"),\r\n        data_dir=data_args.data_dir,\r\n        task_config_dir=data_args.task_config_dir,\r\n        instruction_file=data_args.instruction_file,\r\n        instruction_strategy=data_args.instruction_strategy,\r\n        cache_dir=data_cache_dir,  # for debug, change dataset size, otherwise open it\r\n        max_num_instances_per_task=data_args.max_num_instances_per_task,\r\n        max_num_instances_per_eval_task=data_args.max_num_instances_per_eval_task,\r\n        num_examples=data_args.num_examples,\r\n        over_sampling=data_args.over_sampling\r\n    )\r\n```\r\nFinally, I met the error.\r\n```\r\nBuilderConfig UIEConfig(name='default', version=0.0.0, data_dir=None, data_files=None, description='Default config for NaturalInstructions') doesn't have a 'task_config_dir' key.\r\n```\r\n\r\nI debugged the code, but I find the parameters added by me may not work.\n\n### Steps to reproduce the bug\n\nhttps://github.com/BeyonderXX/InstructUIE/blob/master/src/uie_dataset.py\n\n### Expected behavior\n\n```\r\nBuilderConfig UIEConfig(name='default', version=0.0.0, data_dir=None, data_files=None, description='Default config for NaturalInstructions') doesn't have a 'task_config_dir' key.\r\n```\n\n### Environment info\n\ntorch                     2.3.0+cu118\r\ntransformers              4.40.1\r\npython 3.8<end_of_turn>\n<start_of_turn>model\ndatasets.BuilderConfig does not work.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSort the items in a set according to their `datasets.fingerprint.Hasher.hash` hash to get a deterministic hash of sets.\r\n\r\nThis is useful to get deterministic hashes of tokenizers that use a trie based on python sets.\r\n\r\nreported in https://github.com/huggingface/datasets/issues/3847<end_of_turn>\n<start_of_turn>model\nDeterministic set hash<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nSeems that `dataset` doesn't work with python==3.14. The root cause seems to be something with a `deel` API that was changed.\n```\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\n```\n\n### Steps to reproduce the bug\n\n(on a new folder)\nuv init \nuv python pin 3.14\nuv add datasets\nuv run python \n\n(in REPL)\nimport datasets\ndatasets.load_dataset(\"cais/mmlu\", \"all\")   # will fail on any dataset\n\n```\n>>> datasets.load_dataset(\"cais/mmlu\", \"all\")\nTraceback (most recent call last):\n  File \"<python-input-2>\", line 1, in <module>\n    datasets.load_dataset(\"cais/mmlu\", \"all\")\n    ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^                                                                                                                                \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1397, in load_dataset\n    builder_instance = load_dataset_builder(                          \n        path=path,                                                                                                                                                           \n    ...<10 lines>...                                                               \n        **config_kwargs,                     \n    )                                                                                                                                                                        \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/load.py\", line 1185, in load_dataset_builder\n    builder_instance._use_legacy_cache_dir_if_possible(dataset_module)                                                                                                       \n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^                                                                                                       \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 615, in _use_legacy_cache_dir_if_possible\n    self._check_legacy_cache2(dataset_module) or self._check_legacy_cache() or None\n    ~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^                                                                                                                                \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/builder.py\", line 487, in _check_legacy_cache2\n    config_id = self.config.name + \"-\" + Hasher.hash({\"data_files\": self.config.data_files})\n                                         ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                  \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/fingerprint.py\", line 188, in hash\n    return cls.hash_bytes(dumps(value))  \n                          ~~~~~^^^^^^^                                                                                                                                       \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 120, in dumps\n    dump(obj, file)             \n    ~~~~^^^^^^^^^^^                                                                                                                                                          \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 114, in dump\n    Pickler(file, recurse=True).dump(obj)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^                                                                                                                                    \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 428, in dump\n    StockPickler.dump(self, obj)                                       \n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^                                                                                                                                             \n  File \"/Users/zmoshe/.local/uv/python/cpython-3.14.0rc2-macos-aarch64-none/lib/python3.14/pickle.py\", line 498, in dump\n    self.save(obj)                                  \n    ~~~~~~~~~^^^^^                                                                                                                                                           \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/datasets/utils/_dill.py\", line 70, in save\n    dill.Pickler.save(self, obj, save_persistent_id=save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                      \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 422, in save\n    StockPickler.save(self, obj, save_persistent_id)\n    ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^                                                                                                                         \n  File \"/Users/zmoshe/.local/uv/python/cpython-3.14.0rc2-macos-aarch64-none/lib/python3.14/pickle.py\", line 572, in save\n    f(self, obj)  # Call unbound method with explicit self\n    ~^^^^^^^^^^^                                                                  \n  File \"/Users/zmoshe/temp/test_datasets_py3.14/.venv/lib/python3.14/site-packages/dill/_dill.py\", line 1262, in save_module_dict\n    StockPickler.save_dict(pickler, obj)                                                                                                                                     \n    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^\n  File \"/Users/zmoshe/.local/uv/python/cpython-3.14.0rc2-macos-aarch64-none/lib/python3.14/pickle.py\", line 1064, in save_dict\n    self._batch_setitems(obj.items(), obj)\n    ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\nTypeError: Pickler._batch_setitems() takes 2 positional arguments but 3 were given\n```\n\n### Expected behavior\n\nshould work.\n\n### Environment info\n\ndatasets==v4.3.0\npython==3.14\n<end_of_turn>\n<start_of_turn>model\ndatasets doesn't work with python 3.14<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nreported in https://github.com/huggingface/datasets/pull/7557 (I just reorganized the condition)\r\n\r\nwanted to apply this change to the original PR but github didn't let me apply it directly - merging this one instead<end_of_turn>\n<start_of_turn>model\nfix regression<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nset dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix https://github.com/huggingface/datasets/issues/6141\r\n\r\nColab installs 2023.6.0, so we should be good \ud83d\ude42\r\n<end_of_turn>\n<start_of_turn>model\nSet minimal fsspec version requirement to 2023.1.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAs stated in the title, since Numpy changed in version >2.0 with copy, the stratify parameters break.\n\ne.g. `all_dataset.train_test_split(test_size=0.2,stratify_by_column=\"label\")` returns a Numpy error.\n\nIt works if you downgrade Numpy to a version lower than 2.0.\n\n### Steps to reproduce the bug\n\n1. Numpy > 2.0\n2. `all_dataset.train_test_split(test_size=0.2,stratify_by_column=\"label\")`\n\n### Expected behavior\n\nIt returns a stratified split as per the results of Numpy < 2.0\n\n### Environment info\n\n- `datasets` version: 2.14.4\n- Platform: Linux-6.8.0-85-generic-x86_64-with-glibc2.35\n- Python version: 3.13.7\n- Huggingface_hub version: 0.34.4\n- PyArrow version: 19.0.0\n- Pandas version: 2.3.2\n<end_of_turn>\n<start_of_turn>model\ntrain_test_split and stratify breaks with Numpy 2.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nRemove tasks, as part of the 3.0 release.<end_of_turn>\n<start_of_turn>model\nRemove tasks<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSmall addition to video docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nThis issue is also reported here:\r\nhttps://discuss.huggingface.co/t/create-a-new-dataset-repository-broken-page/102644\r\n\r\nThis page is broken.\r\nhttps://huggingface.co/new-dataset\r\n\r\nI fill in the form with my text, and click `Create Dataset`.\r\n![Screenshot 2024-08-16 at 15 55 37](https://github.com/user-attachments/assets/de16627b-7a55-4bcf-9f0b-a48227aabfe6)\r\n\r\nThen the form gets wiped. And no repo got created. No error message visible in the developer console.\r\n![Screenshot 2024-08-16 at 15 56 54](https://github.com/user-attachments/assets/0520164b-431c-40a5-9634-11fd62c4f4c3)\r\n\r\n# Idea for improvement\r\n\r\nFor better UX, if the repo cannot be created, then show an error message, that something went wrong.\r\n\r\n# Work around, that works for me\r\n\r\n```python\r\nfrom huggingface_hub import HfApi, HfFolder\r\nrepo_id = 'simon-arc-solve-fractal-v3'\r\napi = HfApi()\r\nusername = api.whoami()['name']\r\nrepo_url = api.create_repo(repo_id=repo_id, exist_ok=True, private=True, repo_type=\"dataset\")\r\n```\r\n\r\n### Steps to reproduce the bug\r\n\r\nGo https://huggingface.co/new-dataset\r\n\r\nFill in the form.\r\n\r\nClick `Create dataset`.\r\n\r\nNow the form is cleared. And the page doesn't jump anywhere.\r\n\r\n### Expected behavior\r\n\r\nThe moment the user clicks `Create dataset`, the repo gets created and the page jumps to the created repo.\r\n\r\n\r\n### Environment info\r\n\r\nFirefox 128.0.3 (64-bit)\r\nmacOS Sonoma 14.5\r\n<end_of_turn>\n<start_of_turn>model\nwebsite broken: Create a new dataset repository, doesn't create a new repo in Firefox<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7837\r\n\r\nWe currently have the [mono attribute for Audio documented](https://github.com/huggingface/datasets/blob/41c05299348a499807432ab476e1cdc4143c8772/src/datasets/features/audio.py#L52C1-L54C22) but not used anywhere resulting in confusion for users. Since torchcodec does not know this attribute I suggest using `num_channels` (currently supported `None` (leave unchanged), mono: `1`, stereo: `2`).\r\n\r\nI could also add a mono attribute but found that to be more confusing for developers and would restrict us if at any point in the future more than 2 channels are supported.<end_of_turn>\n<start_of_turn>model\nAdd num channels to audio<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCurrently type inference doesn't know what to do with a Pandas Series of PIL.Image objects, though it would be nice to get a Dataset with the Image type this way<end_of_turn>\n<start_of_turn>model\nDataset.from_pandas with a DataFrame of PIL.Images<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdded some checks to prevent errors that arrise when using evaluate.py on only a portion of the squad 2.0 dataset.<end_of_turn>\n<start_of_turn>model\nadd safety checks when using only part of dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nWhen I load the ArmelR/stack-exchange-instruction dataset, I encounter a bug that may be raised by confusing the dir name string and the split string about the dataset. \r\n\r\nWhen I use the script \"datasets.load_dataset('ArmelR/stack-exchange-instruction', data_dir=\"data/finetune\", split=\"train\", use_auth_token=True)\", it fails. But it succeeds when I add the \"streaming = True\" parameter. \r\n\r\nThe website of the dataset is https://huggingface.co/datasets/ArmelR/stack-exchange-instruction/ .\r\n\r\nThe traceback logs are as below: \r\n\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/load.py\", line 1797, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/builder.py\", line 890, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/builder.py\", line 985, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/builder.py\", line 1706, in _prepare_split\r\n    split_info = self.info.splits[split_generator.name]\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/splits.py\", line 530, in __getitem__\r\n    instructions = make_file_instructions(\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/arrow_reader.py\", line 112, in make_file_instructions\r\n    name2filenames = {\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/arrow_reader.py\", line 113, in <dictcomp>\r\n    info.name: filenames_for_dataset_split(\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/naming.py\", line 70, in filenames_for_dataset_split\r\n    prefix = filename_prefix_for_split(dataset_name, split)\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/site-packages/datasets/naming.py\", line 54, in filename_prefix_for_split\r\n    if os.path.basename(name) != name:\r\n  File \"/home/xxx/miniconda3/envs/code/lib/python3.9/posixpath.py\", line 142, in basename\r\n    p = os.fspath(p)\r\nTypeError: expected str, bytes or os.PathLike object, not NoneType\r\n\r\n\r\n\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\n1. import datasets library function: ```from datasets import load_dataset```\r\n2. load dataset: ```ds=load_dataset('ArmelR/stack-exchange-instruction', data_dir=\"data/finetune\", split=\"train\", use_auth_token=True)```\r\n\r\n### Expected behavior\r\n\r\nThe dataset can be loaded successfully without the streaming setting. \r\n\r\n### Environment info\r\n\r\nLinux, \r\npython=3.9\r\ndatasets=2.12.0<end_of_turn>\n<start_of_turn>model\nThe dir name and split strings are confused when loading ArmelR/stack-exchange-instruction dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm trying to read the parquet file from the private s3 bucket using the `load_dataset` function, but I receive `TypeError: cannot pickle '_contextvars.Context' object` error\r\n\r\nI'm working on a machine with `~/.aws/credentials` file. I can't give credentials and the path to a file in a private bucket for obvious reasons, but I'll try to give all possible outputs. \n\n### Steps to reproduce the bug\n\n```python\r\nimport s3fs\r\nfrom datasets import load_dataset\r\nfrom aiobotocore.session import get_session\r\n\r\n\r\nDATA_PATH = \"s3://bucket_name/path/validation.parquet\"\r\nfs = s3fs.S3FileSystem(session=get_session())\r\n```\r\n`fs.stat` returns the data, so we can say that fs is working and we have all permissions\r\n```python\r\nfs.stat(DATA_PATH)\r\n# Returns:\r\n# {'ETag': '\"123123a-19\"',\r\n# 'LastModified': datetime.datetime(2023, 11, 1, 10, 16, 57, tzinfo=tzutc()),\r\n# 'size': 312237170,\r\n# 'name': 'bucket_name/path/validation.parquet',\r\n# 'type': 'file',\r\n# 'StorageClass': 'STANDARD',\r\n# 'VersionId': 'Abc.HtmsC9h.as',\r\n# 'ContentType': 'binary/octet-stream'}\r\n```\r\n\r\n```python\r\nfs.storage_options\r\n# Returns:\r\n# {'session': <aiobotocore.session.AioSession at 0x7f9193fa53c0>}\r\n```\r\n\r\n```python\r\nds = load_dataset(\"parquet\", data_files={\"train\": DATA_PATH}, storage_options=fs.storage_options)\r\n```\r\n\r\n<details>\r\n  <summary>Returns such error (expandable)</summary>\r\n  \r\n```python\r\n  ---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\nCell In[88], line 1\r\n----> 1 ds = load_dataset(\"parquet\", data_files={\"train\": DATA_PATH}, storage_options=fs.storage_options)\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/load.py:2153, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\r\n   2150 try_from_hf_gcs = path not in _PACKAGED_DATASETS_MODULES\r\n   2152 # Download and prepare data\r\n-> 2153 builder_instance.download_and_prepare(\r\n   2154     download_config=download_config,\r\n   2155     download_mode=download_mode,\r\n   2156     verification_mode=verification_mode,\r\n   2157     try_from_hf_gcs=try_from_hf_gcs,\r\n   2158     num_proc=num_proc,\r\n   2159     storage_options=storage_options,\r\n   2160 )\r\n   2162 # Build dataset for splits\r\n   2163 keep_in_memory = (\r\n   2164     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\r\n   2165 )\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/builder.py:954, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\r\n    952     if num_proc is not None:\r\n    953         prepare_split_kwargs[\"num_proc\"] = num_proc\r\n--> 954     self._download_and_prepare(\r\n    955         dl_manager=dl_manager,\r\n    956         verification_mode=verification_mode,\r\n    957         **prepare_split_kwargs,\r\n    958         **download_and_prepare_kwargs,\r\n    959     )\r\n    960 # Sync info\r\n    961 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/builder.py:1027, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\r\n   1025 split_dict = SplitDict(dataset_name=self.dataset_name)\r\n   1026 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\r\n-> 1027 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\r\n   1029 # Checksums verification\r\n   1030 if verification_mode == VerificationMode.ALL_CHECKS and dl_manager.record_checksums:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/packaged_modules/parquet/parquet.py:34, in Parquet._split_generators(self, dl_manager)\r\n     32 if not self.config.data_files:\r\n     33     raise ValueError(f\"At least one data file must be specified, but got data_files={self.config.data_files}\")\r\n---> 34 data_files = dl_manager.download_and_extract(self.config.data_files)\r\n     35 if isinstance(data_files, (str, list, tuple)):\r\n     36     files = data_files\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/download/download_manager.py:565, in DownloadManager.download_and_extract(self, url_or_urls)\r\n    549 def download_and_extract(self, url_or_urls):\r\n    550     \"\"\"Download and extract given `url_or_urls`.\r\n    551 \r\n    552     Is roughly equivalent to:\r\n   (...)\r\n    563         extracted_path(s): `str`, extracted paths of given URL(s).\r\n    564     \"\"\"\r\n--> 565     return self.extract(self.download(url_or_urls))\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/download/download_manager.py:420, in DownloadManager.download(self, url_or_urls)\r\n    401 def download(self, url_or_urls):\r\n    402     \"\"\"Download given URL(s).\r\n    403 \r\n    404     By default, only one process is used for download. Pass customized `download_config.num_proc` to change this behavior.\r\n   (...)\r\n    418     ```\r\n    419     \"\"\"\r\n--> 420     download_config = self.download_config.copy()\r\n    421     download_config.extract_compressed_file = False\r\n    422     if download_config.download_desc is None:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/download/download_config.py:94, in DownloadConfig.copy(self)\r\n     93 def copy(self) -> \"DownloadConfig\":\r\n---> 94     return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/site-packages/datasets/download/download_config.py:94, in <dictcomp>(.0)\r\n     93 def copy(self) -> \"DownloadConfig\":\r\n---> 94     return self.__class__(**{k: copy.deepcopy(v) for k, v in self.__dict__.items()})\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: _deepcopy_dict at line 231 (2 times), deepcopy at line 146 (2 times)]\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: deepcopy at line 146 (1 times)]\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:206, in _deepcopy_list(x, memo, deepcopy)\r\n    204 append = y.append\r\n    205 for a in x:\r\n--> 206     append(deepcopy(a, memo))\r\n    207 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:238, in _deepcopy_method(x, memo)\r\n    237 def _deepcopy_method(x, memo): # Copy instance methods\r\n--> 238     return type(x)(x.__func__, deepcopy(x.__self__, memo))\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: _deepcopy_dict at line 231 (3 times), deepcopy at line 146 (3 times), deepcopy at line 172 (3 times), _reconstruct at line 271 (2 times)]\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\n    [... skipping similar frames: _deepcopy_dict at line 231 (1 times), deepcopy at line 146 (1 times)]\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:265, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    263 if deep and args:\r\n    264     args = (deepcopy(arg, memo) for arg in args)\r\n--> 265 y = func(*args)\r\n    266 if deep:\r\n    267     memo[id(x)] = y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:264, in <genexpr>(.0)\r\n    262 deep = memo is not None\r\n    263 if deep and args:\r\n--> 264     args = (deepcopy(arg, memo) for arg in args)\r\n    265 y = func(*args)\r\n    266 if deep:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:211, in _deepcopy_tuple(x, memo, deepcopy)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:211, in <listcomp>(.0)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:172, in deepcopy(x, memo, _nil)\r\n    170                 y = x\r\n    171             else:\r\n--> 172                 y = _reconstruct(x, memo, *rv)\r\n    174 # If is its own copy, don't memoize.\r\n    175 if y is not x:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:271, in _reconstruct(x, memo, func, args, state, listiter, dictiter, deepcopy)\r\n    269 if state is not None:\r\n    270     if deep:\r\n--> 271         state = deepcopy(state, memo)\r\n    272     if hasattr(y, '__setstate__'):\r\n    273         y.__setstate__(state)\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:211, in _deepcopy_tuple(x, memo, deepcopy)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:211, in <listcomp>(.0)\r\n    210 def _deepcopy_tuple(x, memo, deepcopy=deepcopy):\r\n--> 211     y = [deepcopy(a, memo) for a in x]\r\n    212     # We're not going to put the tuple in the memo, but it's still important we\r\n    213     # check for it, in case the tuple contains recursive mutable structures.\r\n    214     try:\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:146, in deepcopy(x, memo, _nil)\r\n    144 copier = _deepcopy_dispatch.get(cls)\r\n    145 if copier is not None:\r\n--> 146     y = copier(x, memo)\r\n    147 else:\r\n    148     if issubclass(cls, type):\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:231, in _deepcopy_dict(x, memo, deepcopy)\r\n    229 memo[id(x)] = y\r\n    230 for key, value in x.items():\r\n--> 231     y[deepcopy(key, memo)] = deepcopy(value, memo)\r\n    232 return y\r\n\r\nFile ~/miniconda3/envs/test-env/lib/python3.10/copy.py:161, in deepcopy(x, memo, _nil)\r\n    159 reductor = getattr(x, \"__reduce_ex__\", None)\r\n    160 if reductor is not None:\r\n--> 161     rv = reductor(4)\r\n    162 else:\r\n    163     reductor = getattr(x, \"__reduce__\", None)\r\n\r\nTypeError: cannot pickle '_contextvars.Context' object\r\n```\r\n</details>\r\n\n\n### Expected behavior\n\nIf I choose to load the file from the public bucket with `anon=True` passed - everything works, so I expected loading from the private bucket to work as well\n\n### Environment info\n\n- `datasets` version: 2.14.6\r\n- Platform: macOS-10.16-x86_64-i386-64bit\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.19.1\r\n- PyArrow version: 14.0.1\r\n- Pandas version: 1.5.3\r\n- s3fs version: 2023.10.0\r\n- fsspec version: 2023.10.0 \r\n- aiobotocore version: 2.7.0<end_of_turn>\n<start_of_turn>model\nLoading the dataset from private S3 bucket gives \"TypeError: cannot pickle '_contextvars.Context' object\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAllow specifying [a multiprocessing context](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods) to functions that support `num_proc` or use multiprocessing pools. For example, the following could be done:\r\n\r\n```python\r\ndataset = dataset.map(_func, num_proc=2, mp_context=multiprocess.get_context(\"spawn\"))\r\n```\r\n\r\nOr at least the multiprocessing start method (\"fork\", \"spawn\", \"fork_server\" or `None`):\r\n\r\n```python\r\ndataset = dataset.map(_func, num_proc=2, mp_start_method=\"spawn\")\r\n```\r\n\r\nAnother option could be passing the `pool` as an argument.\n\n### Motivation\n\nBy default, `multiprocess` (the `multiprocessing`-fork library that this repo uses) uses the \"fork\" start method for multiprocessing pools (for the default context). It could be changed by using `set_start_method`. However, this conditions the multiprocessing start method from all processing in a Python program that uses the default context, because [you can't call that function more than once](https://docs.python.org/3/library/multiprocessing.html#contexts-and-start-methods:~:text=set_start_method()%20should%20not%20be%20used%20more%20than%20once%20in%20the%20program.). My proposal is to allow using a different multiprocessing context, not to condition the whole Python program.\r\n\r\nOne reason to change the start method is that \"fork\" (the default) makes child processes likely deadlock if thread pools were created before (and also this is not supported by POSIX). For example, this happens when using PyTorch because OpenMP threads are used for CPU intra-op parallelism, which is enabled by default (e.g., for context see [`torch.set_num_threads`](https://pytorch.org/docs/stable/generated/torch.set_num_threads.html)). This can also be fixed by setting `torch.set_num_threads(1)` (or similarly by other methods) but this conditions the whole Python program as it can only be set once to guarantee its behavior (similarly to). There are noticeable performance differences when setting this number to 1 even when using GPU(s). Using, e.g., a \"spawn\" start method would solve this issue.\r\n\r\nFor more context, see:\r\n\r\n* https://discuss.huggingface.co/t/dataset-map-stuck-with-torch-set-num-threads-set-to-2-or-larger/37984\r\n* https://discuss.huggingface.co/t/using-num-proc-1-in-dataset-map-hangs/44310\n\n### Your contribution\n\nI'd be happy to review a PR that makes such a change. And if you really don't have the bandwidth for it, I'd consider creating one.<end_of_turn>\n<start_of_turn>model\nAllow passing a multiprocessing context to functions that support `num_proc`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix ci compressionfs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am encountering an IndexError while training a T5-small model on the Squad dataset using the transformers and datasets libraries. The error occurs even with a minimal reproducible example, suggesting a potential bug or incompatibility.\n\n### Steps to reproduce the bug\n\n1.Install the required libraries: !pip install transformers datasets\r\n2.Run the following code:\r\n!pip install transformers datasets\r\n\r\nimport torch\r\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer, DataCollatorWithPadding\r\n\r\n# Load a small, publicly available dataset\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"squad\", split=\"train[:100]\") # Use a small subset for testing\r\n\r\n# Load a pre-trained model and tokenizer\r\nmodel_name = \"t5-small\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_name)\r\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)\r\n\r\n# Define a basic data collator\r\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\r\n\r\n# Define training arguments\r\ntraining_args = TrainingArguments(\r\n    output_dir=\"./results\",\r\n    per_device_train_batch_size=2,\r\n    num_train_epochs=1,\r\n)\r\n\r\n# Create a trainer\r\ntrainer = Trainer(\r\n    model=model,\r\n    args=training_args,\r\n    train_dataset=dataset,\r\n    data_collator=data_collator,\r\n)\r\n\r\n# Train the model\r\ntrainer.train()\n\n### Expected behavior\n\n---------------------------------------------------------------------------\r\nIndexError                                Traceback (most recent call last)\r\n[<ipython-input-23-f13a4b23c001>](https://localhost:8080/#) in <cell line: 34>()\r\n     32 \r\n     33 # Train the model\r\n---> 34 trainer.train()\r\n\r\n10 frames\r\n[/usr/local/lib/python3.10/dist-packages/datasets/formatting/formatting.py](https://localhost:8080/#) in _check_valid_index_key(key, size)\r\n    427     if isinstance(key, int):\r\n    428         if (key < 0 and key + size < 0) or (key >= size):\r\n--> 429             raise IndexError(f\"Invalid key: {key} is out of bounds for size {size}\")\r\n    430         return\r\n    431     elif isinstance(key, slice):\r\n\r\nIndexError: Invalid key: 42 is out of bounds for size 0\n\n### Environment info\n\ntransformers version:4.41.2\r\ndatasets version:1.18.4\r\nPython version:3.10.12\r\n<end_of_turn>\n<start_of_turn>model\nIndexError during training with Squad dataset and T5-small model<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI am working with the following demo code:\r\n```\r\nfrom datasets import load_dataset\r\nfrom datasets.features import Sequence, Value, ClassLabel, Features\r\n\r\nais_dataset = load_dataset(\"/data/ryan.gao/ais_dataset_cache/raw/1978/\")\r\nais_dataset = ais_dataset[\"train\"]\r\n\r\ndef add_class(example):\r\n    example[\"my_labeled_bbox\"] = {\"bbox\": [100,100,200,200], \"label\": \"cat\"}\r\n    return example\r\n\r\nais_dataset = ais_dataset.map(add_class, batched=False, num_proc=32)\r\nais_dataset = ais_dataset.cast_column(\"my_labeled_bbox\", Sequence(\r\n                                                        {\r\n                                                            \"bbox\": Sequence(Value(dtype=\"int64\")),\r\n                                                            \"label\": ClassLabel(names=[\"cat\", \"dog\"])\r\n                                                        }))\r\n\r\nprint(ais_dataset[0])\r\n```\r\nHowever, executing this code results in an error:\r\n\r\n```\r\n  File \"/home/protoss.gao/.local/lib/python3.9/site-packages/datasets/table.py\", line 2111, in cast_array_to_feature\r\n    raise TypeError(f\"Couldn't cast array of type\\n{array.type}\\nto\\n{feature}\")\r\nTypeError: Couldn't cast array of type\r\nint64\r\nto\r\nSequence(feature=Value(dtype='int64', id=None), length=-1, id=None)\r\n```\r\nUpon examining the source code in datasets/table.py at line 2035:\r\n\r\n```\r\n        if isinstance(feature, Sequence) and isinstance(feature.feature, dict):\r\n            feature = {\r\n                name: Sequence(subfeature, length=feature.length) for name, subfeature in feature.feature.items()\r\n            }\r\n```\r\n\r\nI noticed that if subfeature is of type Sequence, the code results in Sequence(Sequence(...), ...) and Sequence(ClassLabel(...), ...), which appears to be the source of the error.\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nrun my demo code\r\n\r\n### Expected behavior\r\n\r\nno exception\r\n\r\n### Environment info\r\n\r\npython 3.9 \r\ndatasets: 2.16.1<end_of_turn>\n<start_of_turn>model\ncast_column to Sequence(subfeatures_dict) has err<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI'm aware of the issue #5695 .\r\n\r\nI'm using a modified SDXL trainer: https://github.com/kopyl/diffusers/blob/5e70f604155aeecee254a5c63c5e4236ad4a0d3d/examples/text_to_image/train_text_to_image_sdxl.py#L1027C16-L1027C16\r\n\r\nSo i\r\n1. Map dataset\r\n2. Save to disk\r\n3. Try to upload:\r\n```\r\nimport datasets\r\nfrom datasets import load_from_disk\r\n\r\n\r\ndataset = load_from_disk(\"ds\")\r\ndatasets.config.DEFAULT_MAX_BATCH_SIZE = 1\r\ndataset.push_to_hub(\"kopyl/ds\", private=True, max_shard_size=\"500MB\")\r\n```\r\n\r\nAnd i get this error:\r\n`pyarrow.lib.ArrowNotImplementedError: Unhandled type for Arrow to Parquet schema conversion: halffloat`\r\n\r\nFull traceback:\r\n```\r\n>>> dataset.push_to_hub(\"kopyl/3M_icons_monochrome_only_no_captioning_mapped-for-SDXL-2\", private=True, max_shard_size=\"500MB\")\r\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1451/1451 [00:00<00:00, 6827.40 examples/s]\r\nUploading the dataset shards:   0%|                                                                                                                        | 0/2099 [00:00<?, ?it/s]\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/dataset_dict.py\", line 1705, in push_to_hub\r\n    split_additions, uploaded_size, dataset_nbytes = self[split]._push_parquet_shards_to_hub(\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 5208, in _push_parquet_shards_to_hub\r\n    shard.to_parquet(buffer)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\", line 4931, in to_parquet\r\n    return ParquetDatasetWriter(self, path_or_buf, batch_size=batch_size, **parquet_writer_kwargs).write()\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/io/parquet.py\", line 129, in write\r\n    written = self._write(file_obj=self.path_or_buf, batch_size=batch_size, **self.parquet_writer_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/datasets/io/parquet.py\", line 141, in _write\r\n    writer = pq.ParquetWriter(file_obj, schema=schema, **parquet_writer_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/pyarrow/parquet/core.py\", line 1016, in __init__\r\n    self.writer = _parquet.ParquetWriter(\r\n  File \"pyarrow/_parquet.pyx\", line 1869, in pyarrow._parquet.ParquetWriter.__cinit__\r\n  File \"pyarrow/error.pxi\", line 154, in pyarrow.lib.pyarrow_internal_check_status\r\n  File \"pyarrow/error.pxi\", line 91, in pyarrow.lib.check_status\r\npyarrow.lib.ArrowNotImplementedError: Unhandled type for Arrow to Parquet schema conversion: halffloat\r\n```\r\n\r\nSmaller datasets with the same way of saving and pushing work wonders. Big ones are not.\r\n\r\nI'm currently trying to upload dataset like this:\r\n`HfApi().upload_folder...`\r\nBut i'm not sure that in this case \"load_dataset\" would work well.\r\n\r\nThis setting num_shards does not help too:\r\n```\r\ndataset.push_to_hub(\"kopyl/3M_icons_monochrome_only_no_captioning_mapped-for-SDXL-2\", private=True, num_shards={'train': 500})\r\n```\r\nTried 3000, 500, 478, 100\r\n\r\nAlso do you know if it's possible to push a dataset with multiple processes? It would take an eternity pushing 1TB...\r\n\r\n### Steps to reproduce the bug\r\n\r\nDescribed above\r\n\r\n### Expected behavior\r\n\r\nShould be able to upload...\r\n\r\n### Environment info\r\n\r\nTotal dataset size: 978G\r\nAmount of `.arrow` files: 2101\r\nEach `.arrow` file size: 477M (i know 477 megabytes * 2101 does not equal 978G, but i just checked the size of a couple `.arrow` files, i don't know if some might have different size)\r\n\r\nSome files:\r\n- \"ds/train/state.json\": https://pastebin.com/tJ3ZLGAg\r\n- \"ds/train/dataset_info.json\": https://pastebin.com/JdXMQ5ih<end_of_turn>\n<start_of_turn>model\nLoading big dataset raises pyarrow.lib.ArrowNotImplementedError 2<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n**Is your feature request related to a problem? Please describe.**\r\nI am working on a project, where I would like to test different preprocessing methods for my ML-data. Thus, I would like to work a lot with revisions and compare them. Currently, I was not able to make it work with the revision keyword because it was not redownloading the data, it was reading in some cached data, until I put  `download_mode=\"force_redownload\"`, even though the reversion was different. \r\nOf course, I may have done something wrong or missed a setting somewhere! \r\n\r\n**Describe the solution you'd like**\r\nThe solution would allow me to easily work with revisions: \r\n- create a new dataset (by combining things, different preprocessing, ..) and give it a new revision (v.1.2.3), maybe like this:\r\n`dataset_audio.push_to_hub('kenfus/xy', revision='v1.0.2')`\r\n\r\n- then, get the current revision as follows:         \r\n```\r\ndataset = load_dataset(\r\n            'kenfus/xy', revision='v1.0.2',\r\n        )\r\n```\r\nthis downloads the new version and does not load in a different revision and all future map, filter, .. operations are done on this dataset and not loaded from cache produced from a different revision. \r\n- if I rerun the run, the caching should be smart enough in every step to not reuse a mapping operation on a different revision. \r\n\r\n**Describe alternatives you've considered**\r\nI created my own caching, putting `download_mode=\"force_redownload\"` and `load_from_cache_file=False,` everywhere.\r\n\r\n**Additional context**\r\nThanks a lot for your great work! Creating NLP datasets and training a model with them is really easy and straightforward with huggingface.\r\n\r\nThis is the data loading in my script:\r\n\r\n```\r\n    ## CREATE PATHS\r\n    prepared_dataset_path = os.path.join(\r\n        DATA_FOLDER, str(DATA_VERSION), \"prepared_dataset\"\r\n    )\r\n    os.makedirs(os.path.join(DATA_FOLDER, str(DATA_VERSION)), exist_ok=True)\r\n\r\n    ## LOAD DATASET\r\n    if os.path.exists(prepared_dataset_path):\r\n        print(\"Loading prepared dataset from disk...\")\r\n        dataset_prepared = load_from_disk(prepared_dataset_path)\r\n    else:\r\n        print(\"Loading dataset from HuggingFace Datasets...\")\r\n        dataset = load_dataset(\r\n            PATH_TO_DATASET, revision=DATA_VERSION, download_mode=\"force_redownload\"\r\n        )\r\n\r\n        print(\"Preparing dataset...\")\r\n        dataset_prepared = dataset.map(\r\n            prepare_dataset,\r\n            remove_columns=[\"audio\", \"transcription\"],\r\n            num_proc=os.cpu_count(),\r\n            load_from_cache_file=False,\r\n        )\r\n        dataset_prepared.save_to_disk(prepared_dataset_path)\r\n        del dataset\r\n\r\n    if CHECK_DATASET:\r\n        ## CHECK DATASET\r\n        dataset_prepared = dataset_prepared.map(\r\n            check_dimensions, num_proc=os.cpu_count(), load_from_cache_file=False\r\n        )\r\n        dataset_filtered = dataset_prepared.filter(\r\n            lambda example: not example[\"incorrect_dimension\"],\r\n            load_from_cache_file=False,\r\n        )\r\n\r\n        for example in dataset_prepared.filter(\r\n            lambda example: example[\"incorrect_dimension\"], load_from_cache_file=False\r\n        ):\r\n            print(example[\"path\"])\r\n\r\n        print(\r\n            f\"Number of examples with incorrect dimension:  {len(dataset_prepared) - len(dataset_filtered)}\"\r\n        )\r\n\r\n        print(\"Number of examples train: \", len(dataset_filtered[\"train\"]))\r\n        print(\"Number of examples test: \", len(dataset_filtered[\"test\"]))\r\n\r\n```\r\n<end_of_turn>\n<start_of_turn>model\n[Feature Request] Dataset versioning<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n- Implemented TSV-based dataset loaders:\r\n   - WMT21Dataset (local_datasets/wmt21/wmt21_dataset.py)\r\n   - WMT22Dataset (local_datasets/wmt22/wmt22_dataset.py)\r\n   These classes load source-target pairs from .tsv files for train, validation, and test splits.\r\n- Created sample dummy data for both datasets:\r\n   - dummy_data/train.tsv, dummy_data/validation.tsv, dummy_data/test.tsv\r\n   - Includes a few realistic example lines to allow CI and local tests to pass without downloading full datasets.\r\n- Added automated tests for robust validation:\r\n   - tests/test_wmt21.py and tests/test_wmt22.py\r\n   - Checks that all splits load correctly, empty lines are ignored, and the number of examples matches the number of lines in the .tsv files.\r\n   - Edge cases handled: empty lines, malformed lines, extra tabs.\r\n- Added README.md files for both datasets:\r\n   - Provides dataset structure, usage instructions, and placeholders for citation & license information.\r\n   - Ensures that other developers and reviewers can understand dataset usage immediately.\r\n- Ensured easy local testing:\r\n   - Load datasets programmatically using WMT21Dataset / WMT22Dataset.\r\n   - Verified train/validation/test splits are correctly returned as Python dictionaries of Dataset objects.\r\n\r\n\r\n-Provides initial support for WMT21 and WMT22 NLP/translation experiments.\r\n-Allows contributors and reviewers to test dataset loading locally or in CI without downloading large datasets.\r\n-Serves as a template to extend to other WMT datasets in the future.\r\n\r\nTesting instructions:\r\n```\r\n# Activate your environment\r\npytest tests/test_wmt21.py -v\r\npytest tests/test_wmt22.py -v\r\n```\r\n```\r\nfrom local_datasets.wmt21.wmt21_dataset import WMT21Dataset\r\nfrom local_datasets.wmt22.wmt22_dataset import WMT22Dataset\r\n\r\n# WMT21\r\nwmt21 = WMT21Dataset(\"local_datasets/wmt21/dummy_data\")\r\nds21 = wmt21.load()\r\nprint(ds21[\"train\"][0])\r\nprint(ds21[\"validation\"][0])\r\nprint(ds21[\"test\"][0])\r\n\r\n# WMT22\r\nwmt22 = WMT22Dataset(\"local_datasets/wmt22/dummy_data\")\r\nds22 = wmt22.load()\r\nprint(ds22[\"train\"][0])\r\nprint(ds22[\"validation\"][0])\r\nprint(ds22[\"test\"][0])\r\n```<end_of_turn>\n<start_of_turn>model\nDatasets: Add WMT21 & WMT22 loaders (basic TSV loaders, sample data, tests)<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI would like to be able to iterate (and re-iterate if needed) over a column of an `IterableDataset` instance. The following example shows the supposed API:\n```python\ndef gen():\n    yield {\"text\": \"Good\", \"label\": 0}\n    yield {\"text\": \"Bad\", \"label\": 1}\n\nds = IterableDataset.from_generator(gen)\ntexts = ds[\"text\"]\n\nfor v in texts:\n    print(v)  # Prints \"Good\" and \"Bad\"\n\nfor v in texts:\n    print(v)  # Prints \"Good\" and \"Bad\" again\n```\n\n### Motivation\n\nIn the real world problems, huge NNs like Transformer are not always the best option, so there is a need to conduct experiments with different methods. While \ud83e\udd17Datasets is perfectly adapted to \ud83e\udd17Transformers, it may be inconvenient when being used with other libraries. The ability to retrieve a particular column is the case (e.g., gensim's FastText [requires](https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText.train) only lists of strings, not dictionaries).\nWhile there are ways to achieve the desired functionality, they are not good ([forum](https://discuss.huggingface.co/t/how-to-iterate-over-values-of-a-column-in-the-iterabledataset/135649)). It would be great if there was a built-in solution.\n\n### Your contribution\n\nTheoretically, I can submit a PR, but I have very little knowledge of the internal structure of \ud83e\udd17Datasets, so some help may be needed.\nMoreover, I can only work on weekends, since I have a full-time job. However, the feature does not seem to be popular, so there is no need to implement it as fast as possible.<end_of_turn>\n<start_of_turn>model\nIterating over values of a column in the IterableDataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n__repr__ and _repr_html_ now both are similar to that of Polars <end_of_turn>\n<start_of_turn>model\nDataset representation<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.1.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\n1. I ran dataset mapping with `num_proc=6` in it and got this error:\r\n`RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method`\r\n\r\nI can't actually find a way to run multi-GPU dataset mapping. Can you help?\n\n### Steps to reproduce the bug\n\n1. Rund SDXL training with `num_proc=6`: https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image_sdxl.py\n\n### Expected behavior\n\nShould work well\n\n### Environment info\n\n6x A100 SXM, Linux<end_of_turn>\n<start_of_turn>model\nCannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI propose adding a dedicated feature to the datasets library that allows for the efficient storage and retrieval of multi-dimensional ndarray with dynamic shapes. Similar to how Image columns handle variable-sized images, this feature would provide a structured way to store array data where the dimensions are not fixed.\n\nA possible implementation could be a new Array or Tensor feature type that stores the data in a structured format, for example, \n\n```python\n{\n  \"shape\": (5, 224, 224), \n  \"dtype\": \"uint8\", \n  \"data\": [...]\n}\n```\n\nThis would allow the datasets library to handle heterogeneous array sizes within a single column without requiring a fixed shape definition in the feature schema.\n\n### Motivation\n\nI am currently trying to upload data from astronomical telescopes, specifically FITS files, to the Hugging Face Hub. This type of data is very similar to images but often has more than three dimensions. For example, data from the SDSS project contains five channels (u, g, r, i, z), and the pixel values can exceed 255, making the Pillow based Image feature unsuitable.\n\nThe current datasets library requires a fixed shape to be defined in the feature schema for multi-dimensional arrays, which is a major roadblock. This prevents me from saving my data, as the dimensions of the arrays can vary across different FITS files.\n\nhttps://github.com/huggingface/datasets/blob/985c9bee6bfc345787a8b9dd316e1d4f3b930503/src/datasets/features/features.py#L613-L614\n\nA feature that supports dynamic shapes would be incredibly beneficial for the astronomy community and other fields dealing with similar high-dimensional, variable-sized data (e.g., medical imaging, scientific simulations).\n\n### Your contribution\n\nI am willing to create a PR to help implement this feature if the proposal is accepted.<end_of_turn>\n<start_of_turn>model\nAllow saving multi-dimensional ndarray with dynamic shapes<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI'm using the datasets \r\n\r\n\r\n```\r\nfrom datasets import load_dataset, Audio\r\ndataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\r\n```\r\n\r\n\r\nAnd it seems that sometimes when I imagine a lot of users are accessing the same resources, the Dropbox host fails:\r\n\r\n`raise ConnectionError(f\"Couldn't reach {url} (error {response.status_code})\") ConnectionError: Couldn't reach https://www.dropbox.com/s/e2us0hcs3ilr20e/MInDS-14.zip?dl=1 (error 429)`\r\n\r\nMy question is if we can somehow host these files elsewhere or can you change the limit of simultaneous users accessing those resources or any other solution?\r\n\r\nAlso, has anyone had this issue before?\r\n\r\nThanks\n\n### Steps to reproduce the bug\n\n1: Create a python script like so:\r\n```\r\n\r\nfrom datasets import load_dataset, Audio\r\ndataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\")\r\n```\r\n\r\n2: Execute this by a certain number of users at the same time\n\n### Expected behavior\n\nI woudl expect that this shouldnt happen unless its a huge amount of users, which it is not the case\n\n### Environment info\n\nThis was done in an Ubuntu 22 environment.<end_of_turn>\n<start_of_turn>model\nThe datasets models housed in Dropbox can't support a lot of users downloading them<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\n.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nupdate tips in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nclose https://github.com/huggingface/datasets/issues/7458<end_of_turn>\n<start_of_turn>model\nFix data_files filtering<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nAdd push_to_hub with multiple configs docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nfsspec.callbacks.TqdmCallback (used in https://github.com/huggingface/datasets/blob/73bed12ecda17d1573fd3bf73ed5db24d3622f86/src/datasets/utils/file_utils.py#L338) was first released in fsspec [2022.3.0](https://github.com/fsspec/filesystem_spec/releases/tag/2022.3.0, commit where it was added: https://github.com/fsspec/filesystem_spec/commit/9577c8a482eb0a69092913b81580942a68d66a76#diff-906155c7e926a9ff58b9f23369bb513b09b445f5b0f41fa2a84015d0b471c68cR180), however the dependency is set to 2021.11.1 https://github.com/huggingface/datasets/blob/main/setup.py#L129\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. Install fsspec==2021.11.1\r\n2. Install latest datasets==2.14.1\r\n3. Import datasets, import fails due to lack of `fsspec.callbacks.TqdmCallback`\n\n### Expected behavior\n\nNo import issue\n\n### Environment info\n\nN/A<end_of_turn>\n<start_of_turn>model\nfsspec dependency is set too low<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am trying to fine-tune w2v-bert for ASR task. Since my dataset is so big, I preferred to use the on-the-fly method with set_transform. So i change the preprocessing function to this:\r\n```\r\ndef prepare_dataset(batch):\r\n    input_features = processor(batch[\"audio\"], sampling_rate=16000).input_features[0]\r\n    input_length = len(input_features)\r\n    labels = processor.tokenizer(batch[\"text\"], padding=False).input_ids\r\n\r\n    batch = {\r\n        \"input_features\": [input_features],\r\n        \"input_length\": [input_length],\r\n        \"labels\": [labels]\r\n    }\r\n    \r\n    return batch\r\n\r\n\r\ntrain_ds.set_transform(prepare_dataset)\r\nval_ds.set_transform(prepare_dataset)\r\n```\r\n\r\nAfter this, I also had to change the DataCollatorCTCWithPadding class like this:\r\n```\r\n@dataclass\r\nclass DataCollatorCTCWithPadding:\r\n    processor: Wav2Vec2BertProcessor\r\n    padding: Union[bool, str] = True\r\n\r\n    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\r\n        # Separate input_features and labels\r\n        input_features = [{\"input_features\": feature[\"input_features\"][0]} for feature in features]\r\n        labels = [feature[\"labels\"][0] for feature in features]\r\n\r\n        # Pad input features\r\n        batch = self.processor.pad(\r\n            input_features,\r\n            padding=self.padding,\r\n            return_tensors=\"pt\",\r\n        )\r\n\r\n        # Pad and process labels\r\n        label_features = self.processor.tokenizer.pad(\r\n            {\"input_ids\": labels},\r\n            padding=self.padding,\r\n            return_tensors=\"pt\",\r\n        )\r\n\r\n        labels = label_features[\"input_ids\"]\r\n        attention_mask = label_features[\"attention_mask\"]\r\n\r\n        # Replace padding with -100 to ignore these tokens during loss calculation\r\n        labels = labels.masked_fill(attention_mask.ne(1), -100)\r\n\r\n        batch[\"labels\"] = labels\r\n\r\n        return batch\r\n```\r\n        \r\nBut now a strange thing is happening, no matter how much I increase the batch size, the amount of V-RAM GPU usage does not change, while the number of total steps in the progress-bar (logging) changes. Is this normal or have I made a mistake?\n\n### Steps to reproduce the bug\n\ni can share my code if needed\n\n### Expected behavior\n\nEqual to the batch size value, the set_transform function is applied to the dataset and given to the model as a batch.\n\n### Environment info\n\nall updated versions<end_of_turn>\n<start_of_turn>model\nhow set_transform affects batch size?<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFollowing the polars integration :)\r\n\r\nAllow super fast processing using pandas or polars functions in `IterableDataset.map()` by adding support to pandas and polars formatting in `IterableDataset`\r\n\r\n```python\r\nimport polars as pl\r\nfrom datasets import Dataset\r\n\r\nds = Dataset.from_dict({\"i\": range(10)}).to_iterable_dataset()\r\nds = ds.with_format(\"polars\")\r\nds = ds.map(lambda df: df.with_columns(pl.col(\"i\").add(1).alias(\"i+1\")), batched=True)\r\nds = ds.with_format(None)\r\nprint(next(iter(ds)))\r\n# {'i': 0, 'i+1': 1}\r\n```\r\n\r\nIt leverages arrow's zero-copy features from/to pandas and polars.\r\n\r\nrelated to https://github.com/huggingface/datasets/issues/3444 #6762 <end_of_turn>\n<start_of_turn>model\nSupport faster processing using pandas or polars functions in `IterableDataset.map()`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI use `torchrun --standalone --nproc_per_node=2 train.py` to start training. And write the code following the [docs](https://huggingface.co/docs/datasets/process#distributed-usage). The trick about using `torch.distributed.barrier()` to only execute map at the main process doesn't always work. When I am training model, it will map twice. When I am running a test for dataset and dataloader (just print the batches), it can work. Their code about loading dataset are same.\r\n\r\nAnd on another server with 30 CPU cores, I use 2 GPUs and it can't work neither. \r\n\r\nI have tried to use `rank` and `local_rank` to check, they all didn't make sense.\r\n\r\n### Steps to reproduce the bug\r\n\r\nuse `torchrun --standalone --nproc_per_node=2 train.py` or `torchrun --standalone train.py` to run\r\n\r\nThis is my code:\r\n\r\n```python\r\n            if args.distributed and world_size > 1:\r\n                if args.local_rank > 0:\r\n                    print(f\"Rank {args.rank}: Gpu {args.gpu} waiting for main process to perform the mapping\", force=True)\r\n                    torch.distributed.barrier()\r\n                print(\"Mapping dataset\")\r\n                dataset = dataset.map(lambda x: cut_reorder_keys(x, num_stations_list=args.num_stations_list, is_pad=True, is_train=True), num_proc=8, desc=\"cut_reorder_keys\")\r\n                dataset = dataset.map(lambda x: random_shift(x, shift_range=(-160, 0), feature_scale=16), num_proc=8, desc=\"random_shift\")\r\n                dataset_test = dataset_test.map(lambda x: cut_reorder_keys(x, num_stations_list=args.num_stations_list, is_pad=True, is_train=False), num_proc=8, desc=\"cut_reorder_keys\")\r\n                if args.local_rank == 0:\r\n                    print(\"Mapping finished, loading results from main process\")\r\n                    torch.distributed.barrier()\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nOnly the main process will execute `map`, while the sub process will load cache from disk.\r\n\r\n### Environment info\r\n\r\nserver with 64 CPU cores (AMD Ryzen Threadripper PRO 5995WX 64-Cores) and 2 RTX 4090\r\n\r\n- `python==3.9.16`\r\n- `datasets==2.13.1`\r\n- `torch==2.0.1+cu117`\r\n- `22.04.1-Ubuntu`\r\n\r\nserver with 30 CPU cores (Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz) and 2 RTX 4090\r\n\r\n- `python==3.9.0`\r\n- `datasets==2.13.1`\r\n- `torch==2.0.1+cu117`\r\n- `Ubuntu 20.04`<end_of_turn>\n<start_of_turn>model\nDataset.map() execute twice when in PyTorch DDP mode<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ncontinuation of https://github.com/huggingface/datasets/pull/6431\r\n\r\nthis should fix the CI in https://github.com/huggingface/datasets/pull/6458 too<end_of_turn>\n<start_of_turn>model\nMissing DatasetNotFoundError<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nAdd `with_rank` to `Dataset.from_generator` similar to `Dataset.map` and `Dataset.filter`. \r\n\r\n### Motivation\r\n\r\nAs for `Dataset.map` and `Dataset.filter`, this is useful when creating cache files using multi-GPU, where the rank can be used to select GPU IDs. For now, rank can be added in the `gen_kwars` argument; however, this, in turn, includes the rank when computing the fingerprint.\r\n\r\n### Your contribution\r\n\r\nAdded #7199 which passes rank based on the `job_id` set by `num_proc`.<end_of_turn>\n<start_of_turn>model\nAdd with_rank to Dataset.from_generator<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nRelease: 3.0.1<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nSet dev version<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nI wrote `tokenize(examples)` function as an argument for `map` function for `IterableDataset`.\r\nIt process dictionary type `examples` as a parameter.\r\nIt is used in `train_dataset = train_dataset.map(tokenize, batched=True)`\r\nNo error is raised.\r\n\r\nAnd then, I found some unnecessary keys and values in `examples` so I added `input_columns` argument to `map` function to select keys and values.\r\nIt gives me an error saying\r\n```\r\nTypeError: tokenize() takes 1 positional argument but 3 were given.\r\n```\r\n\r\nThe code below matters.\r\nhttps://github.com/huggingface/datasets/blob/406b2212263c0d33f267e35b917f410ff6b3bc00/src/datasets/iterable_dataset.py#L687\r\n\r\nFor example, `inputs = {\"a\":1, \"b\":2, \"c\":3}`.\r\nIf `self.input_coluns` is `None`,\r\n`inputs` is a dictionary type variable and `function_args` becomes a `list` of a single `dict` variable.\r\n`function_args` becomes `[{\"a\":1, \"b\":2, \"c\":3}]`\r\n\r\nOtherwise, lets say `self.input_columns = [\"a\", \"c\"]`\r\n`[inputs[col] for col in self.input_columns]` results in `[1, 3]`.\r\nI think it should be `[{\"a\":1, \"c\":3}]`.\r\n\r\nI want to ask if the resulting format is intended.\r\nMaybe I can modify `tokenize()` to have 2 parameters in this case instead of having 1 dictionary.\r\nBut this is confusing to me.\r\nOr it should be fixed as `[{col:inputs[col] for col in self.input_columns}]`\r\n\r\n### Steps to reproduce the bug\r\n\r\nRun `map` function of `IterableDataset` with `input_columns` argument.\r\n\r\n### Expected behavior\r\n\r\n`function_args` looks better to have same format.\r\nI think it should be `[{\"a\":1, \"c\":3}]`.\r\n\r\n### Environment info\r\n\r\ndataset version: 2.12\r\npython: 3.8<end_of_turn>\n<start_of_turn>model\nArgument type for map function changes when using `input_columns` for `IterableDataset`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\n```\r\n(datasets) mruserbox@guru-X99:/media/10TB_HHD/_LLM_DATASETS$ python dataset.py\r\nDownloading readme: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 360/360 [00:00<00:00, 2.16MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 65.1M/65.1M [00:19<00:00, 3.38MB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6.35k/6.35k [00:00<00:00, 20.7kB/s]\r\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 7.29M/7.29M [00:01<00:00, 3.99MB/s]\r\nDownloading data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:21<00:00,  7.14s/it]\r\nExtracting data files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 1624.23it/s]\r\nGenerating train split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 314294/314294 [00:00<00:00, 668186.58 examples/s]\r\nGenerating validation split: 120 examples [00:00, 100422.28 examples/s]\r\nGenerating test split: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 34922/34922 [00:00<00:00, 754683.41 examples/s]\r\nTraceback (most recent call last):\r\n  File \"/media/10TB_HHD/_LLM_DATASETS/dataset.py\", line 3, in <module>\r\n    dataset = load_dataset(\"Hyperspace-Technologies/scp-wiki-text\")\r\n  File \"/home/mruserbox/miniconda3/envs/datasets/lib/python3.10/site-packages/datasets/load.py\", line 2153, in load_dataset\r\n    builder_instance.download_and_prepare(\r\n  File \"/home/mruserbox/miniconda3/envs/datasets/lib/python3.10/site-packages/datasets/builder.py\", line 954, in download_and_prepare\r\n    self._download_and_prepare(\r\n  File \"/home/mruserbox/miniconda3/envs/datasets/lib/python3.10/site-packages/datasets/builder.py\", line 1067, in _download_and_prepare\r\n    verify_splits(self.info.splits, split_dict)\r\n  File \"/home/mruserbox/miniconda3/envs/datasets/lib/python3.10/site-packages/datasets/utils/info_utils.py\", line 93, in verify_splits\r\n    raise UnexpectedSplits(str(set(recorded_splits) - set(expected_splits)))\r\ndatasets.utils.info_utils.UnexpectedSplits: {'validation'}\r\n```\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nName:\r\n`dataset.py`\r\n\r\nCode:\r\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset(\"Hyperspace-Technologies/scp-wiki-text\")\r\n```\r\n\r\n### Expected behavior\r\n\r\nRun without errors\r\n\r\n### Environment info\r\n\r\n```\r\nname: datasets\r\nchannels:\r\n  - defaults\r\ndependencies:\r\n  - _libgcc_mutex=0.1=main\r\n  - _openmp_mutex=5.1=1_gnu\r\n  - bzip2=1.0.8=h7b6447c_0\r\n  - ca-certificates=2023.08.22=h06a4308_0\r\n  - ld_impl_linux-64=2.38=h1181459_1\r\n  - libffi=3.4.4=h6a678d5_0\r\n  - libgcc-ng=11.2.0=h1234567_1\r\n  - libgomp=11.2.0=h1234567_1\r\n  - libstdcxx-ng=11.2.0=h1234567_1\r\n  - libuuid=1.41.5=h5eee18b_0\r\n  - ncurses=6.4=h6a678d5_0\r\n  - openssl=3.0.12=h7f8727e_0\r\n  - python=3.10.13=h955ad1f_0\r\n  - readline=8.2=h5eee18b_0\r\n  - setuptools=68.0.0=py310h06a4308_0\r\n  - sqlite=3.41.2=h5eee18b_0\r\n  - tk=8.6.12=h1ccaba5_0\r\n  - wheel=0.41.2=py310h06a4308_0\r\n  - xz=5.4.2=h5eee18b_0\r\n  - zlib=1.2.13=h5eee18b_0\r\n  - pip:\r\n      - aiohttp==3.8.6\r\n      - aiosignal==1.3.1\r\n      - async-timeout==4.0.3\r\n      - attrs==23.1.0\r\n      - certifi==2023.7.22\r\n      - charset-normalizer==3.3.2\r\n      - click==8.1.7\r\n      - datasets==2.14.6\r\n      - dill==0.3.7\r\n      - filelock==3.13.1\r\n      - frozenlist==1.4.0\r\n      - fsspec==2023.10.0\r\n      - huggingface-hub==0.19.0\r\n      - idna==3.4\r\n      - multidict==6.0.4\r\n      - multiprocess==0.70.15\r\n      - numpy==1.26.1\r\n      - openai==0.27.8\r\n      - packaging==23.2\r\n      - pandas==2.1.3\r\n      - pip==23.3.1\r\n      - platformdirs==4.0.0\r\n      - pyarrow==14.0.1\r\n      - python-dateutil==2.8.2\r\n      - pytz==2023.3.post1\r\n      - pyyaml==6.0.1\r\n      - requests==2.31.0\r\n      - six==1.16.0\r\n      - tomli==2.0.1\r\n      - tqdm==4.66.1\r\n      - typer==0.9.0\r\n      - typing-extensions==4.8.0\r\n      - tzdata==2023.3\r\n      - urllib3==2.0.7\r\n      - xxhash==3.4.1\r\n      - yarl==1.9.2\r\nprefix: /home/mruserbox/miniconda3/envs/datasets\r\n```<end_of_turn>\n<start_of_turn>model\ndataset = load_dataset(\"Hyperspace-Technologies/scp-wiki-text\") not working<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix ci compressionfs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix https://discuss.huggingface.co/t/attributeerror-enter/60509<end_of_turn>\n<start_of_turn>model\nFix time measuring snippet in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nadd an s to be consistent with pandas' on_bad_lines<end_of_turn>\n<start_of_turn>model\ntypo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI encounter bug when running the example command line \r\n\r\n```python\r\n    python main.py \\\r\n    --model decapoda-research/llama-7b-hf \\\r\n    --prune_method wanda \\\r\n    --sparsity_ratio 0.5 \\\r\n    --sparsity_type unstructured \\\r\n    --save out/llama_7b/unstructured/wanda/ \r\n```\r\n\r\nThe bug occurred at these lines of code (when loading c4 dataset) \r\n\r\n```python\r\ntraindata = load_dataset('allenai/c4', 'allenai--c4', data_files={'train': 'en/c4-train.00000-of-01024.json.gz'}, split='train')\r\nvaldata = load_dataset('allenai/c4', 'allenai--c4', data_files={'validation': 'en/c4-validation.00000-of-00008.json.gz'}, split='validation')\r\n```\r\n\r\nThe error message states:\r\n```    \r\nraise ExpectedMoreSplits(str(set(expected_splits) - set(recorded_splits)))                                                                           \r\ndatasets.utils.info_utils.ExpectedMoreSplits: {'validation'}\r\n```\r\n\r\n\n\n### Steps to reproduce the bug\n\n1. I encounter bug when running the example command line \r\n\n\n### Expected behavior\n\nThe error message states:\r\n```    \r\nraise ExpectedMoreSplits(str(set(expected_splits) - set(recorded_splits)))                                                                           \r\ndatasets.utils.info_utils.ExpectedMoreSplits: {'validation'}\r\n```\r\n\r\n\n\n### Environment info\n\nI'm using cuda 12.4, so I use ```pip install pytorch``` instead of conda provided in install.md\r\n\r\nAlso, I've tried another environment using the same commands in install.md, but the same bug occured <end_of_turn>\n<start_of_turn>model\nExpectedMoreSplits error when loading C4 dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi all,\r\n\r\nafter an update of the `datasets` library, we observer crashes in our code. We relied on `datasets.list_datasets` returning a `list`. Now, after the API of the HfApi.list_datasets was changed and it returns a `list` instead of an `Iterable`, the `datasets.list_datasets` now sometimes returns a `list` and somesimes an `Iterable`. \r\n\r\nIt would be helpful to indicate that by the return type of the `datasets.list_datasets` function.\r\n\r\nThanks,\r\nMartin \n\n### Steps to reproduce the bug\n\nHere, the code crashed after we updated the `datasets` library:\r\n\r\n```python\r\n# list_datasets no longer returns a list, which leads to an error when one tries to slice it\r\nfor datasets.list_datasets(with_details=True)[:limit]:\r\n    ...\r\n```\n\n### Expected behavior\n\nIt would be helpful to indicate that by the return type of the `datasets.list_datasets` function.\n\n### Environment info\n\nUbuntu 22.04\r\ndatasets 2.12.0<end_of_turn>\n<start_of_turn>model\nBreaking API change in datasets.list_datasets caused by change in HfApi.list_datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nList of formats and libraries we can use to load the data in `datasets`:\n\n- [ ] DICOMs: pydicom\n- [x] NIfTIs: nibabel\n- [ ] WFDB: wfdb\n\ncc @zaRizk7 for viz\n\nFeel free to comment / suggest other formats and libs you'd like to see or to share your interest in one of the mentioned format<end_of_turn>\n<start_of_turn>model\nSupport scientific data formats<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFollowing this tutorial locally using a macboko and VSCode: https://huggingface.co/docs/diffusers/en/tutorials/basic_training\r\nThis line of code: for i, image in enumerate(dataset[:4][\"image\"]):\r\nthrows: ImportError: To support decoding images, please install 'Pillow'.\r\n\r\nPillow is installed.\n\n### Steps to reproduce the bug\n\nRun the tutorial\r\n\n\n### Expected behavior\n\nImages should be rendered\n\n### Environment info\n\nMacBook, VSCode<end_of_turn>\n<start_of_turn>model\nImportError: To support decoding images, please install 'Pillow'.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI have a list of dictionaries with valid/JSON-serializable values. \r\n\r\nOne key is the denominator for a paragraph. In 99.9% of cases its a number, but there are some occurences of '1a', '2b' and so on.\r\n\r\nIf trying to convert this list to a dataset with `Dataset.from_list()`, I always get\r\n`ArrowInvalid: Could not convert '1' with type str: tried to convert to int64`, presumably because pyarrow tries to convert the keys to integers.\r\n\r\nIs there any way to circumvent this and fix dtypes? I didn't find anything in the documentation.\n\n### Steps to reproduce the bug\n\n* create a list of dicts with one key being a string of an integer for the first few thousand occurences and try to convert to dataset.\r\n\n\n### Expected behavior\n\nThere shouldn't be an error (e.g. some flag to turn off automatic str to numeric conversion).\n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.0-84-generic-x86_64-with-glibc2.35\r\n- Python version: 3.9.18\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.1<end_of_turn>\n<start_of_turn>model\nConversion to Arrow fails due to wrong type heuristic<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\r\n\r\nFollowing the (Cache management)[https://huggingface.co/docs/datasets/en/cache] docu and previous behaviour from datasets version 2.18.0, one is able to change the cache directory. Previously, all downloaded/extracted/etc files were found in this folder. As i have recently update to the latest version this is not the case anymore. Downloaded files are stored in `~/.cache/huggingface/hub`.\r\nProviding the `cache_dir` argument in `load_dataset` the cache directory is created and there are some files but the bulk is still in `~/.cache/huggingface/hub`.\r\n\r\nI believe this could be solved by adding the cache_dir argument [here](https://github.com/huggingface/datasets/blob/fdda5585ab18ea1292547f36c969d12c408ab842/src/datasets/utils/file_utils.py#L188)\r\n\r\n\r\n### Steps to reproduce the bug\r\n\r\nFor example using https://huggingface.co/datasets/ashraq/esc50:\r\n```python\r\nfrom datasets import load_dataset\r\nds = load_dataset(\"ashraq/esc50\", \"default\", cache_dir=\"~/custom/cache/path/esc50\")\r\n```\r\n\r\n\r\n### Expected behavior\r\n\r\nI would expect the bulk of files related to the dataset to be stored somewhere in `~/custom/cache/path/esc50`, but it seems they are in `~/.cache/huggingface/hub/datasets--ashraq--esc50`. \r\n\r\n### Environment info\r\n\r\n- `datasets` version: 3.2.0\r\n- Platform: Linux-5.14.0-503.15.1.el9_5.x86_64-x86_64-with-glibc2.34\r\n- Python version: 3.10.14\r\n- `huggingface_hub` version: 0.26.5\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1<end_of_turn>\n<start_of_turn>model\nUnexpected cache behaviour using load_dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6796.<end_of_turn>\n<start_of_turn>model\nFix CI test_load_dataset_distributed_with_script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes huggingface/datasets/issues/6598\r\n\r\nI've added a new test case and a solution. Before applying the solution the test case was failing with the same error described in the linked issue.\r\n\r\nMRE:\r\n```\r\npip install \"datasets[s3]\"\r\npython -c \"from datasets import load_dataset; load_dataset('csv', data_files={'train': 's3://noaa-gsod-pds/2024/A5125600451.csv'})\"\r\n```<end_of_turn>\n<start_of_turn>model\nFix load_dataset for data_files with protocols other than HF<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis adds support for creating a new branch when pushing a dataset to the hub.  Tested both methods locally and branches are created.<end_of_turn>\n<start_of_turn>model\nSupport creating new branch in push_to_hub<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI see this error all the time and it was starting to get to me.<end_of_turn>\n<start_of_turn>model\nfix grammar in fingerprint.py<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen loading a HF dataset in streaming mode and removing some columns, it is impossible to load a sample if the audio contains more than one channel. I have the impression that the time axis and channels are swapped or concatenated.\n\n### Steps to reproduce the bug\n\nMinimal error code:\r\n```python\r\nfrom datasets import load_dataset\r\n\r\ndataset_name = \"zinc75/Vibravox_dummy\"\r\nconfig_name = \"BWE_Larynx_microphone\"\r\n# if we use \"ASR_Larynx_microphone\" subset which is a monochannel audio, no error is thrown.\r\n\r\ndataset = load_dataset(\r\n    path=dataset_name, name=config_name, split=\"train\", streaming=True\r\n)\r\n\r\n\r\ndataset = dataset.remove_columns([\"sensor_id\"])\r\n#  dataset = dataset.map(lambda x:x, remove_columns=[\"sensor_id\"])\r\n# The commented version does not produce an error, but loses the dataset features.\r\nsample = next(iter(dataset))\r\n\r\n\r\n```\r\n\r\n\r\nError:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/julien/Bureau/github/vibravox/tmp.py\", line 15, in <module>\r\n    sample = next(iter(dataset))\r\n             ^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1392, in __iter__\r\n    example = _apply_feature_types_on_example(\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/datasets/iterable_dataset.py\", line 1080, in _apply_feature_types_on_example\r\n    encoded_example = features.encode_example(example)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/datasets/features/features.py\", line 1889, in encode_example\r\n    return encode_nested_example(self, example)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/datasets/features/features.py\", line 1244, in encode_nested_example\r\n    {k: encode_nested_example(schema[k], obj.get(k), level=level + 1) for k in schema}\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/datasets/features/features.py\", line 1244, in <dictcomp>\r\n    {k: encode_nested_example(schema[k], obj.get(k), level=level + 1) for k in schema}\r\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/datasets/features/features.py\", line 1300, in encode_nested_example\r\n    return schema.encode_example(obj) if obj is not None else None\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/datasets/features/audio.py\", line 98, in encode_example\r\n    sf.write(buffer, value[\"array\"], value[\"sampling_rate\"], format=\"wav\")\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/soundfile.py\", line 343, in write\r\n    with SoundFile(file, 'w', samplerate, channels,\r\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/soundfile.py\", line 658, in __init__\r\n    self._file = self._open(file, mode_int, closefd)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/home/julien/.pyenv/versions/vibravox/lib/python3.11/site-packages/soundfile.py\", line 1216, in _open\r\n    raise LibsndfileError(err, prefix=\"Error opening {0!r}: \".format(self.name))\r\nsoundfile.LibsndfileError: Error opening <_io.BytesIO object at 0x7fd795d24680>: Format not recognised.\r\n\r\nProcess finished with exit code 1\r\n```\n\n### Expected behavior\n\nI would expect this code to run without error.\n\n### Environment info\n\n- `datasets` version: 2.18.0\r\n- Platform: Linux-6.5.0-21-generic-x86_64-with-glibc2.35\r\n- Python version: 3.11.0\r\n- `huggingface_hub` version: 0.21.3\r\n- PyArrow version: 15.0.0\r\n- Pandas version: 2.2.1\r\n- `fsspec` version: 2023.10.0<end_of_turn>\n<start_of_turn>model\n`remove_columns` method used with a streaming enable dataset mode produces a LibsndfileError on multichannel audio<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\ngot `NotADirectoryError` whtn loading gigawords dataset\n\n### Steps to reproduce the bug\n\nWhen running\r\n```\r\nimport datasets\r\ndatasets.load_dataset('gigaword')\r\n```\r\n\r\nGot the following exception:\r\n```bash\r\nTraceback (most recent call last):                                                                                                    [0/1862]\r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/builder.py\", line 1629, in _prepare_split_single          \r\n    for key, record in generator:                  \r\n  File \"/home/x/.cache/huggingface/modules/datasets_modules/datasets/gigaword/ea83a8b819190acac5f2dae011fad51dccf269a0604ec5dd24795b\r\n64efb424b6/gigaword.py\", line 115, in _generate_examples            \r\n    with open(src_path, encoding=\"utf-8\") as f_d, open(tgt_path, encoding=\"utf-8\") as f_s:\r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/streaming.py\", line 71, in wrapper\r\n    return function(*args, use_auth_token=use_auth_token, **kwargs)\r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/download/streaming_download_manager.py\", line 493, in xope\r\nn                                     \r\n    return open(main_hop, mode, *args, **kwargs)                                     \r\nNotADirectoryError: [Errno 20] Not a directory: '/home/x/.cache/huggingface/datasets/downloads/6da52431bb5124d90cf51a0187d2dbee9046e\r\n89780c4be7599794a4f559048ec/org_data/train.src.txt'\r\n                                              \r\nThe above exception was the direct cause of the following exception:\r\n                                                                                                                        \r\nTraceback (most recent call last):        \r\n  File \"gigaword.py\", line 38, in <module>                                                                                        \r\n    main()                     \r\n  File \"gigaword.py\", line 35, in main                                                                                              \r\n    train, dev, test = dataset.generate_k_shot_data(k=32, seed=seed, path=\"../data/\")\r\n  File \"/home/x/MICL/preprocess/fewshot_gym_dataset.py\", line 199, in generate_k_shot_data                                \r\n    dataset = self.load_dataset()                               \r\n  File \"gigaword.py\", line 29, in load_dataset                                                                               \r\n    return datasets.load_dataset('gigaword')                \r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/load.py\", line 1809, in load_dataset            \r\n    builder_instance.download_and_prepare(                                               \r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/builder.py\", line 909, in download_and_prepare\r\n    self._download_and_prepare(                                                                                                               \r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/builder.py\", line 1670, in _download_and_prepare\r\n    super()._download_and_prepare(                                                                                  \r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/builder.py\", line 1004, in _download_and_prepare\r\n    self._prepare_split(split_generator, **prepare_split_kwargs)                                                                              \r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/builder.py\", line 1508, in _prepare_split\r\n    for job_id, done, content in self._prepare_split_single(                                                \r\n  File \"/home/x/.conda/envs/dataproc/lib/python3.8/site-packages/datasets/builder.py\", line 1665, in _prepare_split_single          \r\n    raise DatasetGenerationError(\"An error occurred while generating the dataset\") from e\r\ndatasets.builder.DatasetGenerationError: An error occurred while generating the dataset\r\n```\r\n\n\n### Expected behavior\n\nDownload and process the dataset successfully\n\n### Environment info\n\n- `datasets` version: 2.13.1\r\n- Platform: Linux-5.0.0-1032-azure-x86_64-with-glibc2.10\r\n- Python version: 3.8.0\r\n- Huggingface_hub version: 0.15.1\r\n- PyArrow version: 12.0.1\r\n- Pandas version: 2.0.3\r\n<end_of_turn>\n<start_of_turn>model\nNotADirectoryError when loading gigawords<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee: https://github.com/huggingface/datasets/actions/runs/6730567226/job/18293518039\r\n```\r\nFAILED tests/test_table.py::test_cast_sliced_fixed_size_array_to_features - TypeError: Couldn't cast array of type\r\nfixed_size_list<item: int32>[3]\r\nto\r\nSequence(feature=Value(dtype='int64', id=None), length=3, id=None)\r\n```<end_of_turn>\n<start_of_turn>model\nCI is broken: TypeError: Couldn't cast array<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nHello, I was downloading the following dataset and after 20% of data was downloaded, I started getting error 429. It is not resolved since a few days. How should I resolve it?\r\nThanks\r\n\r\nDataset:\r\nhttps://huggingface.co/datasets/cerebras/SlimPajama-627B\r\n\r\nError:\r\n`requests.exceptions.HTTPError: 429 Client Error: Too Many Requests for url: https://huggingface.co/datasets/cerebras/SlimPajama-627B/resolve/2d0accdd58c5d5511943ca1f5ff0e3eb5e293543/train/chunk1/example_train_3300.jsonl.zst`\r\n<end_of_turn>\n<start_of_turn>model\n429 Client Error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #5677 <end_of_turn>\n<start_of_turn>model\nFix cast for dictionaries with no keys<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFollowing this tutorial: https://huggingface.co/docs/diffusers/en/tutorials/basic_training and running it locally using VSCode on my MacBook.  The first line in the tutorial fails:  from datasets import load_dataset\r\ndataset = load_dataset('huggan/smithsonian_butterflies_subset', split=\"train\"). with this error: \r\nA module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2. and ImportError: numpy.core.multiarray failed to import.   \r\n\r\nDoes from datasets import load_dataset really use NumPy 1.x?  \n\n### Steps to reproduce the bug\n\nOpen VSCode.  create a new venv.  Create a new ipynb file.  Import pip install diffusers[training] try to run this line of code: from datasets import load_dataset\n\n### Expected behavior\n\ndata is loaded\n\n### Environment info\n\nran this: datasets-cli env\r\nand got A module that was compiled using NumPy 1.x cannot be run in\r\nNumPy 2.0.2 as it may crash. To support both 1.x and 2.x\r\nversions of NumPy, modules must be compiled with NumPy 2.0.\r\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\r\n\r\nIf you are a user of the module, the easiest solution will be to\r\ndowngrade to 'numpy<2' or try to upgrade the affected module.\r\nWe expect that some modules will need time to support NumPy 2.<end_of_turn>\n<start_of_turn>model\nA module that was compiled using NumPy 1.x cannot be run in NumPy 2.0.2 as it may crash. To support both 1.x and 2.x versions of NumPy, modules must be compiled with NumPy 2.0. Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nPreserve column order when loading from a JSON file with a list of dict (or with a field containing a list of dicts).\r\n\r\nAdditionally, support JSON file with a list of strings field.\r\n\r\nFix #6913.<end_of_turn>\n<start_of_turn>model\nPreserve JSON column order and support list of strings field<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nImproved the tutorial by letting readers know about loading datasets with common files and including a link. I left the local files section alone because the methods were already listed with code snippets.<end_of_turn>\n<start_of_turn>model\nImproved the tutorial by adding a link for loading datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nProposes adding uv to installation docs (see Slack thread [here](https://huggingface.slack.com/archives/C01N44FJDHT/p1737377177709279) for more context) if you're interested!<end_of_turn>\n<start_of_turn>model\n[docs] uv install<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTODO:\r\n- [x] remplace fixtures based on script with no-script fixtures\r\n- [x] windaube<end_of_turn>\n<start_of_turn>model\nRemove scripts altogether<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n`xxx if xxx is not None else None` is no-op.<end_of_turn>\n<start_of_turn>model\nDrop redundant None guard.<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nI want to add my custom dataset in huggingface dataset. Please guide me how to achieve that.<end_of_turn>\n<start_of_turn>model\nHow can I add my custom data to huggingface datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6025 <end_of_turn>\n<start_of_turn>model\nDelete `task_templates` in `IterableDataset` when they are no longer valid<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAllow `storage_options` to be passed in\n1. `cast` related operations (e.g., `cast_columns, cast`)\n2. `info` related reading (e.g., `from_dict, from_pandas, from_polars`) together with `info.features`\n```python3\nimport datasets\n\nimage_path = \"s3://bucket/sample.png\"\ndataset = datasets.Dataset.from_dict({\"image_path\": [image_path]})\n\n# dataset = dataset.cast_column(\"image_path\", datasets.Image())  # now works without `storage_options`\n\n# expected behavior\ndataset = dataset.cast_column(\"image_path\", datasets.Image(), storage_options={\"anon\": True})\n```\n\n### Motivation\n\nI'm using my own registered fsspec filesystem (s3 with customized local cache support). I need to pass cache folder paths `cache_dirs: list[str]` to the filesystem when I read the remote images (cast from file_paths).\n\n### Your contribution\n\nCould help with a PR at weekends<end_of_turn>\n<start_of_turn>model\ncast columns to Image/Audio/Video with `storage_options`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix tests<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\r\n\r\nSupport NumPy 2.0.\r\n\r\n### Motivation\r\n\r\nNumPy introduces the Array API, which bridges the gap between machine learning libraries.  Many clients of HuggingFace are eager to start using the Array API.\r\n\r\nBesides that, NumPy 2 provides a cleaner interface than NumPy 1.\r\n\r\n### Tasks\r\n\r\nNumPy 2.0 was released for testing so that libraries could ensure compatibility [since mid-March](https://github.com/numpy/numpy/issues/24300#issuecomment-1986815755).  What needs to be done for HuggingFace to support Numpy 2?\r\n\r\n- [x] Fix use of `array`: https://github.com/huggingface/datasets/pull/6976\r\n- [ ] Remove [NumPy version limit](https://github.com/huggingface/datasets/pull/6975): https://github.com/huggingface/datasets/pull/6991<end_of_turn>\n<start_of_turn>model\nSupport NumPy 2.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCasting features with Audio for numpy arrays - done here with `ds.map(gen_sine, features=features)` fails\nin version 4.0.0 but not in version 3.6.0\n\n\n### Steps to reproduce the bug\n\nThe following `uv script` should be able to reproduce the bug in version 4.0.0\nand pass in version 3.6.0 on a macOS Sequoia 15.5\n\n```python\n# /// script\n# requires-python = \">=3.13\"\n# dependencies = [\n#   \"datasets[audio]==4.0.0\",\n#   \"librosa>=0.11.0\",\n# ]\n# ///\n# NAME\n#   create_audio_dataset.py - create an audio dataset of sine waves\n#\n# SYNOPSIS\n#   uv run create_audio_dataset.py\n#\n# DESCRIPTION\n#   Create an audio dataset using the Hugging Face [datasets] library.\n#   Illustrates how to create synthetic audio datasets using the [map]\n#   datasets function.\n#\n#   The strategy is to first create a dataset with the input to the\n#   generation function, then execute the map function that generates\n#   the result, and finally cast the final features.\n#\n# BUG\n#   Casting features with Audio for numpy arrays -\n#   done here with `ds.map(gen_sine, features=features)` fails\n#   in version 4.0.0 but not in version 3.6.0\n#\n#   This happens both in cases where --extra audio is provided and where is not.\n#   When audio is not provided i've installed the latest compatible version\n#   of soundfile.\n#\n#   The error when soundfile is installed but the audio --extra is not\n#   indicates that the array values do not have the `.T` property,\n#   whilst also indicating that the value is a list instead of a numpy array.\n#\n#   Last lines of error report when for datasets + soundfile case\n#   ...\n#\n#      File \"/Users/luasantilli/.cache/uv/archive-v0/tc_5IhQe7Zpw8ZXgQWpnl/lib/python3.13/site-packages/datasets/features/audio.py\", line 239, in cast_storage\n#          storage = pa.array([Audio().encode_example(x) if x is not None else None for x in storage.to_pylist()])\n#                              ~~~~~~~~~~~~~~~~~~~~~~^^^\n#        File \"/Users/luasantilli/.cache/uv/archive-v0/tc_5IhQe7Zpw8ZXgQWpnl/lib/python3.13/site-packages/datasets/features/audio.py\", line 122, in encode_example\n#          sf.write(buffer, value[\"array\"].T, value[\"sampling_rate\"], format=\"wav\")\n#                           ^^^^^^^^^^^^^^^^\n#      AttributeError: 'list' object has no attribute 'T'\n#   ...\n#\n#   For the case of datasets[audio] without explicit adding soundfile I get an FFmpeg\n#   error.\n#\n#   Last lines of error report:\n#\n#   ...\n#     RuntimeError: Could not load libtorchcodec. Likely causes:\n#               1. FFmpeg is not properly installed in your environment. We support\n#                  versions 4, 5, 6 and 7.\n#               2. The PyTorch version (2.7.1) is not compatible with\n#                  this version of TorchCodec. Refer to the version compatibility\n#                  table:\n#                  https://github.com/pytorch/torchcodec?tab=readme-ov-file#installing-torchcodec.\n#               3. Another runtime dependency; see exceptions below.\n#             The following exceptions were raised as we tried to load libtorchcodec:\n#\n#     [start of libtorchcodec loading traceback]\n#     FFmpeg version 7: dlopen(/Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder7.dylib, 0x0006): Library not loaded: @rpath/libavutil.59.dylib\n#       Referenced from: <6DB21246-F28A-31A6-910A-D8F3355D1064> /Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder7.dylib\n#       Reason: no LC_RPATH's found\n#     FFmpeg version 6: dlopen(/Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder6.dylib, 0x0006): Library not loaded: @rpath/libavutil.58.dylib\n#       Referenced from: <BD3B44FC-E14B-3ABF-800F-BB54B6CCA3B1> /Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder6.dylib\n#       Reason: no LC_RPATH's found\n#     FFmpeg version 5: dlopen(/Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder5.dylib, 0x0006): Library not loaded: @rpath/libavutil.57.dylib\n#       Referenced from: <F06EBF8A-238C-3A96-BFBB-B34E0BBDABF0> /Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder5.dylib\n#       Reason: no LC_RPATH's found\n#     FFmpeg version 4: dlopen(/Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder4.dylib, 0x0006): Library not loaded: @rpath/libavutil.56.dylib\n#       Referenced from: <6E59F017-C703-3AF6-A271-6277DD5F8170> /Users/luasantilli/.cache/uv/archive-v0/RK3IAlGfiICwDkHm2guLC/lib/python3.13/site-packages/torchcodec/libtorchcodec_decoder4.dylib\n#       Reason: no LC_RPATH's found\n#   ...\n#\n#   This is strange because the the same error does not happen when using version 3.6.0 with datasets[audio].\n#\n#   The same error appears in python3.12\n## Imports\nimport numpy as np\nfrom datasets import Dataset, Features, Audio, Value\n\n## Parameters\nNUM_WAVES = 128\nSAMPLE_RATE = 16_000\nDURATION = 1.0\n\n## Input dataset arguments\nfreqs = np.linspace(100, 2000, NUM_WAVES).tolist()\nds = Dataset.from_dict({\"frequency\": freqs})\n\n## Features for the final dataset\nfeatures = Features(\n    {\"frequency\": Value(\"float32\"), \"audio\": Audio(sampling_rate=SAMPLE_RATE)}\n)\n\n\n## Generate audio sine waves and cast features\ndef gen_sine(example):\n    t = np.linspace(0, DURATION, int(SAMPLE_RATE * DURATION), endpoint=False)\n    wav = np.sin(2 * np.pi * example[\"frequency\"] * t)\n    return {\n        \"frequency\": example[\"frequency\"],\n        \"audio\": {\"array\": wav, \"sampling_rate\": SAMPLE_RATE},\n    }\n\n\nds = ds.map(gen_sine, features=features)\n\nprint(ds)\nprint(ds.features)\n```\n\n### Expected behavior\n\nI expect the result of version `4.0.0` to be the same of that in version `3.6.0`. Switching the value\nof the script above to `3.6.0` I get the following, expected, result: \n\n```\n$ uv run bug_report.py\nMap: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 128/128 [00:00<00:00, 204.58 examples/s]\nDataset({\n    features: ['frequency', 'audio'],\n    num_rows: 128\n})\n{'frequency': Value(dtype='float32', id=None), 'audio': Audio(sampling_rate=16000, mono=True, decode=True, id=None)}\n```\n\n### Environment info\n\n- `datasets` version: 4.0.0\n- Platform: macOS-15.5-arm64-arm-64bit-Mach-O\n- Python version: 3.13.1\n- `huggingface_hub` version: 0.33.4\n- PyArrow version: 20.0.0\n- Pandas version: 2.3.1\n- `fsspec` version: 2025.3.0<end_of_turn>\n<start_of_turn>model\nFail to cast Audio feature for numpy arrays in datasets 4.0.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix push_to_hub by not calling create_branch if PR branch (e.g. `refs/pr/1`).\r\n\r\nNote that currently create_branch raises a 400 Bad Request error if the user passes a PR branch (e.g. `refs/pr/1`).\r\n\r\nEDIT:\r\n~~Fix push_to_hub by not calling create_branch if branch exists.~~\r\n\r\nNote that currently create_branch raises a 403 Forbidden error even if all these conditions are met:\r\n- exist_ok is passed\r\n- the branch already exists\r\n- the user does not have WRITE permission\r\n\r\nFix #7067.\r\n\r\nRelated issue:\r\n- https://github.com/huggingface/huggingface_hub/issues/2419<end_of_turn>\n<start_of_turn>model\nFix push_to_hub by not calling create_branch if PR branch<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using a webdataset each sample can be a collection of different \"fields\" \nlike this:\n```\nimages17/image194.left.jpg\nimages17/image194.right.jpg\nimages17/image194.json\nimages17/image12.left.jpg\nimages17/image12.right.jpg\nimages17/image12.json\n```\n\nif the field_name contains upper case characters, the HF webdataset integration throws a key error when trying to load the dataset:\ne.g. from a dataset (now updated so that it doesn't throw this error)\n\n```\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[1], line 2\n      1 from datasets import load_dataset\n----> 2 ds = load_dataset(\"commaai/comma2k19\", data_files={'train': ['data-00000.tar.gz']}, num_proc=1)\n\nFile ~/xx/.venv/lib/python3.11/site-packages/datasets/load.py:1412, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\n   1409     return builder_instance.as_streaming_dataset(split=split)\n   1411 # Download and prepare data\n-> 1412 builder_instance.download_and_prepare(\n   1413     download_config=download_config,\n   1414     download_mode=download_mode,\n   1415     verification_mode=verification_mode,\n   1416     num_proc=num_proc,\n   1417     storage_options=storage_options,\n   1418 )\n   1420 # Build dataset for splits\n   1421 keep_in_memory = (\n   1422     keep_in_memory if keep_in_memory is not None else is_small_dataset(builder_instance.info.dataset_size)\n   1423 )\n\nFile ~/xx/.venv/lib/python3.11/site-packages/datasets/builder.py:894, in DatasetBuilder.download_and_prepare(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\n    892 if num_proc is not None:\n    893     prepare_split_kwargs[\"num_proc\"] = num_proc\n--> 894 self._download_and_prepare(\n    895     dl_manager=dl_manager,\n    896     verification_mode=verification_mode,\n    897     **prepare_split_kwargs,\n    898     **download_and_prepare_kwargs,\n    899 )\n    900 # Sync info\n    901 self.info.dataset_size = sum(split.num_bytes for split in self.info.splits.values())\n\nFile ~/xx/.venv/lib/python3.11/site-packages/datasets/builder.py:1609, in GeneratorBasedBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs)\n   1608 def _download_and_prepare(self, dl_manager, verification_mode, **prepare_splits_kwargs):\n-> 1609     super()._download_and_prepare(\n   1610         dl_manager,\n   1611         verification_mode,\n   1612         check_duplicate_keys=verification_mode == VerificationMode.BASIC_CHECKS\n   1613         or verification_mode == VerificationMode.ALL_CHECKS,\n   1614         **prepare_splits_kwargs,\n   1615     )\n\nFile ~/xx/.venv/lib/python3.11/site-packages/datasets/builder.py:948, in DatasetBuilder._download_and_prepare(self, dl_manager, verification_mode, **prepare_split_kwargs)\n    946 split_dict = SplitDict(dataset_name=self.dataset_name)\n    947 split_generators_kwargs = self._make_split_generators_kwargs(prepare_split_kwargs)\n--> 948 split_generators = self._split_generators(dl_manager, **split_generators_kwargs)\n    950 # Checksums verification\n    951 if verification_mode == VerificationMode.ALL_CHECKS and dl_manager.record_checksums:\n\nFile ~/xx/.venv/lib/python3.11/site-packages/datasets/packaged_modules/webdataset/webdataset.py:81, in WebDataset._split_generators(self, dl_manager)\n     78 if not self.info.features:\n     79     # Get one example to get the feature types\n     80     pipeline = self._get_pipeline_from_tar(tar_paths[0], tar_iterators[0])\n---> 81     first_examples = list(islice(pipeline, self.NUM_EXAMPLES_FOR_FEATURES_INFERENCE))\n     82     if any(example.keys() != first_examples[0].keys() for example in first_examples):\n     83         raise ValueError(\n     84             \"The TAR archives of the dataset should be in WebDataset format, \"\n     85             \"but the files in the archive don't share the same prefix or the same types.\"\n     86         )\n\nFile ~/xx/.venv/lib/python3.11/site-packages/datasets/packaged_modules/webdataset/webdataset.py:55, in WebDataset._get_pipeline_from_tar(cls, tar_path, tar_iterator)\n     53         data_extension = field_name.split(\".\")[-1]\n     54     if data_extension in cls.DECODERS:\n---> 55         current_example[field_name] = cls.DECODERS[data_extension](current_example[field_name])\n     56 if current_example:\n     57     yield current_example\n\nKeyError: 'processed_log_IMU_magnetometer_value.npy'\n```\n\n### Steps to reproduce the bug\n\nunit test was added in: https://github.com/huggingface/datasets/pull/7726\nit fails without the fixed proposed in the same PR\n\n### Expected behavior\n\nNot throwing a key error. \n\n\n### Environment info\n\n```\n- `datasets` version: 4.0.0\n- Platform: Linux-6.8.0-51-generic-x86_64-with-glibc2.39\n- Python version: 3.11.4\n- `huggingface_hub` version: 0.33.4\n- PyArrow version: 21.0.0\n- Pandas version: 2.3.1\n- `fsspec` version: 2025.7.0\n```<end_of_turn>\n<start_of_turn>model\nwebdataset: key errors when `field_name` has upper case characters<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Introduction\r\n\r\nHello there,\r\nI noticed that there are two ways to combine multiple datasets: Either through `datasets.concatenate_datasets` or `datasets.interleave_datasets`. However, to my knowledge (please correct me if I am wrong) both approaches require the datasets that are combined to have the same features.\r\n\r\nI think it would be a great idea to add support for combining multiple datasets that might not follow the same schema (i.e. have different features), for example an image and text dataset. That is why I propose a third function of the `datasets.combine` module called `stack_datasets`, which can be used to combine a list of datasets with (potentially) different features. This would look as follows:\r\n\r\n```python\r\n>>> from datasets import stack_datasets\r\n>>> image_dataset = ...\r\n>>> next(iter(image_dataset))\r\n{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=555x416 at 0x313E79CD0> }\r\n>>> text_dataset = ...\r\n>>> next(iter(text_dataset))\r\n{'text': \"This is a test.\"}\r\n>>> stacked = stack_datasets(datasets={'i_ds': image_dataset, 't_ds': text_dataset}, stopping_strategy='all_exhausted')\r\n>>> next(iter(stacked))\r\n{\r\n'i_ds': {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=555x416 at 0x313E79CD0> }\r\n't_ds': {'text': \"This is a test.\"}\r\n}\r\n```\r\n<br />\r\n\r\n### Motivation\r\n\r\nI motivate this by:\r\n\r\n**A**: The fact that Pytorch offers a similar functionality under `torch.utils.data.StackDataset` ([link](https://pytorch.org/docs/stable/data.html#torch.utils.data.StackDataset)).\r\n\r\n**B**: In settings where one would like to e.g. train a Vision-Language model using an image-text dataset, an image dataset, and a text dataset, this functionality would offer a clean and intuitive solution to create multimodal datasets. I am aware that the aforementioned is also feasible without my proposed function, but I believe this offers a nice approach that aligns with existing functionality and is directly provided within the `datasets` package.\r\n\r\n### API\r\n`stack_datasets` has two arguments: `datasets` and `stopping_strategy `.\r\n<br />\r\n\r\n`datasets` is a dictionary of either type `Dict[str, Dataset]` or `Dict[str, IterableDatasets]`, a mixture is not allowed. It contains the names of the datasets (the keys) and the datasets themselves (the values) that should be stacked. Each item returned is a dictionary with one key-value pair for each dataset. The keys are the names of the datasets as provided in the argument `datasets`, and the values are the respective examples from the datasets.\r\n<br />\r\n\r\n`stopping_strategy` is the same as for `interleave_datasets`. If it is `first_exhausted` we stop if the smallest dataset runs out of examples, if it is `all_exhausted` we stop if all datasets ran out of examples at least once. For `all_exhausted` that means that we may visit examples from datasets multiple times. \r\n\r\n### Docs\r\nI saw that there are multiple documentations and guides on the HuggingFace website that introduce `concatenate_datasets` and `interleave_datasets`, for example [here](https://huggingface.co/docs/datasets/process#concatenate). If this request is merged I would be willing to add the new functionality at the appropriate points in the documentation (if desired).\r\n\r\n### Tests\r\nI also added some tests to ensure correctness. Some tests I wrote in [tests/test_iterable_dataset.py](https://github.com/TimCares/datasets/blob/fadc1159debf2a65d44e40cbf7758f2bd2cc8b08/tests/test_iterable_dataset.py#L2169)\r\nrun for both `Dataset` and `IterableDataset` even though tests for `Dataset` technically do not belong in this script, but I found that this was a nice way to cover more cases with mostly the same code.\r\n\r\n### Additional information\r\nI tried to write the code in a way so that it is similar to that of `concatenate_datasets` and `interleave_datasets`.\r\nI\u2019m open to feedback and willing to make adjustments based on your suggestions, so feel free to give me your take. :)\r\n<end_of_turn>\n<start_of_turn>model\nFeature proposal: Stacking, potentially heterogeneous, datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI've encountered a memory leak when wrapping the HuggingFace dataset into a PyTorch Dataset. The RAM usage constantly increases during iteration if items are not explicitly deleted after use.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\n\n### Steps to reproduce the bug\n\n\r\nSteps to reproduce:\r\n\r\nCreate a PyTorch Dataset wrapper for 'nebula/cc12m':\r\n````\r\nfrom torch.utils.data import Dataset\r\nfrom tqdm import tqdm\r\nfrom datasets import load_dataset\r\nfrom torchvision import transforms\r\n\r\nImage.MAX_IMAGE_PIXELS = None\r\n\r\nclass CC12M(Dataset):\r\n    def __init__(self, path_or_name='nebula/cc12m', split='train', transform=None, single_caption=True):\r\n        self.raw_dataset = load_dataset(path_or_name)[split]\r\n\r\n        if transform is None:\r\n            self.transform = transforms.Compose([\r\n                transforms.Resize((224, 224)),\r\n                transforms.CenterCrop(224),\r\n                transforms.ToTensor(),\r\n                transforms.Normalize(\r\n                    mean=[0.48145466, 0.4578275, 0.40821073],\r\n                    std=[0.26862954, 0.26130258, 0.27577711]\r\n                )\r\n            ])\r\n        else:\r\n            self.transform = transforms.Compose(transform)\r\n\r\n        self.single_caption = single_caption\r\n        self.length = len(self.raw_dataset)\r\n\r\n    def __len__(self):\r\n        return self.length\r\n\r\n    def __getitem__(self, index):\r\n        item = self.raw_dataset[index]\r\n        caption = item['txt']\r\n        with io.BytesIO(item['webp']) as buffer:\r\n            image = Image.open(buffer).convert('RGB')\r\n            if self.transform:\r\n                image = self.transform(image)\r\n        # del item  # Uncomment this line to prevent the memory leak\r\n        return image, caption\r\n````\r\n\r\nIterate through the dataset without the del item line in __getitem__.\r\n\r\nObserve RAM usage increasing constantly.\r\n\r\nAdd del item at the end of __getitem__:\r\n\r\n```\r\ndef __getitem__(self, index):\r\n    item = self.raw_dataset[index]\r\n    caption = item['txt']\r\n    with io.BytesIO(item['webp']) as buffer:\r\n        image = Image.open(buffer).convert('RGB')\r\n        if self.transform:\r\n            image = self.transform(image)\r\n    del item  # This line prevents the memory leak\r\n    return image, caption\r\n```\r\n\r\n\r\nIterate through the dataset again and observe that RAM usage remains stable.\r\n\r\n\n\n### Expected behavior\n\n\r\nExpected behavior:\r\nRAM usage should remain stable during iteration without needing to explicitly delete items.\r\n\r\nActual behavior:\r\nRAM usage constantly increases unless items are explicitly deleted after use\n\n### Environment info\n\n- `datasets` version: 2.21.0\r\n- Platform: Linux-4.18.0-513.5.1.el8_9.x86_64-x86_64-with-glibc2.28\r\n- Python version: 3.12.4\r\n- `huggingface_hub` version: 0.24.6\r\n- PyArrow version: 17.0.0\r\n- Pandas version: 2.2.2\r\n- `fsspec` version: 2024.6.1\r\n\r\n<end_of_turn>\n<start_of_turn>model\nMemory leak when wrapping datasets into PyTorch Dataset without explicit deletion<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSee: https://github.com/huggingface/datasets/actions/runs/7100835633/job/19327734359\r\n```\r\nWould reformat: src/datasets/features/image.py\r\n1 file would be reformatted, 253 files left unchanged\r\n```<end_of_turn>\n<start_of_turn>model\nCI quality is broken<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nCloses #7741. Followup to #7690\r\n\r\n- Recursive parsing and feature inference, to preserve the tree structure of the file. Note this means we now visit all links in the file. It also means we have to call` combine_chunks` on any large non-root datasets.\r\n- Support for `complex64` (two `float32`s, used to be converted to two `float64`s)\r\n- Support for ndim complex, compound, more field types for compound (due to reusing the main parser, compound types are treated like groups)\r\n- Cleaned up varlen support\r\n- Always do feature inference and always cast to features (used to cast to schema)\r\n- Updated tests to use `load_dataset` instead of internal APIs\r\n- Removed `columns` in config. Have to give Features (i.e., must specify types) if filtering<end_of_turn>\n<start_of_turn>model\nRefactor HDF5 and preserve tree structure<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nchore: fix typos in docs<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSupport fsspec 2023.12 by handling previous and new glob behavior.<end_of_turn>\n<start_of_turn>model\nSupport fsspec 2023.12<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nThe [latest `multiprocess` release](https://github.com/uqfoundation/multiprocess/releases/tag/0.70.17) requires `dill>=0.3.9` which causes issues when installing `datasets` without backtracking during package version resolution. Is it possible to add a pin for multiprocess so something like `multiprocess<=0.70.16` so that the `dill` version is compatible?\n\n### Steps to reproduce the bug\n\nNA\n\n### Expected behavior\n\nNA\n\n### Environment info\n\nNA<end_of_turn>\n<start_of_turn>model\npinning `dill<0.3.9` without pinning `multiprocess` <end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a small typo in an error message in `src/datasets/fingerprint.py`:\r\n\r\nhttps://github.com/huggingface/datasets/blob/910fab20606893f69b4fccac5fcc883dddf5a14d/src/datasets/fingerprint.py#L63\r\n\r\n```diff\r\n- occured\r\n+ occurred\r\n```<end_of_turn>\n<start_of_turn>model\nFix typo in error message for cache directory deletion<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes issues with casting/embedding PyArrow list arrays with null values. It also bumps the required PyArrow version to 12.0.0 (over 9 months old) to simplify the implementation.\r\n\r\nFix #6280, fix #6311, fix #6360 \r\n\r\n(Also fixes https://github.com/huggingface/datasets/issues/5430 to make Beam compatible with PyArrow>=12.0.0)<end_of_turn>\n<start_of_turn>model\nFix array cast/embed with null values<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\ntypo<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdd the possibility of a backend for audio decoding. Before version 4.0.0, soundfile was used, and now torchcodec is used, but the problem is that torchcodec requires ffmpeg, which is problematic to install on the same colab. Therefore, I suggest adding a decoder selection when loading the dataset.\n\n### Motivation\n\nI use a service for training models in which ffmpeg cannot be installed.\n\n### Your contribution\n\nI use a service for training models in which ffmpeg cannot be installed.<end_of_turn>\n<start_of_turn>model\nAdd the possibility of a backend for audio decoding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFixes #7611\r\n\r\nThis PR fixes the usage example in the Dataset.add_column() docstring, which previously implied that add_column() modifies the dataset in-place.\r\n\r\nWhy:\r\nThe method returns a new dataset with the additional column, and users must assign the result to a variable to preserve the change.\r\n\r\nThis should make the behavior clearer for users.\r\n@lhoestq @davanstrien <end_of_turn>\n<start_of_turn>model\nFix misleading add_column() usage example in docstring<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am working with a dataset I am trying to publish. \r\n\r\nThe path is Antreas/TALI. \r\n\r\nIt's a fairly large dataset, and contains images, video, audio and text. \r\n\r\nI have been having multiple problems when the dataset is being downloaded using the load_dataset function -- even with 64 workers taking more than 7 days to process. \r\n\r\nWith snapshot download it takes 12 hours, and that includes the dataset preparation done using load_dataset and passing the dataset parquet file paths. \r\n\r\nFind the script I am using below:\r\n\r\n```python\r\n\r\nimport multiprocessing as mp\r\nimport pathlib\r\nfrom typing import Optional\r\n\r\nimport datasets\r\nfrom rich import print\r\nfrom tqdm import tqdm\r\n\r\n\r\ndef download_dataset_via_hub(\r\n    dataset_name: str,\r\n    dataset_download_path: pathlib.Path,\r\n    num_download_workers: int = mp.cpu_count(),\r\n):\r\n    import huggingface_hub as hf_hub\r\n\r\n    download_folder = hf_hub.snapshot_download(\r\n        repo_id=dataset_name,\r\n        repo_type=\"dataset\",\r\n        cache_dir=dataset_download_path,\r\n        resume_download=True,\r\n        max_workers=num_download_workers,\r\n        ignore_patterns=[],\r\n    )\r\n\r\n    return pathlib.Path(download_folder) / \"data\"\r\n\r\n\r\ndef load_dataset_via_hub(\r\n    dataset_download_path: pathlib.Path,\r\n    num_download_workers: int = mp.cpu_count(),\r\n    dataset_name: Optional[str] = None,\r\n):\r\n    from dataclasses import dataclass, field\r\n\r\n    from datasets import ClassLabel, Features, Image, Sequence, Value\r\n\r\n    dataset_path = download_dataset_via_hub(\r\n        dataset_download_path=dataset_download_path,\r\n        num_download_workers=num_download_workers,\r\n        dataset_name=dataset_name,\r\n    )\r\n    # Building a list of file paths for validation set\r\n\r\n    train_files = [\r\n        file.as_posix()\r\n        for file in pathlib.Path(dataset_path).glob(\"*.parquet\")\r\n        if \"train\" in file.as_posix()\r\n    ]\r\n    val_files = [\r\n        file.as_posix()\r\n        for file in pathlib.Path(dataset_path).glob(\"*.parquet\")\r\n        if \"val\" in file.as_posix()\r\n    ]\r\n    test_files = [\r\n        file.as_posix()\r\n        for file in pathlib.Path(dataset_path).glob(\"*.parquet\")\r\n        if \"test\" in file.as_posix()\r\n    ]\r\n    print(\r\n        f\"Found {len(test_files)} files for testing set, {len(train_files)} for training set and {len(val_files)} for validation set\"\r\n    )\r\n    data_files = {\r\n        \"test\": test_files,\r\n        \"val\": val_files,\r\n        \"train\": train_files,\r\n    }\r\n\r\n    features = Features(\r\n        {\r\n            \"image\": Image(\r\n                decode=True\r\n            ),  # Set `decode=True` if you want to decode the images, otherwise `decode=False`\r\n            \"image_url\": Value(\"string\"),\r\n            \"item_idx\": Value(\"int64\"),\r\n            \"wit_features\": Sequence(\r\n                {\r\n                    \"attribution_passes_lang_id\": Value(\"bool\"),\r\n                    \"caption_alt_text_description\": Value(\"string\"),\r\n                    \"caption_reference_description\": Value(\"string\"),\r\n                    \"caption_title_and_reference_description\": Value(\"string\"),\r\n                    \"context_page_description\": Value(\"string\"),\r\n                    \"context_section_description\": Value(\"string\"),\r\n                    \"hierarchical_section_title\": Value(\"string\"),\r\n                    \"is_main_image\": Value(\"bool\"),\r\n                    \"language\": Value(\"string\"),\r\n                    \"page_changed_recently\": Value(\"bool\"),\r\n                    \"page_title\": Value(\"string\"),\r\n                    \"page_url\": Value(\"string\"),\r\n                    \"section_title\": Value(\"string\"),\r\n                }\r\n            ),\r\n            \"wit_idx\": Value(\"int64\"),\r\n            \"youtube_title_text\": Value(\"string\"),\r\n            \"youtube_description_text\": Value(\"string\"),\r\n            \"youtube_video_content\": Value(\"binary\"),\r\n            \"youtube_video_starting_time\": Value(\"string\"),\r\n            \"youtube_subtitle_text\": Value(\"string\"),\r\n            \"youtube_video_size\": Value(\"int64\"),\r\n            \"youtube_video_file_path\": Value(\"string\"),\r\n        }\r\n    )\r\n\r\n    dataset = datasets.load_dataset(\r\n        \"parquet\" if dataset_name is None else dataset_name,\r\n        data_files=data_files,\r\n        features=features,\r\n        num_proc=1,\r\n        cache_dir=dataset_download_path / \"cache\",\r\n    )\r\n    return dataset\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    dataset_cache = pathlib.Path(\"/disk/scratch_fast0/tali/\")\r\n    dataset = load_dataset_via_hub(dataset_cache, dataset_name=\"Antreas/TALI\")[\r\n        \"test\"\r\n    ]\r\n\r\n    for sample in tqdm(dataset):\r\n        print(list(sample.keys()))\r\n```\r\n\r\nAlso, streaming this dataset has been a very painfully slow process. Streaming the train set takes 15m to start, and streaming the test and val sets takes 3 hours to start! \n\n### Steps to reproduce the bug\n\n1. Run the code I provided to get a sense of how fast snapshot + manual is\r\n2. Run datasets.load_dataset(\"Antreas/TALI\") to get a sense of the speed of that OP. \r\n3. You should now have an appreciation of how long these things take.\r\n\r\n\n\n### Expected behavior\n\nThe load dataset function should be at least as fast as the huggingface snapshot download function in terms of downloading dataset files. Not 20 times slower. \n\n### Environment info\n\n- `datasets` version: 2.14.5\r\n- Platform: Linux-5.15.0-76-generic-x86_64-with-glibc2.35\r\n- Python version: 3.10.13\r\n- Huggingface_hub version: 0.17.3\r\n- PyArrow version: 13.0.0\r\n- Pandas version: 2.1.1<end_of_turn>\n<start_of_turn>model\nDownload + preparation speed of datasets.load_dataset is 20x slower than huggingface hub snapshot and manual loding<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTemporarily pin tensorflow < 2.14.0 until permanent solution is found.\r\n\r\nHot fix #6263.<end_of_turn>\n<start_of_turn>model\nTemporarily pin tensorflow < 2.14.0<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nAccording the doc here: https://huggingface.co/docs/hub/datasets-file-names-and-splits#custom-split-name\nit should infer the split name from the subdir of data or the beg of the name of the files in data.\nWhen doing this manually through web upload it does not work. it uses \"train\" as a unique split.\nexample: https://huggingface.co/datasets/eole-nlp/estimator_chatml\n\n### Steps to reproduce the bug\n\nfollow the link above\n\n### Expected behavior\n\nthere should be two splits \"mlqe\" and \"1720_da\"\n\n### Environment info\n\nwebsite<end_of_turn>\n<start_of_turn>model\nCustom split name with the web interface<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix conda release worlflow<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThe Pandas packaged builder is undocumented and relies on `pickle` to read the data, making it **unsafe**. Moreover, I haven't seen a single instance of this builder being used (not even using the GH/Hub search), so we should deprecate it.<end_of_turn>\n<start_of_turn>model\nDeprecate Pandas builder<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nAdds support for dill 3.7.<end_of_turn>\n<start_of_turn>model\nDill 3.7 support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen using the `load_dataset` function with a folder path containing regex special characters (such as \"[]\"), the issue occurs due to how the path is handled in the `resolve_pattern` function. This function passes the unprocessed path directly to `AbstractFileSystem.glob`, which supports regular expressions. As a result, the globbing mechanism interprets these characters as regex patterns, leading to a traversal of the entire disk partition instead of confining the search to the intended directory.\n\n### Steps to reproduce the bug\n\njust create a folder like `E:\\[D_DATA]\\koch_test`, then `load_dataset(\"parquet\", data_dir=\"E:\\[D_DATA]\\\\test\", split=\"train\")`\nit will keep searching the whole disk.\n\nI add two `print` in `glob` and `resolve_pattern` to see the path\n\n### Expected behavior\n\nit should load the dataset as in normal folders\n\n### Environment info\n\n- `datasets` version: 3.3.2\n- Platform: Windows-10-10.0.22631-SP0\n- Python version: 3.10.16\n- `huggingface_hub` version: 0.29.1\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0<end_of_turn>\n<start_of_turn>model\nfunction `load_dataset`  can't solve folder path with regex characters like \"[]\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nValidate the config attributes `name` and `data_files` in packaged modules by making the derived classes call their parent `__post_init__` method.\r\n\r\nNote that their parent `BuilderConfig` validates its attributes `name` and `data_files` in its `__post_init__` method: https://github.com/huggingface/datasets/blob/60d21efbc01e15d0b596ac1072750cbecd91548a/src/datasets/builder.py#L128-L137\r\n\r\nThis PR makes the derived config classes call their parent `__post_init__` method to validate their `name` and `data_files` attributes.<end_of_turn>\n<start_of_turn>model\nValidate config name and data_files in packaged modules<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\n### Name \r\n\r\n**CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison**\r\n\r\n### Paper\r\n\r\nhttps://arxiv.org/abs/1901.07031\r\n\r\n### Data\r\n\r\nhttps://stanfordaimi.azurewebsites.net/datasets/8cbd9ed4-2eb9-4565-affc-111cf4f7ebe2\n\n### Motivation\n\nCheXpert is one of the fundamental models in medical image classification and can serve as a viable pre-training dataset for radiology classification or low-scale ablation / exploratory studies.\r\n\r\nThis could also serve as a good pre-training dataset for Kaggle competitions.\n\n### Your contribution\n\nWould love to make a PR and pre-process / get this into \ud83e\udd17<end_of_turn>\n<start_of_turn>model\nAdd CheXpert dataset for vision<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nEvery time it runs, somehow, samples increase.\nThis can cause a 12mb dataset to have other built versions of 400 mbs+\n\n<img width=\"363\" height=\"481\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/766ce958-bd2b-41bc-b950-86710259bfdc\" />\n\n### Steps to reproduce the bug\n\n\n`from datasets import load_dataset\ns = load_dataset('~/.cache/huggingface/datasets/databricks___databricks-dolly-15k')['train']\n`\n1. A dataset needs to be available in the .cache folder\n2. Run the code multiple times, and every time it runs, more versions are created\n\n\n### Expected behavior\n\nThe number of samples increases every time the script runs\n\n### Environment info\n\n- `datasets` version: 3.6.0\n- Platform: Windows-11-10.0.26100-SP0\n- Python version: 3.13.3\n- `huggingface_hub` version: 0.32.3\n- PyArrow version: 20.0.0\n- Pandas version: 2.2.3\n- `fsspec` version: 2025.3.0\n<end_of_turn>\n<start_of_turn>model\nDatasets keeps rebuilding the dataset every time i call the python script<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nfixes https://github.com/huggingface/datasets/issues/7660<end_of_turn>\n<start_of_turn>model\nfix del tqdm lock error<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ninstead of the current datasets.filesystems.hffilesystem.HfFileSystem which can be slow in some cases\r\n\r\nrelated to https://github.com/huggingface/datasets/issues/5846 and https://github.com/huggingface/datasets/pull/5919<end_of_turn>\n<start_of_turn>model\nSwitch to huggingface_hub's HfFileSystem<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nFix #6147<end_of_turn>\n<start_of_turn>model\nFix protocol prefix for Beam<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen I try to load a dataset with parquet files (e.g. \"bigcode/the-stack\") the dataset loads, but python interpreter can't exit and hangs\n\n### Steps to reproduce the bug\n\n```python3\nimport datasets\n\nprint('Start')\ndataset = datasets.load_dataset(\"bigcode/the-stack\", data_dir=\"data/yaml\", streaming=True, split=\"train\")\nit = iter(dataset)\nnext(it)\nprint('Finish')\n```\n\nThe program prints finish but doesn't exit and hangs indefinitely.\nI tried this on two different machines and several datasets.\n\n### Expected behavior\n\nThe program exits successfully\n\n### Environment info\n\ndatasets==3.4.1\nPython 3.12.9.\nMacOS and Ubuntu Linux<end_of_turn>\n<start_of_turn>model\nload_dataset with streaming hangs on parquet datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nWhen calling `load_dataset` in a streamlit application running within a docker container, get a failure with the error message:\r\n\r\nEmptyDatasetError: The directory at hf://datasets/fetch-rewards/inc-rings-2000@bea27cf60842b3641eae418f38864a2ec4cde684 doesn't contain any data files\r\nTraceback:\r\nFile \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/scriptrunner/script_runner.py\", line 552, in _run_script\r\n    exec(code, module.__dict__)\r\nFile \"/home/user/app/app.py\", line 62, in <module>\r\n    dashboard()\r\nFile \"/home/user/app/app.py\", line 47, in dashboard\r\n    feat_dict, path_gml = load_data(hf_repo, model_gml_dict[selected_model], hf_token)\r\nFile \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\", line 211, in wrapper\r\n    return cached_func(*args, **kwargs)\r\nFile \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\", line 240, in __call__\r\n    return self._get_or_create_cached_value(args, kwargs)\r\nFile \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\", line 266, in _get_or_create_cached_value\r\n    return self._handle_cache_miss(cache, value_key, func_args, func_kwargs)\r\nFile \"/opt/conda/lib/python3.10/site-packages/streamlit/runtime/caching/cache_utils.py\", line 320, in _handle_cache_miss\r\n    computed_value = self._info.func(*func_args, **func_kwargs)\r\nFile \"/home/user/app/hf_interface.py\", line 16, in load_data\r\n    hf_dataset = load_dataset(repo_id, use_auth_token=hf_token)\r\nFile \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 2109, in load_dataset\r\n    builder_instance = load_dataset_builder(\r\nFile \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1795, in load_dataset_builder\r\n    dataset_module = dataset_module_factory(\r\nFile \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1486, in dataset_module_factory\r\n    raise e1 from None\r\nFile \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1476, in dataset_module_factory\r\n    ).get_module()\r\nFile \"/opt/conda/lib/python3.10/site-packages/datasets/load.py\", line 1032, in get_module\r\n    else get_data_patterns(base_path, download_config=self.download_config)\r\nFile \"/opt/conda/lib/python3.10/site-packages/datasets/data_files.py\", line 458, in get_data_patterns\r\n    raise EmptyDatasetError(f\"The directory at {base_path} doesn't contain any data files\") from None\n\n### Steps to reproduce the bug\n\n```python\r\n@st.cache_resource\r\ndef load_data(repo_id: str, hf_token=None):\r\n    \"\"\"Load data from HuggingFace Hub\r\n    \"\"\"\r\n    \r\n    hf_dataset = load_dataset(repo_id, use_auth_token=hf_token)\r\n    hf_dataset = hf_dataset.map(lambda x: json.loads(x[\"ground_truth\"]), remove_columns=[\"ground_truth\"])\r\n\r\n    return hf_dataset\r\n```\n\n### Expected behavior\n\nExpect to load. \r\nNote: works fine with datasets==2.13.1\n\n### Environment info\n\ndatasets==2.14.2, \r\nUbuntu bionic-based Docker container.<end_of_turn>\n<start_of_turn>model\nload_dataset() fails with streamlit caching inside docker<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nAdding support for loading a file from a DVC repository, tracked remotely on a SCM.\n\n### Motivation\n\nDVC is a popular version control system to version and manage datasets. The files are stored on a remote object storage platform, but they are tracked using Git. Integration with DVC is possible through the `DVCFileSystem`.\r\n\r\nI have a Gitlab repository where multiple files are tracked using DVC and stored in a GCP bucket. I would like to be able to load these files using `datasets` directly using an URL. My goal is to write a generic code that abstracts the storage layer, such that my users will only have to pass in an `fsspec`-compliant URL and the corresponding files will be loaded.\n\n### Your contribution\n\nI managed to instantiate a `DVCFileSystem` pointing to a Gitlab repo from a `fsspec` chained URL in [this pull request](https://github.com/iterative/dvc/pull/9903) to DVC. \r\n\r\n```python\r\nfrom fsspec.core import url_to_fs\r\n\r\nfs, _ = url_to_fs(\"dvc::https://gitlab.com/repository/group/my-repo\")\r\n```\r\n\r\nFrom now I'm not sure how to continue, it seems that `datasets` expects the URL to be fully qualified like so: `dvc::https://gitlab.com/repository/group/my-repo/my-folder/my-file.json` but this fails because `DVCFileSystem` expects the URL to point to the root of an SCM repo. Is there a way to make this work with `datasets`?<end_of_turn>\n<start_of_turn>model\nSupport loading from a DVC remote repository<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nFor the dreprecated notebook found  [here](https://github.com/huggingface/datasets/blob/main/notebooks/Overview.ipynb). The link to the new notebook is broken. \n\n### Steps to reproduce the bug\n\nClick the [Quickstart notebook](https://github.com/huggingface/notebooks/blob/main/datasets_doc/quickstart.ipynb) link in the notebook.\n\n### Expected behavior\n\nI believe is it suposed to link [here](https://github.com/huggingface/notebooks/blob/main/datasets_doc/en/quickstart.ipynb). That is mentioned in the readme.\n\n### Environment info\n\nColab<end_of_turn>\n<start_of_turn>model\n Depprcated Overview.ipynb Link to new Quickstart Notebook invalid<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nSequence is just a utility that we need to keep for backward compatibility. And `[ ]` was used instead but doesn't allow passing the length of the list.\r\n\r\nThis PR removes most mentions of Sequence and usage of `[ ]` and defines a proper List type instead.\r\n\r\nbefore: `Sequence(Value(\"int64\"))` or `[Value(\"int64\")]`\r\nnow: `List(Value(\"int64\"))`\r\n\r\nThis PR conserves full backward compatibility. And it's a good occasion with the release of 4.0.0<end_of_turn>\n<start_of_turn>model\nReplace Sequence by List<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nin data_files.py, line 332, \r\n`fs,  _, _ = get_fs_token_paths(pattern, storage_options=storage_options)`\r\nIf we run the code on AWS, as fs.protocol will be a tuple like: `('file', 'local')`\r\nSo,  `isinstance(fs.protocol, str) == False` and\r\n`protocol_prefix = fs.protocol + \"://\" if fs.protocol != \"file\" else \"\"` will raise \r\n`TypeError: can only concatenate tuple (not \"str\") to tuple`.\n\n### Steps to reproduce the bug\n\nSteps to reproduce:\r\n1. Run on a cloud server like AWS,\r\n2. `import datasets.data_files as datafile`\r\n3. datafile.resolve_pattern('path/to/dataset', '.')\r\n4. `TypeError: can only concatenate tuple (not \"str\") to tuple`\n\n### Expected behavior\n\nShould return path of the dataset, with fs.protocol at the beginning\n\n### Environment info\n\n- `datasets` version: 2.14.0\r\n- Platform: Linux-3.10.0-1160.119.1.el7.x86_64-x86_64-with-glibc2.17\r\n- Python version: 3.8.19\r\n- Huggingface_hub version: 0.23.5\r\n- PyArrow version: 16.1.0\r\n- Pandas version: 1.1.5<end_of_turn>\n<start_of_turn>model\nDatasets.datafiles resolve_pattern `TypeError: can only concatenate tuple (not \"str\") to tuple`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nOur CI Build PR Documentation is broken. See: https://github.com/huggingface/datasets/actions/runs/6799554060/job/18486828777?pr=6390\r\n```\r\nImportError: cannot import name 'TypeAliasType' from 'typing_extensions'\r\n```<end_of_turn>\n<start_of_turn>model\nCI Build PR Documentation is broken: ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nReported in https://github.com/huggingface/datasets-server/issues/2607<end_of_turn>\n<start_of_turn>model\nFix missing download_config in get_data_patterns<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nNone<end_of_turn>\n<start_of_turn>model\nfix iterate nested field<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI'm a huge fan of the current HF Datasets `webdataset` integration (especially the built-in streaming support). However, I'd love to upload some robotics and multimodal datasets I've processed for use with [Mosaic Streaming](https://docs.mosaicml.com/projects/streaming/en/stable/), specifically their [MDS Format](https://docs.mosaicml.com/projects/streaming/en/stable/fundamentals/dataset_format.html#mds). \r\n\r\nBecause the shard files have similar semantics to WebDataset, I'm hoping that adding such support won't be too much trouble?\n\n### Motivation\n\nOne of the downsides with WebDataset is a lack of out-of-the-box determinism (especially for large-scale training and reproducibility), easy job resumption, and the ability to quickly debug / visualize individual examples. \r\n\r\nMosaic Streaming provides a [great interface for this out of the box](https://docs.mosaicml.com/projects/streaming/en/stable/#key-features), so I'd love to see it supported in HF Datasets.\n\n### Your contribution\n\nHappy to help test things / provide example data. Can potentially submit a PR if maintainers could point me to the necessary WebDataset logic / steps for adding a new streaming format!<end_of_turn>\n<start_of_turn>model\nMosaic Streaming (MDS) Support<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nIf the coco annotation file is too large the dataset will not be able to load it, not entirely sure were the issue is but I am guessing it is due to the code trying to load it all as one line into a dataframe. This was for object detections.\n\nMy current work around is the following code but would be nice to be able to do it without worrying about it also probably there is a better way of doing it:\n`\n            dataset_dict = json.load(open(\"./local_data/annotations/train.json\"))\n            df = pd.DataFrame(columns=['images', 'annotations', 'categories'])\n            df = df._append({'images': dataset_dict['images'], 'annotations': dataset_dict['annotations'], 'categories': dataset_dict['categories']}, ignore_index=True)\n            train=Dataset.from_pandas(df)\n\n            dataset_dict = json.load(open(\"./local_data/annotations/validation.json\"))\n            df = pd.DataFrame(columns=['images', 'annotations', 'categories'])\n            df = df._append({'images': dataset_dict['images'], 'annotations': dataset_dict['annotations'],\n                             'categories': dataset_dict['categories']}, ignore_index=True)\n            val = Dataset.from_pandas(df)\n            dataset_dict = json.load(open(\"./local_data/annotations/test.json\"))\n            df = pd.DataFrame(columns=['images', 'annotations', 'categories'])\n            df = df._append({'images': dataset_dict['images'], 'annotations': dataset_dict['annotations'],\n                             'categories': dataset_dict['categories']}, ignore_index=True)\n            test = Dataset.from_pandas(df)\n            dataset = DatasetDict({'train': train, 'validation': val, 'test': test})\n`\n\n### Steps to reproduce the bug\n\n1) step up directory in and have the json files in coco format\n\n-local_data\n|-images\n|---1.jpg\n|---2.jpg\n|---....\n|---n.jpg\n|-annotations\n|---test.json\n|---train.json\n|---validation.json\n\n2) try to load local_data into a dataset if the file is larger than about 300kb it will cause an error.\n\n### Expected behavior\n\nThat it loads the jsons preferably in the same format as it has done with a smaller size.\n\n### Environment info\n\n- `datasets` version: 3.3.3.dev0\n- Platform: Linux-6.11.0-17-generic-x86_64-with-glibc2.39\n- Python version: 3.12.3\n- `huggingface_hub` version: 0.29.0\n- PyArrow version: 19.0.1\n- Pandas version: 2.2.3\n- `fsspec` version: 2024.12.0\n<end_of_turn>\n<start_of_turn>model\nIssues with large Datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### What does this PR do?\r\n\r\nRemove unnecessary permissions granted to the actions workflow.\r\n\r\nSorry for the mishap.<end_of_turn>\n<start_of_turn>model\nfix(ci): remove unnecessary permissions<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nTODO:\r\n- [x] Add docs\r\n- [x] Delete token arg from CLI example\r\n  - See: #6839\r\n\r\nClose #6810.<end_of_turn>\n<start_of_turn>model\nAllow deleting a subset/config from a no-script dataset<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI've loaded a local image dataset with:\r\n`ds = laod_dataset(\"imagefolder\", data_dir=path-to-data)`\r\n\r\nAnd defined a transform to process the data, following the Datasets docs.\r\n\r\nHowever, I get a keyError error, indicating there's no \"image\" key in my dataset. When I printed out the example_batch sent to the transformation function, it shows only the labels are being sent to the function.\r\nFor some reason, the images are not in the example batches.\n\n### Steps to reproduce the bug\n\nI'm using the latest stable version of datasets \n\n### Expected behavior\n\nI expect the example_batches to contain both images and labels\n\n### Environment info\n\nI'm using the latest stable version of datasets <end_of_turn>\n<start_of_turn>model\nKeyError: dataset has no key \"image\"<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\nThis PR fixes a bug in the `generate_examples` function where `datasets.Value` features with a `float` dtype were incorrectly generated using `np.random.randint`. This resulted in integer values being cast to float, which is not representative of true floating-point data.\r\n\r\n**Key changes include:**\r\n1.  Added explicit handling for float features using `np.random.rand` to generate continuous values.\r\n2.  Introduced fail-fast type checks for unsupported dtypes to improve robustness.\r\n3.  Added validation for sequence features to ensure `seq_shapes` is provided.\r\n\r\n\r\n### Before Fix\r\nFloat features were generated incorrectly as integers cast to float:\r\n\r\n```text\r\nExample 0:\r\nint_feature: 0\r\nfloat_feature: 9.0  <-- Incorrect: An integer disguised as a float\r\nstring_feature: The small grey turtle was surprisingly fast...\r\nseq_feature: [0.3048 0.4291 0.4283]\r\n```\r\n### After Fix\r\nFloat features are now correctly generated as continuous numbers in the range [0, 1):\r\n```text\r\n\r\nExample 0:\r\nint_feature: 0\r\nfloat_feature: 0.0183  <-- Correct: A true random float\r\nstring_feature: The small grey turtle was surprisingly fast...\r\nseq_feature: [0.9237 0.7972 0.8526]<end_of_turn>\n<start_of_turn>model\nFix: Correct float feature generation in `generate_examples`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nI am facing an issue since today where HF's dataset is acting weird and in some instances failure to recognise a valid dataset entirely, I think it is happening due to recent change in `fsspec` lib as using this command fixed it for me in one-time: `!pip install -U datasets huggingface_hub fsspec`\n\n\n### Steps to reproduce the bug\n\nfrom datasets import load_dataset\n\ndef download_hf():\n  dataset_name = input(\"Enter the dataset name: \")\n  subset_name = input(\"Enter subset name: \")\n  ds = load_dataset(dataset_name, name=subset_name)\n  for split in ds:\n    ds[split].to_pandas().to_csv(f\"{subset_name}.csv\", index=False)\n\ndownload_hf()\n\n### Expected behavior\n\n```\nDownloading\u2007readme:\u2007100%\n\u20071.55k/1.55k\u2007[00:00<00:00,\u2007121kB/s]\nDownloading\u2007data\u2007files:\u2007100%\n\u20071/1\u2007[00:00<00:00,\u2007\u20072.06it/s]\n\nDownloading data:   0%|          | 0.00/54.2k [00:00<?, ?B/s]\nDownloading data: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 54.2k/54.2k [00:00<00:00, 121kB/s]\nExtracting\u2007data\u2007files:\u2007100%\n\u20071/1\u2007[00:00<00:00,\u200735.17it/s]\nGenerating\u2007test\u2007split:\u2007\n\u2007140/0\u2007[00:00<00:00,\u20072628.62\u2007examples/s]\n---------------------------------------------------------------------------\nNotImplementedError                       Traceback (most recent call last)\n[<ipython-input-2-12ab305b0e77>](https://localhost:8080/#) in <cell line: 0>()\n      8     ds[split].to_pandas().to_csv(f\"{subset_name}.csv\", index=False)\n      9 \n---> 10 download_hf()\n\n2 frames\n[/usr/local/lib/python3.11/dist-packages/datasets/builder.py](https://localhost:8080/#) in as_dataset(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\n   1171         is_local = not is_remote_filesystem(self._fs)\n   1172         if not is_local:\n-> 1173             raise NotImplementedError(f\"Loading a dataset cached in a {type(self._fs).__name__} is not supported.\")\n   1174         if not os.path.exists(self._output_dir):\n   1175             raise FileNotFoundError(\n\nNotImplementedError: Loading a dataset cached in a LocalFileSystem is not supported.\n```\nOR\n```\nTraceback (most recent call last):\n  File \"e:\\Fuck\\download-data\\mcq_dataset.py\", line 10, in <module>\n    download_hf()\n  File \"e:\\Fuck\\download-data\\mcq_dataset.py\", line 6, in download_hf\n    ds = load_dataset(dataset_name, name=subset_name)\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 2606, in load_dataset\n    builder_instance = load_dataset_builder(\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 2277, in load_dataset_builder\n    dataset_module = dataset_module_factory(\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 1917, in dataset_module_factory    \n    raise e1 from None\n  File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py\", line 1867, in dataset_module_factory    \n    raise DatasetNotFoundError(f\"Dataset '{path}' doesn't exist on the Hub or cannot be accessed.\") from e\ndatasets.exceptions.DatasetNotFoundError: Dataset 'dataset repo_id' doesn't exist on the Hub or cannot be accessed.\n```\n\n### Environment info\n\ncolab and 3.10 local system<end_of_turn>\n<start_of_turn>model\nDataset lib seems to broke after fssec lib update<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nHi team, first off, I love the datasets library! \ud83e\udd70\n\nI'm encountering a potential memory leak / increasing memory usage when training a model on a very large DatasetDict.\n\nSetup: I have a DatasetDict containing 362 distinct datasets, which sum up to ~2.8 billion rows.\n\nTraining Task: I'm performing contrastive learning with SentenceTransformer and Accelerate on a single node with 4 H100, which requires me to sample from only one dataset at a time.\n\nTraining Loop: At each training step, I sample ~16,000 examples from a single dataset, and then switch to a different dataset for the next step. I iterate through all 362 datasets this way. \n\nProblem: The process's memory usage continuously increases over time, eventually causing a stale status where GPUs would stop working. It seems memory from previously sampled datasets isn't being released. I've set num_workers=0 for all experiments.\n\nChart 1: Standard DatasetDict The memory usage grows steadily until it make the training stale (RSS memory) <img width=\"773\" height=\"719\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/6606bef5-1153-4f2d-bf08-82da249d6e8d\" />\n\nChart 2: IterableDatasetDict I also tried  to use IterableDatasetDict and IterableDataset. The memory curve is \"smoother,\" but the result is the same: it grows indefinitely and the training become stale. <img width=\"339\" height=\"705\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/ee90c1a1-6c3b-4135-9edc-90955cb1695a\" />\n\nAny feedback or guidance on how to manage this memory would be greatly appreciated!\n\n### Steps to reproduce the bug\n\nWIP, I'll add some code that manage to reproduce this error, but not straightforward. \n\n### Expected behavior\n\nThe memory usage should remain relatively constant or plateau after a few steps. Memory used for sampling one dataset should be released before or during the sampling of the next dataset.\n\n### Environment info\n\nPython: 3.12\nDatasets: 4.3.0\nSentenceTransformers: 5.1.1<end_of_turn>\n<start_of_turn>model\nMemory leak / Large memory usage with num_workers = 0 and numerous dataset within DatasetDict<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Describe the bug\n\nCannot load the dataset https://huggingface.co/datasets/OpenMol/PubChemSFT\n\n### Steps to reproduce the bug\n\n```\r\nfrom datasets import load_dataset\r\ndataset = load_dataset('OpenMol/PubChemSFT')\r\n```\n\n### Expected behavior\n\n```\r\n---------------------------------------------------------------------------\r\nDataFilesNotFoundError                    Traceback (most recent call last)\r\nCell In[7], [line 2](vscode-notebook-cell:?execution_count=7&line=2)\r\n      [1](vscode-notebook-cell:?execution_count=7&line=1) from datasets import load_dataset\r\n----> [2](vscode-notebook-cell:?execution_count=7&line=2) dataset = load_dataset('OpenMol/PubChemSFT')\r\n\r\nFile ~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2587, in load_dataset(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\r\n   [2582](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2582) verification_mode = VerificationMode(\r\n   [2583](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2583)     (verification_mode or VerificationMode.BASIC_CHECKS) if not save_infos else VerificationMode.ALL_CHECKS\r\n   [2584](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2584) )\r\n   [2586](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2586) # Create a dataset builder\r\n-> [2587](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2587) builder_instance = load_dataset_builder(\r\n   [2588](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2588)     path=path,\r\n   [2589](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2589)     name=name,\r\n   [2590](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2590)     data_dir=data_dir,\r\n   [2591](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2591)     data_files=data_files,\r\n   [2592](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2592)     cache_dir=cache_dir,\r\n   [2593](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2593)     features=features,\r\n   [2594](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2594)     download_config=download_config,\r\n   [2595](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2595)     download_mode=download_mode,\r\n   [2596](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2596)     revision=revision,\r\n   [2597](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2597)     token=token,\r\n   [2598](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2598)     storage_options=storage_options,\r\n   [2599](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2599)     trust_remote_code=trust_remote_code,\r\n   [2600](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2600)     _require_default_config_name=name is None,\r\n   [2601](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2601)     **config_kwargs,\r\n   [2602](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2602) )\r\n   [2604](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2604) # Return iterable dataset in case of streaming\r\n   [2605](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2605) if streaming:\r\n\r\nFile ~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2259, in load_dataset_builder(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, use_auth_token, storage_options, trust_remote_code, _require_default_config_name, **config_kwargs)\r\n   [2257](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2257)     download_config = download_config.copy() if download_config else DownloadConfig()\r\n   [2258](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2258)     download_config.storage_options.update(storage_options)\r\n-> [2259](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2259) dataset_module = dataset_module_factory(\r\n   [2260](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2260)     path,\r\n   [2261](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2261)     revision=revision,\r\n   [2262](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2262)     download_config=download_config,\r\n   [2263](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2263)     download_mode=download_mode,\r\n   [2264](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2264)     data_dir=data_dir,\r\n   [2265](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2265)     data_files=data_files,\r\n   [2266](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2266)     cache_dir=cache_dir,\r\n   [2267](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2267)     trust_remote_code=trust_remote_code,\r\n   [2268](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2268)     _require_default_config_name=_require_default_config_name,\r\n   [2269](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2269)     _require_custom_configs=bool(config_kwargs),\r\n   [2270](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2270) )\r\n   [2271](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2271) # Get dataset builder class from the processing script\r\n   [2272](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:2272) builder_kwargs = dataset_module.builder_kwargs\r\n\r\nFile ~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1904, in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\r\n   [1902](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1902)     raise ConnectionError(f\"Couldn't reach the Hugging Face Hub for dataset '{path}': {e1}\") from None\r\n   [1903](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1903) if isinstance(e1, (DataFilesNotFoundError, DatasetNotFoundError, EmptyDatasetError)):\r\n-> [1904](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1904)     raise e1 from None\r\n   [1905](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1905) if isinstance(e1, FileNotFoundError):\r\n   [1906](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1906)     raise FileNotFoundError(\r\n   [1907](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1907)         f\"Couldn't find a dataset script at {relative_to_absolute_path(combined_path)} or any data file in the same directory. \"\r\n   [1908](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1908)         f\"Couldn't find '{path}' on the Hugging Face Hub either: {type(e1).__name__}: {e1}\"\r\n   [1909](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1909)     ) from None\r\n\r\nFile ~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1885, in dataset_module_factory(path, revision, download_config, download_mode, dynamic_modules_path, data_dir, data_files, cache_dir, trust_remote_code, _require_default_config_name, _require_custom_configs, **download_kwargs)\r\n   [1876](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1876)         return HubDatasetModuleFactoryWithScript(\r\n   [1877](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1877)             path,\r\n   [1878](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1878)             revision=revision,\r\n   (...)\r\n   [1882](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1882)             trust_remote_code=trust_remote_code,\r\n   [1883](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1883)         ).get_module()\r\n   [1884](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1884)     else:\r\n-> [1885](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1885)         return HubDatasetModuleFactoryWithoutScript(\r\n   [1886](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1886)             path,\r\n   [1887](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1887)             revision=revision,\r\n   [1888](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1888)             data_dir=data_dir,\r\n   [1889](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1889)             data_files=data_files,\r\n   [1890](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1890)             download_config=download_config,\r\n   [1891](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1891)             download_mode=download_mode,\r\n   [1892](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1892)         ).get_module()\r\n   [1893](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1893) except Exception as e1:\r\n   [1894](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1894)     # All the attempts failed, before raising the error we should check if the module is already cached\r\n   [1895](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1895)     try:\r\n\r\nFile ~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1270, in HubDatasetModuleFactoryWithoutScript.get_module(self)\r\n   [1263](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1263)     patterns = get_data_patterns(base_path, download_config=self.download_config)\r\n   [1264](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1264) data_files = DataFilesDict.from_patterns(\r\n   [1265](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1265)     patterns,\r\n   [1266](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1266)     base_path=base_path,\r\n   [1267](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1267)     allowed_extensions=ALL_ALLOWED_EXTENSIONS,\r\n   [1268](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1268)     download_config=self.download_config,\r\n   [1269](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1269) )\r\n-> [1270](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1270) module_name, default_builder_kwargs = infer_module_for_data_files(\r\n   [1271](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1271)     data_files=data_files,\r\n   [1272](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1272)     path=self.name,\r\n   [1273](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1273)     download_config=self.download_config,\r\n   [1274](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1274) )\r\n   [1275](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1275) data_files = data_files.filter_extensions(_MODULE_TO_EXTENSIONS[module_name])\r\n   [1276](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:1276) # Collect metadata files if the module supports them\r\n\r\nFile ~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:597, in infer_module_for_data_files(data_files, path, download_config)\r\n    [595](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:595)     raise ValueError(f\"Couldn't infer the same data file format for all splits. Got {split_modules}\")\r\n    [596](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:596) if not module_name:\r\n--> [597](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:597)     raise DataFilesNotFoundError(\"No (supported) data files found\" + (f\" in {path}\" if path else \"\"))\r\n    [598](https://file+.vscode-resource.vscode-cdn.net/home/ubuntu/Projects/notebook/~/Softwares/anaconda3/envs/pyg-dev/lib/python3.9/site-packages/datasets/load.py:598) return module_name, default_builder_kwargs\r\n\r\nDataFilesNotFoundError: No (supported) data files found in OpenMol/PubChemSFT\r\n```\n\n### Environment info\n\n```\r\n- `datasets` version: 3.1.0\r\n- Platform: Linux-5.15.0-125-generic-x86_64-with-glibc2.31\r\n- Python version: 3.9.18\r\n- `huggingface_hub` version: 0.25.2\r\n- PyArrow version: 18.0.0\r\n- Pandas version: 2.0.3\r\n- `fsspec` version: 2023.9.2\r\n```<end_of_turn>\n<start_of_turn>model\nDataFilesNotFoundError for datasets `OpenMol/PubChemSFT`<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\n### Feature request\n\nI\u2019d like to request native support in the Hugging Face datasets library for reading .conll files (CoNLL format). This format is widely used in NLP tasks, especially for Named Entity Recognition (NER), POS tagging, and other token classification problems.\n\nRight now `.conll` datasets need to be manually parsed or preprocessed before being loaded into datasets. Having built in support would save time and make workflows smoother for researchers and practitioners.\n\nI propose - \nAdd a conll dataset builder or file parser to datasets that can:\n- Read `.conll` files with customizable delimiters (space, tab).\n- Handle sentence/document boundaries (typically indicated by empty lines).\n- Support common CoNLL variants (e.g., CoNLL-2000 chunking, CoNLL-2003 NER).\n- Output a dataset where each example contains:\n- tokens: list of strings\n- tags (or similar): list of labels aligned with tokens\n\nGiven a .conll snippet like:\n```\nEU NNP B-ORG\nrejects VBZ O\nGerman JJ B-MISC\ncall NN O\n. . O\n```\n\n\nThe dataset should load as:\n```\n{\n  \"tokens\": [\"EU\", \"rejects\", \"German\", \"call\", \".\"],\n  \"tags\": [\"B-ORG\", \"O\", \"B-MISC\", \"O\", \"O\"]\n}\n```\n\n\n\n\n### Motivation\n\n- CoNLL files are a standard benchmark format in NLP (e.g., CoNLL-2003, CoNLL-2000).\n- Many users train NER or sequence labeling models (like BERT for token classification) directly on `.conll` \n- Right now you have to write your own parsing scripts. Built in support would unify this process and would be much more convenient \n\n### Your contribution\n\nI\u2019d be happy to contribute by implementing this feature. My plan is to- \n- Add a new dataset script (conll.py) to handle .conll files.\n- Implement parsing logic that supports sentence/document boundaries and token-label alignment.\n- Write unit tests with small `.conll` examples to ensure correctness.\n- Add documentation and usage examples so new users can easily load `.conll` datasets.\n\nThis would be my first open source contribution, so I\u2019ll follow the `CONTRIBUTING.md` guidelines closely and adjust based on feedback from the maintainers.<end_of_turn>\n<start_of_turn>model\nAdd support for `.conll` file format in datasets<end_of_turn>"}
{"text": "<start_of_turn>user\nSummarize the following GitHub issue in one concise sentence:\n\ncc @cakiki who reported it on [slack](https://huggingface.slack.com/archives/C039P47V1L5/p1717754405578539) (private link)\r\n\r\nThis PR updates how errors are handled in `dataset_module_factory` when the  `dataset_info` cannot be accessed:\r\n1. Use multiple `except ... as e` instead of using `isinstance(e, ...)`\r\n2. Always raise `DatasetNotFoundError` with `from e` so that the initial error is explicitly logged in the stacktrace.\r\n3. Differentiate `RepoNotFoundError` / `GatedRepoError` / `RevisionNotFoundError` cases<end_of_turn>\n<start_of_turn>model\nBetter error handling in `dataset_module_factory`<end_of_turn>"}
